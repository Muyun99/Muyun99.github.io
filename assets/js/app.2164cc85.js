(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(n){function e(e){for(var a,o,s=e[0],l=e[1],c=e[2],d=0,u=[];d<s.length;d++)o=s[d],Object.prototype.hasOwnProperty.call(i,o)&&i[o]&&u.push(i[o][0]),i[o]=0;for(a in l)Object.prototype.hasOwnProperty.call(l,a)&&(n[a]=l[a]);for(p&&p(e);u.length;)u.shift()();return r.push.apply(r,c||[]),t()}function t(){for(var n,e=0;e<r.length;e++){for(var t=r[e],a=!0,s=1;s<t.length;s++){var l=t[s];0!==i[l]&&(a=!1)}a&&(r.splice(e--,1),n=o(o.s=t[0]))}return n}var a={},i={1:0},r=[];function o(e){if(a[e])return a[e].exports;var t=a[e]={i:e,l:!1,exports:{}};return n[e].call(t.exports,t,t.exports,o),t.l=!0,t.exports}o.e=function(n){var e=[],t=i[n];if(0!==t)if(t)e.push(t[2]);else{var a=new Promise((function(e,a){t=i[n]=[e,a]}));e.push(t[2]=a);var r,s=document.createElement("script");s.charset="utf-8",s.timeout=120,o.nc&&s.setAttribute("nonce",o.nc),s.src=function(n){return o.p+"assets/js/"+({}[n]||n)+"."+{2:"3a0f33d3",3:"15429d0b",4:"1cd3e40a",5:"1ef7651e",6:"f2219599",7:"24d16974",8:"0fc65535",9:"a6c1a71e",10:"5f46a7f2",11:"ba219e7b",12:"92eba91b",13:"41875a03",14:"4ea6e58b",15:"6315eebb",16:"a45707b9",17:"84b8189a",18:"7ef19f81",19:"fe8ba7a9",20:"6b22eb76",21:"2b69be72",22:"a171d265",23:"d812c568",24:"72ac623b",25:"fb468557",26:"e7dda852",27:"467f0322",28:"426290bc",29:"225ecc7e",30:"fa7560f8",31:"a8fc66f3",32:"ac50becc",33:"59608a34",34:"68a0dbe3",35:"6336ae0a",36:"f9d748d4",37:"85b39d58",38:"b2d7cfc8",39:"8c3fc57c",40:"b67b358c",41:"c5473fd5",42:"12286784",43:"758498fe",44:"621d6d70",45:"7344cb95",46:"02c79e51",47:"bc1cde90",48:"00eb4363",49:"e0368bdc",50:"11151480",51:"39f64147",52:"58c3f1e5",53:"3689ff3f",54:"9a425b4c",55:"d5837e6a",56:"4f6a4a6d",57:"78ad9bd8",58:"7a4a19db",59:"3cd8b88a",60:"5dfcb925",61:"50246c0b",62:"94b3d244",63:"5d1c7582",64:"7e504dbe",65:"0ab7d7f5",66:"9b1e5832",67:"a49a0072",68:"89a2cdb9",69:"7c51a497",70:"fa17088f",71:"652bc609",72:"a8b644d6",73:"96493a6a",74:"6e32cc3b",75:"5575a872",76:"553e4468",77:"78664d5a",78:"4e68102a",79:"26a5da26",80:"628040bb",81:"f8aa9e53",82:"4d269dab",83:"e451c303",84:"76bc15ac",85:"52941df8",86:"9afbfc8c",87:"da83cc25",88:"f2829fa6",89:"cb138ae1",90:"8192237b",91:"109a9a1b",92:"ffdd8c48",93:"ffbf1b0a",94:"d50a2400",95:"8db673cc",96:"9071a7e6",97:"0f525bfd",98:"dccba0c1",99:"ba1fcb5f",100:"46109e18",101:"0826705d",102:"5129cfd9",103:"e0fad8e4",104:"612f9d15",105:"8f0d5447",106:"59c5fbc8",107:"bebb8c9f",108:"73079467",109:"7e815f7f",110:"6d8ffbe6",111:"0531213b",112:"f2c859c0",113:"bc71ae69",114:"26822f49",115:"6512eecb",116:"e224aaba",117:"05e718a6",118:"597ca212",119:"5da78682",120:"e581a0ee",121:"be2c67fc",122:"70e6b4e5",123:"8296bcad",124:"219f9074",125:"42980d68",126:"e0470fe1",127:"2614a3ff",128:"3894968d",129:"bfd89640",130:"ca6af328",131:"ce3547e9",132:"c7adae59",133:"7b148e67",134:"26ce87d7",135:"c0e6a63b",136:"a846f868",137:"01ef0c79",138:"25845e03",139:"f87d8dfb",140:"a1e0adb4",141:"8d294e46",142:"f31a107d",143:"962c12e1",144:"3e126a2f",145:"e872c634",146:"ca3b3894",147:"4ea23a80",148:"4913751c",149:"674df5e3",150:"afbba495",151:"244001f4",152:"2138506a",153:"096b6935",154:"e3cfefa3",155:"f7b4183e",156:"d45d8fd9",157:"fc653f4e",158:"cad10f7d",159:"adae05f6",160:"828f88c1",161:"20f3e039",162:"cba07e9b",163:"7783a35f",164:"d3618df9",165:"b2823237",166:"11c27ec7",167:"1b89c7f0",168:"70c6c64f",169:"73a1cf13",170:"6529cd0e",171:"eca471df",172:"c37b1d1b",173:"01d9640c",174:"39b4e30a",175:"42d99e11",176:"66f3bf94",177:"2f72339e",178:"157bc1aa",179:"f5fa8abc",180:"2d524cfa",181:"94dd9ddd",182:"e974805c",183:"798e5d4f",184:"6a8c5be6",185:"18142647",186:"84160426",187:"49ee8f2f",188:"836016f5",189:"ff83730a",190:"c7e999fe",191:"fb777610",192:"fe572b1a",193:"ebbb0341",194:"fc098f07",195:"bdb2ad5c",196:"e868eaa0",197:"79638251",198:"9c32057b",199:"5fbb8d5a",200:"fd915906",201:"deb6d35c",202:"4b0e4bb8",203:"9cb27cec",204:"9772dbfe",205:"46709f69",206:"8838ac9a",207:"1207b594",208:"9d545170",209:"48cef5cc",210:"35875a85",211:"1ef507f5",212:"e40db2e4",213:"5d5b825b",214:"e0167929",215:"b3dacfbe",216:"6a6b8867",217:"ad89b973",218:"e6c353a0",219:"78d57472",220:"0587014d",221:"b02bd13b",222:"77d7b68d",223:"989fa28e",224:"f26de2a0",225:"3d0828ec",226:"194d710e",227:"f9768256",228:"41593a37",229:"6a592fa1",230:"3c6fdea6",231:"0fc24bb2",232:"f381122a",233:"d1d5bacf",234:"4af5f5d4",235:"f252f22f",236:"7de67ddb",237:"47ced467",238:"1a33fd51",239:"b696499c",240:"16d96df4",241:"691185aa",242:"377e1fd2",243:"4ed38de5",244:"586adc48",245:"11f2a36a",246:"1ddf3941",247:"d25ea987",248:"71958f1a"}[n]+".js"}(n);var l=new Error;r=function(e){s.onerror=s.onload=null,clearTimeout(c);var t=i[n];if(0!==t){if(t){var a=e&&("load"===e.type?"missing":e.type),r=e&&e.target&&e.target.src;l.message="Loading chunk "+n+" failed.\n("+a+": "+r+")",l.name="ChunkLoadError",l.type=a,l.request=r,t[1](l)}i[n]=void 0}};var c=setTimeout((function(){r({type:"timeout",target:s})}),12e4);s.onerror=s.onload=r,document.head.appendChild(s)}return Promise.all(e)},o.m=n,o.c=a,o.d=function(n,e,t){o.o(n,e)||Object.defineProperty(n,e,{enumerable:!0,get:t})},o.r=function(n){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})},o.t=function(n,e){if(1&e&&(n=o(n)),8&e)return n;if(4&e&&"object"==typeof n&&n&&n.__esModule)return n;var t=Object.create(null);if(o.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:n}),2&e&&"string"!=typeof n)for(var a in n)o.d(t,a,function(e){return n[e]}.bind(null,a));return t},o.n=function(n){var e=n&&n.__esModule?function(){return n.default}:function(){return n};return o.d(e,"a",e),e},o.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},o.p="/",o.oe=function(n){throw console.error(n),n};var s=window.webpackJsonp=window.webpackJsonp||[],l=s.push.bind(s);s.push=e,s=s.slice();for(var c=0;c<s.length;c++)e(s[c]);var p=l;r.push([205,0]),t()}([function(n,e,t){var a=t(2),i=t(31).f,r=t(18),o=t(13),s=t(89),l=t(138),c=t(82);n.exports=function(n,e){var t,p,d,u,m,h=n.target,g=n.global,f=n.stat;if(t=g?a:f?a[h]||s(h,{}):(a[h]||{}).prototype)for(p in e){if(u=e[p],d=n.noTargetGet?(m=i(t,p))&&m.value:t[p],!c(g?p:h+(f?".":"#")+p,n.forced)&&void 0!==d){if(typeof u==typeof d)continue;l(u,d)}(n.sham||d&&d.sham)&&r(u,"sham",!0),o(t,p,u,n)}}},function(n,e){n.exports=function(n){try{return!!n()}catch(n){return!0}}},function(n,e){var t=function(n){return n&&n.Math==Math&&n};n.exports=t("object"==typeof globalThis&&globalThis)||t("object"==typeof window&&window)||t("object"==typeof self&&self)||t("object"==typeof global&&global)||function(){return this}()||Function("return this")()},function(n,e,t){var a=t(2),i=t(67),r=t(9),o=t(68),s=t(91),l=t(133),c=i("wks"),p=a.Symbol,d=l?p:p&&p.withoutSetter||o;n.exports=function(n){return r(c,n)&&(s||"string"==typeof c[n])||(s&&r(p,n)?c[n]=p[n]:c[n]=d("Symbol."+n)),c[n]}},function(n,e){n.exports=function(n){return"object"==typeof n?null!==n:"function"==typeof n}},function(n,e,t){var a=t(4);n.exports=function(n){if(!a(n))throw TypeError(String(n)+" is not an object");return n}},function(n,e,t){"use strict";t.d(e,"a",(function(){return r}));t(66);var a=t(59);t(47),t(57),t(8),t(74),t(12),t(17),t(101);var i=t(80);function r(n){return function(n){if(Array.isArray(n))return Object(a.a)(n)}(n)||function(n){if("undefined"!=typeof Symbol&&Symbol.iterator in Object(n))return Array.from(n)}(n)||Object(i.a)(n)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(n,e,t){var a=t(1);n.exports=!a((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(n,e,t){var a=t(99),i=t(13),r=t(216);a||i(Object.prototype,"toString",r,{unsafe:!0})},function(n,e){var t={}.hasOwnProperty;n.exports=function(n,e){return t.call(n,e)}},function(n,e,t){var a=t(7),i=t(131),r=t(5),o=t(51),s=Object.defineProperty;e.f=a?s:function(n,e,t){if(r(n),e=o(e,!0),r(t),i)try{return s(n,e,t)}catch(n){}if("get"in t||"set"in t)throw TypeError("Accessors not supported");return"value"in t&&(n[e]=t.value),n}},function(n,e,t){var a=t(43),i=Math.min;n.exports=function(n){return n>0?i(a(n),9007199254740991):0}},function(n,e,t){"use strict";var a=t(120).charAt,i=t(42),r=t(137),o=i.set,s=i.getterFor("String Iterator");r(String,"String",(function(n){o(this,{type:"String Iterator",string:String(n),index:0})}),(function(){var n,e=s(this),t=e.string,i=e.index;return i>=t.length?{value:void 0,done:!0}:(n=a(t,i),e.index+=n.length,{value:n,done:!1})}))},function(n,e,t){var a=t(2),i=t(18),r=t(9),o=t(89),s=t(94),l=t(42),c=l.get,p=l.enforce,d=String(String).split("String");(n.exports=function(n,e,t,s){var l,c=!!s&&!!s.unsafe,u=!!s&&!!s.enumerable,m=!!s&&!!s.noTargetGet;"function"==typeof t&&("string"!=typeof e||r(t,"name")||i(t,"name",e),(l=p(t)).source||(l.source=d.join("string"==typeof e?e:""))),n!==a?(c?!m&&n[e]&&(u=!0):delete n[e],u?n[e]=t:i(n,e,t)):u?n[e]=t:o(e,t)})(Function.prototype,"toString",(function(){return"function"==typeof this&&c(this).source||s(this)}))},function(n,e){n.exports=function(n){if(null==n)throw TypeError("Can't call method on "+n);return n}},function(n,e,t){var a=t(14);n.exports=function(n){return Object(a(n))}},function(n,e,t){"use strict";var a=t(0),i=t(44).filter;a({target:"Array",proto:!0,forced:!t(73)("filter")},{filter:function(n){return i(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){var a=t(2),i=t(150),r=t(117),o=t(18),s=t(3),l=s("iterator"),c=s("toStringTag"),p=r.values;for(var d in i){var u=a[d],m=u&&u.prototype;if(m){if(m[l]!==p)try{o(m,l,p)}catch(n){m[l]=p}if(m[c]||o(m,c,d),i[d])for(var h in r)if(m[h]!==r[h])try{o(m,h,r[h])}catch(n){m[h]=r[h]}}}},function(n,e,t){var a=t(7),i=t(10),r=t(45);n.exports=a?function(n,e,t){return i.f(n,e,r(1,t))}:function(n,e,t){return n[e]=t,n}},function(n,e,t){"use strict";var a=t(0),i=t(102);a({target:"RegExp",proto:!0,forced:/./.exec!==i},{exec:i})},function(n,e,t){var a=t(50),i=t(14);n.exports=function(n){return a(i(n))}},function(n,e,t){"use strict";var a=t(0),i=t(154);a({target:"Array",proto:!0,forced:[].forEach!=i},{forEach:i})},function(n,e,t){var a=t(2),i=t(150),r=t(154),o=t(18);for(var s in i){var l=a[s],c=l&&l.prototype;if(c&&c.forEach!==r)try{o(c,"forEach",r)}catch(n){c.forEach=r}}},function(n,e){var t=Array.isArray;n.exports=t},function(n,e,t){var a=t(160),i="object"==typeof self&&self&&self.Object===Object&&self,r=a||i||Function("return this")();n.exports=r},function(n,e,t){"use strict";function a(n,e,t,a,i,r,o,s){var l,c="function"==typeof n?n.options:n;if(e&&(c.render=e,c.staticRenderFns=t,c._compiled=!0),a&&(c.functional=!0),r&&(c._scopeId="data-v-"+r),o?(l=function(n){(n=n||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(n=__VUE_SSR_CONTEXT__),i&&i.call(this,n),n&&n._registeredComponents&&n._registeredComponents.add(o)},c._ssrRegister=l):i&&(l=s?function(){i.call(this,(c.functional?this.parent:this).$root.$options.shadowRoot)}:i),l)if(c.functional){c._injectStyles=l;var p=c.render;c.render=function(n,e){return l.call(e),p(n,e)}}else{var d=c.beforeCreate;c.beforeCreate=d?[].concat(d,l):[l]}return{exports:n,options:c}}t.d(e,"a",(function(){return a}))},function(n,e,t){var a=t(132),i=t(2),r=function(n){return"function"==typeof n?n:void 0};n.exports=function(n,e){return arguments.length<2?r(a[n])||r(i[n]):a[n]&&a[n][e]||i[n]&&i[n][e]}},function(n,e){n.exports=function(n){if("function"!=typeof n)throw TypeError(String(n)+" is not a function");return n}},function(n,e,t){"use strict";var a=t(0),i=t(44).map;a({target:"Array",proto:!0,forced:!t(73)("map")},{map:function(n){return i(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){"use strict";var a=t(84),i=t(5),r=t(11),o=t(43),s=t(14),l=t(103),c=t(224),p=t(85),d=Math.max,u=Math.min;a("replace",2,(function(n,e,t,a){var m=a.REGEXP_REPLACE_SUBSTITUTES_UNDEFINED_CAPTURE,h=a.REPLACE_KEEPS_$0,g=m?"$":"$0";return[function(t,a){var i=s(this),r=null==t?void 0:t[n];return void 0!==r?r.call(t,i,a):e.call(String(i),t,a)},function(n,a){if(!m&&h||"string"==typeof a&&-1===a.indexOf(g)){var s=t(e,n,this,a);if(s.done)return s.value}var f=i(n),v=String(this),b="function"==typeof a;b||(a=String(a));var y=f.global;if(y){var _=f.unicode;f.lastIndex=0}for(var E=[];;){var A=p(f,v);if(null===A)break;if(E.push(A),!y)break;""===String(A[0])&&(f.lastIndex=l(v,r(f.lastIndex),_))}for(var x,k="",w=0,C=0;C<E.length;C++){A=E[C];for(var S=String(A[0]),B=d(u(o(A.index),v.length),0),T=[],P=1;P<A.length;P++)T.push(void 0===(x=A[P])?x:String(x));var D=A.groups;if(b){var z=[S].concat(T,B,v);void 0!==D&&z.push(D);var L=String(a.apply(void 0,z))}else L=c(S,v,B,T,D,a);B>=w&&(k+=v.slice(w,B)+L,w=B+S.length)}return k+v.slice(w)}]}))},function(n,e){n.exports=!1},function(n,e,t){var a=t(7),i=t(95),r=t(45),o=t(20),s=t(51),l=t(9),c=t(131),p=Object.getOwnPropertyDescriptor;e.f=a?p:function(n,e){if(n=o(n),e=s(e,!0),c)try{return p(n,e)}catch(n){}if(l(n,e))return r(!i.f.call(n,e),n[e])}},function(n,e){var t={}.toString;n.exports=function(n){return t.call(n).slice(8,-1)}},function(n,e,t){var a=t(243),i=t(246);n.exports=function(n,e){var t=i(n,e);return a(t)?t:void 0}},function(n,e,t){"use strict";t.d(e,"a",(function(){return i}));t(66);t(47),t(57),t(8),t(74),t(12),t(17);var a=t(80);function i(n,e){return function(n){if(Array.isArray(n))return n}(n)||function(n,e){if("undefined"!=typeof Symbol&&Symbol.iterator in Object(n)){var t=[],a=!0,i=!1,r=void 0;try{for(var o,s=n[Symbol.iterator]();!(a=(o=s.next()).done)&&(t.push(o.value),!e||t.length!==e);a=!0);}catch(n){i=!0,r=n}finally{try{a||null==s.return||s.return()}finally{if(i)throw r}}return t}}(n,e)||Object(a.a)(n,e)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(n,e,t){"use strict";t.d(e,"e",(function(){return a})),t.d(e,"b",(function(){return r})),t.d(e,"j",(function(){return o})),t.d(e,"g",(function(){return l})),t.d(e,"h",(function(){return c})),t.d(e,"i",(function(){return p})),t.d(e,"c",(function(){return d})),t.d(e,"f",(function(){return u})),t.d(e,"l",(function(){return m})),t.d(e,"m",(function(){return h})),t.d(e,"d",(function(){return f})),t.d(e,"k",(function(){return v})),t.d(e,"n",(function(){return b})),t.d(e,"a",(function(){return _}));t(19),t(29),t(113),t(60),t(48),t(28),t(21),t(22),t(16),t(66),t(62),t(8),t(130),t(36),t(181),t(116);var a=/#.*$/,i=/\.(md|html)$/,r=/\/$/,o=/^[a-z]+:/i;function s(n){return decodeURI(n).replace(a,"").replace(i,"")}function l(n){return o.test(n)}function c(n){return/^mailto:/.test(n)}function p(n){return/^tel:/.test(n)}function d(n){if(l(n))return n;var e=n.match(a),t=e?e[0]:"",i=s(n);return r.test(i)?n:i+".html"+t}function u(n,e){var t=n.hash,i=function(n){var e=n.match(a);if(e)return e[0]}(e);return(!i||t===i)&&s(n.path)===s(e)}function m(n,e,t){if(l(e))return{type:"external",path:e};t&&(e=function(n,e,t){var a=n.charAt(0);if("/"===a)return n;if("?"===a||"#"===a)return e+n;var i=e.split("/");t&&i[i.length-1]||i.pop();for(var r=n.replace(/^\//,"").split("/"),o=0;o<r.length;o++){var s=r[o];".."===s?i.pop():"."!==s&&i.push(s)}""!==i[0]&&i.unshift("");return i.join("/")}(e,t));for(var a=s(e),i=0;i<n.length;i++)if(s(n[i].regularPath)===a)return Object.assign({},n[i],{type:"page",path:d(n[i].path)});return console.error('[vuepress] No matching page found for sidebar item "'.concat(e,'"')),{}}function h(n,e,t,a){var i=t.pages,r=t.themeConfig,o=a&&r.locales&&r.locales[a]||r;if("auto"===(n.frontmatter.sidebar||o.sidebar||r.sidebar))return g(n);var s=o.sidebar||r.sidebar;if(s){var l=function(n,e){if(Array.isArray(e))return{base:"/",config:e};for(var t in e)if(0===(a=n,/(\.html|\/)$/.test(a)?a:a+"/").indexOf(encodeURI(t)))return{base:t,config:e[t]};var a;return{}}(e,s),c=l.base,p=l.config;return"auto"===p?g(n):p?p.map((function(n){return function n(e,t,a){var i=arguments.length>3&&void 0!==arguments[3]?arguments[3]:1;if("string"==typeof e)return m(t,e,a);if(Array.isArray(e))return Object.assign(m(t,e[0],a),{title:e[1]});i>3&&console.error("[vuepress] detected a too deep nested sidebar group.");var r=e.children||[];return 0===r.length&&e.path?Object.assign(m(t,e.path,a),{title:e.title}):{type:"group",path:e.path,title:e.title,sidebarDepth:e.sidebarDepth,initialOpenGroupIndex:e.initialOpenGroupIndex,children:r.map((function(e){return n(e,t,a,i+1)})),collapsable:!1!==e.collapsable}}(n,i,c)})):[]}return[]}function g(n){var e=f(n.headers||[]);return[{type:"group",collapsable:!1,title:n.title,path:null,children:e.map((function(e){return{type:"auto",title:e.title,basePath:n.path,path:n.path+"#"+e.slug,children:e.children||[]}}))}]}function f(n){var e;return(n=n.map((function(n){return Object.assign({},n)}))).forEach((function(n){2===n.level?e=n:e&&(e.children||(e.children=[])).push(n)})),n.filter((function(n){return 2===n.level}))}function v(n){return Object.assign(n,{type:n.items&&n.items.length?"links":"link"})}function b(n){return Object.prototype.toString.call(n).match(/\[object (.*?)\]/)[1].toLowerCase()}function y(n){var e=n.frontmatter.date||n.lastUpdated,t=new Date(e);return"Invalid Date"==t&&(t=new Date(e.replace(/-/g,"/"))),t.getTime()}function _(n,e){return y(e)-y(n)}},function(n,e,t){"use strict";var a=t(0),i=t(1),r=t(46),o=t(4),s=t(15),l=t(11),c=t(72),p=t(121),d=t(73),u=t(3),m=t(53),h=u("isConcatSpreadable"),g=m>=51||!i((function(){var n=[];return n[h]=!1,n.concat()[0]!==n})),f=d("concat"),v=function(n){if(!o(n))return!1;var e=n[h];return void 0!==e?!!e:r(n)};a({target:"Array",proto:!0,forced:!g||!f},{concat:function(n){var e,t,a,i,r,o=s(this),d=p(o,0),u=0;for(e=-1,a=arguments.length;e<a;e++)if(v(r=-1===e?o:arguments[e])){if(u+(i=l(r.length))>9007199254740991)throw TypeError("Maximum allowed index exceeded");for(t=0;t<i;t++,u++)t in r&&c(d,u,r[t])}else{if(u>=9007199254740991)throw TypeError("Maximum allowed index exceeded");c(d,u++,r)}return d.length=u,d}})},function(n,e,t){"use strict";var a=t(1);n.exports=function(n,e){var t=[][n];return!!t&&a((function(){t.call(null,e||function(){throw 1},1)}))}},function(n,e,t){var a,i=t(5),r=t(194),o=t(93),s=t(55),l=t(136),c=t(90),p=t(70),d=p("IE_PROTO"),u=function(){},m=function(n){return"<script>"+n+"<\/script>"},h=function(){try{a=document.domain&&new ActiveXObject("htmlfile")}catch(n){}var n,e;h=a?function(n){n.write(m("")),n.close();var e=n.parentWindow.Object;return n=null,e}(a):((e=c("iframe")).style.display="none",l.appendChild(e),e.src=String("javascript:"),(n=e.contentWindow.document).open(),n.write(m("document.F=Object")),n.close(),n.F);for(var t=o.length;t--;)delete h.prototype[o[t]];return h()};s[d]=!0,n.exports=Object.create||function(n,e){var t;return null!==n?(u.prototype=i(n),t=new u,u.prototype=null,t[d]=n):t=h(),void 0===e?t:r(t,e)}},function(n,e){n.exports=function(n){return null!=n&&"object"==typeof n}},function(n,e,t){"use strict";var a=t(0),i=t(4),r=t(46),o=t(135),s=t(11),l=t(20),c=t(72),p=t(3),d=t(73)("slice"),u=p("species"),m=[].slice,h=Math.max;a({target:"Array",proto:!0,forced:!d},{slice:function(n,e){var t,a,p,d=l(this),g=s(d.length),f=o(n,g),v=o(void 0===e?g:e,g);if(r(d)&&("function"!=typeof(t=d.constructor)||t!==Array&&!r(t.prototype)?i(t)&&null===(t=t[u])&&(t=void 0):t=void 0,t===Array||void 0===t))return m.call(d,f,v);for(a=new(void 0===t?Array:t)(h(v-f,0)),p=0;f<v;f++,p++)f in d&&c(a,p,d[f]);return a.length=p,a}})},function(n,e,t){var a=t(0),i=t(2),r=t(54),o=[].slice,s=function(n){return function(e,t){var a=arguments.length>2,i=a?o.call(arguments,2):void 0;return n(a?function(){("function"==typeof e?e:Function(e)).apply(this,i)}:e,t)}};a({global:!0,bind:!0,forced:/MSIE .\./.test(r)},{setTimeout:s(i.setTimeout),setInterval:s(i.setInterval)})},function(n,e,t){var a,i,r,o=t(206),s=t(2),l=t(4),c=t(18),p=t(9),d=t(88),u=t(70),m=t(55),h=s.WeakMap;if(o){var g=d.state||(d.state=new h),f=g.get,v=g.has,b=g.set;a=function(n,e){return e.facade=n,b.call(g,n,e),e},i=function(n){return f.call(g,n)||{}},r=function(n){return v.call(g,n)}}else{var y=u("state");m[y]=!0,a=function(n,e){return e.facade=n,c(n,y,e),e},i=function(n){return p(n,y)?n[y]:{}},r=function(n){return p(n,y)}}n.exports={set:a,get:i,has:r,enforce:function(n){return r(n)?i(n):a(n,{})},getterFor:function(n){return function(e){var t;if(!l(e)||(t=i(e)).type!==n)throw TypeError("Incompatible receiver, "+n+" required");return t}}}},function(n,e){var t=Math.ceil,a=Math.floor;n.exports=function(n){return isNaN(n=+n)?0:(n>0?a:t)(n)}},function(n,e,t){var a=t(64),i=t(50),r=t(15),o=t(11),s=t(121),l=[].push,c=function(n){var e=1==n,t=2==n,c=3==n,p=4==n,d=6==n,u=7==n,m=5==n||d;return function(h,g,f,v){for(var b,y,_=r(h),E=i(_),A=a(g,f,3),x=o(E.length),k=0,w=v||s,C=e?w(h,x):t||u?w(h,0):void 0;x>k;k++)if((m||k in E)&&(y=A(b=E[k],k,_),n))if(e)C[k]=y;else if(y)switch(n){case 3:return!0;case 5:return b;case 6:return k;case 2:l.call(C,b)}else switch(n){case 4:return!1;case 7:l.call(C,b)}return d?-1:c||p?p:C}};n.exports={forEach:c(0),map:c(1),filter:c(2),some:c(3),every:c(4),find:c(5),findIndex:c(6),filterOut:c(7)}},function(n,e){n.exports=function(n,e){return{enumerable:!(1&n),configurable:!(2&n),writable:!(4&n),value:e}}},function(n,e,t){var a=t(32);n.exports=Array.isArray||function(n){return"Array"==a(n)}},function(n,e,t){"use strict";var a=t(0),i=t(2),r=t(26),o=t(30),s=t(7),l=t(91),c=t(133),p=t(1),d=t(9),u=t(46),m=t(4),h=t(5),g=t(15),f=t(20),v=t(51),b=t(45),y=t(38),_=t(69),E=t(65),A=t(222),x=t(96),k=t(31),w=t(10),C=t(95),S=t(18),B=t(13),T=t(67),P=t(70),D=t(55),z=t(68),L=t(3),I=t(156),N=t(157),F=t(63),j=t(42),M=t(44).forEach,R=P("hidden"),O=L("toPrimitive"),U=j.set,V=j.getterFor("Symbol"),G=Object.prototype,W=i.Symbol,H=r("JSON","stringify"),q=k.f,$=w.f,X=A.f,Z=C.f,K=T("symbols"),Y=T("op-symbols"),J=T("string-to-symbol-registry"),Q=T("symbol-to-string-registry"),nn=T("wks"),en=i.QObject,tn=!en||!en.prototype||!en.prototype.findChild,an=s&&p((function(){return 7!=y($({},"a",{get:function(){return $(this,"a",{value:7}).a}})).a}))?function(n,e,t){var a=q(G,e);a&&delete G[e],$(n,e,t),a&&n!==G&&$(G,e,a)}:$,rn=function(n,e){var t=K[n]=y(W.prototype);return U(t,{type:"Symbol",tag:n,description:e}),s||(t.description=e),t},on=c?function(n){return"symbol"==typeof n}:function(n){return Object(n)instanceof W},sn=function(n,e,t){n===G&&sn(Y,e,t),h(n);var a=v(e,!0);return h(t),d(K,a)?(t.enumerable?(d(n,R)&&n[R][a]&&(n[R][a]=!1),t=y(t,{enumerable:b(0,!1)})):(d(n,R)||$(n,R,b(1,{})),n[R][a]=!0),an(n,a,t)):$(n,a,t)},ln=function(n,e){h(n);var t=f(e),a=_(t).concat(un(t));return M(a,(function(e){s&&!cn.call(t,e)||sn(n,e,t[e])})),n},cn=function(n){var e=v(n,!0),t=Z.call(this,e);return!(this===G&&d(K,e)&&!d(Y,e))&&(!(t||!d(this,e)||!d(K,e)||d(this,R)&&this[R][e])||t)},pn=function(n,e){var t=f(n),a=v(e,!0);if(t!==G||!d(K,a)||d(Y,a)){var i=q(t,a);return!i||!d(K,a)||d(t,R)&&t[R][a]||(i.enumerable=!0),i}},dn=function(n){var e=X(f(n)),t=[];return M(e,(function(n){d(K,n)||d(D,n)||t.push(n)})),t},un=function(n){var e=n===G,t=X(e?Y:f(n)),a=[];return M(t,(function(n){!d(K,n)||e&&!d(G,n)||a.push(K[n])})),a};(l||(B((W=function(){if(this instanceof W)throw TypeError("Symbol is not a constructor");var n=arguments.length&&void 0!==arguments[0]?String(arguments[0]):void 0,e=z(n),t=function(n){this===G&&t.call(Y,n),d(this,R)&&d(this[R],e)&&(this[R][e]=!1),an(this,e,b(1,n))};return s&&tn&&an(G,e,{configurable:!0,set:t}),rn(e,n)}).prototype,"toString",(function(){return V(this).tag})),B(W,"withoutSetter",(function(n){return rn(z(n),n)})),C.f=cn,w.f=sn,k.f=pn,E.f=A.f=dn,x.f=un,I.f=function(n){return rn(L(n),n)},s&&($(W.prototype,"description",{configurable:!0,get:function(){return V(this).description}}),o||B(G,"propertyIsEnumerable",cn,{unsafe:!0}))),a({global:!0,wrap:!0,forced:!l,sham:!l},{Symbol:W}),M(_(nn),(function(n){N(n)})),a({target:"Symbol",stat:!0,forced:!l},{for:function(n){var e=String(n);if(d(J,e))return J[e];var t=W(e);return J[e]=t,Q[t]=e,t},keyFor:function(n){if(!on(n))throw TypeError(n+" is not a symbol");if(d(Q,n))return Q[n]},useSetter:function(){tn=!0},useSimple:function(){tn=!1}}),a({target:"Object",stat:!0,forced:!l,sham:!s},{create:function(n,e){return void 0===e?y(n):ln(y(n),e)},defineProperty:sn,defineProperties:ln,getOwnPropertyDescriptor:pn}),a({target:"Object",stat:!0,forced:!l},{getOwnPropertyNames:dn,getOwnPropertySymbols:un}),a({target:"Object",stat:!0,forced:p((function(){x.f(1)}))},{getOwnPropertySymbols:function(n){return x.f(g(n))}}),H)&&a({target:"JSON",stat:!0,forced:!l||p((function(){var n=W();return"[null]"!=H([n])||"{}"!=H({a:n})||"{}"!=H(Object(n))}))},{stringify:function(n,e,t){for(var a,i=[n],r=1;arguments.length>r;)i.push(arguments[r++]);if(a=e,(m(e)||void 0!==n)&&!on(n))return u(e)||(e=function(n,e){if("function"==typeof a&&(e=a.call(this,n,e)),!on(e))return e}),i[1]=e,H.apply(null,i)}});W.prototype[O]||S(W.prototype,O,W.prototype.valueOf),F(W,"Symbol"),D[R]=!0},function(n,e,t){"use strict";var a=t(0),i=t(50),r=t(20),o=t(37),s=[].join,l=i!=Object,c=o("join",",");a({target:"Array",proto:!0,forced:l||!c},{join:function(n){return s.call(r(this),void 0===n?",":n)}})},function(n,e,t){var a=t(58),i=t(228),r=t(229),o=a?a.toStringTag:void 0;n.exports=function(n){return null==n?void 0===n?"[object Undefined]":"[object Null]":o&&o in Object(n)?i(n):r(n)}},function(n,e,t){var a=t(1),i=t(32),r="".split;n.exports=a((function(){return!Object("z").propertyIsEnumerable(0)}))?function(n){return"String"==i(n)?r.call(n,""):Object(n)}:Object},function(n,e,t){var a=t(4);n.exports=function(n,e){if(!a(n))return n;var t,i;if(e&&"function"==typeof(t=n.toString)&&!a(i=t.call(n)))return i;if("function"==typeof(t=n.valueOf)&&!a(i=t.call(n)))return i;if(!e&&"function"==typeof(t=n.toString)&&!a(i=t.call(n)))return i;throw TypeError("Can't convert object to primitive value")}},function(n,e,t){var a=t(32),i=t(2);n.exports="process"==a(i.process)},function(n,e,t){var a,i,r=t(2),o=t(54),s=r.process,l=s&&s.versions,c=l&&l.v8;c?i=(a=c.split("."))[0]+a[1]:o&&(!(a=o.match(/Edge\/(\d+)/))||a[1]>=74)&&(a=o.match(/Chrome\/(\d+)/))&&(i=a[1]),n.exports=i&&+i},function(n,e,t){var a=t(26);n.exports=a("navigator","userAgent")||""},function(n,e){n.exports={}},function(n,e){n.exports={}},function(n,e,t){"use strict";var a=t(0),i=t(7),r=t(2),o=t(9),s=t(4),l=t(10).f,c=t(138),p=r.Symbol;if(i&&"function"==typeof p&&(!("description"in p.prototype)||void 0!==p().description)){var d={},u=function(){var n=arguments.length<1||void 0===arguments[0]?void 0:String(arguments[0]),e=this instanceof u?new p(n):void 0===n?p():p(n);return""===n&&(d[e]=!0),e};c(u,p);var m=u.prototype=p.prototype;m.constructor=u;var h=m.toString,g="Symbol(test)"==String(p("test")),f=/^Symbol\((.*)\)[^)]+$/;l(m,"description",{configurable:!0,get:function(){var n=s(this)?this.valueOf():this,e=h.call(n);if(o(d,n))return"";var t=g?e.slice(7,-1):e.replace(f,"$1");return""===t?void 0:t}}),a({global:!0,forced:!0},{Symbol:u})}},function(n,e,t){var a=t(24).Symbol;n.exports=a},function(n,e,t){"use strict";function a(n,e){(null==e||e>n.length)&&(e=n.length);for(var t=0,a=new Array(e);t<e;t++)a[t]=n[t];return a}t.d(e,"a",(function(){return a}))},function(n,e,t){"use strict";var a=t(84),i=t(122),r=t(5),o=t(14),s=t(100),l=t(103),c=t(11),p=t(85),d=t(102),u=t(124).UNSUPPORTED_Y,m=[].push,h=Math.min;a("split",2,(function(n,e,t){var a;return a="c"=="abbc".split(/(b)*/)[1]||4!="test".split(/(?:)/,-1).length||2!="ab".split(/(?:ab)*/).length||4!=".".split(/(.?)(.?)/).length||".".split(/()()/).length>1||"".split(/.?/).length?function(n,t){var a=String(o(this)),r=void 0===t?4294967295:t>>>0;if(0===r)return[];if(void 0===n)return[a];if(!i(n))return e.call(a,n,r);for(var s,l,c,p=[],u=(n.ignoreCase?"i":"")+(n.multiline?"m":"")+(n.unicode?"u":"")+(n.sticky?"y":""),h=0,g=new RegExp(n.source,u+"g");(s=d.call(g,a))&&!((l=g.lastIndex)>h&&(p.push(a.slice(h,s.index)),s.length>1&&s.index<a.length&&m.apply(p,s.slice(1)),c=s[0].length,h=l,p.length>=r));)g.lastIndex===s.index&&g.lastIndex++;return h===a.length?!c&&g.test("")||p.push(""):p.push(a.slice(h)),p.length>r?p.slice(0,r):p}:"0".split(void 0,0).length?function(n,t){return void 0===n&&0===t?[]:e.call(this,n,t)}:e,[function(e,t){var i=o(this),r=null==e?void 0:e[n];return void 0!==r?r.call(e,i,t):a.call(String(i),e,t)},function(n,i){var o=t(a,n,this,i,a!==e);if(o.done)return o.value;var d=r(n),m=String(this),g=s(d,RegExp),f=d.unicode,v=(d.ignoreCase?"i":"")+(d.multiline?"m":"")+(d.unicode?"u":"")+(u?"g":"y"),b=new g(u?"^(?:"+d.source+")":d,v),y=void 0===i?4294967295:i>>>0;if(0===y)return[];if(0===m.length)return null===p(b,m)?[m]:[];for(var _=0,E=0,A=[];E<m.length;){b.lastIndex=u?0:E;var x,k=p(b,u?m.slice(E):m);if(null===k||(x=h(c(b.lastIndex+(u?E:0)),m.length))===_)E=l(m,E,f);else{if(A.push(m.slice(_,E)),A.length===y)return A;for(var w=1;w<=k.length-1;w++)if(A.push(k[w]),A.length===y)return A;E=_=x}}return A.push(m.slice(_)),A}]}),u)},function(n,e,t){var a=t(0),i=t(15),r=t(69);a({target:"Object",stat:!0,forced:t(1)((function(){r(1)}))},{keys:function(n){return r(i(n))}})},function(n,e,t){"use strict";var a=t(0),i=t(92).indexOf,r=t(37),o=[].indexOf,s=!!o&&1/[1].indexOf(1,-0)<0,l=r("indexOf");a({target:"Array",proto:!0,forced:s||!l},{indexOf:function(n){return s?o.apply(this,arguments)||0:i(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){var a=t(10).f,i=t(9),r=t(3)("toStringTag");n.exports=function(n,e,t){n&&!i(n=t?n:n.prototype,r)&&a(n,r,{configurable:!0,value:e})}},function(n,e,t){var a=t(27);n.exports=function(n,e,t){if(a(n),void 0===e)return n;switch(t){case 0:return function(){return n.call(e)};case 1:return function(t){return n.call(e,t)};case 2:return function(t,a){return n.call(e,t,a)};case 3:return function(t,a,i){return n.call(e,t,a,i)}}return function(){return n.apply(e,arguments)}}},function(n,e,t){var a=t(134),i=t(93).concat("length","prototype");e.f=Object.getOwnPropertyNames||function(n){return a(n,i)}},function(n,e,t){t(0)({target:"Array",stat:!0},{isArray:t(46)})},function(n,e,t){var a=t(30),i=t(88);(n.exports=function(n,e){return i[n]||(i[n]=void 0!==e?e:{})})("versions",[]).push({version:"3.10.1",mode:a?"pure":"global",copyright:"© 2021 Denis Pushkarev (zloirock.ru)"})},function(n,e){var t=0,a=Math.random();n.exports=function(n){return"Symbol("+String(void 0===n?"":n)+")_"+(++t+a).toString(36)}},function(n,e,t){var a=t(134),i=t(93);n.exports=Object.keys||function(n){return a(n,i)}},function(n,e,t){var a=t(67),i=t(68),r=a("keys");n.exports=function(n){return r[n]||(r[n]=i(n))}},function(n,e,t){"use strict";t.d(e,"a",(function(){return i}));t(8);function a(n,e,t,a,i,r,o){try{var s=n[r](o),l=s.value}catch(n){return void t(n)}s.done?e(l):Promise.resolve(l).then(a,i)}function i(n){return function(){var e=this,t=arguments;return new Promise((function(i,r){var o=n.apply(e,t);function s(n){a(o,i,r,s,l,"next",n)}function l(n){a(o,i,r,s,l,"throw",n)}s(void 0)}))}}},function(n,e,t){"use strict";var a=t(51),i=t(10),r=t(45);n.exports=function(n,e,t){var o=a(e);o in n?i.f(n,o,r(0,t)):n[o]=t}},function(n,e,t){var a=t(1),i=t(3),r=t(53),o=i("species");n.exports=function(n){return r>=51||!a((function(){var e=[];return(e.constructor={})[o]=function(){return{foo:1}},1!==e[n](Boolean).foo}))}},function(n,e,t){t(157)("iterator")},function(n,e,t){var a=t(233),i=t(234),r=t(235),o=t(236),s=t(237);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var a=n[e];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=i,l.prototype.get=r,l.prototype.has=o,l.prototype.set=s,n.exports=l},function(n,e,t){var a=t(162);n.exports=function(n,e){for(var t=n.length;t--;)if(a(n[t][0],e))return t;return-1}},function(n,e,t){var a=t(33)(Object,"create");n.exports=a},function(n,e,t){var a=t(255);n.exports=function(n,e){var t=n.__data__;return a(e)?t["string"==typeof e?"string":"hash"]:t.map}},function(n,e,t){var a=t(111);n.exports=function(n){if("string"==typeof n||a(n))return n;var e=n+"";return"0"==e&&1/n==-1/0?"-0":e}},function(n,e,t){"use strict";t.d(e,"a",(function(){return i}));t(40),t(8),t(83),t(101),t(12);var a=t(59);function i(n,e){if(n){if("string"==typeof n)return Object(a.a)(n,e);var t=Object.prototype.toString.call(n).slice(8,-1);return"Object"===t&&n.constructor&&(t=n.constructor.name),"Map"===t||"Set"===t?Array.from(n):"Arguments"===t||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(t)?Object(a.a)(n,e):void 0}}},function(n,e,t){var a,i;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(i="function"==typeof(a=function(){var n,e,t={version:"0.2.0"},a=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function i(n,e,t){return n<e?e:n>t?t:n}function r(n){return 100*(-1+n)}t.configure=function(n){var e,t;for(e in n)void 0!==(t=n[e])&&n.hasOwnProperty(e)&&(a[e]=t);return this},t.status=null,t.set=function(n){var e=t.isStarted();n=i(n,a.minimum,1),t.status=1===n?null:n;var l=t.render(!e),c=l.querySelector(a.barSelector),p=a.speed,d=a.easing;return l.offsetWidth,o((function(e){""===a.positionUsing&&(a.positionUsing=t.getPositioningCSS()),s(c,function(n,e,t){var i;return(i="translate3d"===a.positionUsing?{transform:"translate3d("+r(n)+"%,0,0)"}:"translate"===a.positionUsing?{transform:"translate("+r(n)+"%,0)"}:{"margin-left":r(n)+"%"}).transition="all "+e+"ms "+t,i}(n,p,d)),1===n?(s(l,{transition:"none",opacity:1}),l.offsetWidth,setTimeout((function(){s(l,{transition:"all "+p+"ms linear",opacity:0}),setTimeout((function(){t.remove(),e()}),p)}),p)):setTimeout(e,p)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var n=function(){setTimeout((function(){t.status&&(t.trickle(),n())}),a.trickleSpeed)};return a.trickle&&n(),this},t.done=function(n){return n||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(n){var e=t.status;return e?("number"!=typeof n&&(n=(1-e)*i(Math.random()*e,.1,.95)),e=i(e+n,0,.994),t.set(e)):t.start()},t.trickle=function(){return t.inc(Math.random()*a.trickleRate)},n=0,e=0,t.promise=function(a){return a&&"resolved"!==a.state()?(0===e&&t.start(),n++,e++,a.always((function(){0==--e?(n=0,t.done()):t.set((n-e)/n)})),this):this},t.render=function(n){if(t.isRendered())return document.getElementById("nprogress");c(document.documentElement,"nprogress-busy");var e=document.createElement("div");e.id="nprogress",e.innerHTML=a.template;var i,o=e.querySelector(a.barSelector),l=n?"-100":r(t.status||0),p=document.querySelector(a.parent);return s(o,{transition:"all 0 linear",transform:"translate3d("+l+"%,0,0)"}),a.showSpinner||(i=e.querySelector(a.spinnerSelector))&&u(i),p!=document.body&&c(p,"nprogress-custom-parent"),p.appendChild(e),e},t.remove=function(){p(document.documentElement,"nprogress-busy"),p(document.querySelector(a.parent),"nprogress-custom-parent");var n=document.getElementById("nprogress");n&&u(n)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var n=document.body.style,e="WebkitTransform"in n?"Webkit":"MozTransform"in n?"Moz":"msTransform"in n?"ms":"OTransform"in n?"O":"";return e+"Perspective"in n?"translate3d":e+"Transform"in n?"translate":"margin"};var o=function(){var n=[];function e(){var t=n.shift();t&&t(e)}return function(t){n.push(t),1==n.length&&e()}}(),s=function(){var n=["Webkit","O","Moz","ms"],e={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(n,e){return e.toUpperCase()})),e[t]||(e[t]=function(e){var t=document.body.style;if(e in t)return e;for(var a,i=n.length,r=e.charAt(0).toUpperCase()+e.slice(1);i--;)if((a=n[i]+r)in t)return a;return e}(t))}function a(n,e,a){e=t(e),n.style[e]=a}return function(n,e){var t,i,r=arguments;if(2==r.length)for(t in e)void 0!==(i=e[t])&&e.hasOwnProperty(t)&&a(n,t,i);else a(n,r[1],r[2])}}();function l(n,e){return("string"==typeof n?n:d(n)).indexOf(" "+e+" ")>=0}function c(n,e){var t=d(n),a=t+e;l(t,e)||(n.className=a.substring(1))}function p(n,e){var t,a=d(n);l(n,e)&&(t=a.replace(" "+e+" "," "),n.className=t.substring(1,t.length-1))}function d(n){return(" "+(n.className||"")+" ").replace(/\s+/gi," ")}function u(n){n&&n.parentNode&&n.parentNode.removeChild(n)}return t})?a.call(e,t,e,n):a)||(n.exports=i)},function(n,e,t){var a=t(1),i=/#|\.prototype\./,r=function(n,e){var t=s[o(n)];return t==c||t!=l&&("function"==typeof e?a(e):!!e)},o=r.normalize=function(n){return String(n).replace(i,".").toLowerCase()},s=r.data={},l=r.NATIVE="N",c=r.POLYFILL="P";n.exports=r},function(n,e,t){var a=t(7),i=t(10).f,r=Function.prototype,o=r.toString,s=/^\s*function ([^ (]*)/;a&&!("name"in r)&&i(r,"name",{configurable:!0,get:function(){try{return o.call(this).match(s)[1]}catch(n){return""}}})},function(n,e,t){"use strict";t(19);var a=t(13),i=t(1),r=t(3),o=t(18),s=r("species"),l=!i((function(){var n=/./;return n.exec=function(){var n=[];return n.groups={a:"7"},n},"7"!=="".replace(n,"$<a>")})),c="$0"==="a".replace(/./,"$0"),p=r("replace"),d=!!/./[p]&&""===/./[p]("a","$0"),u=!i((function(){var n=/(?:)/,e=n.exec;n.exec=function(){return e.apply(this,arguments)};var t="ab".split(n);return 2!==t.length||"a"!==t[0]||"b"!==t[1]}));n.exports=function(n,e,t,p){var m=r(n),h=!i((function(){var e={};return e[m]=function(){return 7},7!=""[n](e)})),g=h&&!i((function(){var e=!1,t=/a/;return"split"===n&&((t={}).constructor={},t.constructor[s]=function(){return t},t.flags="",t[m]=/./[m]),t.exec=function(){return e=!0,null},t[m](""),!e}));if(!h||!g||"replace"===n&&(!l||!c||d)||"split"===n&&!u){var f=/./[m],v=t(m,""[n],(function(n,e,t,a,i){return e.exec===RegExp.prototype.exec?h&&!i?{done:!0,value:f.call(e,t,a)}:{done:!0,value:n.call(t,e,a)}:{done:!1}}),{REPLACE_KEEPS_$0:c,REGEXP_REPLACE_SUBSTITUTES_UNDEFINED_CAPTURE:d}),b=v[0],y=v[1];a(String.prototype,n,b),a(RegExp.prototype,m,2==e?function(n,e){return y.call(n,this,e)}:function(n){return y.call(n,this)})}p&&o(RegExp.prototype[m],"sham",!0)}},function(n,e,t){var a=t(32),i=t(102);n.exports=function(n,e){var t=n.exec;if("function"==typeof t){var r=t.call(n,e);if("object"!=typeof r)throw TypeError("RegExp exec method returned something other than an Object or null");return r}if("RegExp"!==a(n))throw TypeError("RegExp#exec called on incompatible receiver");return i.call(n,e)}},function(n,e,t){"use strict";var a=t(0),i=t(92).includes,r=t(118);a({target:"Array",proto:!0},{includes:function(n){return i(this,n,arguments.length>1?arguments[1]:void 0)}}),r("includes")},function(n,e,t){"use strict";var a=t(0),i=t(152),r=t(14);a({target:"String",proto:!0,forced:!t(153)("includes")},{includes:function(n){return!!~String(r(this)).indexOf(i(n),arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){var a=t(2),i=t(89),r=a["__core-js_shared__"]||i("__core-js_shared__",{});n.exports=r},function(n,e,t){var a=t(2),i=t(18);n.exports=function(n,e){try{i(a,n,e)}catch(t){a[n]=e}return e}},function(n,e,t){var a=t(2),i=t(4),r=a.document,o=i(r)&&i(r.createElement);n.exports=function(n){return o?r.createElement(n):{}}},function(n,e,t){var a=t(52),i=t(53),r=t(1);n.exports=!!Object.getOwnPropertySymbols&&!r((function(){return!Symbol.sham&&(a?38===i:i>37&&i<41)}))},function(n,e,t){var a=t(20),i=t(11),r=t(135),o=function(n){return function(e,t,o){var s,l=a(e),c=i(l.length),p=r(o,c);if(n&&t!=t){for(;c>p;)if((s=l[p++])!=s)return!0}else for(;c>p;p++)if((n||p in l)&&l[p]===t)return n||p||0;return!n&&-1}};n.exports={includes:o(!0),indexOf:o(!1)}},function(n,e){n.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(n,e,t){var a=t(88),i=Function.toString;"function"!=typeof a.inspectSource&&(a.inspectSource=function(n){return i.call(n)}),n.exports=a.inspectSource},function(n,e,t){"use strict";var a={}.propertyIsEnumerable,i=Object.getOwnPropertyDescriptor,r=i&&!a.call({1:2},1);e.f=r?function(n){var e=i(this,n);return!!e&&e.enumerable}:a},function(n,e){e.f=Object.getOwnPropertySymbols},function(n,e,t){var a=t(9),i=t(15),r=t(70),o=t(141),s=r("IE_PROTO"),l=Object.prototype;n.exports=o?Object.getPrototypeOf:function(n){return n=i(n),a(n,s)?n[s]:"function"==typeof n.constructor&&n instanceof n.constructor?n.constructor.prototype:n instanceof Object?l:null}},function(n,e,t){var a=t(5),i=t(207);n.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var n,e=!1,t={};try{(n=Object.getOwnPropertyDescriptor(Object.prototype,"__proto__").set).call(t,[]),e=t instanceof Array}catch(n){}return function(t,r){return a(t),i(r),e?n.call(t,r):t.__proto__=r,t}}():void 0)},function(n,e,t){var a={};a[t(3)("toStringTag")]="z",n.exports="[object z]"===String(a)},function(n,e,t){var a=t(5),i=t(27),r=t(3)("species");n.exports=function(n,e){var t,o=a(n).constructor;return void 0===o||null==(t=a(o)[r])?e:i(t)}},function(n,e,t){var a=t(0),i=t(202);a({target:"Array",stat:!0,forced:!t(145)((function(n){Array.from(n)}))},{from:i})},function(n,e,t){"use strict";var a,i,r=t(123),o=t(124),s=t(67),l=RegExp.prototype.exec,c=s("native-string-replace",String.prototype.replace),p=l,d=(a=/a/,i=/b*/g,l.call(a,"a"),l.call(i,"a"),0!==a.lastIndex||0!==i.lastIndex),u=o.UNSUPPORTED_Y||o.BROKEN_CARET,m=void 0!==/()??/.exec("")[1];(d||m||u)&&(p=function(n){var e,t,a,i,o=this,s=u&&o.sticky,p=r.call(o),h=o.source,g=0,f=n;return s&&(-1===(p=p.replace("y","")).indexOf("g")&&(p+="g"),f=String(n).slice(o.lastIndex),o.lastIndex>0&&(!o.multiline||o.multiline&&"\n"!==n[o.lastIndex-1])&&(h="(?: "+h+")",f=" "+f,g++),t=new RegExp("^(?:"+h+")",p)),m&&(t=new RegExp("^"+h+"$(?!\\s)",p)),d&&(e=o.lastIndex),a=l.call(s?t:o,f),s?a?(a.input=a.input.slice(g),a[0]=a[0].slice(g),a.index=o.lastIndex,o.lastIndex+=a[0].length):o.lastIndex=0:d&&a&&(o.lastIndex=o.global?a.index+a[0].length:e),m&&a&&a.length>1&&c.call(a[0],t,(function(){for(i=1;i<arguments.length-2;i++)void 0===arguments[i]&&(a[i]=void 0)})),a}),n.exports=p},function(n,e,t){"use strict";var a=t(120).charAt;n.exports=function(n,e,t){return e+(t?a(n,e).length:1)}},function(n,e,t){var a=t(227),i=t(39),r=Object.prototype,o=r.hasOwnProperty,s=r.propertyIsEnumerable,l=a(function(){return arguments}())?a:function(n){return i(n)&&o.call(n,"callee")&&!s.call(n,"callee")};n.exports=l},function(n,e,t){var a=t(33)(t(24),"Map");n.exports=a},function(n,e){n.exports=function(n){var e=typeof n;return null!=n&&("object"==e||"function"==e)}},function(n,e,t){var a=t(247),i=t(254),r=t(256),o=t(257),s=t(258);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var a=n[e];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=i,l.prototype.get=r,l.prototype.has=o,l.prototype.set=s,n.exports=l},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n){t[++e]=n})),t}},function(n,e){n.exports=function(n){return"number"==typeof n&&n>-1&&n%1==0&&n<=9007199254740991}},function(n,e,t){var a=t(23),i=t(111),r=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,o=/^\w*$/;n.exports=function(n,e){if(a(n))return!1;var t=typeof n;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=n&&!i(n))||(o.test(n)||!r.test(n)||null!=e&&n in Object(e))}},function(n,e,t){var a=t(49),i=t(39);n.exports=function(n){return"symbol"==typeof n||i(n)&&"[object Symbol]"==a(n)}},function(n,e){n.exports=function(n){return n}},function(n,e,t){"use strict";var a=t(84),i=t(5),r=t(11),o=t(14),s=t(103),l=t(85);a("match",1,(function(n,e,t){return[function(e){var t=o(this),a=null==e?void 0:e[n];return void 0!==a?a.call(e,t):new RegExp(e)[n](String(t))},function(n){var a=t(e,n,this);if(a.done)return a.value;var o=i(n),c=String(this);if(!o.global)return l(o,c);var p=o.unicode;o.lastIndex=0;for(var d,u=[],m=0;null!==(d=l(o,c));){var h=String(d[0]);u[m]=h,""===h&&(o.lastIndex=s(c,r(o.lastIndex),p)),m++}return 0===m?null:u}]}))},function(n,e,t){var a=t(119),i=t(56),r=t(3)("iterator");n.exports=function(n){if(null!=n)return n[r]||n["@@iterator"]||i[a(n)]}},function(n,e,t){var a=function(n){"use strict";var e=Object.prototype,t=e.hasOwnProperty,a="function"==typeof Symbol?Symbol:{},i=a.iterator||"@@iterator",r=a.asyncIterator||"@@asyncIterator",o=a.toStringTag||"@@toStringTag";function s(n,e,t){return Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}),n[e]}try{s({},"")}catch(n){s=function(n,e,t){return n[e]=t}}function l(n,e,t,a){var i=e&&e.prototype instanceof d?e:d,r=Object.create(i.prototype),o=new x(a||[]);return r._invoke=function(n,e,t){var a="suspendedStart";return function(i,r){if("executing"===a)throw new Error("Generator is already running");if("completed"===a){if("throw"===i)throw r;return w()}for(t.method=i,t.arg=r;;){var o=t.delegate;if(o){var s=_(o,t);if(s){if(s===p)continue;return s}}if("next"===t.method)t.sent=t._sent=t.arg;else if("throw"===t.method){if("suspendedStart"===a)throw a="completed",t.arg;t.dispatchException(t.arg)}else"return"===t.method&&t.abrupt("return",t.arg);a="executing";var l=c(n,e,t);if("normal"===l.type){if(a=t.done?"completed":"suspendedYield",l.arg===p)continue;return{value:l.arg,done:t.done}}"throw"===l.type&&(a="completed",t.method="throw",t.arg=l.arg)}}}(n,t,o),r}function c(n,e,t){try{return{type:"normal",arg:n.call(e,t)}}catch(n){return{type:"throw",arg:n}}}n.wrap=l;var p={};function d(){}function u(){}function m(){}var h={};h[i]=function(){return this};var g=Object.getPrototypeOf,f=g&&g(g(k([])));f&&f!==e&&t.call(f,i)&&(h=f);var v=m.prototype=d.prototype=Object.create(h);function b(n){["next","throw","return"].forEach((function(e){s(n,e,(function(n){return this._invoke(e,n)}))}))}function y(n,e){var a;this._invoke=function(i,r){function o(){return new e((function(a,o){!function a(i,r,o,s){var l=c(n[i],n,r);if("throw"!==l.type){var p=l.arg,d=p.value;return d&&"object"==typeof d&&t.call(d,"__await")?e.resolve(d.__await).then((function(n){a("next",n,o,s)}),(function(n){a("throw",n,o,s)})):e.resolve(d).then((function(n){p.value=n,o(p)}),(function(n){return a("throw",n,o,s)}))}s(l.arg)}(i,r,a,o)}))}return a=a?a.then(o,o):o()}}function _(n,e){var t=n.iterator[e.method];if(void 0===t){if(e.delegate=null,"throw"===e.method){if(n.iterator.return&&(e.method="return",e.arg=void 0,_(n,e),"throw"===e.method))return p;e.method="throw",e.arg=new TypeError("The iterator does not provide a 'throw' method")}return p}var a=c(t,n.iterator,e.arg);if("throw"===a.type)return e.method="throw",e.arg=a.arg,e.delegate=null,p;var i=a.arg;return i?i.done?(e[n.resultName]=i.value,e.next=n.nextLoc,"return"!==e.method&&(e.method="next",e.arg=void 0),e.delegate=null,p):i:(e.method="throw",e.arg=new TypeError("iterator result is not an object"),e.delegate=null,p)}function E(n){var e={tryLoc:n[0]};1 in n&&(e.catchLoc=n[1]),2 in n&&(e.finallyLoc=n[2],e.afterLoc=n[3]),this.tryEntries.push(e)}function A(n){var e=n.completion||{};e.type="normal",delete e.arg,n.completion=e}function x(n){this.tryEntries=[{tryLoc:"root"}],n.forEach(E,this),this.reset(!0)}function k(n){if(n){var e=n[i];if(e)return e.call(n);if("function"==typeof n.next)return n;if(!isNaN(n.length)){var a=-1,r=function e(){for(;++a<n.length;)if(t.call(n,a))return e.value=n[a],e.done=!1,e;return e.value=void 0,e.done=!0,e};return r.next=r}}return{next:w}}function w(){return{value:void 0,done:!0}}return u.prototype=v.constructor=m,m.constructor=u,u.displayName=s(m,o,"GeneratorFunction"),n.isGeneratorFunction=function(n){var e="function"==typeof n&&n.constructor;return!!e&&(e===u||"GeneratorFunction"===(e.displayName||e.name))},n.mark=function(n){return Object.setPrototypeOf?Object.setPrototypeOf(n,m):(n.__proto__=m,s(n,o,"GeneratorFunction")),n.prototype=Object.create(v),n},n.awrap=function(n){return{__await:n}},b(y.prototype),y.prototype[r]=function(){return this},n.AsyncIterator=y,n.async=function(e,t,a,i,r){void 0===r&&(r=Promise);var o=new y(l(e,t,a,i),r);return n.isGeneratorFunction(t)?o:o.next().then((function(n){return n.done?n.value:o.next()}))},b(v),s(v,o,"Generator"),v[i]=function(){return this},v.toString=function(){return"[object Generator]"},n.keys=function(n){var e=[];for(var t in n)e.push(t);return e.reverse(),function t(){for(;e.length;){var a=e.pop();if(a in n)return t.value=a,t.done=!1,t}return t.done=!0,t}},n.values=k,x.prototype={constructor:x,reset:function(n){if(this.prev=0,this.next=0,this.sent=this._sent=void 0,this.done=!1,this.delegate=null,this.method="next",this.arg=void 0,this.tryEntries.forEach(A),!n)for(var e in this)"t"===e.charAt(0)&&t.call(this,e)&&!isNaN(+e.slice(1))&&(this[e]=void 0)},stop:function(){this.done=!0;var n=this.tryEntries[0].completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(n){if(this.done)throw n;var e=this;function a(t,a){return o.type="throw",o.arg=n,e.next=t,a&&(e.method="next",e.arg=void 0),!!a}for(var i=this.tryEntries.length-1;i>=0;--i){var r=this.tryEntries[i],o=r.completion;if("root"===r.tryLoc)return a("end");if(r.tryLoc<=this.prev){var s=t.call(r,"catchLoc"),l=t.call(r,"finallyLoc");if(s&&l){if(this.prev<r.catchLoc)return a(r.catchLoc,!0);if(this.prev<r.finallyLoc)return a(r.finallyLoc)}else if(s){if(this.prev<r.catchLoc)return a(r.catchLoc,!0)}else{if(!l)throw new Error("try statement without catch or finally");if(this.prev<r.finallyLoc)return a(r.finallyLoc)}}}},abrupt:function(n,e){for(var a=this.tryEntries.length-1;a>=0;--a){var i=this.tryEntries[a];if(i.tryLoc<=this.prev&&t.call(i,"finallyLoc")&&this.prev<i.finallyLoc){var r=i;break}}r&&("break"===n||"continue"===n)&&r.tryLoc<=e&&e<=r.finallyLoc&&(r=null);var o=r?r.completion:{};return o.type=n,o.arg=e,r?(this.method="next",this.next=r.finallyLoc,p):this.complete(o)},complete:function(n,e){if("throw"===n.type)throw n.arg;return"break"===n.type||"continue"===n.type?this.next=n.arg:"return"===n.type?(this.rval=this.arg=n.arg,this.method="return",this.next="end"):"normal"===n.type&&e&&(this.next=e),p},finish:function(n){for(var e=this.tryEntries.length-1;e>=0;--e){var t=this.tryEntries[e];if(t.finallyLoc===n)return this.complete(t.completion,t.afterLoc),A(t),p}},catch:function(n){for(var e=this.tryEntries.length-1;e>=0;--e){var t=this.tryEntries[e];if(t.tryLoc===n){var a=t.completion;if("throw"===a.type){var i=a.arg;A(t)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(n,e,t){return this.delegate={iterator:k(n),resultName:e,nextLoc:t},"next"===this.method&&(this.arg=void 0),p}},n}(n.exports);try{regeneratorRuntime=a}catch(n){Function("r","regeneratorRuntime = r")(a)}},function(n,e,t){"use strict";var a=t(13),i=t(5),r=t(1),o=t(123),s=RegExp.prototype,l=s.toString,c=r((function(){return"/a/b"!=l.call({source:"a",flags:"b"})})),p="toString"!=l.name;(c||p)&&a(RegExp.prototype,"toString",(function(){var n=i(this),e=String(n.source),t=n.flags;return"/"+e+"/"+String(void 0===t&&n instanceof RegExp&&!("flags"in s)?o.call(n):t)}),{unsafe:!0})},function(n,e,t){"use strict";var a=t(20),i=t(118),r=t(56),o=t(42),s=t(137),l=o.set,c=o.getterFor("Array Iterator");n.exports=s(Array,"Array",(function(n,e){l(this,{type:"Array Iterator",target:a(n),index:0,kind:e})}),(function(){var n=c(this),e=n.target,t=n.kind,a=n.index++;return!e||a>=e.length?(n.target=void 0,{value:void 0,done:!0}):"keys"==t?{value:a,done:!1}:"values"==t?{value:e[a],done:!1}:{value:[a,e[a]],done:!1}}),"values"),r.Arguments=r.Array,i("keys"),i("values"),i("entries")},function(n,e,t){var a=t(3),i=t(38),r=t(10),o=a("unscopables"),s=Array.prototype;null==s[o]&&r.f(s,o,{configurable:!0,value:i(null)}),n.exports=function(n){s[o][n]=!0}},function(n,e,t){var a=t(99),i=t(32),r=t(3)("toStringTag"),o="Arguments"==i(function(){return arguments}());n.exports=a?i:function(n){var e,t,a;return void 0===n?"Undefined":null===n?"Null":"string"==typeof(t=function(n,e){try{return n[e]}catch(n){}}(e=Object(n),r))?t:o?i(e):"Object"==(a=i(e))&&"function"==typeof e.callee?"Arguments":a}},function(n,e,t){var a=t(43),i=t(14),r=function(n){return function(e,t){var r,o,s=String(i(e)),l=a(t),c=s.length;return l<0||l>=c?n?"":void 0:(r=s.charCodeAt(l))<55296||r>56319||l+1===c||(o=s.charCodeAt(l+1))<56320||o>57343?n?s.charAt(l):r:n?s.slice(l,l+2):o-56320+(r-55296<<10)+65536}};n.exports={codeAt:r(!1),charAt:r(!0)}},function(n,e,t){var a=t(4),i=t(46),r=t(3)("species");n.exports=function(n,e){var t;return i(n)&&("function"!=typeof(t=n.constructor)||t!==Array&&!i(t.prototype)?a(t)&&null===(t=t[r])&&(t=void 0):t=void 0),new(void 0===t?Array:t)(0===e?0:e)}},function(n,e,t){var a=t(4),i=t(32),r=t(3)("match");n.exports=function(n){var e;return a(n)&&(void 0!==(e=n[r])?!!e:"RegExp"==i(n))}},function(n,e,t){"use strict";var a=t(5);n.exports=function(){var n=a(this),e="";return n.global&&(e+="g"),n.ignoreCase&&(e+="i"),n.multiline&&(e+="m"),n.dotAll&&(e+="s"),n.unicode&&(e+="u"),n.sticky&&(e+="y"),e}},function(n,e,t){"use strict";var a=t(1);function i(n,e){return RegExp(n,e)}e.UNSUPPORTED_Y=a((function(){var n=i("a","y");return n.lastIndex=2,null!=n.exec("abcd")})),e.BROKEN_CARET=a((function(){var n=i("^r","gy");return n.lastIndex=2,null!=n.exec("str")}))},function(n,e){n.exports=function(n){return n.webpackPolyfill||(n.deprecate=function(){},n.paths=[],n.children||(n.children=[]),Object.defineProperty(n,"loaded",{enumerable:!0,get:function(){return n.l}}),Object.defineProperty(n,"id",{enumerable:!0,get:function(){return n.i}}),n.webpackPolyfill=1),n}},function(n,e){var t=/^\s+|\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,i=/^0b[01]+$/i,r=/^0o[0-7]+$/i,o=parseInt,s="object"==typeof global&&global&&global.Object===Object&&global,l="object"==typeof self&&self&&self.Object===Object&&self,c=s||l||Function("return this")(),p=Object.prototype.toString,d=Math.max,u=Math.min,m=function(){return c.Date.now()};function h(n){var e=typeof n;return!!n&&("object"==e||"function"==e)}function g(n){if("number"==typeof n)return n;if(function(n){return"symbol"==typeof n||function(n){return!!n&&"object"==typeof n}(n)&&"[object Symbol]"==p.call(n)}(n))return NaN;if(h(n)){var e="function"==typeof n.valueOf?n.valueOf():n;n=h(e)?e+"":e}if("string"!=typeof n)return 0===n?n:+n;n=n.replace(t,"");var s=i.test(n);return s||r.test(n)?o(n.slice(2),s?2:8):a.test(n)?NaN:+n}n.exports=function(n,e,t){var a,i,r,o,s,l,c=0,p=!1,f=!1,v=!0;if("function"!=typeof n)throw new TypeError("Expected a function");function b(e){var t=a,r=i;return a=i=void 0,c=e,o=n.apply(r,t)}function y(n){return c=n,s=setTimeout(E,e),p?b(n):o}function _(n){var t=n-l;return void 0===l||t>=e||t<0||f&&n-c>=r}function E(){var n=m();if(_(n))return A(n);s=setTimeout(E,function(n){var t=e-(n-l);return f?u(t,r-(n-c)):t}(n))}function A(n){return s=void 0,v&&a?b(n):(a=i=void 0,o)}function x(){var n=m(),t=_(n);if(a=arguments,i=this,l=n,t){if(void 0===s)return y(l);if(f)return s=setTimeout(E,e),b(l)}return void 0===s&&(s=setTimeout(E,e)),o}return e=g(e)||0,h(t)&&(p=!!t.leading,r=(f="maxWait"in t)?d(g(t.maxWait)||0,e):r,v="trailing"in t?!!t.trailing:v),x.cancel=function(){void 0!==s&&clearTimeout(s),c=0,a=l=i=s=void 0},x.flush=function(){return void 0===s?o:A(m())},x}},function(n,e,t){"use strict";var a=t(0),i=t(128).trim;a({target:"String",proto:!0,forced:t(320)("trim")},{trim:function(){return i(this)}})},function(n,e,t){var a=t(14),i="["+t(129)+"]",r=RegExp("^"+i+i+"*"),o=RegExp(i+i+"*$"),s=function(n){return function(e){var t=String(a(e));return 1&n&&(t=t.replace(r,"")),2&n&&(t=t.replace(o,"")),t}};n.exports={start:s(1),end:s(2),trim:s(3)}},function(n,e){n.exports="\t\n\v\f\r                　\u2028\u2029\ufeff"},function(n,e,t){var a=t(13),i=Date.prototype,r=i.toString,o=i.getTime;new Date(NaN)+""!="Invalid Date"&&a(i,"toString",(function(){var n=o.call(this);return n==n?r.call(this):"Invalid Date"}))},function(n,e,t){var a=t(7),i=t(1),r=t(90);n.exports=!a&&!i((function(){return 7!=Object.defineProperty(r("div"),"a",{get:function(){return 7}}).a}))},function(n,e,t){var a=t(2);n.exports=a},function(n,e,t){var a=t(91);n.exports=a&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(n,e,t){var a=t(9),i=t(20),r=t(92).indexOf,o=t(55);n.exports=function(n,e){var t,s=i(n),l=0,c=[];for(t in s)!a(o,t)&&a(s,t)&&c.push(t);for(;e.length>l;)a(s,t=e[l++])&&(~r(c,t)||c.push(t));return c}},function(n,e,t){var a=t(43),i=Math.max,r=Math.min;n.exports=function(n,e){var t=a(n);return t<0?i(t+e,0):r(t,e)}},function(n,e,t){var a=t(26);n.exports=a("document","documentElement")},function(n,e,t){"use strict";var a=t(0),i=t(197),r=t(97),o=t(98),s=t(63),l=t(18),c=t(13),p=t(3),d=t(30),u=t(56),m=t(140),h=m.IteratorPrototype,g=m.BUGGY_SAFARI_ITERATORS,f=p("iterator"),v=function(){return this};n.exports=function(n,e,t,p,m,b,y){i(t,e,p);var _,E,A,x=function(n){if(n===m&&B)return B;if(!g&&n in C)return C[n];switch(n){case"keys":case"values":case"entries":return function(){return new t(this,n)}}return function(){return new t(this)}},k=e+" Iterator",w=!1,C=n.prototype,S=C[f]||C["@@iterator"]||m&&C[m],B=!g&&S||x(m),T="Array"==e&&C.entries||S;if(T&&(_=r(T.call(new n)),h!==Object.prototype&&_.next&&(d||r(_)===h||(o?o(_,h):"function"!=typeof _[f]&&l(_,f,v)),s(_,k,!0,!0),d&&(u[k]=v))),"values"==m&&S&&"values"!==S.name&&(w=!0,B=function(){return S.call(this)}),d&&!y||C[f]===B||l(C,f,B),u[e]=B,m)if(E={values:x("values"),keys:b?B:x("keys"),entries:x("entries")},y)for(A in E)(g||w||!(A in C))&&c(C,A,E[A]);else a({target:e,proto:!0,forced:g||w},E);return E}},function(n,e,t){var a=t(9),i=t(139),r=t(31),o=t(10);n.exports=function(n,e){for(var t=i(e),s=o.f,l=r.f,c=0;c<t.length;c++){var p=t[c];a(n,p)||s(n,p,l(e,p))}}},function(n,e,t){var a=t(26),i=t(65),r=t(96),o=t(5);n.exports=a("Reflect","ownKeys")||function(n){var e=i.f(o(n)),t=r.f;return t?e.concat(t(n)):e}},function(n,e,t){"use strict";var a,i,r,o=t(1),s=t(97),l=t(18),c=t(9),p=t(3),d=t(30),u=p("iterator"),m=!1;[].keys&&("next"in(r=[].keys())?(i=s(s(r)))!==Object.prototype&&(a=i):m=!0);var h=null==a||o((function(){var n={};return a[u].call(n)!==n}));h&&(a={}),d&&!h||c(a,u)||l(a,u,(function(){return this})),n.exports={IteratorPrototype:a,BUGGY_SAFARI_ITERATORS:m}},function(n,e,t){var a=t(1);n.exports=!a((function(){function n(){}return n.prototype.constructor=null,Object.getPrototypeOf(new n)!==n.prototype}))},function(n,e,t){var a=t(2);n.exports=a.Promise},function(n,e,t){var a=t(3),i=t(56),r=a("iterator"),o=Array.prototype;n.exports=function(n){return void 0!==n&&(i.Array===n||o[r]===n)}},function(n,e,t){var a=t(5);n.exports=function(n){var e=n.return;if(void 0!==e)return a(e.call(n)).value}},function(n,e,t){var a=t(3)("iterator"),i=!1;try{var r=0,o={next:function(){return{done:!!r++}},return:function(){i=!0}};o[a]=function(){return this},Array.from(o,(function(){throw 2}))}catch(n){}n.exports=function(n,e){if(!e&&!i)return!1;var t=!1;try{var r={};r[a]=function(){return{next:function(){return{done:t=!0}}}},n(r)}catch(n){}return t}},function(n,e,t){var a,i,r,o=t(2),s=t(1),l=t(64),c=t(136),p=t(90),d=t(147),u=t(52),m=o.location,h=o.setImmediate,g=o.clearImmediate,f=o.process,v=o.MessageChannel,b=o.Dispatch,y=0,_={},E=function(n){if(_.hasOwnProperty(n)){var e=_[n];delete _[n],e()}},A=function(n){return function(){E(n)}},x=function(n){E(n.data)},k=function(n){o.postMessage(n+"",m.protocol+"//"+m.host)};h&&g||(h=function(n){for(var e=[],t=1;arguments.length>t;)e.push(arguments[t++]);return _[++y]=function(){("function"==typeof n?n:Function(n)).apply(void 0,e)},a(y),y},g=function(n){delete _[n]},u?a=function(n){f.nextTick(A(n))}:b&&b.now?a=function(n){b.now(A(n))}:v&&!d?(r=(i=new v).port2,i.port1.onmessage=x,a=l(r.postMessage,r,1)):o.addEventListener&&"function"==typeof postMessage&&!o.importScripts&&m&&"file:"!==m.protocol&&!s(k)?(a=k,o.addEventListener("message",x,!1)):a="onreadystatechange"in p("script")?function(n){c.appendChild(p("script")).onreadystatechange=function(){c.removeChild(this),E(n)}}:function(n){setTimeout(A(n),0)}),n.exports={set:h,clear:g}},function(n,e,t){var a=t(54);n.exports=/(?:iphone|ipod|ipad).*applewebkit/i.test(a)},function(n,e,t){var a=t(5),i=t(4),r=t(149);n.exports=function(n,e){if(a(n),i(e)&&e.constructor===n)return e;var t=r.f(n);return(0,t.resolve)(e),t.promise}},function(n,e,t){"use strict";var a=t(27),i=function(n){var e,t;this.promise=new n((function(n,a){if(void 0!==e||void 0!==t)throw TypeError("Bad Promise constructor");e=n,t=a})),this.resolve=a(e),this.reject=a(t)};n.exports.f=function(n){return new i(n)}},function(n,e){n.exports={CSSRuleList:0,CSSStyleDeclaration:0,CSSValueList:0,ClientRectList:0,DOMRectList:0,DOMStringList:0,DOMTokenList:1,DataTransferItemList:0,FileList:0,HTMLAllCollection:0,HTMLCollection:0,HTMLFormElement:0,HTMLSelectElement:0,MediaList:0,MimeTypeArray:0,NamedNodeMap:0,NodeList:1,PaintRequestList:0,Plugin:0,PluginArray:0,SVGLengthList:0,SVGNumberList:0,SVGPathSegList:0,SVGPointList:0,SVGStringList:0,SVGTransformList:0,SourceBufferList:0,StyleSheetList:0,TextTrackCueList:0,TextTrackList:0,TouchList:0}},function(n,e,t){var a=t(0),i=t(1),r=t(15),o=t(97),s=t(141);a({target:"Object",stat:!0,forced:i((function(){o(1)})),sham:!s},{getPrototypeOf:function(n){return o(r(n))}})},function(n,e,t){var a=t(122);n.exports=function(n){if(a(n))throw TypeError("The method doesn't accept regular expressions");return n}},function(n,e,t){var a=t(3)("match");n.exports=function(n){var e=/./;try{"/./"[n](e)}catch(t){try{return e[a]=!1,"/./"[n](e)}catch(n){}}return!1}},function(n,e,t){"use strict";var a=t(44).forEach,i=t(37)("forEach");n.exports=i?[].forEach:function(n){return a(this,n,arguments.length>1?arguments[1]:void 0)}},function(n,e,t){var a=t(1);n.exports=!a((function(){return Object.isExtensible(Object.preventExtensions({}))}))},function(n,e,t){var a=t(3);e.f=a},function(n,e,t){var a=t(132),i=t(9),r=t(156),o=t(10).f;n.exports=function(n){var e=a.Symbol||(a.Symbol={});i(e,n)||o(e,n,{value:r.f(n)})}},function(n,e,t){t(0)({target:"Object",stat:!0,sham:!t(7)},{create:t(38)})},function(n,e){n.exports=function(n,e){for(var t=-1,a=e.length,i=n.length;++t<a;)n[i+t]=e[t];return n}},function(n,e){var t="object"==typeof global&&global&&global.Object===Object&&global;n.exports=t},function(n,e,t){var a=t(75),i=t(238),r=t(239),o=t(240),s=t(241),l=t(242);function c(n){var e=this.__data__=new a(n);this.size=e.size}c.prototype.clear=i,c.prototype.delete=r,c.prototype.get=o,c.prototype.has=s,c.prototype.set=l,n.exports=c},function(n,e){n.exports=function(n,e){return n===e||n!=n&&e!=e}},function(n,e,t){var a=t(49),i=t(106);n.exports=function(n){if(!i(n))return!1;var e=a(n);return"[object Function]"==e||"[object GeneratorFunction]"==e||"[object AsyncFunction]"==e||"[object Proxy]"==e}},function(n,e){var t=Function.prototype.toString;n.exports=function(n){if(null!=n){try{return t.call(n)}catch(n){}try{return n+""}catch(n){}}return""}},function(n,e,t){var a=t(259),i=t(39);n.exports=function n(e,t,r,o,s){return e===t||(null==e||null==t||!i(e)&&!i(t)?e!=e&&t!=t:a(e,t,r,o,n,s))}},function(n,e,t){var a=t(167),i=t(262),r=t(168);n.exports=function(n,e,t,o,s,l){var c=1&t,p=n.length,d=e.length;if(p!=d&&!(c&&d>p))return!1;var u=l.get(n),m=l.get(e);if(u&&m)return u==e&&m==n;var h=-1,g=!0,f=2&t?new a:void 0;for(l.set(n,e),l.set(e,n);++h<p;){var v=n[h],b=e[h];if(o)var y=c?o(b,v,h,e,n,l):o(v,b,h,n,e,l);if(void 0!==y){if(y)continue;g=!1;break}if(f){if(!i(e,(function(n,e){if(!r(f,e)&&(v===n||s(v,n,t,o,l)))return f.push(e)}))){g=!1;break}}else if(v!==b&&!s(v,b,t,o,l)){g=!1;break}}return l.delete(n),l.delete(e),g}},function(n,e,t){var a=t(107),i=t(260),r=t(261);function o(n){var e=-1,t=null==n?0:n.length;for(this.__data__=new a;++e<t;)this.add(n[e])}o.prototype.add=o.prototype.push=i,o.prototype.has=r,n.exports=o},function(n,e){n.exports=function(n,e){return n.has(e)}},function(n,e,t){var a=t(272),i=t(278),r=t(173);n.exports=function(n){return r(n)?a(n):i(n)}},function(n,e,t){(function(n){var a=t(24),i=t(274),r=e&&!e.nodeType&&e,o=r&&"object"==typeof n&&n&&!n.nodeType&&n,s=o&&o.exports===r?a.Buffer:void 0,l=(s?s.isBuffer:void 0)||i;n.exports=l}).call(this,t(125)(n))},function(n,e){var t=/^(?:0|[1-9]\d*)$/;n.exports=function(n,e){var a=typeof n;return!!(e=null==e?9007199254740991:e)&&("number"==a||"symbol"!=a&&t.test(n))&&n>-1&&n%1==0&&n<e}},function(n,e,t){var a=t(275),i=t(276),r=t(277),o=r&&r.isTypedArray,s=o?i(o):a;n.exports=s},function(n,e,t){var a=t(163),i=t(109);n.exports=function(n){return null!=n&&i(n.length)&&!a(n)}},function(n,e,t){var a=t(33)(t(24),"Set");n.exports=a},function(n,e,t){var a=t(106);n.exports=function(n){return n==n&&!a(n)}},function(n,e){n.exports=function(n,e){return function(t){return null!=t&&(t[n]===e&&(void 0!==e||n in Object(t)))}}},function(n,e,t){var a=t(178),i=t(79);n.exports=function(n,e){for(var t=0,r=(e=a(e,n)).length;null!=n&&t<r;)n=n[i(e[t++])];return t&&t==r?n:void 0}},function(n,e,t){var a=t(23),i=t(110),r=t(289),o=t(292);n.exports=function(n,e){return a(n)?n:i(n,e)?[n]:r(o(n))}},function(n,e,t){t(0)({target:"Function",proto:!0},{bind:t(180)})},function(n,e,t){"use strict";var a=t(27),i=t(4),r=[].slice,o={},s=function(n,e,t){if(!(e in o)){for(var a=[],i=0;i<e;i++)a[i]="a["+i+"]";o[e]=Function("C,a","return new C("+a.join(",")+")")}return o[e](n,t)};n.exports=Function.bind||function(n){var e=a(this),t=r.call(arguments,1),o=function(){var a=t.concat(r.call(arguments));return this instanceof o?s(e,a.length,a):e.apply(n,a)};return i(e.prototype)&&(o.prototype=e.prototype),o}},function(n,e,t){"use strict";var a=t(0),i=t(323).start;a({target:"String",proto:!0,forced:t(325)},{padStart:function(n){return i(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){},function(n,e,t){},function(n,e,t){t(0)({target:"Object",stat:!0},{setPrototypeOf:t(98)})},function(n,e,t){var a=t(0),i=t(26),r=t(27),o=t(5),s=t(4),l=t(38),c=t(180),p=t(1),d=i("Reflect","construct"),u=p((function(){function n(){}return!(d((function(){}),[],n)instanceof n)})),m=!p((function(){d((function(){}))})),h=u||m;a({target:"Reflect",stat:!0,forced:h,sham:h},{construct:function(n,e){r(n),o(e);var t=arguments.length<3?n:r(arguments[2]);if(m&&!u)return d(n,e,t);if(n==t){switch(e.length){case 0:return new n;case 1:return new n(e[0]);case 2:return new n(e[0],e[1]);case 3:return new n(e[0],e[1],e[2]);case 4:return new n(e[0],e[1],e[2],e[3])}var a=[null];return a.push.apply(a,e),new(c.apply(n,a))}var i=t.prototype,p=l(s(i)?i:Object.prototype),h=Function.apply.call(n,p,e);return s(h)?h:p}})},function(n,e,t){},function(n,e,t){},function(n,e,t){var a=t(225),i=t(230),r=t(301),o=t(309),s=t(318),l=t(203),c=r((function(n){var e=l(n);return s(e)&&(e=void 0),o(a(n,1,s,!0),i(e,2))}));n.exports=c},function(n,e,t){function a(e){return"function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?(n.exports=a=function(n){return typeof n},n.exports.default=n.exports,n.exports.__esModule=!0):(n.exports=a=function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n},n.exports.default=n.exports,n.exports.__esModule=!0),a(e)}t(47),t(57),t(8),t(74),t(12),t(17),n.exports=a,n.exports.default=n.exports,n.exports.__esModule=!0},function(n,e,t){"use strict";t.r(e);var a={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},i=(t(326),t(25)),r=Object(i.a)(a,(function(){var n=this.$createElement;return(this._self._c||n)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"4f1e9d0c",null);e.default=r.exports},function(n,e,t){"use strict";t.r(e);t(21),t(22),t(28),t(16);var a={name:"CodeGroup",data:function(){return{codeTabs:[],activeCodeTabIndex:-1}},watch:{activeCodeTabIndex:function(n){this.codeTabs.forEach((function(n){n.elm.classList.remove("theme-code-block__active")})),this.codeTabs[n].elm.classList.add("theme-code-block__active")}},mounted:function(){var n=this;this.codeTabs=(this.$slots.default||[]).filter((function(n){return Boolean(n.componentOptions)})).map((function(e,t){return""===e.componentOptions.propsData.active&&(n.activeCodeTabIndex=t),{title:e.componentOptions.propsData.title,elm:e.elm}})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab:function(n){this.activeCodeTabIndex=n}}},i=(t(327),t(25)),r=Object(i.a)(a,(function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"theme-code-group"},[t("div",{staticClass:"theme-code-group__nav"},[t("ul",{staticClass:"theme-code-group__ul"},n._l(n.codeTabs,(function(e,a){return t("li",{key:e.title,staticClass:"theme-code-group__li"},[t("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":a===n.activeCodeTabIndex},on:{click:function(e){return n.changeCodeTab(a)}}},[n._v("\n            "+n._s(e.title)+"\n          ")])])})),0)]),n._v(" "),n._t("default"),n._v(" "),n.codeTabs.length<1?t("pre",{staticClass:"pre-blank"},[n._v("// Make sure to add code blocks to your code group")]):n._e()],2)}),[],!1,null,"2f5f1757",null);e.default=r.exports},function(n,e,t){"use strict";var a=t(7),i=t(2),r=t(82),o=t(13),s=t(9),l=t(32),c=t(204),p=t(51),d=t(1),u=t(38),m=t(65).f,h=t(31).f,g=t(10).f,f=t(128).trim,v=i.Number,b=v.prototype,y="Number"==l(u(b)),_=function(n){var e,t,a,i,r,o,s,l,c=p(n,!1);if("string"==typeof c&&c.length>2)if(43===(e=(c=f(c)).charCodeAt(0))||45===e){if(88===(t=c.charCodeAt(2))||120===t)return NaN}else if(48===e){switch(c.charCodeAt(1)){case 66:case 98:a=2,i=49;break;case 79:case 111:a=8,i=55;break;default:return+c}for(o=(r=c.slice(2)).length,s=0;s<o;s++)if((l=r.charCodeAt(s))<48||l>i)return NaN;return parseInt(r,a)}return+c};if(r("Number",!v(" 0o1")||!v("0b1")||v("+0x1"))){for(var E,A=function(n){var e=arguments.length<1?0:n,t=this;return t instanceof A&&(y?d((function(){b.valueOf.call(t)})):"Number"!=l(t))?c(new v(_(e)),t,A):_(e)},x=a?m(v):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger,fromString,range".split(","),k=0;x.length>k;k++)s(v,E=x[k])&&!s(A,E)&&g(A,E,h(v,E));A.prototype=b,b.constructor=A,o(i,"Number",A)}},function(n,e,t){"use strict";var a=t(0),i=t(44).some;a({target:"Array",proto:!0,forced:!t(37)("some")},{some:function(n){return i(this,n,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){var a=t(7),i=t(10),r=t(5),o=t(69);n.exports=a?Object.defineProperties:function(n,e){r(n);for(var t,a=o(e),s=a.length,l=0;s>l;)i.f(n,t=a[l++],e[t]);return n}},function(n,e){n.exports=function(n,e,t){if(!(n instanceof e))throw TypeError("Incorrect "+(t?t+" ":"")+"invocation");return n}},function(n,e,t){var a=t(0),i=t(7);a({target:"Object",stat:!0,forced:!i,sham:!i},{defineProperty:t(10).f})},function(n,e,t){"use strict";var a=t(140).IteratorPrototype,i=t(38),r=t(45),o=t(63),s=t(56),l=function(){return this};n.exports=function(n,e,t){var c=e+" Iterator";return n.prototype=i(a,{next:r(1,t)}),o(n,c,!1,!0),s[c]=l,n}},function(n,e,t){var a=t(13);n.exports=function(n,e,t){for(var i in e)a(n,i,e[i],t);return n}},function(n,e,t){"use strict";var a=t(26),i=t(10),r=t(3),o=t(7),s=r("species");n.exports=function(n){var e=a(n),t=i.f;o&&e&&!e[s]&&t(e,s,{configurable:!0,get:function(){return this}})}},function(n,e,t){"use strict";var a=t(7),i=t(1),r=t(69),o=t(96),s=t(95),l=t(15),c=t(50),p=Object.assign,d=Object.defineProperty;n.exports=!p||i((function(){if(a&&1!==p({b:1},p(d({},"a",{enumerable:!0,get:function(){d(this,"b",{value:3,enumerable:!1})}}),{b:2})).b)return!0;var n={},e={},t=Symbol();return n[t]=7,"abcdefghijklmnopqrst".split("").forEach((function(n){e[n]=n})),7!=p({},n)[t]||"abcdefghijklmnopqrst"!=r(p({},e)).join("")}))?function(n,e){for(var t=l(n),i=arguments.length,p=1,d=o.f,u=s.f;i>p;)for(var m,h=c(arguments[p++]),g=d?r(h).concat(d(h)):r(h),f=g.length,v=0;f>v;)m=g[v++],a&&!u.call(h,m)||(t[m]=h[m]);return t}:p},function(n,e,t){var a=t(0),i=t(7),r=t(139),o=t(20),s=t(31),l=t(72);a({target:"Object",stat:!0,sham:!i},{getOwnPropertyDescriptors:function(n){for(var e,t,a=o(n),i=s.f,c=r(a),p={},d=0;c.length>d;)void 0!==(t=i(a,e=c[d++]))&&l(p,e,t);return p}})},function(n,e,t){"use strict";var a=t(64),i=t(15),r=t(223),o=t(143),s=t(11),l=t(72),c=t(114);n.exports=function(n){var e,t,p,d,u,m,h=i(n),g="function"==typeof this?this:Array,f=arguments.length,v=f>1?arguments[1]:void 0,b=void 0!==v,y=c(h),_=0;if(b&&(v=a(v,f>2?arguments[2]:void 0,2)),null==y||g==Array&&o(y))for(t=new g(e=s(h.length));e>_;_++)m=b?v(h[_],_):h[_],l(t,_,m);else for(u=(d=y.call(h)).next,t=new g;!(p=u.call(d)).done;_++)m=b?r(d,v,[p.value,_],!0):p.value,l(t,_,m);return t.length=_,t}},function(n,e){n.exports=function(n){var e=null==n?0:n.length;return e?n[e-1]:void 0}},function(n,e,t){var a=t(4),i=t(98);n.exports=function(n,e,t){var r,o;return i&&"function"==typeof(r=e.constructor)&&r!==t&&a(o=r.prototype)&&o!==t.prototype&&i(n,o),n}},function(n,e,t){n.exports=t(334)},function(n,e,t){var a=t(2),i=t(94),r=a.WeakMap;n.exports="function"==typeof r&&/native code/.test(i(r))},function(n,e,t){var a=t(4);n.exports=function(n){if(!a(n)&&null!==n)throw TypeError("Can't set "+String(n)+" as a prototype");return n}},function(n,e,t){"use strict";var a,i,r,o,s=t(0),l=t(30),c=t(2),p=t(26),d=t(142),u=t(13),m=t(198),h=t(63),g=t(199),f=t(4),v=t(27),b=t(195),y=t(94),_=t(209),E=t(145),A=t(100),x=t(146).set,k=t(210),w=t(148),C=t(212),S=t(149),B=t(213),T=t(42),P=t(82),D=t(3),z=t(52),L=t(53),I=D("species"),N="Promise",F=T.get,j=T.set,M=T.getterFor(N),R=d,O=c.TypeError,U=c.document,V=c.process,G=p("fetch"),W=S.f,H=W,q=!!(U&&U.createEvent&&c.dispatchEvent),$="function"==typeof PromiseRejectionEvent,X=P(N,(function(){if(!(y(R)!==String(R))){if(66===L)return!0;if(!z&&!$)return!0}if(l&&!R.prototype.finally)return!0;if(L>=51&&/native code/.test(R))return!1;var n=R.resolve(1),e=function(n){n((function(){}),(function(){}))};return(n.constructor={})[I]=e,!(n.then((function(){}))instanceof e)})),Z=X||!E((function(n){R.all(n).catch((function(){}))})),K=function(n){var e;return!(!f(n)||"function"!=typeof(e=n.then))&&e},Y=function(n,e){if(!n.notified){n.notified=!0;var t=n.reactions;k((function(){for(var a=n.value,i=1==n.state,r=0;t.length>r;){var o,s,l,c=t[r++],p=i?c.ok:c.fail,d=c.resolve,u=c.reject,m=c.domain;try{p?(i||(2===n.rejection&&en(n),n.rejection=1),!0===p?o=a:(m&&m.enter(),o=p(a),m&&(m.exit(),l=!0)),o===c.promise?u(O("Promise-chain cycle")):(s=K(o))?s.call(o,d,u):d(o)):u(a)}catch(n){m&&!l&&m.exit(),u(n)}}n.reactions=[],n.notified=!1,e&&!n.rejection&&Q(n)}))}},J=function(n,e,t){var a,i;q?((a=U.createEvent("Event")).promise=e,a.reason=t,a.initEvent(n,!1,!0),c.dispatchEvent(a)):a={promise:e,reason:t},!$&&(i=c["on"+n])?i(a):"unhandledrejection"===n&&C("Unhandled promise rejection",t)},Q=function(n){x.call(c,(function(){var e,t=n.facade,a=n.value;if(nn(n)&&(e=B((function(){z?V.emit("unhandledRejection",a,t):J("unhandledrejection",t,a)})),n.rejection=z||nn(n)?2:1,e.error))throw e.value}))},nn=function(n){return 1!==n.rejection&&!n.parent},en=function(n){x.call(c,(function(){var e=n.facade;z?V.emit("rejectionHandled",e):J("rejectionhandled",e,n.value)}))},tn=function(n,e,t){return function(a){n(e,a,t)}},an=function(n,e,t){n.done||(n.done=!0,t&&(n=t),n.value=e,n.state=2,Y(n,!0))},rn=function(n,e,t){if(!n.done){n.done=!0,t&&(n=t);try{if(n.facade===e)throw O("Promise can't be resolved itself");var a=K(e);a?k((function(){var t={done:!1};try{a.call(e,tn(rn,t,n),tn(an,t,n))}catch(e){an(t,e,n)}})):(n.value=e,n.state=1,Y(n,!1))}catch(e){an({done:!1},e,n)}}};X&&(R=function(n){b(this,R,N),v(n),a.call(this);var e=F(this);try{n(tn(rn,e),tn(an,e))}catch(n){an(e,n)}},(a=function(n){j(this,{type:N,done:!1,notified:!1,parent:!1,reactions:[],rejection:!1,state:0,value:void 0})}).prototype=m(R.prototype,{then:function(n,e){var t=M(this),a=W(A(this,R));return a.ok="function"!=typeof n||n,a.fail="function"==typeof e&&e,a.domain=z?V.domain:void 0,t.parent=!0,t.reactions.push(a),0!=t.state&&Y(t,!1),a.promise},catch:function(n){return this.then(void 0,n)}}),i=function(){var n=new a,e=F(n);this.promise=n,this.resolve=tn(rn,e),this.reject=tn(an,e)},S.f=W=function(n){return n===R||n===r?new i(n):H(n)},l||"function"!=typeof d||(o=d.prototype.then,u(d.prototype,"then",(function(n,e){var t=this;return new R((function(n,e){o.call(t,n,e)})).then(n,e)}),{unsafe:!0}),"function"==typeof G&&s({global:!0,enumerable:!0,forced:!0},{fetch:function(n){return w(R,G.apply(c,arguments))}}))),s({global:!0,wrap:!0,forced:X},{Promise:R}),h(R,N,!1,!0),g(N),r=p(N),s({target:N,stat:!0,forced:X},{reject:function(n){var e=W(this);return e.reject.call(void 0,n),e.promise}}),s({target:N,stat:!0,forced:l||X},{resolve:function(n){return w(l&&this===r?R:this,n)}}),s({target:N,stat:!0,forced:Z},{all:function(n){var e=this,t=W(e),a=t.resolve,i=t.reject,r=B((function(){var t=v(e.resolve),r=[],o=0,s=1;_(n,(function(n){var l=o++,c=!1;r.push(void 0),s++,t.call(e,n).then((function(n){c||(c=!0,r[l]=n,--s||a(r))}),i)})),--s||a(r)}));return r.error&&i(r.value),t.promise},race:function(n){var e=this,t=W(e),a=t.reject,i=B((function(){var i=v(e.resolve);_(n,(function(n){i.call(e,n).then(t.resolve,a)}))}));return i.error&&a(i.value),t.promise}})},function(n,e,t){var a=t(5),i=t(143),r=t(11),o=t(64),s=t(114),l=t(144),c=function(n,e){this.stopped=n,this.result=e};n.exports=function(n,e,t){var p,d,u,m,h,g,f,v=t&&t.that,b=!(!t||!t.AS_ENTRIES),y=!(!t||!t.IS_ITERATOR),_=!(!t||!t.INTERRUPTED),E=o(e,v,1+b+_),A=function(n){return p&&l(p),new c(!0,n)},x=function(n){return b?(a(n),_?E(n[0],n[1],A):E(n[0],n[1])):_?E(n,A):E(n)};if(y)p=n;else{if("function"!=typeof(d=s(n)))throw TypeError("Target is not iterable");if(i(d)){for(u=0,m=r(n.length);m>u;u++)if((h=x(n[u]))&&h instanceof c)return h;return new c(!1)}p=d.call(n)}for(g=p.next;!(f=g.call(p)).done;){try{h=x(f.value)}catch(n){throw l(p),n}if("object"==typeof h&&h&&h instanceof c)return h}return new c(!1)}},function(n,e,t){var a,i,r,o,s,l,c,p,d=t(2),u=t(31).f,m=t(146).set,h=t(147),g=t(211),f=t(52),v=d.MutationObserver||d.WebKitMutationObserver,b=d.document,y=d.process,_=d.Promise,E=u(d,"queueMicrotask"),A=E&&E.value;A||(a=function(){var n,e;for(f&&(n=y.domain)&&n.exit();i;){e=i.fn,i=i.next;try{e()}catch(n){throw i?o():r=void 0,n}}r=void 0,n&&n.enter()},h||f||g||!v||!b?_&&_.resolve?(c=_.resolve(void 0),p=c.then,o=function(){p.call(c,a)}):o=f?function(){y.nextTick(a)}:function(){m.call(d,a)}:(s=!0,l=b.createTextNode(""),new v(a).observe(l,{characterData:!0}),o=function(){l.data=s=!s})),n.exports=A||function(n){var e={fn:n,next:void 0};r&&(r.next=e),i||(i=e,o()),r=e}},function(n,e,t){var a=t(54);n.exports=/web0s(?!.*chrome)/i.test(a)},function(n,e,t){var a=t(2);n.exports=function(n,e){var t=a.console;t&&t.error&&(1===arguments.length?t.error(n):t.error(n,e))}},function(n,e){n.exports=function(n){try{return{error:!1,value:n()}}catch(n){return{error:!0,value:n}}}},function(n,e,t){var a=t(0),i=t(200);a({target:"Object",stat:!0,forced:Object.assign!==i},{assign:i})},function(n,e,t){"use strict";var a=t(0),i=t(30),r=t(142),o=t(1),s=t(26),l=t(100),c=t(148),p=t(13);a({target:"Promise",proto:!0,real:!0,forced:!!r&&o((function(){r.prototype.finally.call({then:function(){}},(function(){}))}))},{finally:function(n){var e=l(this,s("Promise")),t="function"==typeof n;return this.then(t?function(t){return c(e,n()).then((function(){return t}))}:n,t?function(t){return c(e,n()).then((function(){throw t}))}:n)}}),i||"function"!=typeof r||r.prototype.finally||p(r.prototype,"finally",s("Promise").prototype.finally)},function(n,e,t){"use strict";var a=t(99),i=t(119);n.exports=a?{}.toString:function(){return"[object "+i(this)+"]"}},function(n,e,t){"use strict";var a=t(0),i=t(218).left,r=t(37),o=t(53),s=t(52);a({target:"Array",proto:!0,forced:!r("reduce")||!s&&o>79&&o<83},{reduce:function(n){return i(this,n,arguments.length,arguments.length>1?arguments[1]:void 0)}})},function(n,e,t){var a=t(27),i=t(15),r=t(50),o=t(11),s=function(n){return function(e,t,s,l){a(t);var c=i(e),p=r(c),d=o(c.length),u=n?d-1:0,m=n?-1:1;if(s<2)for(;;){if(u in p){l=p[u],u+=m;break}if(u+=m,n?u<0:d<=u)throw TypeError("Reduce of empty array with no initial value")}for(;n?u>=0:d>u;u+=m)u in p&&(l=t(l,p[u],u,c));return l}};n.exports={left:s(!1),right:s(!0)}},function(n,e,t){"use strict";var a,i=t(0),r=t(31).f,o=t(11),s=t(152),l=t(14),c=t(153),p=t(30),d="".startsWith,u=Math.min,m=c("startsWith");i({target:"String",proto:!0,forced:!!(p||m||(a=r(String.prototype,"startsWith"),!a||a.writable))&&!m},{startsWith:function(n){var e=String(l(this));s(n);var t=o(u(arguments.length>1?arguments[1]:void 0,e.length)),a=String(n);return d?d.call(e,a,t):e.slice(t,t+a.length)===a}})},function(n,e,t){var a=t(0),i=t(155),r=t(1),o=t(4),s=t(221).onFreeze,l=Object.freeze;a({target:"Object",stat:!0,forced:r((function(){l(1)})),sham:!i},{freeze:function(n){return l&&o(n)?l(s(n)):n}})},function(n,e,t){var a=t(55),i=t(4),r=t(9),o=t(10).f,s=t(68),l=t(155),c=s("meta"),p=0,d=Object.isExtensible||function(){return!0},u=function(n){o(n,c,{value:{objectID:"O"+ ++p,weakData:{}}})},m=n.exports={REQUIRED:!1,fastKey:function(n,e){if(!i(n))return"symbol"==typeof n?n:("string"==typeof n?"S":"P")+n;if(!r(n,c)){if(!d(n))return"F";if(!e)return"E";u(n)}return n[c].objectID},getWeakData:function(n,e){if(!r(n,c)){if(!d(n))return!0;if(!e)return!1;u(n)}return n[c].weakData},onFreeze:function(n){return l&&m.REQUIRED&&d(n)&&!r(n,c)&&u(n),n}};a[c]=!0},function(n,e,t){var a=t(20),i=t(65).f,r={}.toString,o="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];n.exports.f=function(n){return o&&"[object Window]"==r.call(n)?function(n){try{return i(n)}catch(n){return o.slice()}}(n):i(a(n))}},function(n,e,t){var a=t(5),i=t(144);n.exports=function(n,e,t,r){try{return r?e(a(t)[0],t[1]):e(t)}catch(e){throw i(n),e}}},function(n,e,t){var a=t(15),i=Math.floor,r="".replace,o=/\$([$&'`]|\d{1,2}|<[^>]*>)/g,s=/\$([$&'`]|\d{1,2})/g;n.exports=function(n,e,t,l,c,p){var d=t+n.length,u=l.length,m=s;return void 0!==c&&(c=a(c),m=o),r.call(p,m,(function(a,r){var o;switch(r.charAt(0)){case"$":return"$";case"&":return n;case"`":return e.slice(0,t);case"'":return e.slice(d);case"<":o=c[r.slice(1,-1)];break;default:var s=+r;if(0===s)return a;if(s>u){var p=i(s/10);return 0===p?a:p<=u?void 0===l[p-1]?r.charAt(1):l[p-1]+r.charAt(1):a}o=l[s-1]}return void 0===o?"":o}))}},function(n,e,t){var a=t(159),i=t(226);n.exports=function n(e,t,r,o,s){var l=-1,c=e.length;for(r||(r=i),s||(s=[]);++l<c;){var p=e[l];t>0&&r(p)?t>1?n(p,t-1,r,o,s):a(s,p):o||(s[s.length]=p)}return s}},function(n,e,t){var a=t(58),i=t(104),r=t(23),o=a?a.isConcatSpreadable:void 0;n.exports=function(n){return r(n)||i(n)||!!(o&&n&&n[o])}},function(n,e,t){var a=t(49),i=t(39);n.exports=function(n){return i(n)&&"[object Arguments]"==a(n)}},function(n,e,t){var a=t(58),i=Object.prototype,r=i.hasOwnProperty,o=i.toString,s=a?a.toStringTag:void 0;n.exports=function(n){var e=r.call(n,s),t=n[s];try{n[s]=void 0;var a=!0}catch(n){}var i=o.call(n);return a&&(e?n[s]=t:delete n[s]),i}},function(n,e){var t=Object.prototype.toString;n.exports=function(n){return t.call(n)}},function(n,e,t){var a=t(231),i=t(287),r=t(112),o=t(23),s=t(298);n.exports=function(n){return"function"==typeof n?n:null==n?r:"object"==typeof n?o(n)?i(n[0],n[1]):a(n):s(n)}},function(n,e,t){var a=t(232),i=t(286),r=t(176);n.exports=function(n){var e=i(n);return 1==e.length&&e[0][2]?r(e[0][0],e[0][1]):function(t){return t===n||a(t,n,e)}}},function(n,e,t){var a=t(161),i=t(165);n.exports=function(n,e,t,r){var o=t.length,s=o,l=!r;if(null==n)return!s;for(n=Object(n);o--;){var c=t[o];if(l&&c[2]?c[1]!==n[c[0]]:!(c[0]in n))return!1}for(;++o<s;){var p=(c=t[o])[0],d=n[p],u=c[1];if(l&&c[2]){if(void 0===d&&!(p in n))return!1}else{var m=new a;if(r)var h=r(d,u,p,n,e,m);if(!(void 0===h?i(u,d,3,r,m):h))return!1}}return!0}},function(n,e){n.exports=function(){this.__data__=[],this.size=0}},function(n,e,t){var a=t(76),i=Array.prototype.splice;n.exports=function(n){var e=this.__data__,t=a(e,n);return!(t<0)&&(t==e.length-1?e.pop():i.call(e,t,1),--this.size,!0)}},function(n,e,t){var a=t(76);n.exports=function(n){var e=this.__data__,t=a(e,n);return t<0?void 0:e[t][1]}},function(n,e,t){var a=t(76);n.exports=function(n){return a(this.__data__,n)>-1}},function(n,e,t){var a=t(76);n.exports=function(n,e){var t=this.__data__,i=a(t,n);return i<0?(++this.size,t.push([n,e])):t[i][1]=e,this}},function(n,e,t){var a=t(75);n.exports=function(){this.__data__=new a,this.size=0}},function(n,e){n.exports=function(n){var e=this.__data__,t=e.delete(n);return this.size=e.size,t}},function(n,e){n.exports=function(n){return this.__data__.get(n)}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e,t){var a=t(75),i=t(105),r=t(107);n.exports=function(n,e){var t=this.__data__;if(t instanceof a){var o=t.__data__;if(!i||o.length<199)return o.push([n,e]),this.size=++t.size,this;t=this.__data__=new r(o)}return t.set(n,e),this.size=t.size,this}},function(n,e,t){var a=t(163),i=t(244),r=t(106),o=t(164),s=/^\[object .+?Constructor\]$/,l=Function.prototype,c=Object.prototype,p=l.toString,d=c.hasOwnProperty,u=RegExp("^"+p.call(d).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");n.exports=function(n){return!(!r(n)||i(n))&&(a(n)?u:s).test(o(n))}},function(n,e,t){var a,i=t(245),r=(a=/[^.]+$/.exec(i&&i.keys&&i.keys.IE_PROTO||""))?"Symbol(src)_1."+a:"";n.exports=function(n){return!!r&&r in n}},function(n,e,t){var a=t(24)["__core-js_shared__"];n.exports=a},function(n,e){n.exports=function(n,e){return null==n?void 0:n[e]}},function(n,e,t){var a=t(248),i=t(75),r=t(105);n.exports=function(){this.size=0,this.__data__={hash:new a,map:new(r||i),string:new a}}},function(n,e,t){var a=t(249),i=t(250),r=t(251),o=t(252),s=t(253);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var a=n[e];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=i,l.prototype.get=r,l.prototype.has=o,l.prototype.set=s,n.exports=l},function(n,e,t){var a=t(77);n.exports=function(){this.__data__=a?a(null):{},this.size=0}},function(n,e){n.exports=function(n){var e=this.has(n)&&delete this.__data__[n];return this.size-=e?1:0,e}},function(n,e,t){var a=t(77),i=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;if(a){var t=e[n];return"__lodash_hash_undefined__"===t?void 0:t}return i.call(e,n)?e[n]:void 0}},function(n,e,t){var a=t(77),i=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;return a?void 0!==e[n]:i.call(e,n)}},function(n,e,t){var a=t(77);n.exports=function(n,e){var t=this.__data__;return this.size+=this.has(n)?0:1,t[n]=a&&void 0===e?"__lodash_hash_undefined__":e,this}},function(n,e,t){var a=t(78);n.exports=function(n){var e=a(this,n).delete(n);return this.size-=e?1:0,e}},function(n,e){n.exports=function(n){var e=typeof n;return"string"==e||"number"==e||"symbol"==e||"boolean"==e?"__proto__"!==n:null===n}},function(n,e,t){var a=t(78);n.exports=function(n){return a(this,n).get(n)}},function(n,e,t){var a=t(78);n.exports=function(n){return a(this,n).has(n)}},function(n,e,t){var a=t(78);n.exports=function(n,e){var t=a(this,n),i=t.size;return t.set(n,e),this.size+=t.size==i?0:1,this}},function(n,e,t){var a=t(161),i=t(166),r=t(263),o=t(266),s=t(282),l=t(23),c=t(170),p=t(172),d="[object Object]",u=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,m,h,g){var f=l(n),v=l(e),b=f?"[object Array]":s(n),y=v?"[object Array]":s(e),_=(b="[object Arguments]"==b?d:b)==d,E=(y="[object Arguments]"==y?d:y)==d,A=b==y;if(A&&c(n)){if(!c(e))return!1;f=!0,_=!1}if(A&&!_)return g||(g=new a),f||p(n)?i(n,e,t,m,h,g):r(n,e,b,t,m,h,g);if(!(1&t)){var x=_&&u.call(n,"__wrapped__"),k=E&&u.call(e,"__wrapped__");if(x||k){var w=x?n.value():n,C=k?e.value():e;return g||(g=new a),h(w,C,t,m,g)}}return!!A&&(g||(g=new a),o(n,e,t,m,h,g))}},function(n,e){n.exports=function(n){return this.__data__.set(n,"__lodash_hash_undefined__"),this}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e){n.exports=function(n,e){for(var t=-1,a=null==n?0:n.length;++t<a;)if(e(n[t],t,n))return!0;return!1}},function(n,e,t){var a=t(58),i=t(264),r=t(162),o=t(166),s=t(265),l=t(108),c=a?a.prototype:void 0,p=c?c.valueOf:void 0;n.exports=function(n,e,t,a,c,d,u){switch(t){case"[object DataView]":if(n.byteLength!=e.byteLength||n.byteOffset!=e.byteOffset)return!1;n=n.buffer,e=e.buffer;case"[object ArrayBuffer]":return!(n.byteLength!=e.byteLength||!d(new i(n),new i(e)));case"[object Boolean]":case"[object Date]":case"[object Number]":return r(+n,+e);case"[object Error]":return n.name==e.name&&n.message==e.message;case"[object RegExp]":case"[object String]":return n==e+"";case"[object Map]":var m=s;case"[object Set]":var h=1&a;if(m||(m=l),n.size!=e.size&&!h)return!1;var g=u.get(n);if(g)return g==e;a|=2,u.set(n,e);var f=o(m(n),m(e),a,c,d,u);return u.delete(n),f;case"[object Symbol]":if(p)return p.call(n)==p.call(e)}return!1}},function(n,e,t){var a=t(24).Uint8Array;n.exports=a},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n,a){t[++e]=[a,n]})),t}},function(n,e,t){var a=t(267),i=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,r,o,s){var l=1&t,c=a(n),p=c.length;if(p!=a(e).length&&!l)return!1;for(var d=p;d--;){var u=c[d];if(!(l?u in e:i.call(e,u)))return!1}var m=s.get(n),h=s.get(e);if(m&&h)return m==e&&h==n;var g=!0;s.set(n,e),s.set(e,n);for(var f=l;++d<p;){var v=n[u=c[d]],b=e[u];if(r)var y=l?r(b,v,u,e,n,s):r(v,b,u,n,e,s);if(!(void 0===y?v===b||o(v,b,t,r,s):y)){g=!1;break}f||(f="constructor"==u)}if(g&&!f){var _=n.constructor,E=e.constructor;_==E||!("constructor"in n)||!("constructor"in e)||"function"==typeof _&&_ instanceof _&&"function"==typeof E&&E instanceof E||(g=!1)}return s.delete(n),s.delete(e),g}},function(n,e,t){var a=t(268),i=t(269),r=t(169);n.exports=function(n){return a(n,r,i)}},function(n,e,t){var a=t(159),i=t(23);n.exports=function(n,e,t){var r=e(n);return i(n)?r:a(r,t(n))}},function(n,e,t){var a=t(270),i=t(271),r=Object.prototype.propertyIsEnumerable,o=Object.getOwnPropertySymbols,s=o?function(n){return null==n?[]:(n=Object(n),a(o(n),(function(e){return r.call(n,e)})))}:i;n.exports=s},function(n,e){n.exports=function(n,e){for(var t=-1,a=null==n?0:n.length,i=0,r=[];++t<a;){var o=n[t];e(o,t,n)&&(r[i++]=o)}return r}},function(n,e){n.exports=function(){return[]}},function(n,e,t){var a=t(273),i=t(104),r=t(23),o=t(170),s=t(171),l=t(172),c=Object.prototype.hasOwnProperty;n.exports=function(n,e){var t=r(n),p=!t&&i(n),d=!t&&!p&&o(n),u=!t&&!p&&!d&&l(n),m=t||p||d||u,h=m?a(n.length,String):[],g=h.length;for(var f in n)!e&&!c.call(n,f)||m&&("length"==f||d&&("offset"==f||"parent"==f)||u&&("buffer"==f||"byteLength"==f||"byteOffset"==f)||s(f,g))||h.push(f);return h}},function(n,e){n.exports=function(n,e){for(var t=-1,a=Array(n);++t<n;)a[t]=e(t);return a}},function(n,e){n.exports=function(){return!1}},function(n,e,t){var a=t(49),i=t(109),r=t(39),o={};o["[object Float32Array]"]=o["[object Float64Array]"]=o["[object Int8Array]"]=o["[object Int16Array]"]=o["[object Int32Array]"]=o["[object Uint8Array]"]=o["[object Uint8ClampedArray]"]=o["[object Uint16Array]"]=o["[object Uint32Array]"]=!0,o["[object Arguments]"]=o["[object Array]"]=o["[object ArrayBuffer]"]=o["[object Boolean]"]=o["[object DataView]"]=o["[object Date]"]=o["[object Error]"]=o["[object Function]"]=o["[object Map]"]=o["[object Number]"]=o["[object Object]"]=o["[object RegExp]"]=o["[object Set]"]=o["[object String]"]=o["[object WeakMap]"]=!1,n.exports=function(n){return r(n)&&i(n.length)&&!!o[a(n)]}},function(n,e){n.exports=function(n){return function(e){return n(e)}}},function(n,e,t){(function(n){var a=t(160),i=e&&!e.nodeType&&e,r=i&&"object"==typeof n&&n&&!n.nodeType&&n,o=r&&r.exports===i&&a.process,s=function(){try{var n=r&&r.require&&r.require("util").types;return n||o&&o.binding&&o.binding("util")}catch(n){}}();n.exports=s}).call(this,t(125)(n))},function(n,e,t){var a=t(279),i=t(280),r=Object.prototype.hasOwnProperty;n.exports=function(n){if(!a(n))return i(n);var e=[];for(var t in Object(n))r.call(n,t)&&"constructor"!=t&&e.push(t);return e}},function(n,e){var t=Object.prototype;n.exports=function(n){var e=n&&n.constructor;return n===("function"==typeof e&&e.prototype||t)}},function(n,e,t){var a=t(281)(Object.keys,Object);n.exports=a},function(n,e){n.exports=function(n,e){return function(t){return n(e(t))}}},function(n,e,t){var a=t(283),i=t(105),r=t(284),o=t(174),s=t(285),l=t(49),c=t(164),p=c(a),d=c(i),u=c(r),m=c(o),h=c(s),g=l;(a&&"[object DataView]"!=g(new a(new ArrayBuffer(1)))||i&&"[object Map]"!=g(new i)||r&&"[object Promise]"!=g(r.resolve())||o&&"[object Set]"!=g(new o)||s&&"[object WeakMap]"!=g(new s))&&(g=function(n){var e=l(n),t="[object Object]"==e?n.constructor:void 0,a=t?c(t):"";if(a)switch(a){case p:return"[object DataView]";case d:return"[object Map]";case u:return"[object Promise]";case m:return"[object Set]";case h:return"[object WeakMap]"}return e}),n.exports=g},function(n,e,t){var a=t(33)(t(24),"DataView");n.exports=a},function(n,e,t){var a=t(33)(t(24),"Promise");n.exports=a},function(n,e,t){var a=t(33)(t(24),"WeakMap");n.exports=a},function(n,e,t){var a=t(175),i=t(169);n.exports=function(n){for(var e=i(n),t=e.length;t--;){var r=e[t],o=n[r];e[t]=[r,o,a(o)]}return e}},function(n,e,t){var a=t(165),i=t(288),r=t(295),o=t(110),s=t(175),l=t(176),c=t(79);n.exports=function(n,e){return o(n)&&s(e)?l(c(n),e):function(t){var o=i(t,n);return void 0===o&&o===e?r(t,n):a(e,o,3)}}},function(n,e,t){var a=t(177);n.exports=function(n,e,t){var i=null==n?void 0:a(n,e);return void 0===i?t:i}},function(n,e,t){var a=t(290),i=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,r=/\\(\\)?/g,o=a((function(n){var e=[];return 46===n.charCodeAt(0)&&e.push(""),n.replace(i,(function(n,t,a,i){e.push(a?i.replace(r,"$1"):t||n)})),e}));n.exports=o},function(n,e,t){var a=t(291);n.exports=function(n){var e=a(n,(function(n){return 500===t.size&&t.clear(),n})),t=e.cache;return e}},function(n,e,t){var a=t(107);function i(n,e){if("function"!=typeof n||null!=e&&"function"!=typeof e)throw new TypeError("Expected a function");var t=function(){var a=arguments,i=e?e.apply(this,a):a[0],r=t.cache;if(r.has(i))return r.get(i);var o=n.apply(this,a);return t.cache=r.set(i,o)||r,o};return t.cache=new(i.Cache||a),t}i.Cache=a,n.exports=i},function(n,e,t){var a=t(293);n.exports=function(n){return null==n?"":a(n)}},function(n,e,t){var a=t(58),i=t(294),r=t(23),o=t(111),s=a?a.prototype:void 0,l=s?s.toString:void 0;n.exports=function n(e){if("string"==typeof e)return e;if(r(e))return i(e,n)+"";if(o(e))return l?l.call(e):"";var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}},function(n,e){n.exports=function(n,e){for(var t=-1,a=null==n?0:n.length,i=Array(a);++t<a;)i[t]=e(n[t],t,n);return i}},function(n,e,t){var a=t(296),i=t(297);n.exports=function(n,e){return null!=n&&i(n,e,a)}},function(n,e){n.exports=function(n,e){return null!=n&&e in Object(n)}},function(n,e,t){var a=t(178),i=t(104),r=t(23),o=t(171),s=t(109),l=t(79);n.exports=function(n,e,t){for(var c=-1,p=(e=a(e,n)).length,d=!1;++c<p;){var u=l(e[c]);if(!(d=null!=n&&t(n,u)))break;n=n[u]}return d||++c!=p?d:!!(p=null==n?0:n.length)&&s(p)&&o(u,p)&&(r(n)||i(n))}},function(n,e,t){var a=t(299),i=t(300),r=t(110),o=t(79);n.exports=function(n){return r(n)?a(o(n)):i(n)}},function(n,e){n.exports=function(n){return function(e){return null==e?void 0:e[n]}}},function(n,e,t){var a=t(177);n.exports=function(n){return function(e){return a(e,n)}}},function(n,e,t){var a=t(112),i=t(302),r=t(304);n.exports=function(n,e){return r(i(n,e,a),n+"")}},function(n,e,t){var a=t(303),i=Math.max;n.exports=function(n,e,t){return e=i(void 0===e?n.length-1:e,0),function(){for(var r=arguments,o=-1,s=i(r.length-e,0),l=Array(s);++o<s;)l[o]=r[e+o];o=-1;for(var c=Array(e+1);++o<e;)c[o]=r[o];return c[e]=t(l),a(n,this,c)}}},function(n,e){n.exports=function(n,e,t){switch(t.length){case 0:return n.call(e);case 1:return n.call(e,t[0]);case 2:return n.call(e,t[0],t[1]);case 3:return n.call(e,t[0],t[1],t[2])}return n.apply(e,t)}},function(n,e,t){var a=t(305),i=t(308)(a);n.exports=i},function(n,e,t){var a=t(306),i=t(307),r=t(112),o=i?function(n,e){return i(n,"toString",{configurable:!0,enumerable:!1,value:a(e),writable:!0})}:r;n.exports=o},function(n,e){n.exports=function(n){return function(){return n}}},function(n,e,t){var a=t(33),i=function(){try{var n=a(Object,"defineProperty");return n({},"",{}),n}catch(n){}}();n.exports=i},function(n,e){var t=Date.now;n.exports=function(n){var e=0,a=0;return function(){var i=t(),r=16-(i-a);if(a=i,r>0){if(++e>=800)return arguments[0]}else e=0;return n.apply(void 0,arguments)}}},function(n,e,t){var a=t(167),i=t(310),r=t(315),o=t(168),s=t(316),l=t(108);n.exports=function(n,e,t){var c=-1,p=i,d=n.length,u=!0,m=[],h=m;if(t)u=!1,p=r;else if(d>=200){var g=e?null:s(n);if(g)return l(g);u=!1,p=o,h=new a}else h=e?[]:m;n:for(;++c<d;){var f=n[c],v=e?e(f):f;if(f=t||0!==f?f:0,u&&v==v){for(var b=h.length;b--;)if(h[b]===v)continue n;e&&h.push(v),m.push(f)}else p(h,v,t)||(h!==m&&h.push(v),m.push(f))}return m}},function(n,e,t){var a=t(311);n.exports=function(n,e){return!!(null==n?0:n.length)&&a(n,e,0)>-1}},function(n,e,t){var a=t(312),i=t(313),r=t(314);n.exports=function(n,e,t){return e==e?r(n,e,t):a(n,i,t)}},function(n,e){n.exports=function(n,e,t,a){for(var i=n.length,r=t+(a?1:-1);a?r--:++r<i;)if(e(n[r],r,n))return r;return-1}},function(n,e){n.exports=function(n){return n!=n}},function(n,e){n.exports=function(n,e,t){for(var a=t-1,i=n.length;++a<i;)if(n[a]===e)return a;return-1}},function(n,e){n.exports=function(n,e,t){for(var a=-1,i=null==n?0:n.length;++a<i;)if(t(e,n[a]))return!0;return!1}},function(n,e,t){var a=t(174),i=t(317),r=t(108),o=a&&1/r(new a([,-0]))[1]==1/0?function(n){return new a(n)}:i;n.exports=o},function(n,e){n.exports=function(){}},function(n,e,t){var a=t(173),i=t(39);n.exports=function(n){return i(n)&&a(n)}},function(n,e,t){},function(n,e,t){var a=t(1),i=t(129);n.exports=function(n){return a((function(){return!!i[n]()||"​᠎"!="​᠎"[n]()||i[n].name!==n}))}},function(n,e,t){},function(n,e,t){},function(n,e,t){var a=t(11),i=t(324),r=t(14),o=Math.ceil,s=function(n){return function(e,t,s){var l,c,p=String(r(e)),d=p.length,u=void 0===s?" ":String(s),m=a(t);return m<=d||""==u?p:(l=m-d,(c=i.call(u,o(l/u.length))).length>l&&(c=c.slice(0,l)),n?p+c:c+p)}};n.exports={start:s(!1),end:s(!0)}},function(n,e,t){"use strict";var a=t(43),i=t(14);n.exports=function(n){var e=String(i(this)),t="",r=a(n);if(r<0||r==1/0)throw RangeError("Wrong number of repetitions");for(;r>0;(r>>>=1)&&(e+=e))1&r&&(t+=e);return t}},function(n,e,t){var a=t(54);n.exports=/Version\/10(?:\.\d+){1,2}(?: [\w./]+)?(?: Mobile\/\w+)? Safari\//.test(a)},function(n,e,t){"use strict";t(182)},function(n,e,t){"use strict";t(183)},function(n,e,t){"use strict";var a=t(0),i=t(27),r=t(15),o=t(1),s=t(37),l=[],c=l.sort,p=o((function(){l.sort(void 0)})),d=o((function(){l.sort(null)})),u=s("sort");a({target:"Array",proto:!0,forced:p||!d||!u},{sort:function(n){return void 0===n?c.call(r(this)):c.call(r(this),i(n))}})},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){"use strict";t(186)},function(n,e,t){"use strict";t(187)},function(n,e,t){"use strict";t.r(e);t(117),t(208),t(214),t(215);var a=t(71),i=(t(115),t(40),t(8),t(12),t(17),t(28),t(16),Object.freeze({}));function r(n){return null==n}function o(n){return null!=n}function s(n){return!0===n}function l(n){return"string"==typeof n||"number"==typeof n||"symbol"==typeof n||"boolean"==typeof n}function c(n){return null!==n&&"object"==typeof n}var p=Object.prototype.toString;function d(n){return"[object Object]"===p.call(n)}function u(n){return"[object RegExp]"===p.call(n)}function m(n){var e=parseFloat(String(n));return e>=0&&Math.floor(e)===e&&isFinite(n)}function h(n){return o(n)&&"function"==typeof n.then&&"function"==typeof n.catch}function g(n){return null==n?"":Array.isArray(n)||d(n)&&n.toString===p?JSON.stringify(n,null,2):String(n)}function f(n){var e=parseFloat(n);return isNaN(e)?n:e}function v(n,e){for(var t=Object.create(null),a=n.split(","),i=0;i<a.length;i++)t[a[i]]=!0;return e?function(n){return t[n.toLowerCase()]}:function(n){return t[n]}}v("slot,component",!0);var b=v("key,ref,slot,slot-scope,is");function y(n,e){if(n.length){var t=n.indexOf(e);if(t>-1)return n.splice(t,1)}}var _=Object.prototype.hasOwnProperty;function E(n,e){return _.call(n,e)}function A(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var x=/-(\w)/g,k=A((function(n){return n.replace(x,(function(n,e){return e?e.toUpperCase():""}))})),w=A((function(n){return n.charAt(0).toUpperCase()+n.slice(1)})),C=/\B([A-Z])/g,S=A((function(n){return n.replace(C,"-$1").toLowerCase()}));var B=Function.prototype.bind?function(n,e){return n.bind(e)}:function(n,e){function t(t){var a=arguments.length;return a?a>1?n.apply(e,arguments):n.call(e,t):n.call(e)}return t._length=n.length,t};function T(n,e){e=e||0;for(var t=n.length-e,a=new Array(t);t--;)a[t]=n[t+e];return a}function P(n,e){for(var t in e)n[t]=e[t];return n}function D(n){for(var e={},t=0;t<n.length;t++)n[t]&&P(e,n[t]);return e}function z(n,e,t){}var L=function(n,e,t){return!1},I=function(n){return n};function N(n,e){if(n===e)return!0;var t=c(n),a=c(e);if(!t||!a)return!t&&!a&&String(n)===String(e);try{var i=Array.isArray(n),r=Array.isArray(e);if(i&&r)return n.length===e.length&&n.every((function(n,t){return N(n,e[t])}));if(n instanceof Date&&e instanceof Date)return n.getTime()===e.getTime();if(i||r)return!1;var o=Object.keys(n),s=Object.keys(e);return o.length===s.length&&o.every((function(t){return N(n[t],e[t])}))}catch(n){return!1}}function F(n,e){for(var t=0;t<n.length;t++)if(N(n[t],e))return t;return-1}function j(n){var e=!1;return function(){e||(e=!0,n.apply(this,arguments))}}var M=["component","directive","filter"],R=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch"],O={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:L,isReservedAttr:L,isUnknownElement:L,getTagNamespace:z,parsePlatformTagName:I,mustUseProp:L,async:!0,_lifecycleHooks:R},U=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function V(n,e,t,a){Object.defineProperty(n,e,{value:t,enumerable:!!a,writable:!0,configurable:!0})}var G=new RegExp("[^"+U.source+".$_\\d]");var W,H="__proto__"in{},q="undefined"!=typeof window,$="undefined"!=typeof WXEnvironment&&!!WXEnvironment.platform,X=$&&WXEnvironment.platform.toLowerCase(),Z=q&&window.navigator.userAgent.toLowerCase(),K=Z&&/msie|trident/.test(Z),Y=Z&&Z.indexOf("msie 9.0")>0,J=Z&&Z.indexOf("edge/")>0,Q=(Z&&Z.indexOf("android"),Z&&/iphone|ipad|ipod|ios/.test(Z)||"ios"===X),nn=(Z&&/chrome\/\d+/.test(Z),Z&&/phantomjs/.test(Z),Z&&Z.match(/firefox\/(\d+)/)),en={}.watch,tn=!1;if(q)try{var an={};Object.defineProperty(an,"passive",{get:function(){tn=!0}}),window.addEventListener("test-passive",null,an)}catch(n){}var rn=function(){return void 0===W&&(W=!q&&!$&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),W},on=q&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function sn(n){return"function"==typeof n&&/native code/.test(n.toString())}var ln,cn="undefined"!=typeof Symbol&&sn(Symbol)&&"undefined"!=typeof Reflect&&sn(Reflect.ownKeys);ln="undefined"!=typeof Set&&sn(Set)?Set:function(){function n(){this.set=Object.create(null)}return n.prototype.has=function(n){return!0===this.set[n]},n.prototype.add=function(n){this.set[n]=!0},n.prototype.clear=function(){this.set=Object.create(null)},n}();var pn=z,dn=0,un=function(){this.id=dn++,this.subs=[]};un.prototype.addSub=function(n){this.subs.push(n)},un.prototype.removeSub=function(n){y(this.subs,n)},un.prototype.depend=function(){un.target&&un.target.addDep(this)},un.prototype.notify=function(){var n=this.subs.slice();for(var e=0,t=n.length;e<t;e++)n[e].update()},un.target=null;var mn=[];function hn(n){mn.push(n),un.target=n}function gn(){mn.pop(),un.target=mn[mn.length-1]}var fn=function(n,e,t,a,i,r,o,s){this.tag=n,this.data=e,this.children=t,this.text=a,this.elm=i,this.ns=void 0,this.context=r,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=e&&e.key,this.componentOptions=o,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1},vn={child:{configurable:!0}};vn.child.get=function(){return this.componentInstance},Object.defineProperties(fn.prototype,vn);var bn=function(n){void 0===n&&(n="");var e=new fn;return e.text=n,e.isComment=!0,e};function yn(n){return new fn(void 0,void 0,void 0,String(n))}function _n(n){var e=new fn(n.tag,n.data,n.children&&n.children.slice(),n.text,n.elm,n.context,n.componentOptions,n.asyncFactory);return e.ns=n.ns,e.isStatic=n.isStatic,e.key=n.key,e.isComment=n.isComment,e.fnContext=n.fnContext,e.fnOptions=n.fnOptions,e.fnScopeId=n.fnScopeId,e.asyncMeta=n.asyncMeta,e.isCloned=!0,e}var En=Array.prototype,An=Object.create(En);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(n){var e=En[n];V(An,n,(function(){for(var t=[],a=arguments.length;a--;)t[a]=arguments[a];var i,r=e.apply(this,t),o=this.__ob__;switch(n){case"push":case"unshift":i=t;break;case"splice":i=t.slice(2)}return i&&o.observeArray(i),o.dep.notify(),r}))}));var xn=Object.getOwnPropertyNames(An),kn=!0;function wn(n){kn=n}var Cn=function(n){this.value=n,this.dep=new un,this.vmCount=0,V(n,"__ob__",this),Array.isArray(n)?(H?function(n,e){n.__proto__=e}(n,An):function(n,e,t){for(var a=0,i=t.length;a<i;a++){var r=t[a];V(n,r,e[r])}}(n,An,xn),this.observeArray(n)):this.walk(n)};function Sn(n,e){var t;if(c(n)&&!(n instanceof fn))return E(n,"__ob__")&&n.__ob__ instanceof Cn?t=n.__ob__:kn&&!rn()&&(Array.isArray(n)||d(n))&&Object.isExtensible(n)&&!n._isVue&&(t=new Cn(n)),e&&t&&t.vmCount++,t}function Bn(n,e,t,a,i){var r=new un,o=Object.getOwnPropertyDescriptor(n,e);if(!o||!1!==o.configurable){var s=o&&o.get,l=o&&o.set;s&&!l||2!==arguments.length||(t=n[e]);var c=!i&&Sn(t);Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){var e=s?s.call(n):t;return un.target&&(r.depend(),c&&(c.dep.depend(),Array.isArray(e)&&Dn(e))),e},set:function(e){var a=s?s.call(n):t;e===a||e!=e&&a!=a||s&&!l||(l?l.call(n,e):t=e,c=!i&&Sn(e),r.notify())}})}}function Tn(n,e,t){if(Array.isArray(n)&&m(e))return n.length=Math.max(n.length,e),n.splice(e,1,t),t;if(e in n&&!(e in Object.prototype))return n[e]=t,t;var a=n.__ob__;return n._isVue||a&&a.vmCount?t:a?(Bn(a.value,e,t),a.dep.notify(),t):(n[e]=t,t)}function Pn(n,e){if(Array.isArray(n)&&m(e))n.splice(e,1);else{var t=n.__ob__;n._isVue||t&&t.vmCount||E(n,e)&&(delete n[e],t&&t.dep.notify())}}function Dn(n){for(var e=void 0,t=0,a=n.length;t<a;t++)(e=n[t])&&e.__ob__&&e.__ob__.dep.depend(),Array.isArray(e)&&Dn(e)}Cn.prototype.walk=function(n){for(var e=Object.keys(n),t=0;t<e.length;t++)Bn(n,e[t])},Cn.prototype.observeArray=function(n){for(var e=0,t=n.length;e<t;e++)Sn(n[e])};var zn=O.optionMergeStrategies;function Ln(n,e){if(!e)return n;for(var t,a,i,r=cn?Reflect.ownKeys(e):Object.keys(e),o=0;o<r.length;o++)"__ob__"!==(t=r[o])&&(a=n[t],i=e[t],E(n,t)?a!==i&&d(a)&&d(i)&&Ln(a,i):Tn(n,t,i));return n}function In(n,e,t){return t?function(){var a="function"==typeof e?e.call(t,t):e,i="function"==typeof n?n.call(t,t):n;return a?Ln(a,i):i}:e?n?function(){return Ln("function"==typeof e?e.call(this,this):e,"function"==typeof n?n.call(this,this):n)}:e:n}function Nn(n,e){var t=e?n?n.concat(e):Array.isArray(e)?e:[e]:n;return t?function(n){for(var e=[],t=0;t<n.length;t++)-1===e.indexOf(n[t])&&e.push(n[t]);return e}(t):t}function Fn(n,e,t,a){var i=Object.create(n||null);return e?P(i,e):i}zn.data=function(n,e,t){return t?In(n,e,t):e&&"function"!=typeof e?n:In(n,e)},R.forEach((function(n){zn[n]=Nn})),M.forEach((function(n){zn[n+"s"]=Fn})),zn.watch=function(n,e,t,a){if(n===en&&(n=void 0),e===en&&(e=void 0),!e)return Object.create(n||null);if(!n)return e;var i={};for(var r in P(i,n),e){var o=i[r],s=e[r];o&&!Array.isArray(o)&&(o=[o]),i[r]=o?o.concat(s):Array.isArray(s)?s:[s]}return i},zn.props=zn.methods=zn.inject=zn.computed=function(n,e,t,a){if(!n)return e;var i=Object.create(null);return P(i,n),e&&P(i,e),i},zn.provide=In;var jn=function(n,e){return void 0===e?n:e};function Mn(n,e,t){if("function"==typeof e&&(e=e.options),function(n,e){var t=n.props;if(t){var a,i,r={};if(Array.isArray(t))for(a=t.length;a--;)"string"==typeof(i=t[a])&&(r[k(i)]={type:null});else if(d(t))for(var o in t)i=t[o],r[k(o)]=d(i)?i:{type:i};else 0;n.props=r}}(e),function(n,e){var t=n.inject;if(t){var a=n.inject={};if(Array.isArray(t))for(var i=0;i<t.length;i++)a[t[i]]={from:t[i]};else if(d(t))for(var r in t){var o=t[r];a[r]=d(o)?P({from:r},o):{from:o}}else 0}}(e),function(n){var e=n.directives;if(e)for(var t in e){var a=e[t];"function"==typeof a&&(e[t]={bind:a,update:a})}}(e),!e._base&&(e.extends&&(n=Mn(n,e.extends,t)),e.mixins))for(var a=0,i=e.mixins.length;a<i;a++)n=Mn(n,e.mixins[a],t);var r,o={};for(r in n)s(r);for(r in e)E(n,r)||s(r);function s(a){var i=zn[a]||jn;o[a]=i(n[a],e[a],t,a)}return o}function Rn(n,e,t,a){if("string"==typeof t){var i=n[e];if(E(i,t))return i[t];var r=k(t);if(E(i,r))return i[r];var o=w(r);return E(i,o)?i[o]:i[t]||i[r]||i[o]}}function On(n,e,t,a){var i=e[n],r=!E(t,n),o=t[n],s=Gn(Boolean,i.type);if(s>-1)if(r&&!E(i,"default"))o=!1;else if(""===o||o===S(n)){var l=Gn(String,i.type);(l<0||s<l)&&(o=!0)}if(void 0===o){o=function(n,e,t){if(!E(e,"default"))return;var a=e.default;0;if(n&&n.$options.propsData&&void 0===n.$options.propsData[t]&&void 0!==n._props[t])return n._props[t];return"function"==typeof a&&"Function"!==Un(e.type)?a.call(n):a}(a,i,n);var c=kn;wn(!0),Sn(o),wn(c)}return o}function Un(n){var e=n&&n.toString().match(/^\s*function (\w+)/);return e?e[1]:""}function Vn(n,e){return Un(n)===Un(e)}function Gn(n,e){if(!Array.isArray(e))return Vn(e,n)?0:-1;for(var t=0,a=e.length;t<a;t++)if(Vn(e[t],n))return t;return-1}function Wn(n,e,t){hn();try{if(e)for(var a=e;a=a.$parent;){var i=a.$options.errorCaptured;if(i)for(var r=0;r<i.length;r++)try{if(!1===i[r].call(a,n,e,t))return}catch(n){qn(n,a,"errorCaptured hook")}}qn(n,e,t)}finally{gn()}}function Hn(n,e,t,a,i){var r;try{(r=t?n.apply(e,t):n.call(e))&&!r._isVue&&h(r)&&!r._handled&&(r.catch((function(n){return Wn(n,a,i+" (Promise/async)")})),r._handled=!0)}catch(n){Wn(n,a,i)}return r}function qn(n,e,t){if(O.errorHandler)try{return O.errorHandler.call(null,n,e,t)}catch(e){e!==n&&$n(e,null,"config.errorHandler")}$n(n,e,t)}function $n(n,e,t){if(!q&&!$||"undefined"==typeof console)throw n;console.error(n)}var Xn,Zn=!1,Kn=[],Yn=!1;function Jn(){Yn=!1;var n=Kn.slice(0);Kn.length=0;for(var e=0;e<n.length;e++)n[e]()}if("undefined"!=typeof Promise&&sn(Promise)){var Qn=Promise.resolve();Xn=function(){Qn.then(Jn),Q&&setTimeout(z)},Zn=!0}else if(K||"undefined"==typeof MutationObserver||!sn(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Xn="undefined"!=typeof setImmediate&&sn(setImmediate)?function(){setImmediate(Jn)}:function(){setTimeout(Jn,0)};else{var ne=1,ee=new MutationObserver(Jn),te=document.createTextNode(String(ne));ee.observe(te,{characterData:!0}),Xn=function(){ne=(ne+1)%2,te.data=String(ne)},Zn=!0}function ae(n,e){var t;if(Kn.push((function(){if(n)try{n.call(e)}catch(n){Wn(n,e,"nextTick")}else t&&t(e)})),Yn||(Yn=!0,Xn()),!n&&"undefined"!=typeof Promise)return new Promise((function(n){t=n}))}var ie=new ln;function re(n){!function n(e,t){var a,i,r=Array.isArray(e);if(!r&&!c(e)||Object.isFrozen(e)||e instanceof fn)return;if(e.__ob__){var o=e.__ob__.dep.id;if(t.has(o))return;t.add(o)}if(r)for(a=e.length;a--;)n(e[a],t);else for(i=Object.keys(e),a=i.length;a--;)n(e[i[a]],t)}(n,ie),ie.clear()}var oe=A((function(n){var e="&"===n.charAt(0),t="~"===(n=e?n.slice(1):n).charAt(0),a="!"===(n=t?n.slice(1):n).charAt(0);return{name:n=a?n.slice(1):n,once:t,capture:a,passive:e}}));function se(n,e){function t(){var n=arguments,a=t.fns;if(!Array.isArray(a))return Hn(a,null,arguments,e,"v-on handler");for(var i=a.slice(),r=0;r<i.length;r++)Hn(i[r],null,n,e,"v-on handler")}return t.fns=n,t}function le(n,e,t,a,i,o){var l,c,p,d;for(l in n)c=n[l],p=e[l],d=oe(l),r(c)||(r(p)?(r(c.fns)&&(c=n[l]=se(c,o)),s(d.once)&&(c=n[l]=i(d.name,c,d.capture)),t(d.name,c,d.capture,d.passive,d.params)):c!==p&&(p.fns=c,n[l]=p));for(l in e)r(n[l])&&a((d=oe(l)).name,e[l],d.capture)}function ce(n,e,t){var a;n instanceof fn&&(n=n.data.hook||(n.data.hook={}));var i=n[e];function l(){t.apply(this,arguments),y(a.fns,l)}r(i)?a=se([l]):o(i.fns)&&s(i.merged)?(a=i).fns.push(l):a=se([i,l]),a.merged=!0,n[e]=a}function pe(n,e,t,a,i){if(o(e)){if(E(e,t))return n[t]=e[t],i||delete e[t],!0;if(E(e,a))return n[t]=e[a],i||delete e[a],!0}return!1}function de(n){return l(n)?[yn(n)]:Array.isArray(n)?function n(e,t){var a,i,c,p,d=[];for(a=0;a<e.length;a++)r(i=e[a])||"boolean"==typeof i||(c=d.length-1,p=d[c],Array.isArray(i)?i.length>0&&(ue((i=n(i,(t||"")+"_"+a))[0])&&ue(p)&&(d[c]=yn(p.text+i[0].text),i.shift()),d.push.apply(d,i)):l(i)?ue(p)?d[c]=yn(p.text+i):""!==i&&d.push(yn(i)):ue(i)&&ue(p)?d[c]=yn(p.text+i.text):(s(e._isVList)&&o(i.tag)&&r(i.key)&&o(t)&&(i.key="__vlist"+t+"_"+a+"__"),d.push(i)));return d}(n):void 0}function ue(n){return o(n)&&o(n.text)&&!1===n.isComment}function me(n,e){if(n){for(var t=Object.create(null),a=cn?Reflect.ownKeys(n):Object.keys(n),i=0;i<a.length;i++){var r=a[i];if("__ob__"!==r){for(var o=n[r].from,s=e;s;){if(s._provided&&E(s._provided,o)){t[r]=s._provided[o];break}s=s.$parent}if(!s)if("default"in n[r]){var l=n[r].default;t[r]="function"==typeof l?l.call(e):l}else 0}}return t}}function he(n,e){if(!n||!n.length)return{};for(var t={},a=0,i=n.length;a<i;a++){var r=n[a],o=r.data;if(o&&o.attrs&&o.attrs.slot&&delete o.attrs.slot,r.context!==e&&r.fnContext!==e||!o||null==o.slot)(t.default||(t.default=[])).push(r);else{var s=o.slot,l=t[s]||(t[s]=[]);"template"===r.tag?l.push.apply(l,r.children||[]):l.push(r)}}for(var c in t)t[c].every(ge)&&delete t[c];return t}function ge(n){return n.isComment&&!n.asyncFactory||" "===n.text}function fe(n,e,t){var a,r=Object.keys(e).length>0,o=n?!!n.$stable:!r,s=n&&n.$key;if(n){if(n._normalized)return n._normalized;if(o&&t&&t!==i&&s===t.$key&&!r&&!t.$hasNormal)return t;for(var l in a={},n)n[l]&&"$"!==l[0]&&(a[l]=ve(e,l,n[l]))}else a={};for(var c in e)c in a||(a[c]=be(e,c));return n&&Object.isExtensible(n)&&(n._normalized=a),V(a,"$stable",o),V(a,"$key",s),V(a,"$hasNormal",r),a}function ve(n,e,t){var a=function(){var n=arguments.length?t.apply(null,arguments):t({});return(n=n&&"object"==typeof n&&!Array.isArray(n)?[n]:de(n))&&(0===n.length||1===n.length&&n[0].isComment)?void 0:n};return t.proxy&&Object.defineProperty(n,e,{get:a,enumerable:!0,configurable:!0}),a}function be(n,e){return function(){return n[e]}}function ye(n,e){var t,a,i,r,s;if(Array.isArray(n)||"string"==typeof n)for(t=new Array(n.length),a=0,i=n.length;a<i;a++)t[a]=e(n[a],a);else if("number"==typeof n)for(t=new Array(n),a=0;a<n;a++)t[a]=e(a+1,a);else if(c(n))if(cn&&n[Symbol.iterator]){t=[];for(var l=n[Symbol.iterator](),p=l.next();!p.done;)t.push(e(p.value,t.length)),p=l.next()}else for(r=Object.keys(n),t=new Array(r.length),a=0,i=r.length;a<i;a++)s=r[a],t[a]=e(n[s],s,a);return o(t)||(t=[]),t._isVList=!0,t}function _e(n,e,t,a){var i,r=this.$scopedSlots[n];r?(t=t||{},a&&(t=P(P({},a),t)),i=r(t)||e):i=this.$slots[n]||e;var o=t&&t.slot;return o?this.$createElement("template",{slot:o},i):i}function Ee(n){return Rn(this.$options,"filters",n)||I}function Ae(n,e){return Array.isArray(n)?-1===n.indexOf(e):n!==e}function xe(n,e,t,a,i){var r=O.keyCodes[e]||t;return i&&a&&!O.keyCodes[e]?Ae(i,a):r?Ae(r,n):a?S(a)!==e:void 0}function ke(n,e,t,a,i){if(t)if(c(t)){var r;Array.isArray(t)&&(t=D(t));var o=function(o){if("class"===o||"style"===o||b(o))r=n;else{var s=n.attrs&&n.attrs.type;r=a||O.mustUseProp(e,s,o)?n.domProps||(n.domProps={}):n.attrs||(n.attrs={})}var l=k(o),c=S(o);l in r||c in r||(r[o]=t[o],i&&((n.on||(n.on={}))["update:"+o]=function(n){t[o]=n}))};for(var s in t)o(s)}else;return n}function we(n,e){var t=this._staticTrees||(this._staticTrees=[]),a=t[n];return a&&!e||Se(a=t[n]=this.$options.staticRenderFns[n].call(this._renderProxy,null,this),"__static__"+n,!1),a}function Ce(n,e,t){return Se(n,"__once__"+e+(t?"_"+t:""),!0),n}function Se(n,e,t){if(Array.isArray(n))for(var a=0;a<n.length;a++)n[a]&&"string"!=typeof n[a]&&Be(n[a],e+"_"+a,t);else Be(n,e,t)}function Be(n,e,t){n.isStatic=!0,n.key=e,n.isOnce=t}function Te(n,e){if(e)if(d(e)){var t=n.on=n.on?P({},n.on):{};for(var a in e){var i=t[a],r=e[a];t[a]=i?[].concat(i,r):r}}else;return n}function Pe(n,e,t,a){e=e||{$stable:!t};for(var i=0;i<n.length;i++){var r=n[i];Array.isArray(r)?Pe(r,e,t):r&&(r.proxy&&(r.fn.proxy=!0),e[r.key]=r.fn)}return a&&(e.$key=a),e}function De(n,e){for(var t=0;t<e.length;t+=2){var a=e[t];"string"==typeof a&&a&&(n[e[t]]=e[t+1])}return n}function ze(n,e){return"string"==typeof n?e+n:n}function Le(n){n._o=Ce,n._n=f,n._s=g,n._l=ye,n._t=_e,n._q=N,n._i=F,n._m=we,n._f=Ee,n._k=xe,n._b=ke,n._v=yn,n._e=bn,n._u=Pe,n._g=Te,n._d=De,n._p=ze}function Ie(n,e,t,a,r){var o,l=this,c=r.options;E(a,"_uid")?(o=Object.create(a))._original=a:(o=a,a=a._original);var p=s(c._compiled),d=!p;this.data=n,this.props=e,this.children=t,this.parent=a,this.listeners=n.on||i,this.injections=me(c.inject,a),this.slots=function(){return l.$slots||fe(n.scopedSlots,l.$slots=he(t,a)),l.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return fe(n.scopedSlots,this.slots())}}),p&&(this.$options=c,this.$slots=this.slots(),this.$scopedSlots=fe(n.scopedSlots,this.$slots)),c._scopeId?this._c=function(n,e,t,i){var r=Ue(o,n,e,t,i,d);return r&&!Array.isArray(r)&&(r.fnScopeId=c._scopeId,r.fnContext=a),r}:this._c=function(n,e,t,a){return Ue(o,n,e,t,a,d)}}function Ne(n,e,t,a,i){var r=_n(n);return r.fnContext=t,r.fnOptions=a,e.slot&&((r.data||(r.data={})).slot=e.slot),r}function Fe(n,e){for(var t in e)n[k(t)]=e[t]}Le(Ie.prototype);var je={init:function(n,e){if(n.componentInstance&&!n.componentInstance._isDestroyed&&n.data.keepAlive){var t=n;je.prepatch(t,t)}else{(n.componentInstance=function(n,e){var t={_isComponent:!0,_parentVnode:n,parent:e},a=n.data.inlineTemplate;o(a)&&(t.render=a.render,t.staticRenderFns=a.staticRenderFns);return new n.componentOptions.Ctor(t)}(n,Ye)).$mount(e?n.elm:void 0,e)}},prepatch:function(n,e){var t=e.componentOptions;!function(n,e,t,a,r){0;var o=a.data.scopedSlots,s=n.$scopedSlots,l=!!(o&&!o.$stable||s!==i&&!s.$stable||o&&n.$scopedSlots.$key!==o.$key),c=!!(r||n.$options._renderChildren||l);n.$options._parentVnode=a,n.$vnode=a,n._vnode&&(n._vnode.parent=a);if(n.$options._renderChildren=r,n.$attrs=a.data.attrs||i,n.$listeners=t||i,e&&n.$options.props){wn(!1);for(var p=n._props,d=n.$options._propKeys||[],u=0;u<d.length;u++){var m=d[u],h=n.$options.props;p[m]=On(m,h,e,n)}wn(!0),n.$options.propsData=e}t=t||i;var g=n.$options._parentListeners;n.$options._parentListeners=t,Ke(n,t,g),c&&(n.$slots=he(r,a.context),n.$forceUpdate());0}(e.componentInstance=n.componentInstance,t.propsData,t.listeners,e,t.children)},insert:function(n){var e,t=n.context,a=n.componentInstance;a._isMounted||(a._isMounted=!0,et(a,"mounted")),n.data.keepAlive&&(t._isMounted?((e=a)._inactive=!1,at.push(e)):nt(a,!0))},destroy:function(n){var e=n.componentInstance;e._isDestroyed||(n.data.keepAlive?function n(e,t){if(t&&(e._directInactive=!0,Qe(e)))return;if(!e._inactive){e._inactive=!0;for(var a=0;a<e.$children.length;a++)n(e.$children[a]);et(e,"deactivated")}}(e,!0):e.$destroy())}},Me=Object.keys(je);function Re(n,e,t,a,l){if(!r(n)){var p=t.$options._base;if(c(n)&&(n=p.extend(n)),"function"==typeof n){var d;if(r(n.cid)&&void 0===(n=function(n,e){if(s(n.error)&&o(n.errorComp))return n.errorComp;if(o(n.resolved))return n.resolved;var t=Ge;t&&o(n.owners)&&-1===n.owners.indexOf(t)&&n.owners.push(t);if(s(n.loading)&&o(n.loadingComp))return n.loadingComp;if(t&&!o(n.owners)){var a=n.owners=[t],i=!0,l=null,p=null;t.$on("hook:destroyed",(function(){return y(a,t)}));var d=function(n){for(var e=0,t=a.length;e<t;e++)a[e].$forceUpdate();n&&(a.length=0,null!==l&&(clearTimeout(l),l=null),null!==p&&(clearTimeout(p),p=null))},u=j((function(t){n.resolved=We(t,e),i?a.length=0:d(!0)})),m=j((function(e){o(n.errorComp)&&(n.error=!0,d(!0))})),g=n(u,m);return c(g)&&(h(g)?r(n.resolved)&&g.then(u,m):h(g.component)&&(g.component.then(u,m),o(g.error)&&(n.errorComp=We(g.error,e)),o(g.loading)&&(n.loadingComp=We(g.loading,e),0===g.delay?n.loading=!0:l=setTimeout((function(){l=null,r(n.resolved)&&r(n.error)&&(n.loading=!0,d(!1))}),g.delay||200)),o(g.timeout)&&(p=setTimeout((function(){p=null,r(n.resolved)&&m(null)}),g.timeout)))),i=!1,n.loading?n.loadingComp:n.resolved}}(d=n,p)))return function(n,e,t,a,i){var r=bn();return r.asyncFactory=n,r.asyncMeta={data:e,context:t,children:a,tag:i},r}(d,e,t,a,l);e=e||{},xt(n),o(e.model)&&function(n,e){var t=n.model&&n.model.prop||"value",a=n.model&&n.model.event||"input";(e.attrs||(e.attrs={}))[t]=e.model.value;var i=e.on||(e.on={}),r=i[a],s=e.model.callback;o(r)?(Array.isArray(r)?-1===r.indexOf(s):r!==s)&&(i[a]=[s].concat(r)):i[a]=s}(n.options,e);var u=function(n,e,t){var a=e.options.props;if(!r(a)){var i={},s=n.attrs,l=n.props;if(o(s)||o(l))for(var c in a){var p=S(c);pe(i,l,c,p,!0)||pe(i,s,c,p,!1)}return i}}(e,n);if(s(n.options.functional))return function(n,e,t,a,r){var s=n.options,l={},c=s.props;if(o(c))for(var p in c)l[p]=On(p,c,e||i);else o(t.attrs)&&Fe(l,t.attrs),o(t.props)&&Fe(l,t.props);var d=new Ie(t,l,r,a,n),u=s.render.call(null,d._c,d);if(u instanceof fn)return Ne(u,t,d.parent,s,d);if(Array.isArray(u)){for(var m=de(u)||[],h=new Array(m.length),g=0;g<m.length;g++)h[g]=Ne(m[g],t,d.parent,s,d);return h}}(n,u,e,t,a);var m=e.on;if(e.on=e.nativeOn,s(n.options.abstract)){var g=e.slot;e={},g&&(e.slot=g)}!function(n){for(var e=n.hook||(n.hook={}),t=0;t<Me.length;t++){var a=Me[t],i=e[a],r=je[a];i===r||i&&i._merged||(e[a]=i?Oe(r,i):r)}}(e);var f=n.options.name||l;return new fn("vue-component-"+n.cid+(f?"-"+f:""),e,void 0,void 0,void 0,t,{Ctor:n,propsData:u,listeners:m,tag:l,children:a},d)}}}function Oe(n,e){var t=function(t,a){n(t,a),e(t,a)};return t._merged=!0,t}function Ue(n,e,t,a,i,p){return(Array.isArray(t)||l(t))&&(i=a,a=t,t=void 0),s(p)&&(i=2),function(n,e,t,a,i){if(o(t)&&o(t.__ob__))return bn();o(t)&&o(t.is)&&(e=t.is);if(!e)return bn();0;Array.isArray(a)&&"function"==typeof a[0]&&((t=t||{}).scopedSlots={default:a[0]},a.length=0);2===i?a=de(a):1===i&&(a=function(n){for(var e=0;e<n.length;e++)if(Array.isArray(n[e]))return Array.prototype.concat.apply([],n);return n}(a));var l,p;if("string"==typeof e){var d;p=n.$vnode&&n.$vnode.ns||O.getTagNamespace(e),l=O.isReservedTag(e)?new fn(O.parsePlatformTagName(e),t,a,void 0,void 0,n):t&&t.pre||!o(d=Rn(n.$options,"components",e))?new fn(e,t,a,void 0,void 0,n):Re(d,t,n,a,e)}else l=Re(e,t,n,a);return Array.isArray(l)?l:o(l)?(o(p)&&function n(e,t,a){e.ns=t,"foreignObject"===e.tag&&(t=void 0,a=!0);if(o(e.children))for(var i=0,l=e.children.length;i<l;i++){var c=e.children[i];o(c.tag)&&(r(c.ns)||s(a)&&"svg"!==c.tag)&&n(c,t,a)}}(l,p),o(t)&&function(n){c(n.style)&&re(n.style);c(n.class)&&re(n.class)}(t),l):bn()}(n,e,t,a,i)}var Ve,Ge=null;function We(n,e){return(n.__esModule||cn&&"Module"===n[Symbol.toStringTag])&&(n=n.default),c(n)?e.extend(n):n}function He(n){return n.isComment&&n.asyncFactory}function qe(n){if(Array.isArray(n))for(var e=0;e<n.length;e++){var t=n[e];if(o(t)&&(o(t.componentOptions)||He(t)))return t}}function $e(n,e){Ve.$on(n,e)}function Xe(n,e){Ve.$off(n,e)}function Ze(n,e){var t=Ve;return function a(){var i=e.apply(null,arguments);null!==i&&t.$off(n,a)}}function Ke(n,e,t){Ve=n,le(e,t||{},$e,Xe,Ze,n),Ve=void 0}var Ye=null;function Je(n){var e=Ye;return Ye=n,function(){Ye=e}}function Qe(n){for(;n&&(n=n.$parent);)if(n._inactive)return!0;return!1}function nt(n,e){if(e){if(n._directInactive=!1,Qe(n))return}else if(n._directInactive)return;if(n._inactive||null===n._inactive){n._inactive=!1;for(var t=0;t<n.$children.length;t++)nt(n.$children[t]);et(n,"activated")}}function et(n,e){hn();var t=n.$options[e],a=e+" hook";if(t)for(var i=0,r=t.length;i<r;i++)Hn(t[i],n,null,n,a);n._hasHookEvent&&n.$emit("hook:"+e),gn()}var tt=[],at=[],it={},rt=!1,ot=!1,st=0;var lt=0,ct=Date.now;if(q&&!K){var pt=window.performance;pt&&"function"==typeof pt.now&&ct()>document.createEvent("Event").timeStamp&&(ct=function(){return pt.now()})}function dt(){var n,e;for(lt=ct(),ot=!0,tt.sort((function(n,e){return n.id-e.id})),st=0;st<tt.length;st++)(n=tt[st]).before&&n.before(),e=n.id,it[e]=null,n.run();var t=at.slice(),a=tt.slice();st=tt.length=at.length=0,it={},rt=ot=!1,function(n){for(var e=0;e<n.length;e++)n[e]._inactive=!0,nt(n[e],!0)}(t),function(n){var e=n.length;for(;e--;){var t=n[e],a=t.vm;a._watcher===t&&a._isMounted&&!a._isDestroyed&&et(a,"updated")}}(a),on&&O.devtools&&on.emit("flush")}var ut=0,mt=function(n,e,t,a,i){this.vm=n,i&&(n._watcher=this),n._watchers.push(this),a?(this.deep=!!a.deep,this.user=!!a.user,this.lazy=!!a.lazy,this.sync=!!a.sync,this.before=a.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++ut,this.active=!0,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new ln,this.newDepIds=new ln,this.expression="","function"==typeof e?this.getter=e:(this.getter=function(n){if(!G.test(n)){var e=n.split(".");return function(n){for(var t=0;t<e.length;t++){if(!n)return;n=n[e[t]]}return n}}}(e),this.getter||(this.getter=z)),this.value=this.lazy?void 0:this.get()};mt.prototype.get=function(){var n;hn(this);var e=this.vm;try{n=this.getter.call(e,e)}catch(n){if(!this.user)throw n;Wn(n,e,'getter for watcher "'+this.expression+'"')}finally{this.deep&&re(n),gn(),this.cleanupDeps()}return n},mt.prototype.addDep=function(n){var e=n.id;this.newDepIds.has(e)||(this.newDepIds.add(e),this.newDeps.push(n),this.depIds.has(e)||n.addSub(this))},mt.prototype.cleanupDeps=function(){for(var n=this.deps.length;n--;){var e=this.deps[n];this.newDepIds.has(e.id)||e.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},mt.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():function(n){var e=n.id;if(null==it[e]){if(it[e]=!0,ot){for(var t=tt.length-1;t>st&&tt[t].id>n.id;)t--;tt.splice(t+1,0,n)}else tt.push(n);rt||(rt=!0,ae(dt))}}(this)},mt.prototype.run=function(){if(this.active){var n=this.get();if(n!==this.value||c(n)||this.deep){var e=this.value;if(this.value=n,this.user)try{this.cb.call(this.vm,n,e)}catch(n){Wn(n,this.vm,'callback for watcher "'+this.expression+'"')}else this.cb.call(this.vm,n,e)}}},mt.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},mt.prototype.depend=function(){for(var n=this.deps.length;n--;)this.deps[n].depend()},mt.prototype.teardown=function(){if(this.active){this.vm._isBeingDestroyed||y(this.vm._watchers,this);for(var n=this.deps.length;n--;)this.deps[n].removeSub(this);this.active=!1}};var ht={enumerable:!0,configurable:!0,get:z,set:z};function gt(n,e,t){ht.get=function(){return this[e][t]},ht.set=function(n){this[e][t]=n},Object.defineProperty(n,t,ht)}function ft(n){n._watchers=[];var e=n.$options;e.props&&function(n,e){var t=n.$options.propsData||{},a=n._props={},i=n.$options._propKeys=[];n.$parent&&wn(!1);var r=function(r){i.push(r);var o=On(r,e,t,n);Bn(a,r,o),r in n||gt(n,"_props",r)};for(var o in e)r(o);wn(!0)}(n,e.props),e.methods&&function(n,e){n.$options.props;for(var t in e)n[t]="function"!=typeof e[t]?z:B(e[t],n)}(n,e.methods),e.data?function(n){var e=n.$options.data;d(e=n._data="function"==typeof e?function(n,e){hn();try{return n.call(e,e)}catch(n){return Wn(n,e,"data()"),{}}finally{gn()}}(e,n):e||{})||(e={});var t=Object.keys(e),a=n.$options.props,i=(n.$options.methods,t.length);for(;i--;){var r=t[i];0,a&&E(a,r)||(o=void 0,36!==(o=(r+"").charCodeAt(0))&&95!==o&&gt(n,"_data",r))}var o;Sn(e,!0)}(n):Sn(n._data={},!0),e.computed&&function(n,e){var t=n._computedWatchers=Object.create(null),a=rn();for(var i in e){var r=e[i],o="function"==typeof r?r:r.get;0,a||(t[i]=new mt(n,o||z,z,vt)),i in n||bt(n,i,r)}}(n,e.computed),e.watch&&e.watch!==en&&function(n,e){for(var t in e){var a=e[t];if(Array.isArray(a))for(var i=0;i<a.length;i++)Et(n,t,a[i]);else Et(n,t,a)}}(n,e.watch)}var vt={lazy:!0};function bt(n,e,t){var a=!rn();"function"==typeof t?(ht.get=a?yt(e):_t(t),ht.set=z):(ht.get=t.get?a&&!1!==t.cache?yt(e):_t(t.get):z,ht.set=t.set||z),Object.defineProperty(n,e,ht)}function yt(n){return function(){var e=this._computedWatchers&&this._computedWatchers[n];if(e)return e.dirty&&e.evaluate(),un.target&&e.depend(),e.value}}function _t(n){return function(){return n.call(this,this)}}function Et(n,e,t,a){return d(t)&&(a=t,t=t.handler),"string"==typeof t&&(t=n[t]),n.$watch(e,t,a)}var At=0;function xt(n){var e=n.options;if(n.super){var t=xt(n.super);if(t!==n.superOptions){n.superOptions=t;var a=function(n){var e,t=n.options,a=n.sealedOptions;for(var i in t)t[i]!==a[i]&&(e||(e={}),e[i]=t[i]);return e}(n);a&&P(n.extendOptions,a),(e=n.options=Mn(t,n.extendOptions)).name&&(e.components[e.name]=n)}}return e}function kt(n){this._init(n)}function wt(n){n.cid=0;var e=1;n.extend=function(n){n=n||{};var t=this,a=t.cid,i=n._Ctor||(n._Ctor={});if(i[a])return i[a];var r=n.name||t.options.name;var o=function(n){this._init(n)};return(o.prototype=Object.create(t.prototype)).constructor=o,o.cid=e++,o.options=Mn(t.options,n),o.super=t,o.options.props&&function(n){var e=n.options.props;for(var t in e)gt(n.prototype,"_props",t)}(o),o.options.computed&&function(n){var e=n.options.computed;for(var t in e)bt(n.prototype,t,e[t])}(o),o.extend=t.extend,o.mixin=t.mixin,o.use=t.use,M.forEach((function(n){o[n]=t[n]})),r&&(o.options.components[r]=o),o.superOptions=t.options,o.extendOptions=n,o.sealedOptions=P({},o.options),i[a]=o,o}}function Ct(n){return n&&(n.Ctor.options.name||n.tag)}function St(n,e){return Array.isArray(n)?n.indexOf(e)>-1:"string"==typeof n?n.split(",").indexOf(e)>-1:!!u(n)&&n.test(e)}function Bt(n,e){var t=n.cache,a=n.keys,i=n._vnode;for(var r in t){var o=t[r];if(o){var s=Ct(o.componentOptions);s&&!e(s)&&Tt(t,r,a,i)}}}function Tt(n,e,t,a){var i=n[e];!i||a&&i.tag===a.tag||i.componentInstance.$destroy(),n[e]=null,y(t,e)}!function(n){n.prototype._init=function(n){var e=this;e._uid=At++,e._isVue=!0,n&&n._isComponent?function(n,e){var t=n.$options=Object.create(n.constructor.options),a=e._parentVnode;t.parent=e.parent,t._parentVnode=a;var i=a.componentOptions;t.propsData=i.propsData,t._parentListeners=i.listeners,t._renderChildren=i.children,t._componentTag=i.tag,e.render&&(t.render=e.render,t.staticRenderFns=e.staticRenderFns)}(e,n):e.$options=Mn(xt(e.constructor),n||{},e),e._renderProxy=e,e._self=e,function(n){var e=n.$options,t=e.parent;if(t&&!e.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(n)}n.$parent=t,n.$root=t?t.$root:n,n.$children=[],n.$refs={},n._watcher=null,n._inactive=null,n._directInactive=!1,n._isMounted=!1,n._isDestroyed=!1,n._isBeingDestroyed=!1}(e),function(n){n._events=Object.create(null),n._hasHookEvent=!1;var e=n.$options._parentListeners;e&&Ke(n,e)}(e),function(n){n._vnode=null,n._staticTrees=null;var e=n.$options,t=n.$vnode=e._parentVnode,a=t&&t.context;n.$slots=he(e._renderChildren,a),n.$scopedSlots=i,n._c=function(e,t,a,i){return Ue(n,e,t,a,i,!1)},n.$createElement=function(e,t,a,i){return Ue(n,e,t,a,i,!0)};var r=t&&t.data;Bn(n,"$attrs",r&&r.attrs||i,null,!0),Bn(n,"$listeners",e._parentListeners||i,null,!0)}(e),et(e,"beforeCreate"),function(n){var e=me(n.$options.inject,n);e&&(wn(!1),Object.keys(e).forEach((function(t){Bn(n,t,e[t])})),wn(!0))}(e),ft(e),function(n){var e=n.$options.provide;e&&(n._provided="function"==typeof e?e.call(n):e)}(e),et(e,"created"),e.$options.el&&e.$mount(e.$options.el)}}(kt),function(n){var e={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(n.prototype,"$data",e),Object.defineProperty(n.prototype,"$props",t),n.prototype.$set=Tn,n.prototype.$delete=Pn,n.prototype.$watch=function(n,e,t){if(d(e))return Et(this,n,e,t);(t=t||{}).user=!0;var a=new mt(this,n,e,t);if(t.immediate)try{e.call(this,a.value)}catch(n){Wn(n,this,'callback for immediate watcher "'+a.expression+'"')}return function(){a.teardown()}}}(kt),function(n){var e=/^hook:/;n.prototype.$on=function(n,t){var a=this;if(Array.isArray(n))for(var i=0,r=n.length;i<r;i++)a.$on(n[i],t);else(a._events[n]||(a._events[n]=[])).push(t),e.test(n)&&(a._hasHookEvent=!0);return a},n.prototype.$once=function(n,e){var t=this;function a(){t.$off(n,a),e.apply(t,arguments)}return a.fn=e,t.$on(n,a),t},n.prototype.$off=function(n,e){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(Array.isArray(n)){for(var a=0,i=n.length;a<i;a++)t.$off(n[a],e);return t}var r,o=t._events[n];if(!o)return t;if(!e)return t._events[n]=null,t;for(var s=o.length;s--;)if((r=o[s])===e||r.fn===e){o.splice(s,1);break}return t},n.prototype.$emit=function(n){var e=this,t=e._events[n];if(t){t=t.length>1?T(t):t;for(var a=T(arguments,1),i='event handler for "'+n+'"',r=0,o=t.length;r<o;r++)Hn(t[r],e,a,e,i)}return e}}(kt),function(n){n.prototype._update=function(n,e){var t=this,a=t.$el,i=t._vnode,r=Je(t);t._vnode=n,t.$el=i?t.__patch__(i,n):t.__patch__(t.$el,n,e,!1),r(),a&&(a.__vue__=null),t.$el&&(t.$el.__vue__=t),t.$vnode&&t.$parent&&t.$vnode===t.$parent._vnode&&(t.$parent.$el=t.$el)},n.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},n.prototype.$destroy=function(){var n=this;if(!n._isBeingDestroyed){et(n,"beforeDestroy"),n._isBeingDestroyed=!0;var e=n.$parent;!e||e._isBeingDestroyed||n.$options.abstract||y(e.$children,n),n._watcher&&n._watcher.teardown();for(var t=n._watchers.length;t--;)n._watchers[t].teardown();n._data.__ob__&&n._data.__ob__.vmCount--,n._isDestroyed=!0,n.__patch__(n._vnode,null),et(n,"destroyed"),n.$off(),n.$el&&(n.$el.__vue__=null),n.$vnode&&(n.$vnode.parent=null)}}}(kt),function(n){Le(n.prototype),n.prototype.$nextTick=function(n){return ae(n,this)},n.prototype._render=function(){var n,e=this,t=e.$options,a=t.render,i=t._parentVnode;i&&(e.$scopedSlots=fe(i.data.scopedSlots,e.$slots,e.$scopedSlots)),e.$vnode=i;try{Ge=e,n=a.call(e._renderProxy,e.$createElement)}catch(t){Wn(t,e,"render"),n=e._vnode}finally{Ge=null}return Array.isArray(n)&&1===n.length&&(n=n[0]),n instanceof fn||(n=bn()),n.parent=i,n}}(kt);var Pt=[String,RegExp,Array],Dt={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:Pt,exclude:Pt,max:[String,Number]},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var n in this.cache)Tt(this.cache,n,this.keys)},mounted:function(){var n=this;this.$watch("include",(function(e){Bt(n,(function(n){return St(e,n)}))})),this.$watch("exclude",(function(e){Bt(n,(function(n){return!St(e,n)}))}))},render:function(){var n=this.$slots.default,e=qe(n),t=e&&e.componentOptions;if(t){var a=Ct(t),i=this.include,r=this.exclude;if(i&&(!a||!St(i,a))||r&&a&&St(r,a))return e;var o=this.cache,s=this.keys,l=null==e.key?t.Ctor.cid+(t.tag?"::"+t.tag:""):e.key;o[l]?(e.componentInstance=o[l].componentInstance,y(s,l),s.push(l)):(o[l]=e,s.push(l),this.max&&s.length>parseInt(this.max)&&Tt(o,s[0],s,this._vnode)),e.data.keepAlive=!0}return e||n&&n[0]}}};!function(n){var e={get:function(){return O}};Object.defineProperty(n,"config",e),n.util={warn:pn,extend:P,mergeOptions:Mn,defineReactive:Bn},n.set=Tn,n.delete=Pn,n.nextTick=ae,n.observable=function(n){return Sn(n),n},n.options=Object.create(null),M.forEach((function(e){n.options[e+"s"]=Object.create(null)})),n.options._base=n,P(n.options.components,Dt),function(n){n.use=function(n){var e=this._installedPlugins||(this._installedPlugins=[]);if(e.indexOf(n)>-1)return this;var t=T(arguments,1);return t.unshift(this),"function"==typeof n.install?n.install.apply(n,t):"function"==typeof n&&n.apply(null,t),e.push(n),this}}(n),function(n){n.mixin=function(n){return this.options=Mn(this.options,n),this}}(n),wt(n),function(n){M.forEach((function(e){n[e]=function(n,t){return t?("component"===e&&d(t)&&(t.name=t.name||n,t=this.options._base.extend(t)),"directive"===e&&"function"==typeof t&&(t={bind:t,update:t}),this.options[e+"s"][n]=t,t):this.options[e+"s"][n]}}))}(n)}(kt),Object.defineProperty(kt.prototype,"$isServer",{get:rn}),Object.defineProperty(kt.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(kt,"FunctionalRenderContext",{value:Ie}),kt.version="2.6.12";var zt=v("style,class"),Lt=v("input,textarea,option,select,progress"),It=v("contenteditable,draggable,spellcheck"),Nt=v("events,caret,typing,plaintext-only"),Ft=v("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,translate,truespeed,typemustmatch,visible"),jt="http://www.w3.org/1999/xlink",Mt=function(n){return":"===n.charAt(5)&&"xlink"===n.slice(0,5)},Rt=function(n){return Mt(n)?n.slice(6,n.length):""},Ot=function(n){return null==n||!1===n};function Ut(n){for(var e=n.data,t=n,a=n;o(a.componentInstance);)(a=a.componentInstance._vnode)&&a.data&&(e=Vt(a.data,e));for(;o(t=t.parent);)t&&t.data&&(e=Vt(e,t.data));return function(n,e){if(o(n)||o(e))return Gt(n,Wt(e));return""}(e.staticClass,e.class)}function Vt(n,e){return{staticClass:Gt(n.staticClass,e.staticClass),class:o(n.class)?[n.class,e.class]:e.class}}function Gt(n,e){return n?e?n+" "+e:n:e||""}function Wt(n){return Array.isArray(n)?function(n){for(var e,t="",a=0,i=n.length;a<i;a++)o(e=Wt(n[a]))&&""!==e&&(t&&(t+=" "),t+=e);return t}(n):c(n)?function(n){var e="";for(var t in n)n[t]&&(e&&(e+=" "),e+=t);return e}(n):"string"==typeof n?n:""}var Ht={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},qt=v("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),$t=v("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignObject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),Xt=function(n){return qt(n)||$t(n)};var Zt=Object.create(null);var Kt=v("text,number,password,search,email,tel,url");var Yt=Object.freeze({createElement:function(n,e){var t=document.createElement(n);return"select"!==n||e.data&&e.data.attrs&&void 0!==e.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(n,e){return document.createElementNS(Ht[n],e)},createTextNode:function(n){return document.createTextNode(n)},createComment:function(n){return document.createComment(n)},insertBefore:function(n,e,t){n.insertBefore(e,t)},removeChild:function(n,e){n.removeChild(e)},appendChild:function(n,e){n.appendChild(e)},parentNode:function(n){return n.parentNode},nextSibling:function(n){return n.nextSibling},tagName:function(n){return n.tagName},setTextContent:function(n,e){n.textContent=e},setStyleScope:function(n,e){n.setAttribute(e,"")}}),Jt={create:function(n,e){Qt(e)},update:function(n,e){n.data.ref!==e.data.ref&&(Qt(n,!0),Qt(e))},destroy:function(n){Qt(n,!0)}};function Qt(n,e){var t=n.data.ref;if(o(t)){var a=n.context,i=n.componentInstance||n.elm,r=a.$refs;e?Array.isArray(r[t])?y(r[t],i):r[t]===i&&(r[t]=void 0):n.data.refInFor?Array.isArray(r[t])?r[t].indexOf(i)<0&&r[t].push(i):r[t]=[i]:r[t]=i}}var na=new fn("",{},[]),ea=["create","activate","update","remove","destroy"];function ta(n,e){return n.key===e.key&&(n.tag===e.tag&&n.isComment===e.isComment&&o(n.data)===o(e.data)&&function(n,e){if("input"!==n.tag)return!0;var t,a=o(t=n.data)&&o(t=t.attrs)&&t.type,i=o(t=e.data)&&o(t=t.attrs)&&t.type;return a===i||Kt(a)&&Kt(i)}(n,e)||s(n.isAsyncPlaceholder)&&n.asyncFactory===e.asyncFactory&&r(e.asyncFactory.error))}function aa(n,e,t){var a,i,r={};for(a=e;a<=t;++a)o(i=n[a].key)&&(r[i]=a);return r}var ia={create:ra,update:ra,destroy:function(n){ra(n,na)}};function ra(n,e){(n.data.directives||e.data.directives)&&function(n,e){var t,a,i,r=n===na,o=e===na,s=sa(n.data.directives,n.context),l=sa(e.data.directives,e.context),c=[],p=[];for(t in l)a=s[t],i=l[t],a?(i.oldValue=a.value,i.oldArg=a.arg,ca(i,"update",e,n),i.def&&i.def.componentUpdated&&p.push(i)):(ca(i,"bind",e,n),i.def&&i.def.inserted&&c.push(i));if(c.length){var d=function(){for(var t=0;t<c.length;t++)ca(c[t],"inserted",e,n)};r?ce(e,"insert",d):d()}p.length&&ce(e,"postpatch",(function(){for(var t=0;t<p.length;t++)ca(p[t],"componentUpdated",e,n)}));if(!r)for(t in s)l[t]||ca(s[t],"unbind",n,n,o)}(n,e)}var oa=Object.create(null);function sa(n,e){var t,a,i=Object.create(null);if(!n)return i;for(t=0;t<n.length;t++)(a=n[t]).modifiers||(a.modifiers=oa),i[la(a)]=a,a.def=Rn(e.$options,"directives",a.name);return i}function la(n){return n.rawName||n.name+"."+Object.keys(n.modifiers||{}).join(".")}function ca(n,e,t,a,i){var r=n.def&&n.def[e];if(r)try{r(t.elm,n,t,a,i)}catch(a){Wn(a,t.context,"directive "+n.name+" "+e+" hook")}}var pa=[Jt,ia];function da(n,e){var t=e.componentOptions;if(!(o(t)&&!1===t.Ctor.options.inheritAttrs||r(n.data.attrs)&&r(e.data.attrs))){var a,i,s=e.elm,l=n.data.attrs||{},c=e.data.attrs||{};for(a in o(c.__ob__)&&(c=e.data.attrs=P({},c)),c)i=c[a],l[a]!==i&&ua(s,a,i);for(a in(K||J)&&c.value!==l.value&&ua(s,"value",c.value),l)r(c[a])&&(Mt(a)?s.removeAttributeNS(jt,Rt(a)):It(a)||s.removeAttribute(a))}}function ua(n,e,t){n.tagName.indexOf("-")>-1?ma(n,e,t):Ft(e)?Ot(t)?n.removeAttribute(e):(t="allowfullscreen"===e&&"EMBED"===n.tagName?"true":e,n.setAttribute(e,t)):It(e)?n.setAttribute(e,function(n,e){return Ot(e)||"false"===e?"false":"contenteditable"===n&&Nt(e)?e:"true"}(e,t)):Mt(e)?Ot(t)?n.removeAttributeNS(jt,Rt(e)):n.setAttributeNS(jt,e,t):ma(n,e,t)}function ma(n,e,t){if(Ot(t))n.removeAttribute(e);else{if(K&&!Y&&"TEXTAREA"===n.tagName&&"placeholder"===e&&""!==t&&!n.__ieph){var a=function(e){e.stopImmediatePropagation(),n.removeEventListener("input",a)};n.addEventListener("input",a),n.__ieph=!0}n.setAttribute(e,t)}}var ha={create:da,update:da};function ga(n,e){var t=e.elm,a=e.data,i=n.data;if(!(r(a.staticClass)&&r(a.class)&&(r(i)||r(i.staticClass)&&r(i.class)))){var s=Ut(e),l=t._transitionClasses;o(l)&&(s=Gt(s,Wt(l))),s!==t._prevClass&&(t.setAttribute("class",s),t._prevClass=s)}}var fa,va={create:ga,update:ga};function ba(n,e,t){var a=fa;return function i(){var r=e.apply(null,arguments);null!==r&&Ea(n,i,t,a)}}var ya=Zn&&!(nn&&Number(nn[1])<=53);function _a(n,e,t,a){if(ya){var i=lt,r=e;e=r._wrapper=function(n){if(n.target===n.currentTarget||n.timeStamp>=i||n.timeStamp<=0||n.target.ownerDocument!==document)return r.apply(this,arguments)}}fa.addEventListener(n,e,tn?{capture:t,passive:a}:t)}function Ea(n,e,t,a){(a||fa).removeEventListener(n,e._wrapper||e,t)}function Aa(n,e){if(!r(n.data.on)||!r(e.data.on)){var t=e.data.on||{},a=n.data.on||{};fa=e.elm,function(n){if(o(n.__r)){var e=K?"change":"input";n[e]=[].concat(n.__r,n[e]||[]),delete n.__r}o(n.__c)&&(n.change=[].concat(n.__c,n.change||[]),delete n.__c)}(t),le(t,a,_a,Ea,ba,e.context),fa=void 0}}var xa,ka={create:Aa,update:Aa};function wa(n,e){if(!r(n.data.domProps)||!r(e.data.domProps)){var t,a,i=e.elm,s=n.data.domProps||{},l=e.data.domProps||{};for(t in o(l.__ob__)&&(l=e.data.domProps=P({},l)),s)t in l||(i[t]="");for(t in l){if(a=l[t],"textContent"===t||"innerHTML"===t){if(e.children&&(e.children.length=0),a===s[t])continue;1===i.childNodes.length&&i.removeChild(i.childNodes[0])}if("value"===t&&"PROGRESS"!==i.tagName){i._value=a;var c=r(a)?"":String(a);Ca(i,c)&&(i.value=c)}else if("innerHTML"===t&&$t(i.tagName)&&r(i.innerHTML)){(xa=xa||document.createElement("div")).innerHTML="<svg>"+a+"</svg>";for(var p=xa.firstChild;i.firstChild;)i.removeChild(i.firstChild);for(;p.firstChild;)i.appendChild(p.firstChild)}else if(a!==s[t])try{i[t]=a}catch(n){}}}}function Ca(n,e){return!n.composing&&("OPTION"===n.tagName||function(n,e){var t=!0;try{t=document.activeElement!==n}catch(n){}return t&&n.value!==e}(n,e)||function(n,e){var t=n.value,a=n._vModifiers;if(o(a)){if(a.number)return f(t)!==f(e);if(a.trim)return t.trim()!==e.trim()}return t!==e}(n,e))}var Sa={create:wa,update:wa},Ba=A((function(n){var e={},t=/:(.+)/;return n.split(/;(?![^(]*\))/g).forEach((function(n){if(n){var a=n.split(t);a.length>1&&(e[a[0].trim()]=a[1].trim())}})),e}));function Ta(n){var e=Pa(n.style);return n.staticStyle?P(n.staticStyle,e):e}function Pa(n){return Array.isArray(n)?D(n):"string"==typeof n?Ba(n):n}var Da,za=/^--/,La=/\s*!important$/,Ia=function(n,e,t){if(za.test(e))n.style.setProperty(e,t);else if(La.test(t))n.style.setProperty(S(e),t.replace(La,""),"important");else{var a=Fa(e);if(Array.isArray(t))for(var i=0,r=t.length;i<r;i++)n.style[a]=t[i];else n.style[a]=t}},Na=["Webkit","Moz","ms"],Fa=A((function(n){if(Da=Da||document.createElement("div").style,"filter"!==(n=k(n))&&n in Da)return n;for(var e=n.charAt(0).toUpperCase()+n.slice(1),t=0;t<Na.length;t++){var a=Na[t]+e;if(a in Da)return a}}));function ja(n,e){var t=e.data,a=n.data;if(!(r(t.staticStyle)&&r(t.style)&&r(a.staticStyle)&&r(a.style))){var i,s,l=e.elm,c=a.staticStyle,p=a.normalizedStyle||a.style||{},d=c||p,u=Pa(e.data.style)||{};e.data.normalizedStyle=o(u.__ob__)?P({},u):u;var m=function(n,e){var t,a={};if(e)for(var i=n;i.componentInstance;)(i=i.componentInstance._vnode)&&i.data&&(t=Ta(i.data))&&P(a,t);(t=Ta(n.data))&&P(a,t);for(var r=n;r=r.parent;)r.data&&(t=Ta(r.data))&&P(a,t);return a}(e,!0);for(s in d)r(m[s])&&Ia(l,s,"");for(s in m)(i=m[s])!==d[s]&&Ia(l,s,null==i?"":i)}}var Ma={create:ja,update:ja},Ra=/\s+/;function Oa(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(Ra).forEach((function(e){return n.classList.add(e)})):n.classList.add(e);else{var t=" "+(n.getAttribute("class")||"")+" ";t.indexOf(" "+e+" ")<0&&n.setAttribute("class",(t+e).trim())}}function Ua(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(Ra).forEach((function(e){return n.classList.remove(e)})):n.classList.remove(e),n.classList.length||n.removeAttribute("class");else{for(var t=" "+(n.getAttribute("class")||"")+" ",a=" "+e+" ";t.indexOf(a)>=0;)t=t.replace(a," ");(t=t.trim())?n.setAttribute("class",t):n.removeAttribute("class")}}function Va(n){if(n){if("object"==typeof n){var e={};return!1!==n.css&&P(e,Ga(n.name||"v")),P(e,n),e}return"string"==typeof n?Ga(n):void 0}}var Ga=A((function(n){return{enterClass:n+"-enter",enterToClass:n+"-enter-to",enterActiveClass:n+"-enter-active",leaveClass:n+"-leave",leaveToClass:n+"-leave-to",leaveActiveClass:n+"-leave-active"}})),Wa=q&&!Y,Ha="transition",qa="transitionend",$a="animation",Xa="animationend";Wa&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(Ha="WebkitTransition",qa="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&($a="WebkitAnimation",Xa="webkitAnimationEnd"));var Za=q?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(n){return n()};function Ka(n){Za((function(){Za(n)}))}function Ya(n,e){var t=n._transitionClasses||(n._transitionClasses=[]);t.indexOf(e)<0&&(t.push(e),Oa(n,e))}function Ja(n,e){n._transitionClasses&&y(n._transitionClasses,e),Ua(n,e)}function Qa(n,e,t){var a=ei(n,e),i=a.type,r=a.timeout,o=a.propCount;if(!i)return t();var s="transition"===i?qa:Xa,l=0,c=function(){n.removeEventListener(s,p),t()},p=function(e){e.target===n&&++l>=o&&c()};setTimeout((function(){l<o&&c()}),r+1),n.addEventListener(s,p)}var ni=/\b(transform|all)(,|$)/;function ei(n,e){var t,a=window.getComputedStyle(n),i=(a[Ha+"Delay"]||"").split(", "),r=(a[Ha+"Duration"]||"").split(", "),o=ti(i,r),s=(a[$a+"Delay"]||"").split(", "),l=(a[$a+"Duration"]||"").split(", "),c=ti(s,l),p=0,d=0;return"transition"===e?o>0&&(t="transition",p=o,d=r.length):"animation"===e?c>0&&(t="animation",p=c,d=l.length):d=(t=(p=Math.max(o,c))>0?o>c?"transition":"animation":null)?"transition"===t?r.length:l.length:0,{type:t,timeout:p,propCount:d,hasTransform:"transition"===t&&ni.test(a[Ha+"Property"])}}function ti(n,e){for(;n.length<e.length;)n=n.concat(n);return Math.max.apply(null,e.map((function(e,t){return ai(e)+ai(n[t])})))}function ai(n){return 1e3*Number(n.slice(0,-1).replace(",","."))}function ii(n,e){var t=n.elm;o(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var a=Va(n.data.transition);if(!r(a)&&!o(t._enterCb)&&1===t.nodeType){for(var i=a.css,s=a.type,l=a.enterClass,p=a.enterToClass,d=a.enterActiveClass,u=a.appearClass,m=a.appearToClass,h=a.appearActiveClass,g=a.beforeEnter,v=a.enter,b=a.afterEnter,y=a.enterCancelled,_=a.beforeAppear,E=a.appear,A=a.afterAppear,x=a.appearCancelled,k=a.duration,w=Ye,C=Ye.$vnode;C&&C.parent;)w=C.context,C=C.parent;var S=!w._isMounted||!n.isRootInsert;if(!S||E||""===E){var B=S&&u?u:l,T=S&&h?h:d,P=S&&m?m:p,D=S&&_||g,z=S&&"function"==typeof E?E:v,L=S&&A||b,I=S&&x||y,N=f(c(k)?k.enter:k);0;var F=!1!==i&&!Y,M=si(z),R=t._enterCb=j((function(){F&&(Ja(t,P),Ja(t,T)),R.cancelled?(F&&Ja(t,B),I&&I(t)):L&&L(t),t._enterCb=null}));n.data.show||ce(n,"insert",(function(){var e=t.parentNode,a=e&&e._pending&&e._pending[n.key];a&&a.tag===n.tag&&a.elm._leaveCb&&a.elm._leaveCb(),z&&z(t,R)})),D&&D(t),F&&(Ya(t,B),Ya(t,T),Ka((function(){Ja(t,B),R.cancelled||(Ya(t,P),M||(oi(N)?setTimeout(R,N):Qa(t,s,R)))}))),n.data.show&&(e&&e(),z&&z(t,R)),F||M||R()}}}function ri(n,e){var t=n.elm;o(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var a=Va(n.data.transition);if(r(a)||1!==t.nodeType)return e();if(!o(t._leaveCb)){var i=a.css,s=a.type,l=a.leaveClass,p=a.leaveToClass,d=a.leaveActiveClass,u=a.beforeLeave,m=a.leave,h=a.afterLeave,g=a.leaveCancelled,v=a.delayLeave,b=a.duration,y=!1!==i&&!Y,_=si(m),E=f(c(b)?b.leave:b);0;var A=t._leaveCb=j((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[n.key]=null),y&&(Ja(t,p),Ja(t,d)),A.cancelled?(y&&Ja(t,l),g&&g(t)):(e(),h&&h(t)),t._leaveCb=null}));v?v(x):x()}function x(){A.cancelled||(!n.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[n.key]=n),u&&u(t),y&&(Ya(t,l),Ya(t,d),Ka((function(){Ja(t,l),A.cancelled||(Ya(t,p),_||(oi(E)?setTimeout(A,E):Qa(t,s,A)))}))),m&&m(t,A),y||_||A())}}function oi(n){return"number"==typeof n&&!isNaN(n)}function si(n){if(r(n))return!1;var e=n.fns;return o(e)?si(Array.isArray(e)?e[0]:e):(n._length||n.length)>1}function li(n,e){!0!==e.data.show&&ii(e)}var ci=function(n){var e,t,a={},i=n.modules,c=n.nodeOps;for(e=0;e<ea.length;++e)for(a[ea[e]]=[],t=0;t<i.length;++t)o(i[t][ea[e]])&&a[ea[e]].push(i[t][ea[e]]);function p(n){var e=c.parentNode(n);o(e)&&c.removeChild(e,n)}function d(n,e,t,i,r,l,p){if(o(n.elm)&&o(l)&&(n=l[p]=_n(n)),n.isRootInsert=!r,!function(n,e,t,i){var r=n.data;if(o(r)){var l=o(n.componentInstance)&&r.keepAlive;if(o(r=r.hook)&&o(r=r.init)&&r(n,!1),o(n.componentInstance))return u(n,e),m(t,n.elm,i),s(l)&&function(n,e,t,i){var r,s=n;for(;s.componentInstance;)if(s=s.componentInstance._vnode,o(r=s.data)&&o(r=r.transition)){for(r=0;r<a.activate.length;++r)a.activate[r](na,s);e.push(s);break}m(t,n.elm,i)}(n,e,t,i),!0}}(n,e,t,i)){var d=n.data,g=n.children,v=n.tag;o(v)?(n.elm=n.ns?c.createElementNS(n.ns,v):c.createElement(v,n),b(n),h(n,g,e),o(d)&&f(n,e),m(t,n.elm,i)):s(n.isComment)?(n.elm=c.createComment(n.text),m(t,n.elm,i)):(n.elm=c.createTextNode(n.text),m(t,n.elm,i))}}function u(n,e){o(n.data.pendingInsert)&&(e.push.apply(e,n.data.pendingInsert),n.data.pendingInsert=null),n.elm=n.componentInstance.$el,g(n)?(f(n,e),b(n)):(Qt(n),e.push(n))}function m(n,e,t){o(n)&&(o(t)?c.parentNode(t)===n&&c.insertBefore(n,e,t):c.appendChild(n,e))}function h(n,e,t){if(Array.isArray(e)){0;for(var a=0;a<e.length;++a)d(e[a],t,n.elm,null,!0,e,a)}else l(n.text)&&c.appendChild(n.elm,c.createTextNode(String(n.text)))}function g(n){for(;n.componentInstance;)n=n.componentInstance._vnode;return o(n.tag)}function f(n,t){for(var i=0;i<a.create.length;++i)a.create[i](na,n);o(e=n.data.hook)&&(o(e.create)&&e.create(na,n),o(e.insert)&&t.push(n))}function b(n){var e;if(o(e=n.fnScopeId))c.setStyleScope(n.elm,e);else for(var t=n;t;)o(e=t.context)&&o(e=e.$options._scopeId)&&c.setStyleScope(n.elm,e),t=t.parent;o(e=Ye)&&e!==n.context&&e!==n.fnContext&&o(e=e.$options._scopeId)&&c.setStyleScope(n.elm,e)}function y(n,e,t,a,i,r){for(;a<=i;++a)d(t[a],r,n,e,!1,t,a)}function _(n){var e,t,i=n.data;if(o(i))for(o(e=i.hook)&&o(e=e.destroy)&&e(n),e=0;e<a.destroy.length;++e)a.destroy[e](n);if(o(e=n.children))for(t=0;t<n.children.length;++t)_(n.children[t])}function E(n,e,t){for(;e<=t;++e){var a=n[e];o(a)&&(o(a.tag)?(A(a),_(a)):p(a.elm))}}function A(n,e){if(o(e)||o(n.data)){var t,i=a.remove.length+1;for(o(e)?e.listeners+=i:e=function(n,e){function t(){0==--t.listeners&&p(n)}return t.listeners=e,t}(n.elm,i),o(t=n.componentInstance)&&o(t=t._vnode)&&o(t.data)&&A(t,e),t=0;t<a.remove.length;++t)a.remove[t](n,e);o(t=n.data.hook)&&o(t=t.remove)?t(n,e):e()}else p(n.elm)}function x(n,e,t,a){for(var i=t;i<a;i++){var r=e[i];if(o(r)&&ta(n,r))return i}}function k(n,e,t,i,l,p){if(n!==e){o(e.elm)&&o(i)&&(e=i[l]=_n(e));var u=e.elm=n.elm;if(s(n.isAsyncPlaceholder))o(e.asyncFactory.resolved)?S(n.elm,e,t):e.isAsyncPlaceholder=!0;else if(s(e.isStatic)&&s(n.isStatic)&&e.key===n.key&&(s(e.isCloned)||s(e.isOnce)))e.componentInstance=n.componentInstance;else{var m,h=e.data;o(h)&&o(m=h.hook)&&o(m=m.prepatch)&&m(n,e);var f=n.children,v=e.children;if(o(h)&&g(e)){for(m=0;m<a.update.length;++m)a.update[m](n,e);o(m=h.hook)&&o(m=m.update)&&m(n,e)}r(e.text)?o(f)&&o(v)?f!==v&&function(n,e,t,a,i){var s,l,p,u=0,m=0,h=e.length-1,g=e[0],f=e[h],v=t.length-1,b=t[0],_=t[v],A=!i;for(0;u<=h&&m<=v;)r(g)?g=e[++u]:r(f)?f=e[--h]:ta(g,b)?(k(g,b,a,t,m),g=e[++u],b=t[++m]):ta(f,_)?(k(f,_,a,t,v),f=e[--h],_=t[--v]):ta(g,_)?(k(g,_,a,t,v),A&&c.insertBefore(n,g.elm,c.nextSibling(f.elm)),g=e[++u],_=t[--v]):ta(f,b)?(k(f,b,a,t,m),A&&c.insertBefore(n,f.elm,g.elm),f=e[--h],b=t[++m]):(r(s)&&(s=aa(e,u,h)),r(l=o(b.key)?s[b.key]:x(b,e,u,h))?d(b,a,n,g.elm,!1,t,m):ta(p=e[l],b)?(k(p,b,a,t,m),e[l]=void 0,A&&c.insertBefore(n,p.elm,g.elm)):d(b,a,n,g.elm,!1,t,m),b=t[++m]);u>h?y(n,r(t[v+1])?null:t[v+1].elm,t,m,v,a):m>v&&E(e,u,h)}(u,f,v,t,p):o(v)?(o(n.text)&&c.setTextContent(u,""),y(u,null,v,0,v.length-1,t)):o(f)?E(f,0,f.length-1):o(n.text)&&c.setTextContent(u,""):n.text!==e.text&&c.setTextContent(u,e.text),o(h)&&o(m=h.hook)&&o(m=m.postpatch)&&m(n,e)}}}function w(n,e,t){if(s(t)&&o(n.parent))n.parent.data.pendingInsert=e;else for(var a=0;a<e.length;++a)e[a].data.hook.insert(e[a])}var C=v("attrs,class,staticClass,staticStyle,key");function S(n,e,t,a){var i,r=e.tag,l=e.data,c=e.children;if(a=a||l&&l.pre,e.elm=n,s(e.isComment)&&o(e.asyncFactory))return e.isAsyncPlaceholder=!0,!0;if(o(l)&&(o(i=l.hook)&&o(i=i.init)&&i(e,!0),o(i=e.componentInstance)))return u(e,t),!0;if(o(r)){if(o(c))if(n.hasChildNodes())if(o(i=l)&&o(i=i.domProps)&&o(i=i.innerHTML)){if(i!==n.innerHTML)return!1}else{for(var p=!0,d=n.firstChild,m=0;m<c.length;m++){if(!d||!S(d,c[m],t,a)){p=!1;break}d=d.nextSibling}if(!p||d)return!1}else h(e,c,t);if(o(l)){var g=!1;for(var v in l)if(!C(v)){g=!0,f(e,t);break}!g&&l.class&&re(l.class)}}else n.data!==e.text&&(n.data=e.text);return!0}return function(n,e,t,i){if(!r(e)){var l,p=!1,u=[];if(r(n))p=!0,d(e,u);else{var m=o(n.nodeType);if(!m&&ta(n,e))k(n,e,u,null,null,i);else{if(m){if(1===n.nodeType&&n.hasAttribute("data-server-rendered")&&(n.removeAttribute("data-server-rendered"),t=!0),s(t)&&S(n,e,u))return w(e,u,!0),n;l=n,n=new fn(c.tagName(l).toLowerCase(),{},[],void 0,l)}var h=n.elm,f=c.parentNode(h);if(d(e,u,h._leaveCb?null:f,c.nextSibling(h)),o(e.parent))for(var v=e.parent,b=g(e);v;){for(var y=0;y<a.destroy.length;++y)a.destroy[y](v);if(v.elm=e.elm,b){for(var A=0;A<a.create.length;++A)a.create[A](na,v);var x=v.data.hook.insert;if(x.merged)for(var C=1;C<x.fns.length;C++)x.fns[C]()}else Qt(v);v=v.parent}o(f)?E([n],0,0):o(n.tag)&&_(n)}}return w(e,u,p),e.elm}o(n)&&_(n)}}({nodeOps:Yt,modules:[ha,va,ka,Sa,Ma,q?{create:li,activate:li,remove:function(n,e){!0!==n.data.show?ri(n,e):e()}}:{}].concat(pa)});Y&&document.addEventListener("selectionchange",(function(){var n=document.activeElement;n&&n.vmodel&&vi(n,"input")}));var pi={inserted:function(n,e,t,a){"select"===t.tag?(a.elm&&!a.elm._vOptions?ce(t,"postpatch",(function(){pi.componentUpdated(n,e,t)})):di(n,e,t.context),n._vOptions=[].map.call(n.options,hi)):("textarea"===t.tag||Kt(n.type))&&(n._vModifiers=e.modifiers,e.modifiers.lazy||(n.addEventListener("compositionstart",gi),n.addEventListener("compositionend",fi),n.addEventListener("change",fi),Y&&(n.vmodel=!0)))},componentUpdated:function(n,e,t){if("select"===t.tag){di(n,e,t.context);var a=n._vOptions,i=n._vOptions=[].map.call(n.options,hi);if(i.some((function(n,e){return!N(n,a[e])})))(n.multiple?e.value.some((function(n){return mi(n,i)})):e.value!==e.oldValue&&mi(e.value,i))&&vi(n,"change")}}};function di(n,e,t){ui(n,e,t),(K||J)&&setTimeout((function(){ui(n,e,t)}),0)}function ui(n,e,t){var a=e.value,i=n.multiple;if(!i||Array.isArray(a)){for(var r,o,s=0,l=n.options.length;s<l;s++)if(o=n.options[s],i)r=F(a,hi(o))>-1,o.selected!==r&&(o.selected=r);else if(N(hi(o),a))return void(n.selectedIndex!==s&&(n.selectedIndex=s));i||(n.selectedIndex=-1)}}function mi(n,e){return e.every((function(e){return!N(e,n)}))}function hi(n){return"_value"in n?n._value:n.value}function gi(n){n.target.composing=!0}function fi(n){n.target.composing&&(n.target.composing=!1,vi(n.target,"input"))}function vi(n,e){var t=document.createEvent("HTMLEvents");t.initEvent(e,!0,!0),n.dispatchEvent(t)}function bi(n){return!n.componentInstance||n.data&&n.data.transition?n:bi(n.componentInstance._vnode)}var yi={model:pi,show:{bind:function(n,e,t){var a=e.value,i=(t=bi(t)).data&&t.data.transition,r=n.__vOriginalDisplay="none"===n.style.display?"":n.style.display;a&&i?(t.data.show=!0,ii(t,(function(){n.style.display=r}))):n.style.display=a?r:"none"},update:function(n,e,t){var a=e.value;!a!=!e.oldValue&&((t=bi(t)).data&&t.data.transition?(t.data.show=!0,a?ii(t,(function(){n.style.display=n.__vOriginalDisplay})):ri(t,(function(){n.style.display="none"}))):n.style.display=a?n.__vOriginalDisplay:"none")},unbind:function(n,e,t,a,i){i||(n.style.display=n.__vOriginalDisplay)}}},_i={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function Ei(n){var e=n&&n.componentOptions;return e&&e.Ctor.options.abstract?Ei(qe(e.children)):n}function Ai(n){var e={},t=n.$options;for(var a in t.propsData)e[a]=n[a];var i=t._parentListeners;for(var r in i)e[k(r)]=i[r];return e}function xi(n,e){if(/\d-keep-alive$/.test(e.tag))return n("keep-alive",{props:e.componentOptions.propsData})}var ki=function(n){return n.tag||He(n)},wi=function(n){return"show"===n.name},Ci={name:"transition",props:_i,abstract:!0,render:function(n){var e=this,t=this.$slots.default;if(t&&(t=t.filter(ki)).length){0;var a=this.mode;0;var i=t[0];if(function(n){for(;n=n.parent;)if(n.data.transition)return!0}(this.$vnode))return i;var r=Ei(i);if(!r)return i;if(this._leaving)return xi(n,i);var o="__transition-"+this._uid+"-";r.key=null==r.key?r.isComment?o+"comment":o+r.tag:l(r.key)?0===String(r.key).indexOf(o)?r.key:o+r.key:r.key;var s=(r.data||(r.data={})).transition=Ai(this),c=this._vnode,p=Ei(c);if(r.data.directives&&r.data.directives.some(wi)&&(r.data.show=!0),p&&p.data&&!function(n,e){return e.key===n.key&&e.tag===n.tag}(r,p)&&!He(p)&&(!p.componentInstance||!p.componentInstance._vnode.isComment)){var d=p.data.transition=P({},s);if("out-in"===a)return this._leaving=!0,ce(d,"afterLeave",(function(){e._leaving=!1,e.$forceUpdate()})),xi(n,i);if("in-out"===a){if(He(r))return c;var u,m=function(){u()};ce(s,"afterEnter",m),ce(s,"enterCancelled",m),ce(d,"delayLeave",(function(n){u=n}))}}return i}}},Si=P({tag:String,moveClass:String},_i);function Bi(n){n.elm._moveCb&&n.elm._moveCb(),n.elm._enterCb&&n.elm._enterCb()}function Ti(n){n.data.newPos=n.elm.getBoundingClientRect()}function Pi(n){var e=n.data.pos,t=n.data.newPos,a=e.left-t.left,i=e.top-t.top;if(a||i){n.data.moved=!0;var r=n.elm.style;r.transform=r.WebkitTransform="translate("+a+"px,"+i+"px)",r.transitionDuration="0s"}}delete Si.mode;var Di={Transition:Ci,TransitionGroup:{props:Si,beforeMount:function(){var n=this,e=this._update;this._update=function(t,a){var i=Je(n);n.__patch__(n._vnode,n.kept,!1,!0),n._vnode=n.kept,i(),e.call(n,t,a)}},render:function(n){for(var e=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),a=this.prevChildren=this.children,i=this.$slots.default||[],r=this.children=[],o=Ai(this),s=0;s<i.length;s++){var l=i[s];if(l.tag)if(null!=l.key&&0!==String(l.key).indexOf("__vlist"))r.push(l),t[l.key]=l,(l.data||(l.data={})).transition=o;else;}if(a){for(var c=[],p=[],d=0;d<a.length;d++){var u=a[d];u.data.transition=o,u.data.pos=u.elm.getBoundingClientRect(),t[u.key]?c.push(u):p.push(u)}this.kept=n(e,null,c),this.removed=p}return n(e,null,r)},updated:function(){var n=this.prevChildren,e=this.moveClass||(this.name||"v")+"-move";n.length&&this.hasMove(n[0].elm,e)&&(n.forEach(Bi),n.forEach(Ti),n.forEach(Pi),this._reflow=document.body.offsetHeight,n.forEach((function(n){if(n.data.moved){var t=n.elm,a=t.style;Ya(t,e),a.transform=a.WebkitTransform=a.transitionDuration="",t.addEventListener(qa,t._moveCb=function n(a){a&&a.target!==t||a&&!/transform$/.test(a.propertyName)||(t.removeEventListener(qa,n),t._moveCb=null,Ja(t,e))})}})))},methods:{hasMove:function(n,e){if(!Wa)return!1;if(this._hasMove)return this._hasMove;var t=n.cloneNode();n._transitionClasses&&n._transitionClasses.forEach((function(n){Ua(t,n)})),Oa(t,e),t.style.display="none",this.$el.appendChild(t);var a=ei(t);return this.$el.removeChild(t),this._hasMove=a.hasTransform}}}};kt.config.mustUseProp=function(n,e,t){return"value"===t&&Lt(n)&&"button"!==e||"selected"===t&&"option"===n||"checked"===t&&"input"===n||"muted"===t&&"video"===n},kt.config.isReservedTag=Xt,kt.config.isReservedAttr=zt,kt.config.getTagNamespace=function(n){return $t(n)?"svg":"math"===n?"math":void 0},kt.config.isUnknownElement=function(n){if(!q)return!0;if(Xt(n))return!1;if(n=n.toLowerCase(),null!=Zt[n])return Zt[n];var e=document.createElement(n);return n.indexOf("-")>-1?Zt[n]=e.constructor===window.HTMLUnknownElement||e.constructor===window.HTMLElement:Zt[n]=/HTMLUnknownElement/.test(e.toString())},P(kt.options.directives,yi),P(kt.options.components,Di),kt.prototype.__patch__=q?ci:z,kt.prototype.$mount=function(n,e){return function(n,e,t){var a;return n.$el=e,n.$options.render||(n.$options.render=bn),et(n,"beforeMount"),a=function(){n._update(n._render(),t)},new mt(n,a,z,{before:function(){n._isMounted&&!n._isDestroyed&&et(n,"beforeUpdate")}},!0),t=!1,null==n.$vnode&&(n._isMounted=!0,et(n,"mounted")),n}(this,n=n&&q?function(n){if("string"==typeof n){var e=document.querySelector(n);return e||document.createElement("div")}return n}(n):void 0,e)},q&&setTimeout((function(){O.devtools&&on&&on.emit("init",kt)}),0);var zi=kt;
/*!
  * vue-router v3.5.1
  * (c) 2021 Evan You
  * @license MIT
  */function Li(n,e){for(var t in e)n[t]=e[t];return n}var Ii=/[!'()*]/g,Ni=function(n){return"%"+n.charCodeAt(0).toString(16)},Fi=/%2C/g,ji=function(n){return encodeURIComponent(n).replace(Ii,Ni).replace(Fi,",")};function Mi(n){try{return decodeURIComponent(n)}catch(n){0}return n}var Ri=function(n){return null==n||"object"==typeof n?n:String(n)};function Oi(n){var e={};return(n=n.trim().replace(/^(\?|#|&)/,""))?(n.split("&").forEach((function(n){var t=n.replace(/\+/g," ").split("="),a=Mi(t.shift()),i=t.length>0?Mi(t.join("=")):null;void 0===e[a]?e[a]=i:Array.isArray(e[a])?e[a].push(i):e[a]=[e[a],i]})),e):e}function Ui(n){var e=n?Object.keys(n).map((function(e){var t=n[e];if(void 0===t)return"";if(null===t)return ji(e);if(Array.isArray(t)){var a=[];return t.forEach((function(n){void 0!==n&&(null===n?a.push(ji(e)):a.push(ji(e)+"="+ji(n)))})),a.join("&")}return ji(e)+"="+ji(t)})).filter((function(n){return n.length>0})).join("&"):null;return e?"?"+e:""}var Vi=/\/?$/;function Gi(n,e,t,a){var i=a&&a.options.stringifyQuery,r=e.query||{};try{r=Wi(r)}catch(n){}var o={name:e.name||n&&n.name,meta:n&&n.meta||{},path:e.path||"/",hash:e.hash||"",query:r,params:e.params||{},fullPath:$i(e,i),matched:n?qi(n):[]};return t&&(o.redirectedFrom=$i(t,i)),Object.freeze(o)}function Wi(n){if(Array.isArray(n))return n.map(Wi);if(n&&"object"==typeof n){var e={};for(var t in n)e[t]=Wi(n[t]);return e}return n}var Hi=Gi(null,{path:"/"});function qi(n){for(var e=[];n;)e.unshift(n),n=n.parent;return e}function $i(n,e){var t=n.path,a=n.query;void 0===a&&(a={});var i=n.hash;return void 0===i&&(i=""),(t||"/")+(e||Ui)(a)+i}function Xi(n,e,t){return e===Hi?n===e:!!e&&(n.path&&e.path?n.path.replace(Vi,"")===e.path.replace(Vi,"")&&(t||n.hash===e.hash&&Zi(n.query,e.query)):!(!n.name||!e.name)&&(n.name===e.name&&(t||n.hash===e.hash&&Zi(n.query,e.query)&&Zi(n.params,e.params))))}function Zi(n,e){if(void 0===n&&(n={}),void 0===e&&(e={}),!n||!e)return n===e;var t=Object.keys(n).sort(),a=Object.keys(e).sort();return t.length===a.length&&t.every((function(t,i){var r=n[t];if(a[i]!==t)return!1;var o=e[t];return null==r||null==o?r===o:"object"==typeof r&&"object"==typeof o?Zi(r,o):String(r)===String(o)}))}function Ki(n){for(var e=0;e<n.matched.length;e++){var t=n.matched[e];for(var a in t.instances){var i=t.instances[a],r=t.enteredCbs[a];if(i&&r){delete t.enteredCbs[a];for(var o=0;o<r.length;o++)i._isBeingDestroyed||r[o](i)}}}}var Yi={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(n,e){var t=e.props,a=e.children,i=e.parent,r=e.data;r.routerView=!0;for(var o=i.$createElement,s=t.name,l=i.$route,c=i._routerViewCache||(i._routerViewCache={}),p=0,d=!1;i&&i._routerRoot!==i;){var u=i.$vnode?i.$vnode.data:{};u.routerView&&p++,u.keepAlive&&i._directInactive&&i._inactive&&(d=!0),i=i.$parent}if(r.routerViewDepth=p,d){var m=c[s],h=m&&m.component;return h?(m.configProps&&Ji(h,r,m.route,m.configProps),o(h,r,a)):o()}var g=l.matched[p],f=g&&g.components[s];if(!g||!f)return c[s]=null,o();c[s]={component:f},r.registerRouteInstance=function(n,e){var t=g.instances[s];(e&&t!==n||!e&&t===n)&&(g.instances[s]=e)},(r.hook||(r.hook={})).prepatch=function(n,e){g.instances[s]=e.componentInstance},r.hook.init=function(n){n.data.keepAlive&&n.componentInstance&&n.componentInstance!==g.instances[s]&&(g.instances[s]=n.componentInstance),Ki(l)};var v=g.props&&g.props[s];return v&&(Li(c[s],{route:l,configProps:v}),Ji(f,r,l,v)),o(f,r,a)}};function Ji(n,e,t,a){var i=e.props=function(n,e){switch(typeof e){case"undefined":return;case"object":return e;case"function":return e(n);case"boolean":return e?n.params:void 0;default:0}}(t,a);if(i){i=e.props=Li({},i);var r=e.attrs=e.attrs||{};for(var o in i)n.props&&o in n.props||(r[o]=i[o],delete i[o])}}function Qi(n,e,t){var a=n.charAt(0);if("/"===a)return n;if("?"===a||"#"===a)return e+n;var i=e.split("/");t&&i[i.length-1]||i.pop();for(var r=n.replace(/^\//,"").split("/"),o=0;o<r.length;o++){var s=r[o];".."===s?i.pop():"."!==s&&i.push(s)}return""!==i[0]&&i.unshift(""),i.join("/")}function nr(n){return n.replace(/\/\//g,"/")}var er=Array.isArray||function(n){return"[object Array]"==Object.prototype.toString.call(n)},tr=fr,ar=lr,ir=function(n,e){return pr(lr(n,e),e)},rr=pr,or=gr,sr=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function lr(n,e){for(var t,a=[],i=0,r=0,o="",s=e&&e.delimiter||"/";null!=(t=sr.exec(n));){var l=t[0],c=t[1],p=t.index;if(o+=n.slice(r,p),r=p+l.length,c)o+=c[1];else{var d=n[r],u=t[2],m=t[3],h=t[4],g=t[5],f=t[6],v=t[7];o&&(a.push(o),o="");var b=null!=u&&null!=d&&d!==u,y="+"===f||"*"===f,_="?"===f||"*"===f,E=t[2]||s,A=h||g;a.push({name:m||i++,prefix:u||"",delimiter:E,optional:_,repeat:y,partial:b,asterisk:!!v,pattern:A?ur(A):v?".*":"[^"+dr(E)+"]+?"})}}return r<n.length&&(o+=n.substr(r)),o&&a.push(o),a}function cr(n){return encodeURI(n).replace(/[\/?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()}))}function pr(n,e){for(var t=new Array(n.length),a=0;a<n.length;a++)"object"==typeof n[a]&&(t[a]=new RegExp("^(?:"+n[a].pattern+")$",hr(e)));return function(e,a){for(var i="",r=e||{},o=(a||{}).pretty?cr:encodeURIComponent,s=0;s<n.length;s++){var l=n[s];if("string"!=typeof l){var c,p=r[l.name];if(null==p){if(l.optional){l.partial&&(i+=l.prefix);continue}throw new TypeError('Expected "'+l.name+'" to be defined')}if(er(p)){if(!l.repeat)throw new TypeError('Expected "'+l.name+'" to not repeat, but received `'+JSON.stringify(p)+"`");if(0===p.length){if(l.optional)continue;throw new TypeError('Expected "'+l.name+'" to not be empty')}for(var d=0;d<p.length;d++){if(c=o(p[d]),!t[s].test(c))throw new TypeError('Expected all "'+l.name+'" to match "'+l.pattern+'", but received `'+JSON.stringify(c)+"`");i+=(0===d?l.prefix:l.delimiter)+c}}else{if(c=l.asterisk?encodeURI(p).replace(/[?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()})):o(p),!t[s].test(c))throw new TypeError('Expected "'+l.name+'" to match "'+l.pattern+'", but received "'+c+'"');i+=l.prefix+c}}else i+=l}return i}}function dr(n){return n.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function ur(n){return n.replace(/([=!:$\/()])/g,"\\$1")}function mr(n,e){return n.keys=e,n}function hr(n){return n&&n.sensitive?"":"i"}function gr(n,e,t){er(e)||(t=e||t,e=[]);for(var a=(t=t||{}).strict,i=!1!==t.end,r="",o=0;o<n.length;o++){var s=n[o];if("string"==typeof s)r+=dr(s);else{var l=dr(s.prefix),c="(?:"+s.pattern+")";e.push(s),s.repeat&&(c+="(?:"+l+c+")*"),r+=c=s.optional?s.partial?l+"("+c+")?":"(?:"+l+"("+c+"))?":l+"("+c+")"}}var p=dr(t.delimiter||"/"),d=r.slice(-p.length)===p;return a||(r=(d?r.slice(0,-p.length):r)+"(?:"+p+"(?=$))?"),r+=i?"$":a&&d?"":"(?="+p+"|$)",mr(new RegExp("^"+r,hr(t)),e)}function fr(n,e,t){return er(e)||(t=e||t,e=[]),t=t||{},n instanceof RegExp?function(n,e){var t=n.source.match(/\((?!\?)/g);if(t)for(var a=0;a<t.length;a++)e.push({name:a,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return mr(n,e)}(n,e):er(n)?function(n,e,t){for(var a=[],i=0;i<n.length;i++)a.push(fr(n[i],e,t).source);return mr(new RegExp("(?:"+a.join("|")+")",hr(t)),e)}(n,e,t):function(n,e,t){return gr(lr(n,t),e,t)}(n,e,t)}tr.parse=ar,tr.compile=ir,tr.tokensToFunction=rr,tr.tokensToRegExp=or;var vr=Object.create(null);function br(n,e,t){e=e||{};try{var a=vr[n]||(vr[n]=tr.compile(n));return"string"==typeof e.pathMatch&&(e[0]=e.pathMatch),a(e,{pretty:!0})}catch(n){return""}finally{delete e[0]}}function yr(n,e,t,a){var i="string"==typeof n?{path:n}:n;if(i._normalized)return i;if(i.name){var r=(i=Li({},n)).params;return r&&"object"==typeof r&&(i.params=Li({},r)),i}if(!i.path&&i.params&&e){(i=Li({},i))._normalized=!0;var o=Li(Li({},e.params),i.params);if(e.name)i.name=e.name,i.params=o;else if(e.matched.length){var s=e.matched[e.matched.length-1].path;i.path=br(s,o,e.path)}else 0;return i}var l=function(n){var e="",t="",a=n.indexOf("#");a>=0&&(e=n.slice(a),n=n.slice(0,a));var i=n.indexOf("?");return i>=0&&(t=n.slice(i+1),n=n.slice(0,i)),{path:n,query:t,hash:e}}(i.path||""),c=e&&e.path||"/",p=l.path?Qi(l.path,c,t||i.append):c,d=function(n,e,t){void 0===e&&(e={});var a,i=t||Oi;try{a=i(n||"")}catch(n){a={}}for(var r in e){var o=e[r];a[r]=Array.isArray(o)?o.map(Ri):Ri(o)}return a}(l.query,i.query,a&&a.options.parseQuery),u=i.hash||l.hash;return u&&"#"!==u.charAt(0)&&(u="#"+u),{_normalized:!0,path:p,query:d,hash:u}}var _r,Er=function(){},Ar={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(n){var e=this,t=this.$router,a=this.$route,i=t.resolve(this.to,a,this.append),r=i.location,o=i.route,s=i.href,l={},c=t.options.linkActiveClass,p=t.options.linkExactActiveClass,d=null==c?"router-link-active":c,u=null==p?"router-link-exact-active":p,m=null==this.activeClass?d:this.activeClass,h=null==this.exactActiveClass?u:this.exactActiveClass,g=o.redirectedFrom?Gi(null,yr(o.redirectedFrom),null,t):o;l[h]=Xi(a,g,this.exactPath),l[m]=this.exact||this.exactPath?l[h]:function(n,e){return 0===n.path.replace(Vi,"/").indexOf(e.path.replace(Vi,"/"))&&(!e.hash||n.hash===e.hash)&&function(n,e){for(var t in e)if(!(t in n))return!1;return!0}(n.query,e.query)}(a,g);var f=l[h]?this.ariaCurrentValue:null,v=function(n){xr(n)&&(e.replace?t.replace(r,Er):t.push(r,Er))},b={click:xr};Array.isArray(this.event)?this.event.forEach((function(n){b[n]=v})):b[this.event]=v;var y={class:l},_=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:s,route:o,navigate:v,isActive:l[m],isExactActive:l[h]});if(_){if(1===_.length)return _[0];if(_.length>1||!_.length)return 0===_.length?n():n("span",{},_)}if("a"===this.tag)y.on=b,y.attrs={href:s,"aria-current":f};else{var E=function n(e){var t;if(e)for(var a=0;a<e.length;a++){if("a"===(t=e[a]).tag)return t;if(t.children&&(t=n(t.children)))return t}}(this.$slots.default);if(E){E.isStatic=!1;var A=E.data=Li({},E.data);for(var x in A.on=A.on||{},A.on){var k=A.on[x];x in b&&(A.on[x]=Array.isArray(k)?k:[k])}for(var w in b)w in A.on?A.on[w].push(b[w]):A.on[w]=v;var C=E.data.attrs=Li({},E.data.attrs);C.href=s,C["aria-current"]=f}else y.on=b}return n(this.tag,y,this.$slots.default)}};function xr(n){if(!(n.metaKey||n.altKey||n.ctrlKey||n.shiftKey||n.defaultPrevented||void 0!==n.button&&0!==n.button)){if(n.currentTarget&&n.currentTarget.getAttribute){var e=n.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(e))return}return n.preventDefault&&n.preventDefault(),!0}}var kr="undefined"!=typeof window;function wr(n,e,t,a,i){var r=e||[],o=t||Object.create(null),s=a||Object.create(null);n.forEach((function(n){!function n(e,t,a,i,r,o){var s=i.path,l=i.name;0;var c=i.pathToRegexpOptions||{},p=function(n,e,t){t||(n=n.replace(/\/$/,""));if("/"===n[0])return n;if(null==e)return n;return nr(e.path+"/"+n)}(s,r,c.strict);"boolean"==typeof i.caseSensitive&&(c.sensitive=i.caseSensitive);var d={path:p,regex:Cr(p,c),components:i.components||{default:i.component},alias:i.alias?"string"==typeof i.alias?[i.alias]:i.alias:[],instances:{},enteredCbs:{},name:l,parent:r,matchAs:o,redirect:i.redirect,beforeEnter:i.beforeEnter,meta:i.meta||{},props:null==i.props?{}:i.components?i.props:{default:i.props}};i.children&&i.children.forEach((function(i){var r=o?nr(o+"/"+i.path):void 0;n(e,t,a,i,d,r)}));t[d.path]||(e.push(d.path),t[d.path]=d);if(void 0!==i.alias)for(var u=Array.isArray(i.alias)?i.alias:[i.alias],m=0;m<u.length;++m){0;var h={path:u[m],children:i.children};n(e,t,a,h,r,d.path||"/")}l&&(a[l]||(a[l]=d))}(r,o,s,n,i)}));for(var l=0,c=r.length;l<c;l++)"*"===r[l]&&(r.push(r.splice(l,1)[0]),c--,l--);return{pathList:r,pathMap:o,nameMap:s}}function Cr(n,e){return tr(n,[],e)}function Sr(n,e){var t=wr(n),a=t.pathList,i=t.pathMap,r=t.nameMap;function o(n,t,o){var s=yr(n,t,!1,e),c=s.name;if(c){var p=r[c];if(!p)return l(null,s);var d=p.regex.keys.filter((function(n){return!n.optional})).map((function(n){return n.name}));if("object"!=typeof s.params&&(s.params={}),t&&"object"==typeof t.params)for(var u in t.params)!(u in s.params)&&d.indexOf(u)>-1&&(s.params[u]=t.params[u]);return s.path=br(p.path,s.params),l(p,s,o)}if(s.path){s.params={};for(var m=0;m<a.length;m++){var h=a[m],g=i[h];if(Br(g.regex,s.path,s.params))return l(g,s,o)}}return l(null,s)}function s(n,t){var a=n.redirect,i="function"==typeof a?a(Gi(n,t,null,e)):a;if("string"==typeof i&&(i={path:i}),!i||"object"!=typeof i)return l(null,t);var s=i,c=s.name,p=s.path,d=t.query,u=t.hash,m=t.params;if(d=s.hasOwnProperty("query")?s.query:d,u=s.hasOwnProperty("hash")?s.hash:u,m=s.hasOwnProperty("params")?s.params:m,c){r[c];return o({_normalized:!0,name:c,query:d,hash:u,params:m},void 0,t)}if(p){var h=function(n,e){return Qi(n,e.parent?e.parent.path:"/",!0)}(p,n);return o({_normalized:!0,path:br(h,m),query:d,hash:u},void 0,t)}return l(null,t)}function l(n,t,a){return n&&n.redirect?s(n,a||t):n&&n.matchAs?function(n,e,t){var a=o({_normalized:!0,path:br(t,e.params)});if(a){var i=a.matched,r=i[i.length-1];return e.params=a.params,l(r,e)}return l(null,e)}(0,t,n.matchAs):Gi(n,t,a,e)}return{match:o,addRoute:function(n,e){var t="object"!=typeof n?r[n]:void 0;wr([e||n],a,i,r,t),t&&wr(t.alias.map((function(n){return{path:n,children:[e]}})),a,i,r,t)},getRoutes:function(){return a.map((function(n){return i[n]}))},addRoutes:function(n){wr(n,a,i,r)}}}function Br(n,e,t){var a=e.match(n);if(!a)return!1;if(!t)return!0;for(var i=1,r=a.length;i<r;++i){var o=n.keys[i-1];o&&(t[o.name||"pathMatch"]="string"==typeof a[i]?Mi(a[i]):a[i])}return!0}var Tr=kr&&window.performance&&window.performance.now?window.performance:Date;function Pr(){return Tr.now().toFixed(3)}var Dr=Pr();function zr(){return Dr}function Lr(n){return Dr=n}var Ir=Object.create(null);function Nr(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var n=window.location.protocol+"//"+window.location.host,e=window.location.href.replace(n,""),t=Li({},window.history.state);return t.key=zr(),window.history.replaceState(t,"",e),window.addEventListener("popstate",Mr),function(){window.removeEventListener("popstate",Mr)}}function Fr(n,e,t,a){if(n.app){var i=n.options.scrollBehavior;i&&n.app.$nextTick((function(){var r=function(){var n=zr();if(n)return Ir[n]}(),o=i.call(n,e,t,a?r:null);o&&("function"==typeof o.then?o.then((function(n){Gr(n,r)})).catch((function(n){0})):Gr(o,r))}))}}function jr(){var n=zr();n&&(Ir[n]={x:window.pageXOffset,y:window.pageYOffset})}function Mr(n){jr(),n.state&&n.state.key&&Lr(n.state.key)}function Rr(n){return Ur(n.x)||Ur(n.y)}function Or(n){return{x:Ur(n.x)?n.x:window.pageXOffset,y:Ur(n.y)?n.y:window.pageYOffset}}function Ur(n){return"number"==typeof n}var Vr=/^#\d/;function Gr(n,e){var t,a="object"==typeof n;if(a&&"string"==typeof n.selector){var i=Vr.test(n.selector)?document.getElementById(n.selector.slice(1)):document.querySelector(n.selector);if(i){var r=n.offset&&"object"==typeof n.offset?n.offset:{};e=function(n,e){var t=document.documentElement.getBoundingClientRect(),a=n.getBoundingClientRect();return{x:a.left-t.left-e.x,y:a.top-t.top-e.y}}(i,r={x:Ur((t=r).x)?t.x:0,y:Ur(t.y)?t.y:0})}else Rr(n)&&(e=Or(n))}else a&&Rr(n)&&(e=Or(n));e&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:e.x,top:e.y,behavior:n.behavior}):window.scrollTo(e.x,e.y))}var Wr,Hr=kr&&((-1===(Wr=window.navigator.userAgent).indexOf("Android 2.")&&-1===Wr.indexOf("Android 4.0")||-1===Wr.indexOf("Mobile Safari")||-1!==Wr.indexOf("Chrome")||-1!==Wr.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function qr(n,e){jr();var t=window.history;try{if(e){var a=Li({},t.state);a.key=zr(),t.replaceState(a,"",n)}else t.pushState({key:Lr(Pr())},"",n)}catch(t){window.location[e?"replace":"assign"](n)}}function $r(n){qr(n,!0)}function Xr(n,e,t){var a=function(i){i>=n.length?t():n[i]?e(n[i],(function(){a(i+1)})):a(i+1)};a(0)}var Zr={redirected:2,aborted:4,cancelled:8,duplicated:16};function Kr(n,e){return Jr(n,e,Zr.redirected,'Redirected when going from "'+n.fullPath+'" to "'+function(n){if("string"==typeof n)return n;if("path"in n)return n.path;var e={};return Qr.forEach((function(t){t in n&&(e[t]=n[t])})),JSON.stringify(e,null,2)}(e)+'" via a navigation guard.')}function Yr(n,e){return Jr(n,e,Zr.cancelled,'Navigation cancelled from "'+n.fullPath+'" to "'+e.fullPath+'" with a new navigation.')}function Jr(n,e,t,a){var i=new Error(a);return i._isRouter=!0,i.from=n,i.to=e,i.type=t,i}var Qr=["params","query","hash"];function no(n){return Object.prototype.toString.call(n).indexOf("Error")>-1}function eo(n,e){return no(n)&&n._isRouter&&(null==e||n.type===e)}function to(n){return function(e,t,a){var i=!1,r=0,o=null;ao(n,(function(n,e,t,s){if("function"==typeof n&&void 0===n.cid){i=!0,r++;var l,c=oo((function(e){var i;((i=e).__esModule||ro&&"Module"===i[Symbol.toStringTag])&&(e=e.default),n.resolved="function"==typeof e?e:_r.extend(e),t.components[s]=e,--r<=0&&a()})),p=oo((function(n){var e="Failed to resolve async component "+s+": "+n;o||(o=no(n)?n:new Error(e),a(o))}));try{l=n(c,p)}catch(n){p(n)}if(l)if("function"==typeof l.then)l.then(c,p);else{var d=l.component;d&&"function"==typeof d.then&&d.then(c,p)}}})),i||a()}}function ao(n,e){return io(n.map((function(n){return Object.keys(n.components).map((function(t){return e(n.components[t],n.instances[t],n,t)}))})))}function io(n){return Array.prototype.concat.apply([],n)}var ro="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function oo(n){var e=!1;return function(){for(var t=[],a=arguments.length;a--;)t[a]=arguments[a];if(!e)return e=!0,n.apply(this,t)}}var so=function(n,e){this.router=n,this.base=function(n){if(!n)if(kr){var e=document.querySelector("base");n=(n=e&&e.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else n="/";"/"!==n.charAt(0)&&(n="/"+n);return n.replace(/\/$/,"")}(e),this.current=Hi,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function lo(n,e,t,a){var i=ao(n,(function(n,a,i,r){var o=function(n,e){"function"!=typeof n&&(n=_r.extend(n));return n.options[e]}(n,e);if(o)return Array.isArray(o)?o.map((function(n){return t(n,a,i,r)})):t(o,a,i,r)}));return io(a?i.reverse():i)}function co(n,e){if(e)return function(){return n.apply(e,arguments)}}so.prototype.listen=function(n){this.cb=n},so.prototype.onReady=function(n,e){this.ready?n():(this.readyCbs.push(n),e&&this.readyErrorCbs.push(e))},so.prototype.onError=function(n){this.errorCbs.push(n)},so.prototype.transitionTo=function(n,e,t){var a,i=this;try{a=this.router.match(n,this.current)}catch(n){throw this.errorCbs.forEach((function(e){e(n)})),n}var r=this.current;this.confirmTransition(a,(function(){i.updateRoute(a),e&&e(a),i.ensureURL(),i.router.afterHooks.forEach((function(n){n&&n(a,r)})),i.ready||(i.ready=!0,i.readyCbs.forEach((function(n){n(a)})))}),(function(n){t&&t(n),n&&!i.ready&&(eo(n,Zr.redirected)&&r===Hi||(i.ready=!0,i.readyErrorCbs.forEach((function(e){e(n)}))))}))},so.prototype.confirmTransition=function(n,e,t){var a=this,i=this.current;this.pending=n;var r,o,s=function(n){!eo(n)&&no(n)&&(a.errorCbs.length?a.errorCbs.forEach((function(e){e(n)})):console.error(n)),t&&t(n)},l=n.matched.length-1,c=i.matched.length-1;if(Xi(n,i)&&l===c&&n.matched[l]===i.matched[c])return this.ensureURL(),s(((o=Jr(r=i,n,Zr.duplicated,'Avoided redundant navigation to current location: "'+r.fullPath+'".')).name="NavigationDuplicated",o));var p=function(n,e){var t,a=Math.max(n.length,e.length);for(t=0;t<a&&n[t]===e[t];t++);return{updated:e.slice(0,t),activated:e.slice(t),deactivated:n.slice(t)}}(this.current.matched,n.matched),d=p.updated,u=p.deactivated,m=p.activated,h=[].concat(function(n){return lo(n,"beforeRouteLeave",co,!0)}(u),this.router.beforeHooks,function(n){return lo(n,"beforeRouteUpdate",co)}(d),m.map((function(n){return n.beforeEnter})),to(m)),g=function(e,t){if(a.pending!==n)return s(Yr(i,n));try{e(n,i,(function(e){!1===e?(a.ensureURL(!0),s(function(n,e){return Jr(n,e,Zr.aborted,'Navigation aborted from "'+n.fullPath+'" to "'+e.fullPath+'" via a navigation guard.')}(i,n))):no(e)?(a.ensureURL(!0),s(e)):"string"==typeof e||"object"==typeof e&&("string"==typeof e.path||"string"==typeof e.name)?(s(Kr(i,n)),"object"==typeof e&&e.replace?a.replace(e):a.push(e)):t(e)}))}catch(n){s(n)}};Xr(h,g,(function(){Xr(function(n){return lo(n,"beforeRouteEnter",(function(n,e,t,a){return function(n,e,t){return function(a,i,r){return n(a,i,(function(n){"function"==typeof n&&(e.enteredCbs[t]||(e.enteredCbs[t]=[]),e.enteredCbs[t].push(n)),r(n)}))}}(n,t,a)}))}(m).concat(a.router.resolveHooks),g,(function(){if(a.pending!==n)return s(Yr(i,n));a.pending=null,e(n),a.router.app&&a.router.app.$nextTick((function(){Ki(n)}))}))}))},so.prototype.updateRoute=function(n){this.current=n,this.cb&&this.cb(n)},so.prototype.setupListeners=function(){},so.prototype.teardown=function(){this.listeners.forEach((function(n){n()})),this.listeners=[],this.current=Hi,this.pending=null};var po=function(n){function e(e,t){n.call(this,e,t),this._startLocation=uo(this.base)}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router,t=e.options.scrollBehavior,a=Hr&&t;a&&this.listeners.push(Nr());var i=function(){var t=n.current,i=uo(n.base);n.current===Hi&&i===n._startLocation||n.transitionTo(i,(function(n){a&&Fr(e,n,t,!0)}))};window.addEventListener("popstate",i),this.listeners.push((function(){window.removeEventListener("popstate",i)}))}},e.prototype.go=function(n){window.history.go(n)},e.prototype.push=function(n,e,t){var a=this,i=this.current;this.transitionTo(n,(function(n){qr(nr(a.base+n.fullPath)),Fr(a.router,n,i,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var a=this,i=this.current;this.transitionTo(n,(function(n){$r(nr(a.base+n.fullPath)),Fr(a.router,n,i,!1),e&&e(n)}),t)},e.prototype.ensureURL=function(n){if(uo(this.base)!==this.current.fullPath){var e=nr(this.base+this.current.fullPath);n?qr(e):$r(e)}},e.prototype.getCurrentLocation=function(){return uo(this.base)},e}(so);function uo(n){var e=window.location.pathname;return n&&0===e.toLowerCase().indexOf(n.toLowerCase())&&(e=e.slice(n.length)),(e||"/")+window.location.search+window.location.hash}var mo=function(n){function e(e,t,a){n.call(this,e,t),a&&function(n){var e=uo(n);if(!/^\/#/.test(e))return window.location.replace(nr(n+"/#"+e)),!0}(this.base)||ho()}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router.options.scrollBehavior,t=Hr&&e;t&&this.listeners.push(Nr());var a=function(){var e=n.current;ho()&&n.transitionTo(go(),(function(a){t&&Fr(n.router,a,e,!0),Hr||bo(a.fullPath)}))},i=Hr?"popstate":"hashchange";window.addEventListener(i,a),this.listeners.push((function(){window.removeEventListener(i,a)}))}},e.prototype.push=function(n,e,t){var a=this,i=this.current;this.transitionTo(n,(function(n){vo(n.fullPath),Fr(a.router,n,i,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var a=this,i=this.current;this.transitionTo(n,(function(n){bo(n.fullPath),Fr(a.router,n,i,!1),e&&e(n)}),t)},e.prototype.go=function(n){window.history.go(n)},e.prototype.ensureURL=function(n){var e=this.current.fullPath;go()!==e&&(n?vo(e):bo(e))},e.prototype.getCurrentLocation=function(){return go()},e}(so);function ho(){var n=go();return"/"===n.charAt(0)||(bo("/"+n),!1)}function go(){var n=window.location.href,e=n.indexOf("#");return e<0?"":n=n.slice(e+1)}function fo(n){var e=window.location.href,t=e.indexOf("#");return(t>=0?e.slice(0,t):e)+"#"+n}function vo(n){Hr?qr(fo(n)):window.location.hash=n}function bo(n){Hr?$r(fo(n)):window.location.replace(fo(n))}var yo=function(n){function e(e,t){n.call(this,e,t),this.stack=[],this.index=-1}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.push=function(n,e,t){var a=this;this.transitionTo(n,(function(n){a.stack=a.stack.slice(0,a.index+1).concat(n),a.index++,e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var a=this;this.transitionTo(n,(function(n){a.stack=a.stack.slice(0,a.index).concat(n),e&&e(n)}),t)},e.prototype.go=function(n){var e=this,t=this.index+n;if(!(t<0||t>=this.stack.length)){var a=this.stack[t];this.confirmTransition(a,(function(){var n=e.current;e.index=t,e.updateRoute(a),e.router.afterHooks.forEach((function(e){e&&e(a,n)}))}),(function(n){eo(n,Zr.duplicated)&&(e.index=t)}))}},e.prototype.getCurrentLocation=function(){var n=this.stack[this.stack.length-1];return n?n.fullPath:"/"},e.prototype.ensureURL=function(){},e}(so),_o=function(n){void 0===n&&(n={}),this.app=null,this.apps=[],this.options=n,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=Sr(n.routes||[],this);var e=n.mode||"hash";switch(this.fallback="history"===e&&!Hr&&!1!==n.fallback,this.fallback&&(e="hash"),kr||(e="abstract"),this.mode=e,e){case"history":this.history=new po(this,n.base);break;case"hash":this.history=new mo(this,n.base,this.fallback);break;case"abstract":this.history=new yo(this,n.base);break;default:0}},Eo={currentRoute:{configurable:!0}};function Ao(n,e){return n.push(e),function(){var t=n.indexOf(e);t>-1&&n.splice(t,1)}}_o.prototype.match=function(n,e,t){return this.matcher.match(n,e,t)},Eo.currentRoute.get=function(){return this.history&&this.history.current},_o.prototype.init=function(n){var e=this;if(this.apps.push(n),n.$once("hook:destroyed",(function(){var t=e.apps.indexOf(n);t>-1&&e.apps.splice(t,1),e.app===n&&(e.app=e.apps[0]||null),e.app||e.history.teardown()})),!this.app){this.app=n;var t=this.history;if(t instanceof po||t instanceof mo){var a=function(n){t.setupListeners(),function(n){var a=t.current,i=e.options.scrollBehavior;Hr&&i&&"fullPath"in n&&Fr(e,n,a,!1)}(n)};t.transitionTo(t.getCurrentLocation(),a,a)}t.listen((function(n){e.apps.forEach((function(e){e._route=n}))}))}},_o.prototype.beforeEach=function(n){return Ao(this.beforeHooks,n)},_o.prototype.beforeResolve=function(n){return Ao(this.resolveHooks,n)},_o.prototype.afterEach=function(n){return Ao(this.afterHooks,n)},_o.prototype.onReady=function(n,e){this.history.onReady(n,e)},_o.prototype.onError=function(n){this.history.onError(n)},_o.prototype.push=function(n,e,t){var a=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){a.history.push(n,e,t)}));this.history.push(n,e,t)},_o.prototype.replace=function(n,e,t){var a=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){a.history.replace(n,e,t)}));this.history.replace(n,e,t)},_o.prototype.go=function(n){this.history.go(n)},_o.prototype.back=function(){this.go(-1)},_o.prototype.forward=function(){this.go(1)},_o.prototype.getMatchedComponents=function(n){var e=n?n.matched?n:this.resolve(n).route:this.currentRoute;return e?[].concat.apply([],e.matched.map((function(n){return Object.keys(n.components).map((function(e){return n.components[e]}))}))):[]},_o.prototype.resolve=function(n,e,t){var a=yr(n,e=e||this.history.current,t,this),i=this.match(a,e),r=i.redirectedFrom||i.fullPath;return{location:a,route:i,href:function(n,e,t){var a="hash"===t?"#"+e:e;return n?nr(n+"/"+a):a}(this.history.base,r,this.mode),normalizedTo:a,resolved:i}},_o.prototype.getRoutes=function(){return this.matcher.getRoutes()},_o.prototype.addRoute=function(n,e){this.matcher.addRoute(n,e),this.history.current!==Hi&&this.history.transitionTo(this.history.getCurrentLocation())},_o.prototype.addRoutes=function(n){this.matcher.addRoutes(n),this.history.current!==Hi&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(_o.prototype,Eo),_o.install=function n(e){if(!n.installed||_r!==e){n.installed=!0,_r=e;var t=function(n){return void 0!==n},a=function(n,e){var a=n.$options._parentVnode;t(a)&&t(a=a.data)&&t(a=a.registerRouteInstance)&&a(n,e)};e.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),e.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,a(this,this)},destroyed:function(){a(this)}}),Object.defineProperty(e.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(e.prototype,"$route",{get:function(){return this._routerRoot._route}}),e.component("RouterView",Yi),e.component("RouterLink",Ar);var i=e.config.optionMergeStrategies;i.beforeRouteEnter=i.beforeRouteLeave=i.beforeRouteUpdate=i.created}},_o.version="3.5.1",_o.isNavigationFailure=eo,_o.NavigationFailureType=Zr,_o.START_LOCATION=Hi,kr&&window.Vue&&window.Vue.use(_o);var xo=_o;t(201),t(151),t(217),t(61),t(219),t(21),t(22),t(220);function ko(n){n.locales&&Object.keys(n.locales).forEach((function(e){n.locales[e].path=e})),Object.freeze(n)}t(47),t(57),t(74);var wo=t(6),Co=(t(158),t(19),t(29),t(86),t(87),{NotFound:function(){return Promise.all([t.e(0),t.e(4)]).then(t.bind(null,429))},Layout:function(){return Promise.all([t.e(0),t.e(2)]).then(t.bind(null,428))}}),So={"v-1dc59a20":function(){return t.e(5).then(t.bind(null,430))},"v-212ad4b4":function(){return t.e(6).then(t.bind(null,431))},"v-5d3b959a":function(){return t.e(7).then(t.bind(null,432))},"v-12ba5629":function(){return t.e(8).then(t.bind(null,433))},"v-57cabcae":function(){return t.e(9).then(t.bind(null,434))},"v-5aaf9a28":function(){return t.e(10).then(t.bind(null,435))},"v-6efe7e76":function(){return t.e(11).then(t.bind(null,436))},"v-86e9fd48":function(){return t.e(12).then(t.bind(null,437))},"v-4230263f":function(){return t.e(13).then(t.bind(null,438))},"v-0ee8bb4e":function(){return t.e(14).then(t.bind(null,439))},"v-42849146":function(){return t.e(15).then(t.bind(null,440))},"v-62525ab4":function(){return t.e(16).then(t.bind(null,441))},"v-13f035ec":function(){return t.e(17).then(t.bind(null,442))},"v-2ff958f8":function(){return t.e(18).then(t.bind(null,443))},"v-141ae27f":function(){return t.e(19).then(t.bind(null,444))},"v-0de9dd06":function(){return t.e(21).then(t.bind(null,445))},"v-3beab5b9":function(){return t.e(22).then(t.bind(null,446))},"v-2bd63fd0":function(){return t.e(23).then(t.bind(null,447))},"v-6d38f8e1":function(){return t.e(20).then(t.bind(null,448))},"v-33e2c2c0":function(){return t.e(24).then(t.bind(null,449))},"v-88f16062":function(){return t.e(25).then(t.bind(null,450))},"v-04892c9a":function(){return t.e(26).then(t.bind(null,451))},"v-6044af56":function(){return t.e(27).then(t.bind(null,452))},"v-79125705":function(){return t.e(28).then(t.bind(null,453))},"v-91bf2bb8":function(){return t.e(29).then(t.bind(null,454))},"v-4be82551":function(){return t.e(30).then(t.bind(null,455))},"v-456339ea":function(){return t.e(31).then(t.bind(null,456))},"v-2ecd8cf6":function(){return t.e(32).then(t.bind(null,457))},"v-04d06b5b":function(){return t.e(33).then(t.bind(null,458))},"v-0e6fb88c":function(){return t.e(34).then(t.bind(null,459))},"v-6d7c880a":function(){return t.e(35).then(t.bind(null,460))},"v-7fc45692":function(){return t.e(36).then(t.bind(null,461))},"v-dce64a46":function(){return t.e(37).then(t.bind(null,462))},"v-a6e70b22":function(){return t.e(38).then(t.bind(null,463))},"v-30f9ac0c":function(){return t.e(39).then(t.bind(null,464))},"v-28c46957":function(){return t.e(40).then(t.bind(null,465))},"v-0d3a7931":function(){return t.e(41).then(t.bind(null,466))},"v-3706fd7a":function(){return t.e(42).then(t.bind(null,467))},"v-2e6c02ba":function(){return t.e(43).then(t.bind(null,468))},"v-b5ba63b2":function(){return t.e(45).then(t.bind(null,469))},"v-a46f98a2":function(){return t.e(46).then(t.bind(null,470))},"v-6f1a4286":function(){return t.e(47).then(t.bind(null,471))},"v-9dd185bc":function(){return t.e(48).then(t.bind(null,472))},"v-e3307746":function(){return t.e(49).then(t.bind(null,473))},"v-40979294":function(){return t.e(50).then(t.bind(null,474))},"v-456b02c8":function(){return t.e(53).then(t.bind(null,475))},"v-502d1918":function(){return t.e(55).then(t.bind(null,476))},"v-343bb2dc":function(){return t.e(54).then(t.bind(null,477))},"v-1c426e16":function(){return t.e(44).then(t.bind(null,478))},"v-05671b78":function(){return t.e(56).then(t.bind(null,479))},"v-5a7f928b":function(){return t.e(58).then(t.bind(null,480))},"v-ac5cebe2":function(){return t.e(57).then(t.bind(null,481))},"v-10762118":function(){return t.e(59).then(t.bind(null,482))},"v-cf4b3f26":function(){return t.e(60).then(t.bind(null,483))},"v-d10e5544":function(){return t.e(61).then(t.bind(null,484))},"v-5d54258d":function(){return t.e(62).then(t.bind(null,485))},"v-798c7e0f":function(){return t.e(65).then(t.bind(null,486))},"v-4b21ce4b":function(){return t.e(64).then(t.bind(null,487))},"v-21b50551":function(){return t.e(66).then(t.bind(null,488))},"v-3d25093c":function(){return t.e(67).then(t.bind(null,489))},"v-74f65bd4":function(){return t.e(68).then(t.bind(null,490))},"v-2243d7fc":function(){return t.e(69).then(t.bind(null,491))},"v-45276cd6":function(){return t.e(70).then(t.bind(null,492))},"v-093768b9":function(){return t.e(71).then(t.bind(null,493))},"v-2e7a230d":function(){return t.e(72).then(t.bind(null,494))},"v-0ab31c4e":function(){return t.e(52).then(t.bind(null,495))},"v-1c7c9f96":function(){return t.e(73).then(t.bind(null,496))},"v-b995e478":function(){return t.e(63).then(t.bind(null,497))},"v-a1f0ce22":function(){return t.e(51).then(t.bind(null,498))},"v-c2fce59a":function(){return t.e(74).then(t.bind(null,499))},"v-0bbe12ea":function(){return t.e(76).then(t.bind(null,500))},"v-801da7ca":function(){return t.e(78).then(t.bind(null,501))},"v-2e36dcd8":function(){return t.e(75).then(t.bind(null,502))},"v-75d523ea":function(){return t.e(77).then(t.bind(null,503))},"v-4a6374f4":function(){return t.e(79).then(t.bind(null,504))},"v-232d2f76":function(){return t.e(80).then(t.bind(null,505))},"v-608f1e1c":function(){return t.e(83).then(t.bind(null,506))},"v-e9f21f7c":function(){return t.e(85).then(t.bind(null,507))},"v-05797640":function(){return t.e(86).then(t.bind(null,508))},"v-1218f70b":function(){return t.e(87).then(t.bind(null,509))},"v-76238680":function(){return t.e(88).then(t.bind(null,510))},"v-7e90e4e9":function(){return t.e(89).then(t.bind(null,511))},"v-60f648f9":function(){return t.e(82).then(t.bind(null,512))},"v-c1d3d84c":function(){return t.e(90).then(t.bind(null,513))},"v-41cb72a9":function(){return t.e(91).then(t.bind(null,514))},"v-ed2e8b86":function(){return t.e(81).then(t.bind(null,515))},"v-0efa04a6":function(){return t.e(92).then(t.bind(null,516))},"v-2d34cbf4":function(){return t.e(93).then(t.bind(null,517))},"v-af5feadc":function(){return t.e(94).then(t.bind(null,518))},"v-789b2126":function(){return t.e(84).then(t.bind(null,519))},"v-048810a1":function(){return t.e(96).then(t.bind(null,520))},"v-c8ffdb4e":function(){return t.e(97).then(t.bind(null,521))},"v-4867182a":function(){return t.e(98).then(t.bind(null,522))},"v-db08b16e":function(){return t.e(101).then(t.bind(null,523))},"v-60eddab5":function(){return t.e(99).then(t.bind(null,524))},"v-68c570d3":function(){return t.e(100).then(t.bind(null,525))},"v-035fdc6d":function(){return t.e(103).then(t.bind(null,526))},"v-7f7ee08c":function(){return t.e(102).then(t.bind(null,527))},"v-03d1a531":function(){return t.e(105).then(t.bind(null,528))},"v-6ed35a41":function(){return t.e(104).then(t.bind(null,529))},"v-026eefc1":function(){return t.e(106).then(t.bind(null,530))},"v-aae41b5e":function(){return t.e(95).then(t.bind(null,531))},"v-51bcc501":function(){return t.e(107).then(t.bind(null,532))},"v-58142f35":function(){return t.e(108).then(t.bind(null,533))},"v-7d0d935a":function(){return t.e(109).then(t.bind(null,534))},"v-087fae29":function(){return t.e(110).then(t.bind(null,535))},"v-25353850":function(){return t.e(111).then(t.bind(null,536))},"v-d022b7f6":function(){return t.e(112).then(t.bind(null,537))},"v-1aa0d565":function(){return t.e(113).then(t.bind(null,538))},"v-7d2a1f50":function(){return t.e(117).then(t.bind(null,539))},"v-735b9b2c":function(){return t.e(118).then(t.bind(null,540))},"v-0ce6a857":function(){return t.e(119).then(t.bind(null,541))},"v-28b40785":function(){return t.e(120).then(t.bind(null,542))},"v-55caee54":function(){return t.e(121).then(t.bind(null,543))},"v-307f090f":function(){return t.e(123).then(t.bind(null,544))},"v-66324249":function(){return t.e(124).then(t.bind(null,545))},"v-1690b4cd":function(){return t.e(122).then(t.bind(null,546))},"v-37c4561a":function(){return t.e(116).then(t.bind(null,547))},"v-7788edac":function(){return t.e(125).then(t.bind(null,548))},"v-a5dc9b2a":function(){return t.e(127).then(t.bind(null,549))},"v-348c00c9":function(){return t.e(126).then(t.bind(null,550))},"v-0f686957":function(){return t.e(129).then(t.bind(null,551))},"v-0bde3425":function(){return t.e(128).then(t.bind(null,552))},"v-380a3df9":function(){return t.e(131).then(t.bind(null,553))},"v-7030c545":function(){return t.e(130).then(t.bind(null,554))},"v-8c039fd8":function(){return t.e(132).then(t.bind(null,555))},"v-5afadf0a":function(){return t.e(133).then(t.bind(null,556))},"v-bf597a0c":function(){return t.e(134).then(t.bind(null,557))},"v-7b4fdfde":function(){return t.e(135).then(t.bind(null,558))},"v-7cba1716":function(){return t.e(114).then(t.bind(null,559))},"v-60c8d04a":function(){return t.e(137).then(t.bind(null,560))},"v-8f837ea0":function(){return t.e(115).then(t.bind(null,561))},"v-1973eb08":function(){return t.e(138).then(t.bind(null,562))},"v-74c89e7c":function(){return t.e(139).then(t.bind(null,563))},"v-56003e12":function(){return t.e(141).then(t.bind(null,564))},"v-5c9bb8c7":function(){return t.e(142).then(t.bind(null,565))},"v-05ab1571":function(){return t.e(140).then(t.bind(null,566))},"v-cc9eb5f0":function(){return t.e(143).then(t.bind(null,567))},"v-232d0ce8":function(){return t.e(144).then(t.bind(null,568))},"v-0fbc1026":function(){return t.e(136).then(t.bind(null,569))},"v-20a0e6d4":function(){return t.e(145).then(t.bind(null,570))},"v-5154597e":function(){return t.e(146).then(t.bind(null,571))},"v-78fd300e":function(){return t.e(147).then(t.bind(null,572))},"v-1fb62b40":function(){return t.e(149).then(t.bind(null,573))},"v-52e3e8cf":function(){return t.e(148).then(t.bind(null,574))},"v-16aa8d4a":function(){return t.e(151).then(t.bind(null,575))},"v-2cb0147f":function(){return t.e(150).then(t.bind(null,576))},"v-5388bbd0":function(){return t.e(152).then(t.bind(null,577))},"v-bfbdbcee":function(){return t.e(153).then(t.bind(null,578))},"v-47a2a4fb":function(){return t.e(156).then(t.bind(null,579))},"v-0e34e394":function(){return t.e(154).then(t.bind(null,580))},"v-8a7101b2":function(){return t.e(157).then(t.bind(null,581))},"v-95b7e762":function(){return t.e(155).then(t.bind(null,582))},"v-d675efea":function(){return t.e(158).then(t.bind(null,583))},"v-90d4e4ee":function(){return t.e(161).then(t.bind(null,584))},"v-da9fda54":function(){return t.e(160).then(t.bind(null,585))},"v-167dd495":function(){return t.e(159).then(t.bind(null,586))},"v-103ba0a2":function(){return t.e(163).then(t.bind(null,587))},"v-8a1baa90":function(){return t.e(164).then(t.bind(null,588))},"v-0e6d37a8":function(){return t.e(162).then(t.bind(null,589))},"v-0a57134c":function(){return t.e(165).then(t.bind(null,590))},"v-2a51b9b5":function(){return t.e(167).then(t.bind(null,591))},"v-29693aaf":function(){return t.e(166).then(t.bind(null,592))},"v-416df733":function(){return t.e(169).then(t.bind(null,593))},"v-966faedc":function(){return t.e(168).then(t.bind(null,594))},"v-1beb24d8":function(){return t.e(170).then(t.bind(null,595))},"v-ed2b6be2":function(){return t.e(172).then(t.bind(null,596))},"v-59d4b975":function(){return t.e(171).then(t.bind(null,597))},"v-05774214":function(){return t.e(173).then(t.bind(null,598))},"v-b8a31364":function(){return t.e(174).then(t.bind(null,599))},"v-39505a4f":function(){return t.e(176).then(t.bind(null,600))},"v-552c5d34":function(){return t.e(175).then(t.bind(null,601))},"v-6ce7636a":function(){return t.e(177).then(t.bind(null,602))},"v-1f470076":function(){return t.e(178).then(t.bind(null,603))},"v-2e51ed34":function(){return t.e(179).then(t.bind(null,604))},"v-6ea858af":function(){return t.e(180).then(t.bind(null,605))},"v-97cae6c4":function(){return t.e(182).then(t.bind(null,606))},"v-718e9ea6":function(){return t.e(183).then(t.bind(null,607))},"v-24a8bf61":function(){return t.e(184).then(t.bind(null,608))},"v-4ee69ba8":function(){return t.e(185).then(t.bind(null,609))},"v-47f5d7b3":function(){return t.e(186).then(t.bind(null,610))},"v-c123a972":function(){return t.e(188).then(t.bind(null,611))},"v-7181a7c3":function(){return t.e(187).then(t.bind(null,612))},"v-502a83ef":function(){return t.e(189).then(t.bind(null,613))},"v-4af706ce":function(){return t.e(190).then(t.bind(null,614))},"v-6fd27afe":function(){return t.e(181).then(t.bind(null,615))},"v-38b4cc66":function(){return t.e(191).then(t.bind(null,616))},"v-19930428":function(){return t.e(192).then(t.bind(null,617))},"v-06a7f062":function(){return t.e(193).then(t.bind(null,618))},"v-2df9e565":function(){return t.e(198).then(t.bind(null,619))},"v-1efee980":function(){return t.e(195).then(t.bind(null,620))},"v-4120d242":function(){return t.e(196).then(t.bind(null,621))},"v-ae94161c":function(){return t.e(197).then(t.bind(null,622))},"v-9f230ad8":function(){return t.e(199).then(t.bind(null,623))},"v-6ed459d2":function(){return t.e(200).then(t.bind(null,624))},"v-49a90feb":function(){return t.e(201).then(t.bind(null,625))},"v-61ae05f4":function(){return t.e(202).then(t.bind(null,626))},"v-20dcdbb3":function(){return t.e(203).then(t.bind(null,627))},"v-44df5588":function(){return t.e(204).then(t.bind(null,628))},"v-644ecf6a":function(){return t.e(205).then(t.bind(null,629))},"v-34007dfb":function(){return t.e(194).then(t.bind(null,630))},"v-903271d4":function(){return t.e(206).then(t.bind(null,631))},"v-176eac29":function(){return t.e(207).then(t.bind(null,632))},"v-3a012f16":function(){return t.e(208).then(t.bind(null,633))},"v-4582675f":function(){return t.e(209).then(t.bind(null,634))},"v-53ea97c3":function(){return t.e(210).then(t.bind(null,635))},"v-08d4a690":function(){return t.e(213).then(t.bind(null,636))},"v-38d61985":function(){return t.e(211).then(t.bind(null,637))},"v-170f256a":function(){return t.e(212).then(t.bind(null,638))},"v-b4b99e96":function(){return t.e(214).then(t.bind(null,639))},"v-22b71997":function(){return t.e(215).then(t.bind(null,640))},"v-58bcda0a":function(){return t.e(216).then(t.bind(null,641))},"v-4d3dd9d1":function(){return t.e(217).then(t.bind(null,642))},"v-e762e6a2":function(){return t.e(218).then(t.bind(null,643))},"v-62fa926d":function(){return t.e(219).then(t.bind(null,644))},"v-4392326d":function(){return t.e(220).then(t.bind(null,645))},"v-7953ded5":function(){return t.e(222).then(t.bind(null,646))},"v-5457056d":function(){return t.e(221).then(t.bind(null,647))},"v-59f183b7":function(){return t.e(224).then(t.bind(null,648))},"v-732238a5":function(){return t.e(223).then(t.bind(null,649))},"v-1c2d5bbc":function(){return t.e(225).then(t.bind(null,650))},"v-d9a68072":function(){return t.e(226).then(t.bind(null,651))},"v-08f68526":function(){return t.e(227).then(t.bind(null,652))},"v-fcbfac08":function(){return t.e(229).then(t.bind(null,653))},"v-82aa7594":function(){return t.e(228).then(t.bind(null,654))},"v-f453b4b0":function(){return t.e(230).then(t.bind(null,655))},"v-4be9171a":function(){return t.e(231).then(t.bind(null,656))},"v-0cefb3e1":function(){return t.e(232).then(t.bind(null,657))},"v-203c8ca1":function(){return t.e(234).then(t.bind(null,658))},"v-b5744878":function(){return t.e(235).then(t.bind(null,659))},"v-4401a9cd":function(){return t.e(233).then(t.bind(null,660))},"v-7e1bdab1":function(){return t.e(236).then(t.bind(null,661))},"v-038a1890":function(){return t.e(237).then(t.bind(null,662))},"v-60771a10":function(){return t.e(238).then(t.bind(null,663))},"v-1acf53d4":function(){return t.e(241).then(t.bind(null,664))},"v-02f11e84":function(){return t.e(239).then(t.bind(null,665))},"v-04b09795":function(){return t.e(240).then(t.bind(null,666))},"v-3e931a1c":function(){return t.e(245).then(t.bind(null,667))},"v-171a9f6a":function(){return t.e(246).then(t.bind(null,668))},"v-78788d52":function(){return t.e(244).then(t.bind(null,669))},"v-4d695430":function(){return t.e(242).then(t.bind(null,670))},"v-20ca5f1c":function(){return t.e(243).then(t.bind(null,671))}};function Bo(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var To=/-(\w)/g,Po=Bo((function(n){return n.replace(To,(function(n,e){return e?e.toUpperCase():""}))})),Do=/\B([A-Z])/g,zo=Bo((function(n){return n.replace(Do,"-$1").toLowerCase()})),Lo=Bo((function(n){return n.charAt(0).toUpperCase()+n.slice(1)}));function Io(n,e){if(e)return n(e)?n(e):e.includes("-")?n(Lo(Po(e))):n(Lo(e))||n(zo(e))}var No=Object.assign({},Co,So),Fo=function(n){return No[n]},jo=function(n){return So[n]},Mo=function(n){return Co[n]},Ro=function(n){return zi.component(n)};function Oo(n){return Io(jo,n)}function Uo(n){return Io(Mo,n)}function Vo(n){return Io(Fo,n)}function Go(n){return Io(Ro,n)}function Wo(){for(var n=arguments.length,e=new Array(n),t=0;t<n;t++)e[t]=arguments[t];return Promise.all(e.filter((function(n){return n})).map(function(){var n=Object(a.a)(regeneratorRuntime.mark((function n(e){var t;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:if(Go(e)||!Vo(e)){n.next=5;break}return n.next=3,Vo(e)();case 3:t=n.sent,zi.component(e,t.default);case 5:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}()))}function Ho(n,e){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[n]=e)}var qo=t(34),$o=(t(48),t(36),t(188)),Xo=t.n($o),Zo={created:function(){if(this.siteMeta=this.$site.headTags.filter((function(n){return"meta"===Object(qo.a)(n,1)[0]})).map((function(n){var e=Object(qo.a)(n,2);e[0];return e[1]})),this.$ssrContext){var n=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(e=n)?e.map((function(n){var e="<meta";return Object.keys(n).forEach((function(t){e+=" ".concat(t,'="').concat(n[t],'"')})),e+">"})).join("\n    "):"",this.$ssrContext.canonicalLink=Yo(this.$canonicalUrl)}var e},mounted:function(){this.currentMetaTags=Object(wo.a)(document.querySelectorAll("meta")),this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta:function(){document.title=this.$title,document.documentElement.lang=this.$lang;var n=this.getMergedMetaTags();this.currentMetaTags=Jo(n,this.currentMetaTags)},getMergedMetaTags:function(){var n=this.$page.frontmatter.meta||[];return Xo()([{name:"description",content:this.$description}],n,this.siteMeta,Qo)},updateCanonicalLink:function(){Ko(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",Yo(this.$canonicalUrl))}},watch:{$page:function(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy:function(){Jo(null,this.currentMetaTags),Ko()}};function Ko(){var n=document.querySelector("link[rel='canonical']");n&&n.remove()}function Yo(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"";return n?'<link href="'.concat(n,'" rel="canonical" />'):""}function Jo(n,e){if(e&&Object(wo.a)(e).filter((function(n){return n.parentNode===document.head})).forEach((function(n){return document.head.removeChild(n)})),n)return n.map((function(n){var e=document.createElement("meta");return Object.keys(n).forEach((function(t){e.setAttribute(t,n[t])})),document.head.appendChild(e),e}))}function Qo(n){for(var e=0,t=["name","property","itemprop"];e<t.length;e++){var a=t[e];if(n.hasOwnProperty(a))return n[a]+a}return JSON.stringify(n)}t(193);var ns=t(126),es={mounted:function(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(ns)()((function(){this.setActiveHash()}),300),setActiveHash:function(){for(var n=this,e=[].slice.call(document.querySelectorAll(".sidebar-link")),t=[].slice.call(document.querySelectorAll(".header-anchor")).filter((function(n){return e.some((function(e){return e.hash===n.hash}))})),a=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),i=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),r=window.innerHeight+a,o=0;o<t.length;o++){var s=t[o],l=t[o+1],c=0===o&&0===a||a>=s.parentElement.offsetTop+10&&(!l||a<l.parentElement.offsetTop-10),p=decodeURIComponent(this.$route.hash);if(c&&p!==decodeURIComponent(s.hash)){var d=s;if(r===i)for(var u=o+1;u<t.length;u++)if(p===decodeURIComponent(t[u].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(d.hash),(function(){n.$nextTick((function(){n.$vuepress.$set("disableScrollBehavior",!1)}))}))}}}},beforeDestroy:function(){window.removeEventListener("scroll",this.onScroll)}},ts=(t(83),t(81)),as=t.n(ts),is={mounted:function(){var n=this;as.a.configure({showSpinner:!1}),this.$router.beforeEach((function(n,e,t){n.path===e.path||zi.component(n.name)||as.a.start(),t()})),this.$router.afterEach((function(){as.a.done(),n.isSidebarOpen=!1}))}},rs=(t(319),t(41),t(101),t(179),t(62),t(127),t(60),{jsLib:[],cssLib:[],codepenLayout:"left",codepenJsProcessor:"babel",codepenEditors:"101",vue:"https://cdn.jsdelivr.net/npm/vue/dist/vue.min.js",react:"https://cdn.jsdelivr.net/npm/react/umd/react.production.min.js",reactDOM:"https://cdn.jsdelivr.net/npm/react-dom/umd/react-dom.production.min.js"}),os=function(n,e,t){var a=document.createElement(n);return e&&Object.keys(e).forEach((function(n){if(n.indexOf("data"))a[n]=e[n];else{var t=n.replace("data","");a.dataset[t]=e[n]}})),t&&t.forEach((function(n){a.appendChild(n)})),a},ss=function(n){return n.replace(/<br \/>/g,"<br>").replace(/<((\S+)[^<]*?)\s+\/>/g,"<$1></$2>")},ls=function(n){return'<div id="app">'.concat(ss(n),"</div>")},cs=function(n){return"".concat(n.replace("export default ","").replace(/App\.__style__(\s*)=(\s*)`([\s\S]*)?`/,""),'ReactDOM.render(React.createElement(App), document.getElementById("app"))')},ps=function(n){return"new Vue({ el: '#app', ".concat(n.replace(/export[\t-\r \xA0\u1680\u2000-\u200A\u2028\u2029\u202F\u205F\u3000\uFEFF]+default[\t-\r \xA0\u1680\u2000-\u200A\u2028\u2029\u202F\u205F\u3000\uFEFF]*\{(\n*(?:[\0-\uD7FF\uE000-\uFFFF]|[\uD800-\uDBFF][\uDC00-\uDFFF]|[\uD800-\uDBFF](?![\uDC00-\uDFFF])|(?:[^\uD800-\uDBFF]|^)[\uDC00-\uDFFF])*)\n*\}[\t-\r \xA0\u1680\u2000-\u200A\u2028\u2029\u202F\u205F\u3000\uFEFF]*;?$/,"$1").replace(/export[\t-\r \xA0\u1680\u2000-\u200A\u2028\u2029\u202F\u205F\u3000\uFEFF]+default[\t-\r \xA0\u1680\u2000-\u200A\u2028\u2029\u202F\u205F\u3000\uFEFF]*Vue\.extend[\t-\r \xA0\u1680\u2000-\u200A\u2028\u2029\u202F\u205F\u3000\uFEFF]*\([\t-\r \xA0\u1680\u2000-\u200A\u2028\u2029\u202F\u205F\u3000\uFEFF]*\{(\n*(?:[\0-\uD7FF\uE000-\uFFFF]|[\uD800-\uDBFF][\uDC00-\uDFFF]|[\uD800-\uDBFF](?![\uDC00-\uDFFF])|(?:[^\uD800-\uDBFF]|^)[\uDC00-\uDFFF])*)\n*\}[\t-\r \xA0\u1680\u2000-\u200A\u2028\u2029\u202F\u205F\u3000\uFEFF]*\)[\t-\r \xA0\u1680\u2000-\u200A\u2028\u2029\u202F\u205F\u3000\uFEFF]*;?$/,"$1").trim()," })")},ds=function(n,e){var t=e.split(/export[\t-\r \xA0\u1680\u2000-\u200A\u2028\u2029\u202F\u205F\u3000\uFEFF]+default/),a="(function() {".concat(t[0]," ; return ").concat(t[1],"})()"),i=window.Babel?window.Babel.transform(a,{presets:["es2015"]}).code:a,r=[eval][0](i);return r.template=n,r},us={html:{types:["html","slim","haml","md","markdown","vue"],map:{html:"none",vue:"none",md:"markdown"}},js:{types:["js","javascript","coffee","coffeescript","ts","typescript","ls","livescript"],map:{js:"none",javascript:"none",coffee:"coffeescript",ls:"livescript",ts:"typescript"}},css:{types:["css","less","sass","scss","stylus","styl"],map:{css:"none",styl:"stylus"}}},ms=function(n,e){return Array.from(n.querySelectorAll(".".concat(e)))},hs=function(n,e,t,a){var i=n.classList.contains("down");e.style.height=i?"".concat(t.clientHeight+13.8,"px"):"0",i?(a.classList.add("show-link"),n.classList.remove("down")):(a.classList.remove("show-link"),n.classList.add("down"))},gs=function(n,e){var t=n.html,a=n.js,i=n.css,r=n.jsLib,o=n.cssLib;return os("form",{className:"code-demo-codepen",target:"_blank",action:"https://codepen.io/pen/define",method:"post"},[os("input",{type:"hidden",name:"data",value:JSON.stringify({html:t,js:a,css:i,js_external:[].concat(Object(wo.a)(r),Object(wo.a)(rs.jsLib)).join(";"),css_external:[].concat(Object(wo.a)(o),Object(wo.a)(rs.cssLib)).join(";"),layout:rs.codepenLayout,html_pre_processor:e?e.html[1]:"none",js_pre_processor:e?e.js[1]:"none",css_pre_processor:e?e.css[1]:"none",editors:rs.editors})}),os("button",{type:"submit",innerHTML:'<svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="200" height="200"><defs><style/></defs><path d="M123.429 668L468 897.714V692.571L277.143 565.143zM88 585.714L198.286 512 88 438.286v147.428zm468 312L900.571 668 746.857 565.143 556 692.57v205.143zM512 616l155.429-104L512 408 356.571 512zM277.143 458.857L468 331.43V126.286L123.429 356zM825.714 512L936 585.714V438.286zm-78.857-53.143L900.571 356 556 126.286v205.143zM1024 356v312q0 23.429-19.429 36.571l-468 312Q524.571 1024 512 1024t-24.571-7.429l-468-312Q0 691.43 0 668V356q0-23.429 19.429-36.571l468-312Q499.429 0 512 0t24.571 7.429l468 312Q1024 332.57 1024 356z"/></svg>',className:"button",datatip:"Codepen"})])},fs=function(n){var e,t,a,i,r,o,s=n.code,l=n.codeType,c=n.container,p=n.title,d=c.id,u=ms(c,"demo-wrapper")[0],m=ms(c,"code-wrapper")[0],h=ms(c,"code")[0],g=ms(c,"code-demo-footer")[0];if(s.script){var f=os("button",{className:"expand arrow down"});g.appendChild(f),g.appendChild(os("span",{className:"title",innerHTML:p})),f.addEventListener("click",hs.bind(null,f,m,h,g)),m.style.height="0",s.css&&function(n,e){for(var t,a=document.querySelector("#".concat(e)),i=/((?:[\0-\uD7FF\uE000-\uFFFF]|[\uD800-\uDBFF][\uDC00-\uDFFF]|[\uD800-\uDBFF](?![\uDC00-\uDFFF])|(?:[^\uD800-\uDBFF]|^)[\uDC00-\uDFFF])*?)\{((?:[\0-\uD7FF\uE000-\uFFFF]|[\uD800-\uDBFF][\uDC00-\uDFFF]|[\uD800-\uDBFF](?![\uDC00-\uDFFF])|(?:[^\uD800-\uDBFF]|^)[\uDC00-\uDFFF])*?)\}/g,r="";t=i.exec(n);){var o=t,s=Object(qo.a)(o,3),l=s[1],c=s[2];r+="".concat(l.replace(/\n/g,"").split(",").map((function(n){return"#".concat(e," .demo-wrapper ").concat(n)})).join(","),"{").concat(c,"}")}var p=os("style",{innerHTML:r});a&&!a.hasAttribute("demo-styled")&&(a.appendChild(p),a.setAttribute("demo-styled",""))}(s.css,d),!1!==rs.jsfiddle&&g.appendChild((t=(e=s).html,a=e.js,i=e.css,r=e.jsLib,o=e.cssLib,os("form",{className:"code-demo-jsfiddle",target:"_blank",action:"https://jsfiddle.net/api/post/library/pure/",method:"post"},[os("input",{type:"hidden",name:"html",value:t}),os("input",{type:"hidden",name:"js",value:a}),os("input",{type:"hidden",name:"css",value:i}),os("input",{type:"hidden",name:"wrap",value:"1"}),os("input",{type:"hidden",name:"panel_js",value:"3"}),os("input",{type:"hidden",name:"resources",value:[].concat(Object(wo.a)(r),Object(wo.a)(o),Object(wo.a)(rs.cssLib),Object(wo.a)(rs.jsLib)).join(",")}),os("button",{type:"submit",className:"button",innerHTML:'<svg class="icon" viewBox="0 0 1170 1024" xmlns="http://www.w3.org/2000/svg" width="228.516" height="200"><defs><style/></defs><path d="M1028.571 441.143q63.429 26.286 102.572 83.143t39.143 126.571q0 93.714-67.429 160.286T940 877.714q-2.286 0-6.571-.285t-6-.286H232q-97.143-5.714-164.571-71.714T0 645.143q0-62.857 31.429-116t84-84q-6.858-22.286-6.858-46.857 0-65.715 46.858-112T269.143 240q54.286 0 98.286 33.143 42.857-88 127.142-141.714t186.572-53.715q94.857 0 174.857 46t126.571 124.857 46.572 172q0 3.429-.286 10.286t-.286 10.286zm-761.142 152q0 69.714 48 110.286T434.286 744q78.285 0 137.143-56.571-9.143-11.429-27.143-32.286t-24.857-28.857q-38.286 37.143-82.286 37.143-31.429 0-53.429-19.143t-22-50q0-30.286 22-49.715T436 525.143q25.143 0 48.286 12T526 568.57t37.143 42.858 39.428 46.857 44 42.857T702 732.57t69.429 12q69.142 0 116.857-40.857T936 594.857q0-69.143-48-109.714T769.714 444.57Q688 444.571 632 500l53.143 61.714q37.714-36.571 81.143-36.571 29.714 0 52.571 18.857t22.857 48q0 32.571-21.143 52.286T766.857 664q-24.571 0-47.143-12t-41.143-31.429-37.428-42.857-39.714-46.857T557.143 488 502 456.571t-67.714-12q-69.715 0-118.286 40.286t-48.571 108.286z"/></svg>',datatip:"JSFiddle"})]))),!1!==rs.codepen&&g.appendChild(gs(s))}else u.style.display="none",m.style.height="auto",g.appendChild(gs(s,l)),g.style.height="40px"},vs=function n(){var e=ms(document,"code-demo-wrapper");e.length?e.forEach((function(n){if(!n.hasAttribute("demo-inited")){var e=ms(n,"code-demo-app")[0],t=decodeURIComponent(n.dataset.title||""),a=decodeURIComponent(n.dataset.type||"normal"),i=JSON.parse(decodeURIComponent(n.dataset.config||"{}")),r=function(n){var e=Object.keys(n),t={html:[],js:[],css:[],isLegal:!1};return["html","js","css"].forEach((function(a){var i=e.filter((function(n){return us[a].types.includes(n)}));if(i.length){var r=i[0];t[a]=[n[r].replace(/^\n|\n$/g,""),us[a].map[r]||r]}})),t.isLegal=!(t.html.length&&"none"!==t.html[1]||t.js.length&&"none"!==t.js[1]||t.css.length&&"none"!==t.css[1]),t}(JSON.parse(decodeURIComponent(n.dataset.code||"{}")));if(a.includes("react")){var o=function(n,e){var t=n.isLegal&&window.Babel?new Function("return (function(exports){var module={};module.exports=exports;".concat(window.Babel.transform(n.js[0]||"",{presets:["es2015","react"]}).code,";return module.exports.__esModule?module.exports.default:module.exports;})({})"))():void 0;return{html:ls(""),js:cs(n.js[0]||""),css:n.css[0]||"",jsLib:[rs.react,rs.reactDOM].concat(Object(wo.a)(e.jsLib||[])),cssLib:e.cssLib||[],script:t}}(r,i);o.script&&window.ReactDOM.render(window.React.createElement(o.script),e),fs({code:o,codeType:r,container:n,title:t})}else if(a.includes("vue")){var s=function(n,e){var t=n.html[0]||"",a=/<template>((?:[\0-\uD7FF\uE000-\uFFFF]|[\uD800-\uDBFF][\uDC00-\uDFFF]|[\uD800-\uDBFF](?![\uDC00-\uDFFF])|(?:[^\uD800-\uDBFF]|^)[\uDC00-\uDFFF])+)<\/template>/.exec(t),i=/<script([\t-\r \xA0\u1680\u2000-\u200A\u2028\u2029\u202F\u205F\u3000\uFEFF]*lang=(["'])((?:[\0-\t\x0B\f\x0E-\u2027\u202A-\uD7FF\uE000-\uFFFF]|[\uD800-\uDBFF][\uDC00-\uDFFF]|[\uD800-\uDBFF](?![\uDC00-\uDFFF])|(?:[^\uD800-\uDBFF]|^)[\uDC00-\uDFFF])*?)\2)?>((?:[\0-\uD7FF\uE000-\uFFFF]|[\uD800-\uDBFF][\uDC00-\uDFFF]|[\uD800-\uDBFF](?![\uDC00-\uDFFF])|(?:[^\uD800-\uDBFF]|^)[\uDC00-\uDFFF])+)<\/script>/.exec(t),r=/<style([\t-\r \xA0\u1680\u2000-\u200A\u2028\u2029\u202F\u205F\u3000\uFEFF]*lang=(["'])((?:[\0-\t\x0B\f\x0E-\u2027\u202A-\uD7FF\uE000-\uFFFF]|[\uD800-\uDBFF][\uDC00-\uDFFF]|[\uD800-\uDBFF](?![\uDC00-\uDFFF])|(?:[^\uD800-\uDBFF]|^)[\uDC00-\uDFFF])*?)\2)?[\t-\r \xA0\u1680\u2000-\u200A\u2028\u2029\u202F\u205F\u3000\uFEFF]*(?:scoped)?>((?:[\0-\uD7FF\uE000-\uFFFF]|[\uD800-\uDBFF][\uDC00-\uDFFF]|[\uD800-\uDBFF](?![\uDC00-\uDFFF])|(?:[^\uD800-\uDBFF]|^)[\uDC00-\uDFFF])+)<\/style>/.exec(t),o=a?a[1].replace(/^\n|\n$/g,""):"",s=i?[i[4].replace(/^\n|\n$/g,""),i[3]]:[],l=Object(qo.a)(s,2),c=l[0],p=void 0===c?"":c,d=l[1],u=void 0===d?"":d,m=r?[r[4].replace(/^\n|\n$/g,""),r[3]]:[],h=Object(qo.a)(m,2),g=h[0],f=void 0===g?"":g,v=h[1],b=void 0===v?"":v,y=""===u&&(""===b||"css"===b);return{html:ls(o),js:ps(p),css:f,jsLib:[rs.vue].concat(Object(wo.a)(e.jsLib||[])),cssLib:e.cssLib||[],script:y?ds(o,p):void 0}}(r,i);if(s.script){var l=(new(window.Vue.extend(s.script))).$mount();e.appendChild(l.$el)}fs({code:s,codeType:r,container:n,title:t})}else{var c=function(n,e){return{html:ss(n.html[0]||""),js:n.js[0]||"",css:n.css[0]||"",jsLib:e.jsLib||[],cssLib:e.cssLib||[],script:n.isLegal?(t=n.js[0]||"",a=window.Babel?window.Babel.transform(t,{presets:["es2015"]}).code:t,new Function("return (function(){".concat(a,"})()"))):void 0};var t,a}(r,i);c.script&&(e.innerHTML=c.html,c.script()),fs({code:c,codeType:r,container:n,title:t})}n.setAttribute("demo-inited","")}})):setTimeout((function(){return n()}),300)},bs={mounted:function(){vs()},updated:function(){vs()}};t(66),t(321);function ys(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}t(196);function _s(n,e){for(var t=0;t<e.length;t++){var a=e[t];a.enumerable=a.enumerable||!1,a.configurable=!0,"value"in a&&(a.writable=!0),Object.defineProperty(n,a.key,a)}}function Es(n,e,t){return e&&_s(n.prototype,e),t&&_s(n,t),n}t(322);var As=function(){function n(){ys(this,n);this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}return Es(n,[{key:"show",value:function(n){var e=this,t=n.text,a=void 0===t?"":t,i=n.duration,r=void 0===i?3e3:i,o=document.createElement("div");o.className="message move-in",o.innerHTML='\n      <i style="fill: #06a35a;font-size: 14px;coolr: #06a35a" style="display:flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">'.concat(a,"</div>\n    "),this.containerEl.appendChild(o),r>0&&setTimeout((function(){e.close(o)}),r)}},{key:"close",value:function(n){n.className=n.className.replace("move-in",""),n.className+="move-out",n.addEventListener("animationend",(function(){n.remove()}))}}]),n}(),xs={mounted:function(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated:function(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy:function(){var n=this;setTimeout((function(){(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach((function(e){document.querySelectorAll(e).forEach(n.generateCopyButton)}))}),1e3)},generateCopyButton:function(n){var e=this;if(!n.classList.contains("codecopy-enabled")){var t=document.createElement("i");t.className="code-copy",t.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',t.title="Click to Copy to Clipboard",t.addEventListener("click",(function(){e.copyToClipboard(n.innerText)})),n.appendChild(t),n.classList.add("codecopy-enabled")}},copyToClipboard:function(n){var e=document.createElement("textarea");e.value=n,e.setAttribute("readonly",""),e.style.position="absolute",e.style.left="-9999px",document.body.appendChild(e);var t=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);e.select(),document.execCommand("copy"),(new As).show({text:"复制成功",duration:1e3}),document.body.removeChild(e),t&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(t))}}};t(113);!function(n,e){void 0===e&&(e={});var t=e.insertAt;if(n&&"undefined"!=typeof document){var a=document.head||document.getElementsByTagName("head")[0],i=document.createElement("style");i.type="text/css","top"===t&&a.firstChild?a.insertBefore(i,a.firstChild):a.appendChild(i),i.styleSheet?i.styleSheet.cssText=n:i.appendChild(document.createTextNode(n))}}("@media (max-width: 1000px) {\n  .vuepress-plugin-demo-block__h_code {\n    display: none;\n  }\n  .vuepress-plugin-demo-block__app {\n    margin-left: auto !important;\n    margin-right: auto !important;\n  }\n}\n.vuepress-plugin-demo-block__wrapper {\n  margin-top: 10px;\n  border: 1px solid #ebebeb;\n  border-radius: 4px;\n  transition: all 0.2s;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display {\n  height: 400px;\n  display: flex;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__app {\n  width: 300px;\n  border: 1px solid #ebebeb;\n  box-shadow: 1px 1px 3px #ebebeb;\n  margin-right: 5px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code {\n  flex: 1;\n  overflow: auto;\n  height: 100%;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code > pre {\n  overflow: visible;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  max-height: 400px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper div {\n  box-sizing: border-box;\n}\n.vuepress-plugin-demo-block__wrapper:hover {\n  box-shadow: 0 0 11px rgba(33, 33, 33, 0.2);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code {\n  overflow: hidden;\n  height: 0;\n  padding: 0 !important;\n  background-color: #282c34;\n  border-radius: 0 !important;\n  transition: height 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code pre {\n  margin: 0 !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  padding: 20px;\n  border-bottom: 1px solid #ebebeb;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer {\n  position: relative;\n  text-align: center;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__codepen {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__expand::before {\n  border-top: none;\n  border-right: 6px solid transparent;\n  border-bottom: 6px solid #ccc;\n  border-left: 6px solid transparent;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__codepen,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand span,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand::before {\n  border-top-color: #3eaf7c !important;\n  border-bottom-color: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover svg {\n  fill: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand-text {\n  transition: all 0.5s;\n  opacity: 0;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:nth-last-child(2) {\n  right: 50px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:last-child {\n  right: 10px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button {\n  border-color: transparent;\n  background-color: transparent;\n  font-size: 14px;\n  color: #3eaf7c;\n  cursor: pointer;\n  outline: none;\n  margin: 0;\n  width: 46px;\n  position: relative;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::before {\n  content: attr(data-tip);\n  white-space: nowrap;\n  position: absolute;\n  top: -30px;\n  left: 50%;\n  color: #eee;\n  line-height: 1;\n  z-index: 1000;\n  border-radius: 4px;\n  padding: 6px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  background-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::after {\n  content: '' !important;\n  display: block;\n  position: absolute;\n  left: 50%;\n  top: -5px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  border: 5px solid transparent;\n  border-top-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button svg {\n  width: 34px;\n  height: 20px;\n  fill: #ccc;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__codepen {\n  position: absolute;\n  top: 10px;\n  transition: all 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand {\n  position: relative;\n  width: 100px;\n  height: 40px;\n  margin: 0;\n  color: #3eaf7c;\n  font-size: 14px;\n  background-color: transparent;\n  border-color: transparent;\n  outline: none;\n  transition: all 0.5s;\n  cursor: pointer;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand::before {\n  content: \"\";\n  position: absolute;\n  top: 50%;\n  left: 50%;\n  width: 0;\n  height: 0;\n  border-top: 6px solid #ccc;\n  border-right: 6px solid transparent;\n  border-left: 6px solid transparent;\n  -webkit-transform: translate(-50%, -50%);\n          transform: translate(-50%, -50%);\n}\n");var ks={jsLib:[],cssLib:[],jsfiddle:!0,codepen:!0,codepenLayout:"left",codepenJsProcessor:"babel",codepenEditors:"101",horizontal:!1,vue:"https://cdn.jsdelivr.net/npm/vue/dist/vue.min.js",react:"https://cdn.jsdelivr.net/npm/react/umd/react.production.min.js",reactDOM:"https://cdn.jsdelivr.net/npm/react-dom/umd/react-dom.production.min.js"},ws={},Cs=function(n){return'<div id="app">\n'.concat(n,"\n</div>")},Ss=function(n){return window.$VUEPRESS_DEMO_BLOCK&&void 0!==window.$VUEPRESS_DEMO_BLOCK[n]?window.$VUEPRESS_DEMO_BLOCK[n]:ks[n]},Bs=function n(e,t,a){var i=document.createElement(e);return t&&Object.keys(t).forEach((function(n){if(n.indexOf("data"))i[n]=t[n];else{var e=n.replace("data","");i.dataset[e]=t[n]}})),a&&a.forEach((function(e){var t=e.tag,a=e.attrs,r=e.children;i.appendChild(n(t,a,r))})),i},Ts=function(n,e,t){var a,i=(a=n.querySelectorAll(".".concat(e)),Array.prototype.slice.call(a));return 1!==i.length||t?i:i[0]},Ps=function(n,e){var t,a,i=n.match(/<style>([\s\S]+)<\/style>/),r=n.match(/<template>([\s\S]+)<\/template>/),o=n.match(/<script>([\s\S]+)<\/script>/),s={css:i&&i[1].replace(/^\n|\n$/g,""),html:r&&r[1].replace(/^\n|\n$/g,""),js:o&&o[1].replace(/^\n|\n$/g,""),jsLib:e.jsLib||[],cssLib:e.cssLib||[]};s.htmlTpl=Cs(s.html),s.jsTpl=(t=s.js,a=t.replace(/export\s+default\s*?\{\n*/,"").replace(/\n*\}\s*$/,"").trim(),"new Vue({\n  el: '#app',\n  ".concat(a,"\n})")),s.script=function(n,e){var t=n.split(/export\s+default/),a="(function() {".concat(t[0]," ; return ").concat(t[1],"})()"),i=window.Babel?window.Babel.transform(a,{presets:["es2015"]}).code:a,r=[eval][0](i);return r.template=e,r}(s.js,s.html);var l=Ss("vue");return s.jsLib.unshift(l),s},Ds=function(n,e){var t,a=n.match(/<style>([\s\S]+)<\/style>/),i=n.match(/<html>([\s\S]+)<\/html>/),r=n.match(/<script>([\s\S]+)<\/script>/),o={css:a&&a[1].replace(/^\n|\n$/g,""),html:i&&i[1].replace(/^\n|\n$/g,""),js:r&&r[1].replace(/^\n|\n$/g,""),jsLib:e.jsLib||[],cssLib:e.cssLib||[]};return o.htmlTpl=o.html,o.jsTpl=o.js,o.script=(t=o.js,window.Babel?window.Babel.transform(t,{presets:["es2015"]}).code:t),o},zs=function(n){return n=n.replace("export default ","").replace(/App\.__style__(\s*)=(\s*)`([\s\S]*)?`/,""),n+='ReactDOM.render(React.createElement(App), document.getElementById("app"))'};function Ls(){var n=Ts(document,"vuepress-plugin-demo-block__wrapper",!0);n.length?n.forEach((function(n){if("true"!==n.dataset.created){n.style.display="block";var e=Ts(n,"vuepress-plugin-demo-block__code"),t=Ts(n,"vuepress-plugin-demo-block__display"),a=Ts(n,"vuepress-plugin-demo-block__footer"),i=Ts(t,"vuepress-plugin-demo-block__app"),r=decodeURIComponent(n.dataset.code),o=decodeURIComponent(n.dataset.config),s=decodeURIComponent(n.dataset.type);o=o?JSON.parse(o):{};var l,c,p,d,u,m,h,g=e.querySelector("div").clientHeight,f="react"===s?function(n,e){var t=(0,window.Babel.transform)(n,{presets:["es2015","react"]}).code,a="(function(exports){var module={};module.exports=exports;".concat(t,";return module.exports.__esModule?module.exports.default:module.exports;})({})"),i=new Function("return ".concat(a))(),r={js:i,css:i.__style__||"",jsLib:e.jsLib||[],cssLib:e.cssLib||[],jsTpl:zs(n),htmlTpl:Cs("")},o=Ss("react"),s=Ss("reactDOM");return r.jsLib.unshift(o,s),r}(r,o):"vanilla"===s?Ds(r,o):Ps(r,o),v=Bs("button",{className:"".concat("vuepress-plugin-demo-block__expand")});if(a.appendChild(v),v.addEventListener("click",Is.bind(null,v,g,e,a)),Ss("jsfiddle")&&a.appendChild((c=(l=f).css,p=l.htmlTpl,d=l.jsTpl,u=l.jsLib,m=l.cssLib,h=u.concat(m).concat(Ss("cssLib")).concat(Ss("jsLib")).join(","),Bs("form",{className:"vuepress-plugin-demo-block__jsfiddle",target:"_blank",action:"https://jsfiddle.net/api/post/library/pure/",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"css",value:c}},{tag:"input",attrs:{type:"hidden",name:"html",value:p}},{tag:"input",attrs:{type:"hidden",name:"js",value:d}},{tag:"input",attrs:{type:"hidden",name:"panel_js",value:3}},{tag:"input",attrs:{type:"hidden",name:"wrap",value:1}},{tag:"input",attrs:{type:"hidden",name:"resources",value:h}},{tag:"button",attrs:{type:"submit",className:"vuepress-plugin-demo-block__button",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088289967" class="icon" style="" viewBox="0 0 1170 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1952" xmlns:xlink="http://www.w3.org/1999/xlink" width="228.515625" height="200"><defs><style type="text/css"></style></defs><path d="M1028.571429 441.142857q63.428571 26.285714 102.571428 83.142857T1170.285714 650.857143q0 93.714286-67.428571 160.285714T940 877.714286q-2.285714 0-6.571429-0.285715t-6-0.285714H232q-97.142857-5.714286-164.571429-71.714286T0 645.142857q0-62.857143 31.428571-116t84-84q-6.857143-22.285714-6.857142-46.857143 0-65.714286 46.857142-112t113.714286-46.285714q54.285714 0 98.285714 33.142857 42.857143-88 127.142858-141.714286t186.571428-53.714285q94.857143 0 174.857143 46T982.571429 248.571429t46.571428 172q0 3.428571-0.285714 10.285714t-0.285714 10.285714zM267.428571 593.142857q0 69.714286 48 110.285714t118.857143 40.571429q78.285714 0 137.142857-56.571429-9.142857-11.428571-27.142857-32.285714T519.428571 626.285714q-38.285714 37.142857-82.285714 37.142857-31.428571 0-53.428571-19.142857T361.714286 594.285714q0-30.285714 22-49.714285t52.285714-19.428572q25.142857 0 48.285714 12t41.714286 31.428572 37.142857 42.857142 39.428572 46.857143 44 42.857143 55.428571 31.428572 69.428571 12q69.142857 0 116.857143-40.857143T936 594.857143q0-69.142857-48-109.714286t-118.285714-40.571428q-81.714286 0-137.714286 55.428571l53.142857 61.714286q37.714286-36.571429 81.142857-36.571429 29.714286 0 52.571429 18.857143t22.857143 48q0 32.571429-21.142857 52.285714t-53.714286 19.714286q-24.571429 0-47.142857-12t-41.142857-31.428571-37.428572-42.857143-39.714286-46.857143-44.285714-42.857143-55.142857-31.428571T434.285714 444.571429q-69.714286 0-118.285714 40.285714T267.428571 593.142857z" p-id="1953"></path></svg>',datatip:"JSFiddle"}}]))),Ss("codepen")&&a.appendChild(function(n){var e=n.css,t=n.htmlTpl,a=n.jsTpl,i=n.jsLib,r=n.cssLib,o=JSON.stringify({css:e,html:t,js:a,js_external:i.concat(Ss("jsLib")).join(";"),css_external:r.concat(Ss("cssLib")).join(";"),layout:Ss("codepenLayout"),js_pre_processor:Ss("codepenJsProcessor"),editors:Ss("codepenEditors")});return Bs("form",{className:"vuepress-plugin-demo-block__codepen",target:"_blank",action:"https://codepen.io/pen/define",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"data",value:o}},{tag:"button",attrs:{type:"submit",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088271207" class="icon" style="" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1737" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><defs><style type="text/css"></style></defs><path d="M123.428571 668l344.571429 229.714286v-205.142857L277.142857 565.142857z m-35.428571-82.285714l110.285714-73.714286-110.285714-73.714286v147.428572z m468 312l344.571429-229.714286-153.714286-102.857143-190.857143 127.428572v205.142857z m-44-281.714286l155.428571-104-155.428571-104-155.428571 104zM277.142857 458.857143l190.857143-127.428572V126.285714L123.428571 356z m548.571429 53.142857l110.285714 73.714286V438.285714z m-78.857143-53.142857l153.714286-102.857143-344.571429-229.714286v205.142857z m277.142857-102.857143v312q0 23.428571-19.428571 36.571429l-468 312q-12 7.428571-24.571429 7.428571t-24.571429-7.428571L19.428571 704.571429q-19.428571-13.142857-19.428571-36.571429V356q0-23.428571 19.428571-36.571429L487.428571 7.428571q12-7.428571 24.571429-7.428571t24.571429 7.428571l468 312q19.428571 13.142857 19.428571 36.571429z" p-id="1738"></path></svg>',className:"vuepress-plugin-demo-block__button",datatip:"Codepen"}}])}(f)),void 0!==o.horizontal?o.horizontal:Ss("horizontal")){n.classList.add("vuepress-plugin-demo-block__horizontal");var b=e.firstChild.cloneNode(!0);b.classList.add("vuepress-plugin-demo-block__h_code"),t.appendChild(b)}if(f.css&&function(n){if(!ws[n]){var e=Bs("style",{innerHTML:n});document.body.appendChild(e),ws[n]=!0}}(f.css),"react"===s)ReactDOM.render(React.createElement(f.js),i);else if("vue"===s){var y=(new(Vue.extend(f.script))).$mount();i.appendChild(y.$el)}else"vanilla"===s&&(i.innerHTML=f.html,new Function("return (function(){".concat(f.script,"})()"))());n.dataset.created="true"}})):setTimeout((function(n){Ls()}),300)}function Is(n,e,t,a){var i="1"!==n.dataset.isExpand;t.style.height=i?"".concat(e,"px"):0,i?a.classList.add("vuepress-plugin-demo-block__show-link"):a.classList.remove("vuepress-plugin-demo-block__show-link"),n.dataset.isExpand=i?"1":"0"}var Ns={mounted:function(){window.$VUEPRESS_DEMO_BLOCK={jsfiddle:!1,codepen:!0,horizontal:!1},Ls()},updated:function(){Ls()}},Fs=(t(192),"auto"),js="zoom-in",Ms="zoom-out",Rs="grab",Os="move";function Us(n,e,t){var a=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],i={passive:!1};a?n.addEventListener(e,t,i):n.removeEventListener(e,t,i)}function Vs(n,e){if(n){var t=new Image;t.onload=function(){e&&e(t)},t.src=n}}function Gs(n){return n.dataset.original?n.dataset.original:"A"===n.parentNode.tagName?n.parentNode.getAttribute("href"):null}function Ws(n,e,t){!function(n){var e=Hs,t=qs;if(n.transition){var a=n.transition;delete n.transition,n[e]=a}if(n.transform){var i=n.transform;delete n.transform,n[t]=i}}(e);var a=n.style,i={};for(var r in e)t&&(i[r]=a[r]||""),a[r]=e[r];return i}var Hs="transition",qs="transform",$s="transform",Xs="transitionend";var Zs=function(){},Ks={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:Zs,onClose:Zs,onGrab:Zs,onMove:Zs,onRelease:Zs,onBeforeOpen:Zs,onBeforeClose:Zs,onBeforeGrab:Zs,onBeforeRelease:Zs,onImageLoading:Zs,onImageLoaded:Zs},Ys={init:function(n){var e,t;e=this,t=n,Object.getOwnPropertyNames(Object.getPrototypeOf(e)).forEach((function(n){e[n]=e[n].bind(t)}))},click:function(n){if(n.preventDefault(),Qs(n))return window.open(this.target.srcOriginal||n.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(n.currentTarget)},scroll:function(){var n=document.documentElement||document.body.parentNode||document.body,e=window.pageXOffset||n.scrollLeft,t=window.pageYOffset||n.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:e,y:t});var a=this.lastScrollPosition.x-e,i=this.lastScrollPosition.y-t,r=this.options.scrollThreshold;(Math.abs(i)>=r||Math.abs(a)>=r)&&(this.lastScrollPosition=null,this.close())},keydown:function(n){(function(n){return"Escape"===(n.key||n.code)||27===n.keyCode})(n)&&(this.released?this.close():this.release(this.close))},mousedown:function(n){if(Js(n)&&!Qs(n)){n.preventDefault();var e=n.clientX,t=n.clientY;this.pressTimer=setTimeout(function(){this.grab(e,t)}.bind(this),200)}},mousemove:function(n){this.released||this.move(n.clientX,n.clientY)},mouseup:function(n){Js(n)&&!Qs(n)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(n){n.preventDefault();var e=n.touches[0],t=e.clientX,a=e.clientY;this.pressTimer=setTimeout(function(){this.grab(t,a)}.bind(this),200)},touchmove:function(n){if(!this.released){var e=n.touches[0],t=e.clientX,a=e.clientY;this.move(t,a)}},touchend:function(n){(function(n){n.targetTouches.length})(n)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function Js(n){return 0===n.button}function Qs(n){return n.metaKey||n.ctrlKey}var nl={init:function(n){this.el=document.createElement("div"),this.instance=n,this.parent=document.body,Ws(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(n.options),Us(this.el,"click",n.handler.clickOverlay.bind(n))},updateStyle:function(n){Ws(this.el,{zIndex:n.zIndex,backgroundColor:n.bgColor,transition:"opacity\n        "+n.transitionDuration+"s\n        "+n.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},el="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n},tl=function(){function n(n,e){for(var t=0;t<e.length;t++){var a=e[t];a.enumerable=a.enumerable||!1,a.configurable=!0,"value"in a&&(a.writable=!0),Object.defineProperty(n,a.key,a)}}return function(e,t,a){return t&&n(e.prototype,t),a&&n(e,a),e}}(),al=Object.assign||function(n){for(var e=1;e<arguments.length;e++){var t=arguments[e];for(var a in t)Object.prototype.hasOwnProperty.call(t,a)&&(n[a]=t[a])}return n},il={init:function(n,e){this.el=n,this.instance=e,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=Gs(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var n=this.instance.options,e=n.zIndex,t=n.enableGrab,a=n.transitionDuration,i=n.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:e+1,cursor:t?Rs:Ms,transition:$s+"\n        "+a+"s\n        "+i,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=Ws(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,Ws(this.el,{transform:"none"})},grab:function(n,e,t){var a=rl(),i=a.x-n,r=a.y-e;Ws(this.el,{cursor:Os,transform:"translate3d(\n        "+(this.translate.x+i)+"px, "+(this.translate.y+r)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},move:function(n,e,t){var a=rl(),i=a.x-n,r=a.y-e;Ws(this.el,{transition:$s,transform:"translate3d(\n        "+(this.translate.x+i)+"px, "+(this.translate.y+r)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},restoreCloseStyle:function(){Ws(this.el,this.styleClose)},restoreOpenStyle:function(){Ws(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var n=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var e=this.el.cloneNode(!1);e.setAttribute("src",this.srcOriginal),e.style.position="fixed",e.style.visibility="hidden",n.appendChild(e),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),n.removeChild(e)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var n=rl(),e=this.rect.left+this.rect.width/2,t=this.rect.top+this.rect.height/2;return{x:n.x-e,y:n.y-t}},calculateScale:function(){var n=this.el.dataset,e=n.zoomingHeight,t=n.zoomingWidth,a=this.instance.options,i=a.customSize,r=a.scaleBase;if(!i&&e&&t)return{x:t/this.rect.width,y:e/this.rect.height};if(i&&"object"===(void 0===i?"undefined":el(i)))return{x:i.width/this.rect.width,y:i.height/this.rect.height};var o=this.rect.width/2,s=this.rect.height/2,l=rl(),c={x:l.x-o,y:l.y-s},p=c.x/o,d=c.y/s,u=r+Math.min(p,d);if(i&&"string"==typeof i){var m=t||this.el.naturalWidth,h=e||this.el.naturalHeight,g=parseFloat(i)*m/(100*this.rect.width),f=parseFloat(i)*h/(100*this.rect.height);if(u>g||u>f)return{x:g,y:f}}return{x:u,y:u}}};function rl(){var n=document.documentElement;return{x:Math.min(n.clientWidth,window.innerWidth)/2,y:Math.min(n.clientHeight,window.innerHeight)/2}}function ol(n,e,t){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(a){Us(n,a,e[a],t)}))}var sl=function(){function n(e){!function(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}(this,n),this.target=Object.create(il),this.overlay=Object.create(nl),this.handler=Object.create(Ys),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=al({},Ks,e),this.overlay.init(this),this.handler.init(this)}return tl(n,[{key:"listen",value:function(n){if("string"==typeof n)for(var e=document.querySelectorAll(n),t=e.length;t--;)this.listen(e[t]);else"IMG"===n.tagName&&(n.style.cursor=js,Us(n,"click",this.handler.click),this.options.preloadImage&&Vs(Gs(n)));return this}},{key:"config",value:function(n){return n?(al(this.options,n),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(n){var e=this,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var a="string"==typeof n?document.querySelector(n):n;if("IMG"===a.tagName){if(this.options.onBeforeOpen(a),this.target.init(a,this),!this.options.preloadImage){var i=this.target.srcOriginal;null!=i&&(this.options.onImageLoading(a),Vs(i,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),Us(document,"scroll",this.handler.scroll),Us(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&Us(window,"resize",this.handler.resizeWindow);var r=function n(){Us(a,Xs,n,!1),e.lock=!1,e.target.upgradeSource(),e.options.enableGrab&&ol(document,e.handler,!0),t(a)};return Us(a,Xs,r),this}}}},{key:"close",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeClose(t),this.lock=!0,this.body.style.cursor=Fs,this.overlay.fadeOut(),this.target.zoomOut(),Us(document,"scroll",this.handler.scroll,!1),Us(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&Us(window,"resize",this.handler.resizeWindow,!1);var a=function a(){Us(t,Xs,a,!1),n.shown=!1,n.lock=!1,n.target.downgradeSource(),n.options.enableGrab&&ol(document,n.handler,!1),n.target.restoreCloseStyle(),n.overlay.remove(),e(t)};return Us(t,Xs,a),this}}},{key:"grab",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,a=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var i=this.target.el;this.options.onBeforeGrab(i),this.released=!1,this.target.grab(n,e,t);var r=function n(){Us(i,Xs,n,!1),a(i)};return Us(i,Xs,r),this}}},{key:"move",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,a=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=Os,this.target.move(n,e,t);var i=this.target.el,r=function n(){Us(i,Xs,n,!1),a(i)};return Us(i,Xs,r),this}}},{key:"release",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeRelease(t),this.lock=!0,this.body.style.cursor=Fs,this.target.restoreOpenStyle();var a=function a(){Us(t,Xs,a,!1),n.lock=!1,n.released=!0,e(t)};return Us(t,Xs,a),this}}}]),n}(),ll=".theme-vdoing-content img:not(.no-zoom)",cl=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),pl=Number("500"),dl=function(){function n(){ys(this,n),this.instance=new sl(cl)}return Es(n,[{key:"update",value:function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:ll;"undefined"!=typeof window&&this.instance.listen(n)}},{key:"updateDelay",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:ll,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:pl;setTimeout((function(){return n.update(e)}),t)}}]),n}(),ul=[Zo,es,is,bs,xs,Ns,{watch:{"$page.path":function(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted:function(){this.$vuepress.zooming=new dl,this.$vuepress.zooming.updateDelay()}}],ml={name:"GlobalLayout",computed:{layout:function(){var n=this.getLayout();return Ho("layout",n),zi.component(n)}},methods:{getLayout:function(){if(this.$page.path){var n=this.$page.frontmatter.layout;return n&&(this.$vuepress.getLayoutAsyncComponent(n)||this.$vuepress.getVueComponent(n))?n:"Layout"}return"NotFound"}}},hl=t(25),gl=Object(hl.a)(ml,(function(){var n=this.$createElement;return(this._self._c||n)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(n,e,t){var a;switch(e){case"components":n[e]||(n[e]={}),Object.assign(n[e],t);break;case"mixins":n[e]||(n[e]=[]),(a=n[e]).push.apply(a,Object(wo.a)(t));break;default:throw new Error("Unknown option name.")}}(gl,"mixins",ul);var fl=[{name:"v-1dc59a20",path:"/research/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-1dc59a20").then(t)}},{path:"/research/index.html",redirect:"/research/"},{path:"/00.目录页/01.学术搬砖.html",redirect:"/research/"},{name:"v-212ad4b4",path:"/notes/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-212ad4b4").then(t)}},{path:"/notes/index.html",redirect:"/notes/"},{path:"/00.目录页/02.学习笔记.html",redirect:"/notes/"},{name:"v-5d3b959a",path:"/life/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-5d3b959a").then(t)}},{path:"/life/index.html",redirect:"/life/"},{path:"/00.目录页/03.生活杂谈.html",redirect:"/life/"},{name:"v-12ba5629",path:"/wiki/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-12ba5629").then(t)}},{path:"/wiki/index.html",redirect:"/wiki/"},{path:"/00.目录页/04.wiki搬运.html",redirect:"/wiki/"},{name:"v-57cabcae",path:"/resources/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-57cabcae").then(t)}},{path:"/resources/index.html",redirect:"/resources/"},{path:"/00.目录页/05.资源收藏.html",redirect:"/resources/"},{name:"v-5aaf9a28",path:"/pages/3fe31b/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-5aaf9a28").then(t)}},{path:"/pages/3fe31b/index.html",redirect:"/pages/3fe31b/"},{path:"/01.学术搬砖/01.论文摘抄/00.论文中值得摘抄的句子.html",redirect:"/pages/3fe31b/"},{name:"v-6efe7e76",path:"/pages/7e3623/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-6efe7e76").then(t)}},{path:"/pages/7e3623/index.html",redirect:"/pages/7e3623/"},{path:"/01.学术搬砖/01.论文摘抄/01.论文常用表达.html",redirect:"/pages/7e3623/"},{name:"v-86e9fd48",path:"/pages/551fae/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-86e9fd48").then(t)}},{path:"/pages/551fae/index.html",redirect:"/pages/551fae/"},{path:"/01.学术搬砖/01.论文摘抄/02.LaTeX常用公式.html",redirect:"/pages/551fae/"},{name:"v-4230263f",path:"/pages/7e460e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-4230263f").then(t)}},{path:"/pages/7e460e/index.html",redirect:"/pages/7e460e/"},{path:"/01.学术搬砖/01.论文摘抄/03.论文阅读笔记范文.html",redirect:"/pages/7e460e/"},{name:"v-0ee8bb4e",path:"/pages/a2c095/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0ee8bb4e").then(t)}},{path:"/pages/a2c095/index.html",redirect:"/pages/a2c095/"},{path:"/01.学术搬砖/01.论文摘抄/04.可能会用到的表达.html",redirect:"/pages/a2c095/"},{name:"v-42849146",path:"/pages/268893/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-42849146").then(t)}},{path:"/pages/268893/index.html",redirect:"/pages/268893/"},{path:"/01.学术搬砖/01.论文摘抄/05.撰写论文工具.html",redirect:"/pages/268893/"},{name:"v-62525ab4",path:"/pages/80034b/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-62525ab4").then(t)}},{path:"/pages/80034b/index.html",redirect:"/pages/80034b/"},{path:"/01.学术搬砖/02.论文阅读-图像分类/00.Query2Label A Simple Transformer Way to Multi-Label Classification.html",redirect:"/pages/80034b/"},{name:"v-13f035ec",path:"/pages/2203ea/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-13f035ec").then(t)}},{path:"/pages/2203ea/index.html",redirect:"/pages/2203ea/"},{path:"/01.学术搬砖/02.论文阅读-图像分类/01.Contextual Transformer Networks for Visual Recognition.html",redirect:"/pages/2203ea/"},{name:"v-2ff958f8",path:"/pages/499d0c/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-2ff958f8").then(t)}},{path:"/pages/499d0c/index.html",redirect:"/pages/499d0c/"},{path:"/01.学术搬砖/02.论文阅读-图像分类/02.General Multi-label Image Classification with Transformers.html",redirect:"/pages/499d0c/"},{name:"v-141ae27f",path:"/pages/a34c56/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-141ae27f").then(t)}},{path:"/pages/a34c56/index.html",redirect:"/pages/a34c56/"},{path:"/01.学术搬砖/02.论文阅读-图像分类/03.RepVGG.html",redirect:"/pages/a34c56/"},{name:"v-0de9dd06",path:"/pages/885a91/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0de9dd06").then(t)}},{path:"/pages/885a91/index.html",redirect:"/pages/885a91/"},{path:"/01.学术搬砖/04.论文阅读-知识蒸馏/00.Awesome-Knowledge-distillation.html",redirect:"/pages/885a91/"},{name:"v-3beab5b9",path:"/pages/b34b2b/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-3beab5b9").then(t)}},{path:"/pages/b34b2b/index.html",redirect:"/pages/b34b2b/"},{path:"/01.学术搬砖/05.论文阅读-Transformer/00.Awesome-Visual-Transformer.html",redirect:"/pages/b34b2b/"},{name:"v-2bd63fd0",path:"/pages/a96d59/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-2bd63fd0").then(t)}},{path:"/pages/a96d59/index.html",redirect:"/pages/a96d59/"},{path:"/01.学术搬砖/05.论文阅读-Transformer/01.An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.html",redirect:"/pages/a96d59/"},{name:"v-6d38f8e1",path:"/pages/8d4552/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-6d38f8e1").then(t)}},{path:"/pages/8d4552/index.html",redirect:"/pages/8d4552/"},{path:"/01.学术搬砖/03.论文阅读-语义分割/00.(DeepLabv1) Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs.html",redirect:"/pages/8d4552/"},{name:"v-33e2c2c0",path:"/pages/7cfb60/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-33e2c2c0").then(t)}},{path:"/pages/7cfb60/index.html",redirect:"/pages/7cfb60/"},{path:"/01.学术搬砖/05.论文阅读-Transformer/02.Do Vision Transformers See Like Convolutional Neural Networks.html",redirect:"/pages/7cfb60/"},{name:"v-88f16062",path:"/pages/2a87f2/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-88f16062").then(t)}},{path:"/pages/2a87f2/index.html",redirect:"/pages/2a87f2/"},{path:"/01.学术搬砖/06.论文阅读-图卷积网络/00.Awesome-Graph-Neural-Network.html",redirect:"/pages/2a87f2/"},{name:"v-04892c9a",path:"/pages/6d34a8/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-04892c9a").then(t)}},{path:"/pages/6d34a8/index.html",redirect:"/pages/6d34a8/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/00.Awesome weakly supervised semantic segmentation.html",redirect:"/pages/6d34a8/"},{name:"v-6044af56",path:"/pages/abd9df/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-6044af56").then(t)}},{path:"/pages/abd9df/index.html",redirect:"/pages/abd9df/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/01.Self-supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation.html",redirect:"/pages/abd9df/"},{name:"v-79125705",path:"/pages/a03587/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-79125705").then(t)}},{path:"/pages/a03587/index.html",redirect:"/pages/a03587/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/02.Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks.html",redirect:"/pages/a03587/"},{name:"v-91bf2bb8",path:"/pages/7232a7/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-91bf2bb8").then(t)}},{path:"/pages/7232a7/index.html",redirect:"/pages/7232a7/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/03.Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation.html",redirect:"/pages/7232a7/"},{name:"v-4be82551",path:"/pages/60761b/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-4be82551").then(t)}},{path:"/pages/60761b/index.html",redirect:"/pages/60761b/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/04.Weakly-Supervised Semantic Segmentation via Sub-category Exploration.html",redirect:"/pages/60761b/"},{name:"v-456339ea",path:"/pages/35ccd6/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-456339ea").then(t)}},{path:"/pages/35ccd6/index.html",redirect:"/pages/35ccd6/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/05.Learning Pixel level Semantic Affinity with Image level Supervision for Weakly Supervised Semantic Segmentation.html",redirect:"/pages/35ccd6/"},{name:"v-2ecd8cf6",path:"/pages/cf4b85/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-2ecd8cf6").then(t)}},{path:"/pages/cf4b85/index.html",redirect:"/pages/cf4b85/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/06.Grad-CAM Visual Explanations from Deep Networks via Gradient-based Localization.html",redirect:"/pages/cf4b85/"},{name:"v-04d06b5b",path:"/pages/fa177d/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-04d06b5b").then(t)}},{path:"/pages/fa177d/index.html",redirect:"/pages/fa177d/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/07.Grad-CAM++ Improved Visual Explanations for Deep Convolutional Networks.html",redirect:"/pages/fa177d/"},{name:"v-0e6fb88c",path:"/pages/1e1493/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0e6fb88c").then(t)}},{path:"/pages/1e1493/index.html",redirect:"/pages/1e1493/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/08.Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation.html",redirect:"/pages/1e1493/"},{name:"v-6d7c880a",path:"/pages/6947ae/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-6d7c880a").then(t)}},{path:"/pages/6947ae/index.html",redirect:"/pages/6947ae/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/09.Embedded Discriminative Attention Mechanism for Weakly Supervised Semantic Segmentation.html",redirect:"/pages/6947ae/"},{name:"v-7fc45692",path:"/pages/dd295d/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-7fc45692").then(t)}},{path:"/pages/dd295d/index.html",redirect:"/pages/dd295d/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/10.Group-Wise Semantic Mining for Weakly Supervised Semantic Segmentation.html",redirect:"/pages/dd295d/"},{name:"v-dce64a46",path:"/pages/27c6f8/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-dce64a46").then(t)}},{path:"/pages/27c6f8/index.html",redirect:"/pages/27c6f8/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/11.Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation.html",redirect:"/pages/27c6f8/"},{name:"v-a6e70b22",path:"/pages/611606/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-a6e70b22").then(t)}},{path:"/pages/611606/index.html",redirect:"/pages/611606/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/12.NoPeopleAllowed The Three-Step Approach to Weakly Supervised SemanticSegmentation.html",redirect:"/pages/611606/"},{name:"v-30f9ac0c",path:"/pages/72359e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-30f9ac0c").then(t)}},{path:"/pages/72359e/index.html",redirect:"/pages/72359e/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/13.Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations.html",redirect:"/pages/72359e/"},{name:"v-28c46957",path:"/pages/632c01/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-28c46957").then(t)}},{path:"/pages/632c01/index.html",redirect:"/pages/632c01/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/14.Learning Deep Features for Discriminative Localization.html",redirect:"/pages/632c01/"},{name:"v-0d3a7931",path:"/pages/25f8e7/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0d3a7931").then(t)}},{path:"/pages/25f8e7/index.html",redirect:"/pages/25f8e7/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/15.Convolutional Random Walk Networks for Semantic Image Segmentation.html",redirect:"/pages/25f8e7/"},{name:"v-3706fd7a",path:"/pages/fc1a12/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-3706fd7a").then(t)}},{path:"/pages/fc1a12/index.html",redirect:"/pages/fc1a12/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/16.Learning random-walk label propagation for weakly-supervised semantic segmentation.html",redirect:"/pages/fc1a12/"},{name:"v-2e6c02ba",path:"/pages/25dbf3/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-2e6c02ba").then(t)}},{path:"/pages/25dbf3/index.html",redirect:"/pages/25dbf3/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/17.Puzzle-CAM Improved localization via matching partial and full features.html",redirect:"/pages/25dbf3/"},{name:"v-b5ba63b2",path:"/pages/9bc70f/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-b5ba63b2").then(t)}},{path:"/pages/9bc70f/index.html",redirect:"/pages/9bc70f/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/19.区域擦除 | Object Region Mining with Adversarial Erasing A Simple Classification to Semantic Segmentation Approach.html",redirect:"/pages/9bc70f/"},{name:"v-a46f98a2",path:"/pages/c691d0/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-a46f98a2").then(t)}},{path:"/pages/c691d0/index.html",redirect:"/pages/c691d0/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/20.CAM 扩散 | Tell Me Where to Look Guided Attention Inference Network.html",redirect:"/pages/c691d0/"},{name:"v-6f1a4286",path:"/pages/742623/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-6f1a4286").then(t)}},{path:"/pages/742623/index.html",redirect:"/pages/742623/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/21.Self-Erasing Network for Integral Object Attention.html",redirect:"/pages/742623/"},{name:"v-9dd185bc",path:"/pages/e9bd5f/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-9dd185bc").then(t)}},{path:"/pages/e9bd5f/index.html",redirect:"/pages/e9bd5f/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/22.Transformer CAM|Transformer Interpretability Beyond Attention Visualization.html",redirect:"/pages/e9bd5f/"},{name:"v-e3307746",path:"/pages/54a25d/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-e3307746").then(t)}},{path:"/pages/54a25d/index.html",redirect:"/pages/54a25d/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/23.GETAM Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation.html",redirect:"/pages/54a25d/"},{name:"v-40979294",path:"/pages/fe1b38/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-40979294").then(t)}},{path:"/pages/fe1b38/index.html",redirect:"/pages/fe1b38/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/24.Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation.html",redirect:"/pages/fe1b38/"},{name:"v-456b02c8",path:"/pages/8109a2/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-456b02c8").then(t)}},{path:"/pages/8109a2/index.html",redirect:"/pages/8109a2/"},{path:"/01.学术搬砖/08.论文阅读-半监督图像分割/02.Semi-Supervised Learning by exploiting unlabeled data correlations in a dual-branch network.html",redirect:"/pages/8109a2/"},{name:"v-502d1918",path:"/pages/c40a2c/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-502d1918").then(t)}},{path:"/pages/c40a2c/index.html",redirect:"/pages/c40a2c/"},{path:"/01.学术搬砖/08.论文阅读-半监督图像分割/04.Semi-supervised semantic segmentation needs strong, varied perturbations.html",redirect:"/pages/c40a2c/"},{name:"v-343bb2dc",path:"/pages/26f3ac/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-343bb2dc").then(t)}},{path:"/pages/26f3ac/index.html",redirect:"/pages/26f3ac/"},{path:"/01.学术搬砖/08.论文阅读-半监督图像分割/03.DMT Dynamic Mutual Training for Semi-Supervised Learning.html",redirect:"/pages/26f3ac/"},{name:"v-1c426e16",path:"/pages/62e38a/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-1c426e16").then(t)}},{path:"/pages/62e38a/index.html",redirect:"/pages/62e38a/"},{path:"/01.学术搬砖/07.论文阅读-弱监督图像分割/18.Learning Visual Words for Weakly-Supervised Semantic Segmentation.html",redirect:"/pages/62e38a/"},{name:"v-05671b78",path:"/pages/247800/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-05671b78").then(t)}},{path:"/pages/247800/index.html",redirect:"/pages/247800/"},{path:"/01.学术搬砖/08.论文阅读-半监督图像分割/05.ClassMix Segmentation-Based Data Augmentation for Semi-Supervised Learning.html",redirect:"/pages/247800/"},{name:"v-5a7f928b",path:"/pages/3f8f22/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-5a7f928b").then(t)}},{path:"/pages/3f8f22/index.html",redirect:"/pages/3f8f22/"},{path:"/01.学术搬砖/08.论文阅读-半监督图像分割/07.Semi-supevised Semantic Segmentation with High- and Low-level Consistency.html",redirect:"/pages/3f8f22/"},{name:"v-ac5cebe2",path:"/pages/e8a5ee/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-ac5cebe2").then(t)}},{path:"/pages/e8a5ee/index.html",redirect:"/pages/e8a5ee/"},{path:"/01.学术搬砖/08.论文阅读-半监督图像分割/06.Social-STGCNN A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction.html",redirect:"/pages/e8a5ee/"},{name:"v-10762118",path:"/pages/a86584/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-10762118").then(t)}},{path:"/pages/a86584/index.html",redirect:"/pages/a86584/"},{path:"/01.学术搬砖/08.论文阅读-半监督图像分割/08.Self-Tuning for Data-Efficient Deep Learning.html",redirect:"/pages/a86584/"},{name:"v-cf4b3f26",path:"/pages/29ae03/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-cf4b3f26").then(t)}},{path:"/pages/29ae03/index.html",redirect:"/pages/29ae03/"},{path:"/01.学术搬砖/08.论文阅读-半监督图像分割/09.FixMatch Simplifying Semi-Supervised Learning with Consistency and Confidence.html",redirect:"/pages/29ae03/"},{name:"v-d10e5544",path:"/pages/fc0825/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-d10e5544").then(t)}},{path:"/pages/fc0825/index.html",redirect:"/pages/fc0825/"},{path:"/01.学术搬砖/08.论文阅读-半监督图像分割/10.Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation A Baseline Investigation.html",redirect:"/pages/fc0825/"},{name:"v-5d54258d",path:"/pages/8246b6/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-5d54258d").then(t)}},{path:"/pages/8246b6/index.html",redirect:"/pages/8246b6/"},{path:"/01.学术搬砖/08.论文阅读-半监督图像分割/11.Mean teachers are better role models Weight-averaged consistency targets improve semi-supervised deep learning results.html",redirect:"/pages/8246b6/"},{name:"v-798c7e0f",path:"/pages/0f51c6/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-798c7e0f").then(t)}},{path:"/pages/0f51c6/index.html",redirect:"/pages/0f51c6/"},{path:"/01.学术搬砖/10.论文阅读-小样本学习/02.SCAN Learning to Classify Images without Labels.html",redirect:"/pages/0f51c6/"},{name:"v-4b21ce4b",path:"/pages/bdd933/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-4b21ce4b").then(t)}},{path:"/pages/bdd933/index.html",redirect:"/pages/bdd933/"},{path:"/01.学术搬砖/10.论文阅读-小样本学习/01.Improving Unsupervised Image Clustering With Robust Learning.html",redirect:"/pages/bdd933/"},{name:"v-21b50551",path:"/pages/83a0c4/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-21b50551").then(t)}},{path:"/pages/83a0c4/index.html",redirect:"/pages/83a0c4/"},{path:"/01.学术搬砖/10.论文阅读-小样本学习/03.Sill-Net Feature Augmentation with Separated Illumination Representation.html",redirect:"/pages/83a0c4/"},{name:"v-3d25093c",path:"/pages/1e8d33/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-3d25093c").then(t)}},{path:"/pages/1e8d33/index.html",redirect:"/pages/1e8d33/"},{path:"/01.学术搬砖/11.论文阅读-自监督学习/00.(MoCov1) Momentum Contrast for Unsupervised Visual Representation Learning.html",redirect:"/pages/1e8d33/"},{name:"v-74f65bd4",path:"/pages/464aed/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-74f65bd4").then(t)}},{path:"/pages/464aed/index.html",redirect:"/pages/464aed/"},{path:"/01.学术搬砖/11.论文阅读-自监督学习/01.(SimCLRv1) A Simple Framework for Contrastive Learning of Visual Representations.html",redirect:"/pages/464aed/"},{name:"v-2243d7fc",path:"/pages/1fa222/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-2243d7fc").then(t)}},{path:"/pages/1fa222/index.html",redirect:"/pages/1fa222/"},{path:"/01.学术搬砖/11.论文阅读-自监督学习/02.(SimCLRv2) Big Self-Supervised Models are Strong Semi-Supervised Learners.html",redirect:"/pages/1fa222/"},{name:"v-45276cd6",path:"/pages/bb05d4/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-45276cd6").then(t)}},{path:"/pages/bb05d4/index.html",redirect:"/pages/bb05d4/"},{path:"/01.学术搬砖/11.论文阅读-自监督学习/03.(InstDis) Unsupervised Feature Learning via Non-Parametric Instance Discrimination.html",redirect:"/pages/bb05d4/"},{name:"v-093768b9",path:"/pages/2b22cb/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-093768b9").then(t)}},{path:"/pages/2b22cb/index.html",redirect:"/pages/2b22cb/"},{path:"/01.学术搬砖/11.论文阅读-自监督学习/04.(CPC) Representation Learning with Contrastive Predictive Coding.html",redirect:"/pages/2b22cb/"},{name:"v-2e7a230d",path:"/pages/f6097d/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-2e7a230d").then(t)}},{path:"/pages/f6097d/index.html",redirect:"/pages/f6097d/"},{path:"/01.学术搬砖/11.论文阅读-自监督学习/05.(CMC) Contrastive Multiview Coding, also contains implementations for MoCo and InstDis.html",redirect:"/pages/f6097d/"},{name:"v-0ab31c4e",path:"/pages/b3d215/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0ab31c4e").then(t)}},{path:"/pages/b3d215/index.html",redirect:"/pages/b3d215/"},{path:"/01.学术搬砖/08.论文阅读-半监督图像分割/01.Semi-supervised Semantic Segmentation via Strong-weak Dual-branch Network.html",redirect:"/pages/b3d215/"},{name:"v-1c7c9f96",path:"/pages/682176/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-1c7c9f96").then(t)}},{path:"/pages/682176/index.html",redirect:"/pages/682176/"},{path:"/01.学术搬砖/11.论文阅读-自监督学习/06.(HPT) Self-Supervised Pretraining Improves Self-Supervised Pretraining.html",redirect:"/pages/682176/"},{name:"v-b995e478",path:"/pages/a5a6bb/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-b995e478").then(t)}},{path:"/pages/a5a6bb/index.html",redirect:"/pages/a5a6bb/"},{path:"/01.学术搬砖/10.论文阅读-小样本学习/00.SPICE Semantic Pseudo-labeling for Image Clustering.html",redirect:"/pages/a5a6bb/"},{name:"v-a1f0ce22",path:"/pages/c672e6/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-a1f0ce22").then(t)}},{path:"/pages/c672e6/index.html",redirect:"/pages/c672e6/"},{path:"/01.学术搬砖/08.论文阅读-半监督图像分割/00.Learning from Pixel-Level Label Noise A NewPerspective for Semi-Supervised SemanticSegmentation.html",redirect:"/pages/c672e6/"},{name:"v-c2fce59a",path:"/pages/f3d018/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-c2fce59a").then(t)}},{path:"/pages/f3d018/index.html",redirect:"/pages/f3d018/"},{path:"/01.学术搬砖/11.论文阅读-自监督学习/07.(SimSiam) SimSiam Exploring Simple Siamese Representation Learning.html",redirect:"/pages/f3d018/"},{name:"v-0bbe12ea",path:"/pages/eb7136/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0bbe12ea").then(t)}},{path:"/pages/eb7136/index.html",redirect:"/pages/eb7136/"},{path:"/01.学术搬砖/12.语义分割中的知识蒸馏/00.Structured Knowledge Distillation for Semantic Segmentation.html",redirect:"/pages/eb7136/"},{name:"v-801da7ca",path:"/pages/5f322e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-801da7ca").then(t)}},{path:"/pages/5f322e/index.html",redirect:"/pages/5f322e/"},{path:"/01.学术搬砖/14.论文阅读-其他文章/01.Decompose to Adapt Domain Disentanglement Faster-RCNN for Cross-domain Object Detection.html",redirect:"/pages/5f322e/"},{name:"v-2e36dcd8",path:"/pages/09e28c/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-2e36dcd8").then(t)}},{path:"/pages/09e28c/index.html",redirect:"/pages/09e28c/"},{path:"/01.学术搬砖/11.论文阅读-自监督学习/13.自监督系列代码.html",redirect:"/pages/09e28c/"},{name:"v-75d523ea",path:"/pages/865735/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-75d523ea").then(t)}},{path:"/pages/865735/index.html",redirect:"/pages/865735/"},{path:"/01.学术搬砖/13.学术文章搜集/00.Awesome-Academic-Articals.html",redirect:"/pages/865735/"},{name:"v-4a6374f4",path:"/pages/f542b4/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-4a6374f4").then(t)}},{path:"/pages/f542b4/index.html",redirect:"/pages/f542b4/"},{path:"/02.学习笔记/01.代码实践-目标检测/00.基于深度学习的目标检测技术.html",redirect:"/pages/f542b4/"},{name:"v-232d2f76",path:"/pages/0d3c60/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-232d2f76").then(t)}},{path:"/pages/0d3c60/index.html",redirect:"/pages/0d3c60/"},{path:"/02.学习笔记/01.代码实践-目标检测/01.mmdetection voc.html",redirect:"/pages/0d3c60/"},{name:"v-608f1e1c",path:"/pages/c06123/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-608f1e1c").then(t)}},{path:"/pages/c06123/index.html",redirect:"/pages/c06123/"},{path:"/02.学习笔记/01.代码实践-目标检测/04.cowfits竞赛记录.html",redirect:"/pages/c06123/"},{name:"v-e9f21f7c",path:"/pages/7302ec/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-e9f21f7c").then(t)}},{path:"/pages/7302ec/index.html",redirect:"/pages/7302ec/"},{path:"/02.学习笔记/02.代码实践-图像分割/01.领域自适应.html",redirect:"/pages/7302ec/"},{name:"v-05797640",path:"/pages/4e1e41/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-05797640").then(t)}},{path:"/pages/4e1e41/index.html",redirect:"/pages/4e1e41/"},{path:"/02.学习笔记/02.代码实践-图像分割/02.如何计算一个模型的FPS,Params,GFLOPs.html",redirect:"/pages/4e1e41/"},{name:"v-1218f70b",path:"/pages/679017/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-1218f70b").then(t)}},{path:"/pages/679017/index.html",redirect:"/pages/679017/"},{path:"/02.学习笔记/02.代码实践-图像分割/03.常见数据集的相关知识.html",redirect:"/pages/679017/"},{name:"v-76238680",path:"/pages/80ffb0/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-76238680").then(t)}},{path:"/pages/80ffb0/index.html",redirect:"/pages/80ffb0/"},{path:"/02.学习笔记/02.代码实践-图像分割/04.如何加载数据集.html",redirect:"/pages/80ffb0/"},{name:"v-7e90e4e9",path:"/pages/b8e080/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-7e90e4e9").then(t)}},{path:"/pages/b8e080/index.html",redirect:"/pages/b8e080/"},{path:"/02.学习笔记/02.代码实践-图像分割/05.半监督与弱监督图像分割.html",redirect:"/pages/b8e080/"},{name:"v-60f648f9",path:"/pages/0bdb48/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-60f648f9").then(t)}},{path:"/pages/0bdb48/index.html",redirect:"/pages/0bdb48/"},{path:"/02.学习笔记/01.代码实践-目标检测/03.常用数据集简介.html",redirect:"/pages/0bdb48/"},{name:"v-c1d3d84c",path:"/pages/15a0e4/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-c1d3d84c").then(t)}},{path:"/pages/15a0e4/index.html",redirect:"/pages/15a0e4/"},{path:"/02.学习笔记/02.代码实践-图像分割/06.PASCAL VOC 2012 调色板 color map 赋值.html",redirect:"/pages/15a0e4/"},{name:"v-41cb72a9",path:"/pages/f64db3/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-41cb72a9").then(t)}},{path:"/pages/f64db3/index.html",redirect:"/pages/f64db3/"},{path:"/02.学习笔记/02.代码实践-图像分割/07.语义分割数据集灰度分割图转彩色分割图代码.html",redirect:"/pages/f64db3/"},{name:"v-ed2e8b86",path:"/pages/1708c0/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-ed2e8b86").then(t)}},{path:"/pages/1708c0/index.html",redirect:"/pages/1708c0/"},{path:"/02.学习笔记/01.代码实践-目标检测/02.mmdetection COCO格式数据集.html",redirect:"/pages/1708c0/"},{name:"v-0efa04a6",path:"/pages/5e185e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0efa04a6").then(t)}},{path:"/pages/5e185e/index.html",redirect:"/pages/5e185e/"},{path:"/02.学习笔记/02.代码实践-图像分割/08.复现PSA.html",redirect:"/pages/5e185e/"},{name:"v-2d34cbf4",path:"/pages/6c2aa7/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-2d34cbf4").then(t)}},{path:"/pages/6c2aa7/index.html",redirect:"/pages/6c2aa7/"},{path:"/02.学习笔记/02.代码实践-图像分割/09.转换cityscapes 到对应的类别.html",redirect:"/pages/6c2aa7/"},{name:"v-af5feadc",path:"/pages/ce4f65/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-af5feadc").then(t)}},{path:"/pages/ce4f65/index.html",redirect:"/pages/ce4f65/"},{path:"/02.学习笔记/02.代码实践-图像分割/10.上采样函数.html",redirect:"/pages/ce4f65/"},{name:"v-789b2126",path:"/pages/14bcdb/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-789b2126").then(t)}},{path:"/pages/14bcdb/index.html",redirect:"/pages/14bcdb/"},{path:"/02.学习笔记/02.代码实践-图像分割/00.基于深度学习的图像分割技术.html",redirect:"/pages/14bcdb/"},{name:"v-048810a1",path:"/pages/f55018/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-048810a1").then(t)}},{path:"/pages/f55018/index.html",redirect:"/pages/f55018/"},{path:"/02.学习笔记/02.代码实践-图像分割/12.mIoU的计算.html",redirect:"/pages/f55018/"},{name:"v-c8ffdb4e",path:"/pages/a0a28d/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-c8ffdb4e").then(t)}},{path:"/pages/a0a28d/index.html",redirect:"/pages/a0a28d/"},{path:"/02.学习笔记/02.代码实践-图像分割/13.Multi-label 分类中如何计算 mAP.html",redirect:"/pages/a0a28d/"},{name:"v-4867182a",path:"/pages/aad696/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-4867182a").then(t)}},{path:"/pages/aad696/index.html",redirect:"/pages/aad696/"},{path:"/02.学习笔记/03.代码实践-自监督学习/00.自监督学习的一些文章.html",redirect:"/pages/aad696/"},{name:"v-db08b16e",path:"/pages/86aedf/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-db08b16e").then(t)}},{path:"/pages/86aedf/index.html",redirect:"/pages/86aedf/"},{path:"/02.学习笔记/04.竞赛笔记-视觉竞赛/00.当我们在谈论图像竞赛EDA时在谈论些什么.html",redirect:"/pages/86aedf/"},{name:"v-60eddab5",path:"/pages/a3f895/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-60eddab5").then(t)}},{path:"/pages/a3f895/index.html",redirect:"/pages/a3f895/"},{path:"/02.学习笔记/03.代码实践-自监督学习/01.名词解释.html",redirect:"/pages/a3f895/"},{name:"v-68c570d3",path:"/pages/e182f4/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-68c570d3").then(t)}},{path:"/pages/e182f4/index.html",redirect:"/pages/e182f4/"},{path:"/02.学习笔记/03.代码实践-自监督学习/02.组会思路.html",redirect:"/pages/e182f4/"},{name:"v-035fdc6d",path:"/pages/7f2968/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-035fdc6d").then(t)}},{path:"/pages/7f2968/index.html",redirect:"/pages/7f2968/"},{path:"/02.学习笔记/04.竞赛笔记-视觉竞赛/02.图像检索orReID 竞赛的 Tricks.html",redirect:"/pages/7f2968/"},{name:"v-7f7ee08c",path:"/pages/e518f8/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-7f7ee08c").then(t)}},{path:"/pages/e518f8/index.html",redirect:"/pages/e518f8/"},{path:"/02.学习笔记/04.竞赛笔记-视觉竞赛/01.kaggle-Classify Leaves 竞赛方案学习.html",redirect:"/pages/e518f8/"},{name:"v-03d1a531",path:"/pages/31d8c4/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-03d1a531").then(t)}},{path:"/pages/31d8c4/index.html",redirect:"/pages/31d8c4/"},{path:"/02.学习笔记/04.竞赛笔记-视觉竞赛/04.kaggle 图像分割竞赛学习.html",redirect:"/pages/31d8c4/"},{name:"v-6ed35a41",path:"/pages/9f35c1/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-6ed35a41").then(t)}},{path:"/pages/9f35c1/index.html",redirect:"/pages/9f35c1/"},{path:"/02.学习笔记/04.竞赛笔记-视觉竞赛/03.kaggle-CowBoy Outfits Detection 竞赛方案学习.html",redirect:"/pages/9f35c1/"},{name:"v-026eefc1",path:"/pages/e4a923/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-026eefc1").then(t)}},{path:"/pages/e4a923/index.html",redirect:"/pages/e4a923/"},{path:"/02.学习笔记/04.竞赛笔记-视觉竞赛/05.few-shot learning 竞赛学习-1.html",redirect:"/pages/e4a923/"},{name:"v-aae41b5e",path:"/pages/181ce5/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-aae41b5e").then(t)}},{path:"/pages/181ce5/index.html",redirect:"/pages/181ce5/"},{path:"/02.学习笔记/02.代码实践-图像分割/11.DeepLab系列代码.html",redirect:"/pages/181ce5/"},{name:"v-51bcc501",path:"/pages/8432b6/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-51bcc501").then(t)}},{path:"/pages/8432b6/index.html",redirect:"/pages/8432b6/"},{path:"/02.学习笔记/04.竞赛笔记-视觉竞赛/06.few-shot learning 竞赛学习-2.html",redirect:"/pages/8432b6/"},{name:"v-58142f35",path:"/pages/59507a/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-58142f35").then(t)}},{path:"/pages/59507a/index.html",redirect:"/pages/59507a/"},{path:"/02.学习笔记/04.竞赛笔记-视觉竞赛/07.遥感图像建筑物变化检测竞赛学习-1.html",redirect:"/pages/59507a/"},{name:"v-7d0d935a",path:"/pages/fbe79c/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-7d0d935a").then(t)}},{path:"/pages/fbe79c/index.html",redirect:"/pages/fbe79c/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/00.MMClassifiction 框架学习导言.html",redirect:"/pages/fbe79c/"},{name:"v-087fae29",path:"/pages/a070a4/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-087fae29").then(t)}},{path:"/pages/a070a4/index.html",redirect:"/pages/a070a4/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/01.mmcls 是如何能够通过config 就搭建好一个模型的？.html",redirect:"/pages/a070a4/"},{name:"v-25353850",path:"/pages/e677b8/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-25353850").then(t)}},{path:"/pages/e677b8/index.html",redirect:"/pages/e677b8/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/02.为自己的 inicls 框架加上 fp16 训练.html",redirect:"/pages/e677b8/"},{name:"v-d022b7f6",path:"/pages/5a9505/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-d022b7f6").then(t)}},{path:"/pages/5a9505/index.html",redirect:"/pages/5a9505/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/03.为自己的 inicls 框架集成 Horovod.html",redirect:"/pages/5a9505/"},{name:"v-1aa0d565",path:"/pages/37ee3e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-1aa0d565").then(t)}},{path:"/pages/37ee3e/index.html",redirect:"/pages/37ee3e/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/04.为自己的 inicls 框架集成 DALI.html",redirect:"/pages/37ee3e/"},{name:"v-7d2a1f50",path:"/pages/944bac/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-7d2a1f50").then(t)}},{path:"/pages/944bac/index.html",redirect:"/pages/944bac/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/08.mmcv 使用（上）--Fileio&Image.html",redirect:"/pages/944bac/"},{name:"v-735b9b2c",path:"/pages/125b38/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-735b9b2c").then(t)}},{path:"/pages/125b38/index.html",redirect:"/pages/125b38/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/09.mmcv使用（中）--Config.html",redirect:"/pages/125b38/"},{name:"v-0ce6a857",path:"/pages/16f307/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0ce6a857").then(t)}},{path:"/pages/16f307/index.html",redirect:"/pages/16f307/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/10.什么是 Register.html",redirect:"/pages/16f307/"},{name:"v-28b40785",path:"/pages/1a8345/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-28b40785").then(t)}},{path:"/pages/1a8345/index.html",redirect:"/pages/1a8345/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/11.什么是 ABCMeta.html",redirect:"/pages/1a8345/"},{name:"v-55caee54",path:"/pages/6b655a/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-55caee54").then(t)}},{path:"/pages/6b655a/index.html",redirect:"/pages/6b655a/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/12.mmseg数据集.html",redirect:"/pages/6b655a/"},{name:"v-307f090f",path:"/pages/9b8ee5/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-307f090f").then(t)}},{path:"/pages/9b8ee5/index.html",redirect:"/pages/9b8ee5/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/14.计算loss和计算metric.html",redirect:"/pages/9b8ee5/"},{name:"v-66324249",path:"/pages/234602/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-66324249").then(t)}},{path:"/pages/234602/index.html",redirect:"/pages/234602/"},{path:"/02.学习笔记/06.讲座记录-有意思的文章集合/00.尚未阅读的各类文章.html",redirect:"/pages/234602/"},{name:"v-1690b4cd",path:"/pages/1789a5/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-1690b4cd").then(t)}},{path:"/pages/1789a5/index.html",redirect:"/pages/1789a5/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/13.mmseg 推理单张图像并保存.html",redirect:"/pages/1789a5/"},{name:"v-37c4561a",path:"/pages/759a29/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-37c4561a").then(t)}},{path:"/pages/759a29/index.html",redirect:"/pages/759a29/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/07.mmsegmentation框架解析（下）.html",redirect:"/pages/759a29/"},{name:"v-7788edac",path:"/pages/1b9e2a/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-7788edac").then(t)}},{path:"/pages/1b9e2a/index.html",redirect:"/pages/1b9e2a/"},{path:"/02.学习笔记/06.讲座记录-有意思的文章集合/01.VALSE Paper 探索简单孪生网络表征学习.html",redirect:"/pages/1b9e2a/"},{name:"v-a5dc9b2a",path:"/pages/ada345/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-a5dc9b2a").then(t)}},{path:"/pages/ada345/index.html",redirect:"/pages/ada345/"},{path:"/02.学习笔记/06.讲座记录-有意思的文章集合/03.VALSE Webinar 21-19 弱监督视觉学习：定位、分割和其他.html",redirect:"/pages/ada345/"},{name:"v-348c00c9",path:"/pages/49a01a/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-348c00c9").then(t)}},{path:"/pages/49a01a/index.html",redirect:"/pages/49a01a/"},{path:"/02.学习笔记/06.讲座记录-有意思的文章集合/02.TeachBeat 不确定性学习在视觉识别中的应用.html",redirect:"/pages/49a01a/"},{name:"v-0f686957",path:"/pages/669e4a/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0f686957").then(t)}},{path:"/pages/669e4a/index.html",redirect:"/pages/669e4a/"},{path:"/02.学习笔记/06.讲座记录-有意思的文章集合/05.TeachBeat 物体检测中的训练样本采样.html",redirect:"/pages/669e4a/"},{name:"v-0bde3425",path:"/pages/cdcabd/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0bde3425").then(t)}},{path:"/pages/cdcabd/index.html",redirect:"/pages/cdcabd/"},{path:"/02.学习笔记/06.讲座记录-有意思的文章集合/04.VALSE Tutorial A Tutorial of Transformers.html",redirect:"/pages/cdcabd/"},{name:"v-380a3df9",path:"/pages/bc39b9/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-380a3df9").then(t)}},{path:"/pages/bc39b9/index.html",redirect:"/pages/bc39b9/"},{path:"/02.学习笔记/07.体会感悟-产品沉思录观后有感/00.控制输入才能更好地控制输出.html",redirect:"/pages/bc39b9/"},{name:"v-7030c545",path:"/pages/7f9724/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-7030c545").then(t)}},{path:"/pages/7f9724/index.html",redirect:"/pages/7f9724/"},{path:"/02.学习笔记/06.讲座记录-有意思的文章集合/06.VALSE Webinar 20-02 元学习与小样本学习.html",redirect:"/pages/7f9724/"},{name:"v-8c039fd8",path:"/pages/8ee315/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-8c039fd8").then(t)}},{path:"/pages/8ee315/index.html",redirect:"/pages/8ee315/"},{path:"/02.学习笔记/07.体会感悟-产品沉思录观后有感/01.差异化才能生存，泯然众生是宇宙引力.html",redirect:"/pages/8ee315/"},{name:"v-5afadf0a",path:"/pages/e6bc11/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-5afadf0a").then(t)}},{path:"/pages/e6bc11/index.html",redirect:"/pages/e6bc11/"},{path:"/02.学习笔记/07.体会感悟-产品沉思录观后有感/02.做加法易，而做减法需要更多的认知努力.html",redirect:"/pages/e6bc11/"},{name:"v-bf597a0c",path:"/pages/18a53e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-bf597a0c").then(t)}},{path:"/pages/18a53e/index.html",redirect:"/pages/18a53e/"},{path:"/02.学习笔记/07.体会感悟-产品沉思录观后有感/03.时间的河入海流.html",redirect:"/pages/18a53e/"},{name:"v-7b4fdfde",path:"/pages/60f54c/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-7b4fdfde").then(t)}},{path:"/pages/60f54c/index.html",redirect:"/pages/60f54c/"},{path:"/02.学习笔记/08.体会感悟-摄影/01.摄影文章收藏.html",redirect:"/pages/60f54c/"},{name:"v-7cba1716",path:"/pages/3fb5c1/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-7cba1716").then(t)}},{path:"/pages/3fb5c1/index.html",redirect:"/pages/3fb5c1/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/05.mmsegmentation框架解析（上）.html",redirect:"/pages/3fb5c1/"},{name:"v-60c8d04a",path:"/pages/0611eb/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-60c8d04a").then(t)}},{path:"/pages/0611eb/index.html",redirect:"/pages/0611eb/"},{path:"/02.学习笔记/08.体会感悟-摄影/03.胶片相机泛谈.html",redirect:"/pages/0611eb/"},{name:"v-8f837ea0",path:"/pages/2c9bb8/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-8f837ea0").then(t)}},{path:"/pages/2c9bb8/index.html",redirect:"/pages/2c9bb8/"},{path:"/02.学习笔记/05.框架解析-mmlab系列/06.mmsegmentation框架解析（中）.html",redirect:"/pages/2c9bb8/"},{name:"v-1973eb08",path:"/pages/736ec2/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-1973eb08").then(t)}},{path:"/pages/736ec2/index.html",redirect:"/pages/736ec2/"},{path:"/02.学习笔记/08.体会感悟-摄影/04.后期泛谈.html",redirect:"/pages/736ec2/"},{name:"v-74c89e7c",path:"/pages/405167/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-74c89e7c").then(t)}},{path:"/pages/405167/index.html",redirect:"/pages/405167/"},{path:"/02.学习笔记/09.系列笔记-.特征可视化/00.特征可视化简介.html",redirect:"/pages/405167/"},{name:"v-56003e12",path:"/pages/7cc0fb/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-56003e12").then(t)}},{path:"/pages/7cc0fb/index.html",redirect:"/pages/7cc0fb/"},{path:"/02.学习笔记/09.系列笔记-.特征可视化/02.特征图可视化.html",redirect:"/pages/7cc0fb/"},{name:"v-5c9bb8c7",path:"/pages/78c106/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-5c9bb8c7").then(t)}},{path:"/pages/78c106/index.html",redirect:"/pages/78c106/"},{path:"/02.学习笔记/09.系列笔记-.特征可视化/03.卷积核可视化.html",redirect:"/pages/78c106/"},{name:"v-05ab1571",path:"/pages/26899e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-05ab1571").then(t)}},{path:"/pages/26899e/index.html",redirect:"/pages/26899e/"},{path:"/02.学习笔记/09.系列笔记-.特征可视化/01.类激活热力图可视化.html",redirect:"/pages/26899e/"},{name:"v-cc9eb5f0",path:"/pages/9867d4/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-cc9eb5f0").then(t)}},{path:"/pages/9867d4/index.html",redirect:"/pages/9867d4/"},{path:"/02.学习笔记/10.系列笔记-乐理和五线谱/00.五线谱基础（上）.html",redirect:"/pages/9867d4/"},{name:"v-232d0ce8",path:"/pages/4ac034/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-232d0ce8").then(t)}},{path:"/pages/4ac034/index.html",redirect:"/pages/4ac034/"},{path:"/02.学习笔记/10.系列笔记-乐理和五线谱/01.五线谱基础（下）.html",redirect:"/pages/4ac034/"},{name:"v-0fbc1026",path:"/pages/5f0c87/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0fbc1026").then(t)}},{path:"/pages/5f0c87/index.html",redirect:"/pages/5f0c87/"},{path:"/02.学习笔记/08.体会感悟-摄影/02.富士相机泛谈.html",redirect:"/pages/5f0c87/"},{name:"v-20a0e6d4",path:"/pages/2cfd0e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-20a0e6d4").then(t)}},{path:"/pages/2cfd0e/index.html",redirect:"/pages/2cfd0e/"},{path:"/02.学习笔记/11.系列笔记-爬虫实践/01.爬虫基础.html",redirect:"/pages/2cfd0e/"},{name:"v-5154597e",path:"/pages/ad2d26/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-5154597e").then(t)}},{path:"/pages/ad2d26/index.html",redirect:"/pages/ad2d26/"},{path:"/02.学习笔记/11.系列笔记-爬虫实践/02.Beautiful-soup4、Xpath、re.html",redirect:"/pages/ad2d26/"},{name:"v-78fd300e",path:"/pages/a36ee1/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-78fd300e").then(t)}},{path:"/pages/a36ee1/index.html",redirect:"/pages/a36ee1/"},{path:"/02.学习笔记/11.系列笔记-爬虫实践/03.session和cookie、代理、selenium自动化.html",redirect:"/pages/a36ee1/"},{name:"v-1fb62b40",path:"/pages/597550/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-1fb62b40").then(t)}},{path:"/pages/597550/index.html",redirect:"/pages/597550/"},{path:"/02.学习笔记/12.系列笔记-Django学习笔记/01.Django学习笔记（一）--简单入门.html",redirect:"/pages/597550/"},{name:"v-52e3e8cf",path:"/pages/359c09/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-52e3e8cf").then(t)}},{path:"/pages/359c09/index.html",redirect:"/pages/359c09/"},{path:"/02.学习笔记/12.系列笔记-Django学习笔记/00.安装云环境的mysql.html",redirect:"/pages/359c09/"},{name:"v-16aa8d4a",path:"/pages/3dcbbc/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-16aa8d4a").then(t)}},{path:"/pages/3dcbbc/index.html",redirect:"/pages/3dcbbc/"},{path:"/02.学习笔记/13.系列笔记-Git 使用笔记/00.常见错误.html",redirect:"/pages/3dcbbc/"},{name:"v-2cb0147f",path:"/pages/fc23c1/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-2cb0147f").then(t)}},{path:"/pages/fc23c1/index.html",redirect:"/pages/fc23c1/"},{path:"/02.学习笔记/12.系列笔记-Django学习笔记/02.Django学习笔记（二）-- 连接MySQL数据库的小应用.html",redirect:"/pages/fc23c1/"},{name:"v-5388bbd0",path:"/pages/c9c01b/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-5388bbd0").then(t)}},{path:"/pages/c9c01b/index.html",redirect:"/pages/c9c01b/"},{path:"/02.学习笔记/13.系列笔记-Git 使用笔记/01.本地分支的新建.html",redirect:"/pages/c9c01b/"},{name:"v-bfbdbcee",path:"/pages/27a54a/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-bfbdbcee").then(t)}},{path:"/pages/27a54a/index.html",redirect:"/pages/27a54a/"},{path:"/02.学习笔记/13.系列笔记-Git 使用笔记/02.fork 代码后想要同步至最新版.html",redirect:"/pages/27a54a/"},{name:"v-47a2a4fb",path:"/pages/1e41a6/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-47a2a4fb").then(t)}},{path:"/pages/1e41a6/index.html",redirect:"/pages/1e41a6/"},{path:"/02.学习笔记/13.系列笔记-Git 使用笔记/05.撤销 git commit.html",redirect:"/pages/1e41a6/"},{name:"v-0e34e394",path:"/pages/83cc49/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0e34e394").then(t)}},{path:"/pages/83cc49/index.html",redirect:"/pages/83cc49/"},{path:"/02.学习笔记/13.系列笔记-Git 使用笔记/03.remote Support for password authentication was removed Please use a personal access token instead.html",redirect:"/pages/83cc49/"},{name:"v-8a7101b2",path:"/pages/02e47d/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-8a7101b2").then(t)}},{path:"/pages/02e47d/index.html",redirect:"/pages/02e47d/"},{path:"/02.学习笔记/13.系列笔记-Git 使用笔记/06.ssh 与 Git 登录.html",redirect:"/pages/02e47d/"},{name:"v-95b7e762",path:"/pages/ecdf24/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-95b7e762").then(t)}},{path:"/pages/ecdf24/index.html",redirect:"/pages/ecdf24/"},{path:"/02.学习笔记/13.系列笔记-Git 使用笔记/04.撤销 git add.html",redirect:"/pages/ecdf24/"},{name:"v-d675efea",path:"/pages/4000c9/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-d675efea").then(t)}},{path:"/pages/4000c9/index.html",redirect:"/pages/4000c9/"},{path:"/02.学习笔记/13.系列笔记-Git 使用笔记/07.Git 设置代理.html",redirect:"/pages/4000c9/"},{name:"v-90d4e4ee",path:"/pages/2da48e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-90d4e4ee").then(t)}},{path:"/pages/2da48e/index.html",redirect:"/pages/2da48e/"},{path:"/02.学习笔记/14.系列笔记-网站搭建/00.flarum 论坛搭建.html",redirect:"/pages/2da48e/"},{name:"v-da9fda54",path:"/pages/83977c/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-da9fda54").then(t)}},{path:"/pages/83977c/index.html",redirect:"/pages/83977c/"},{path:"/02.学习笔记/13.系列笔记-Git 使用笔记/09.README 美化.html",redirect:"/pages/83977c/"},{name:"v-167dd495",path:"/pages/950767/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-167dd495").then(t)}},{path:"/pages/950767/index.html",redirect:"/pages/950767/"},{path:"/02.学习笔记/13.系列笔记-Git 使用笔记/08.Unverify.html",redirect:"/pages/950767/"},{name:"v-103ba0a2",path:"/pages/eef8e4/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-103ba0a2").then(t)}},{path:"/pages/eef8e4/index.html",redirect:"/pages/eef8e4/"},{path:"/02.学习笔记/15.系列笔记-图卷积网络/01.图游走类模型.html",redirect:"/pages/eef8e4/"},{name:"v-8a1baa90",path:"/pages/a12467/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-8a1baa90").then(t)}},{path:"/pages/a12467/index.html",redirect:"/pages/a12467/"},{path:"/02.学习笔记/15.系列笔记-图卷积网络/02.节点分类是如何训练的.html",redirect:"/pages/a12467/"},{name:"v-0e6d37a8",path:"/pages/1a1c0e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0e6d37a8").then(t)}},{path:"/pages/1a1c0e/index.html",redirect:"/pages/1a1c0e/"},{path:"/02.学习笔记/15.系列笔记-图卷积网络/00.图学习初印象.html",redirect:"/pages/1a1c0e/"},{name:"v-0a57134c",path:"/pages/79bd3f/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0a57134c").then(t)}},{path:"/pages/79bd3f/index.html",redirect:"/pages/79bd3f/"},{path:"/02.学习笔记/16.课程笔记-MIT-NULL/00.Course overview and the shell.html",redirect:"/pages/79bd3f/"},{name:"v-2a51b9b5",path:"/pages/a70988/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-2a51b9b5").then(t)}},{path:"/pages/a70988/index.html",redirect:"/pages/a70988/"},{path:"/02.学习笔记/17.系列笔记-OpenCV-Python/02.图像增强-几何及灰度变换.html",redirect:"/pages/a70988/"},{name:"v-29693aaf",path:"/pages/8cab5c/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-29693aaf").then(t)}},{path:"/pages/8cab5c/index.html",redirect:"/pages/8cab5c/"},{path:"/02.学习笔记/17.系列笔记-OpenCV-Python/01.图像插值.html",redirect:"/pages/8cab5c/"},{name:"v-416df733",path:"/pages/846a1d/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-416df733").then(t)}},{path:"/pages/846a1d/index.html",redirect:"/pages/846a1d/"},{path:"/02.学习笔记/17.系列笔记-OpenCV-Python/04.图像滤波.html",redirect:"/pages/846a1d/"},{name:"v-966faedc",path:"/pages/adde71/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-966faedc").then(t)}},{path:"/pages/adde71/index.html",redirect:"/pages/adde71/"},{path:"/02.学习笔记/17.系列笔记-OpenCV-Python/03.彩色空间互转.html",redirect:"/pages/adde71/"},{name:"v-1beb24d8",path:"/pages/b41db5/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-1beb24d8").then(t)}},{path:"/pages/b41db5/index.html",redirect:"/pages/b41db5/"},{path:"/02.学习笔记/17.系列笔记-OpenCV-Python/05.阈值分割及二值化.html",redirect:"/pages/b41db5/"},{name:"v-ed2b6be2",path:"/pages/3a4147/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-ed2b6be2").then(t)}},{path:"/pages/3a4147/index.html",redirect:"/pages/3a4147/"},{path:"/02.学习笔记/18.系列笔记-使用 Beancount 记账/01.利用 GitHub 以及商家的批量导入记账数据.html",redirect:"/pages/3a4147/"},{name:"v-59d4b975",path:"/pages/ac712c/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-59d4b975").then(t)}},{path:"/pages/ac712c/index.html",redirect:"/pages/ac712c/"},{path:"/02.学习笔记/18.系列笔记-使用 Beancount 记账/00.入门 Beancount.html",redirect:"/pages/ac712c/"},{name:"v-05774214",path:"/pages/f53f1f/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-05774214").then(t)}},{path:"/pages/f53f1f/index.html",redirect:"/pages/f53f1f/"},{path:"/02.学习笔记/18.系列笔记-使用 Beancount 记账/02.将 Cashwarden 的数据导入 Beancount.html",redirect:"/pages/f53f1f/"},{name:"v-b8a31364",path:"/pages/eee0ed/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-b8a31364").then(t)}},{path:"/pages/eee0ed/index.html",redirect:"/pages/eee0ed/"},{path:"/02.学习笔记/19.系列笔记-Python设计模式/00.工厂方法模式.html",redirect:"/pages/eee0ed/"},{name:"v-39505a4f",path:"/pages/2bcb1d/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-39505a4f").then(t)}},{path:"/pages/2bcb1d/index.html",redirect:"/pages/2bcb1d/"},{path:"/02.学习笔记/20.系列笔记-MLOps/01.(Notes) A Chat with Andrew on MLOps From Model-centric to Data-centric AI.html",redirect:"/pages/2bcb1d/"},{name:"v-552c5d34",path:"/pages/8a695b/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-552c5d34").then(t)}},{path:"/pages/8a695b/index.html",redirect:"/pages/8a695b/"},{path:"/02.学习笔记/20.系列笔记-MLOps/00.关于MLOps.html",redirect:"/pages/8a695b/"},{name:"v-6ce7636a",path:"/pages/ac6238/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-6ce7636a").then(t)}},{path:"/pages/ac6238/index.html",redirect:"/pages/ac6238/"},{path:"/02.学习笔记/20.系列笔记-MLOps/02.常用的数据治理手段.html",redirect:"/pages/ac6238/"},{name:"v-1f470076",path:"/pages/899a9e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-1f470076").then(t)}},{path:"/pages/899a9e/index.html",redirect:"/pages/899a9e/"},{path:"/02.学习笔记/21.系列笔记-Apollo自动驾驶/00.Apollo 核心模块.html",redirect:"/pages/899a9e/"},{name:"v-2e51ed34",path:"/pages/799e91/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-2e51ed34").then(t)}},{path:"/pages/799e91/index.html",redirect:"/pages/799e91/"},{path:"/02.学习笔记/21.系列笔记-Apollo自动驾驶/01.自动驾驶简介.html",redirect:"/pages/799e91/"},{name:"v-6ea858af",path:"/pages/e423da/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-6ea858af").then(t)}},{path:"/pages/e423da/index.html",redirect:"/pages/e423da/"},{path:"/02.学习笔记/21.系列笔记-Apollo自动驾驶/02.高精度地图.html",redirect:"/pages/e423da/"},{name:"v-97cae6c4",path:"/pages/1d8a53/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-97cae6c4").then(t)}},{path:"/pages/1d8a53/index.html",redirect:"/pages/1d8a53/"},{path:"/02.学习笔记/21.系列笔记-Apollo自动驾驶/04.感知.html",redirect:"/pages/1d8a53/"},{name:"v-718e9ea6",path:"/pages/7dc507/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-718e9ea6").then(t)}},{path:"/pages/7dc507/index.html",redirect:"/pages/7dc507/"},{path:"/02.学习笔记/21.系列笔记-Apollo自动驾驶/05.预测.html",redirect:"/pages/7dc507/"},{name:"v-24a8bf61",path:"/pages/8beaa5/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-24a8bf61").then(t)}},{path:"/pages/8beaa5/index.html",redirect:"/pages/8beaa5/"},{path:"/02.学习笔记/21.系列笔记-Apollo自动驾驶/06.规划.html",redirect:"/pages/8beaa5/"},{name:"v-4ee69ba8",path:"/pages/b43d07/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-4ee69ba8").then(t)}},{path:"/pages/b43d07/index.html",redirect:"/pages/b43d07/"},{path:"/02.学习笔记/21.系列笔记-Apollo自动驾驶/07.控制.html",redirect:"/pages/b43d07/"},{name:"v-47f5d7b3",path:"/pages/0ba4e9/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-47f5d7b3").then(t)}},{path:"/pages/0ba4e9/index.html",redirect:"/pages/0ba4e9/"},{path:"/02.学习笔记/22.系列笔记-PaddlePaddle/00.整体评分.html",redirect:"/pages/0ba4e9/"},{name:"v-c123a972",path:"/pages/b4bbc7/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-c123a972").then(t)}},{path:"/pages/b4bbc7/index.html",redirect:"/pages/b4bbc7/"},{path:"/02.学习笔记/25.深度学习及机器学习理论知识学习笔记/00.极大似然函数.html",redirect:"/pages/b4bbc7/"},{name:"v-7181a7c3",path:"/pages/0f2c42/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-7181a7c3").then(t)}},{path:"/pages/0f2c42/index.html",redirect:"/pages/0f2c42/"},{path:"/02.学习笔记/23.系列笔记-视频操作/00.ffmpeg 库的使用.html",redirect:"/pages/0f2c42/"},{name:"v-502a83ef",path:"/pages/fbd258/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-502a83ef").then(t)}},{path:"/pages/fbd258/index.html",redirect:"/pages/fbd258/"},{path:"/02.学习笔记/25.深度学习及机器学习理论知识学习笔记/01.逻辑回归与sigmoid.html",redirect:"/pages/fbd258/"},{name:"v-4af706ce",path:"/pages/bc5b38/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-4af706ce").then(t)}},{path:"/pages/bc5b38/index.html",redirect:"/pages/bc5b38/"},{path:"/02.学习笔记/25.深度学习及机器学习理论知识学习笔记/02.softmax与交叉熵.html",redirect:"/pages/bc5b38/"},{name:"v-6fd27afe",path:"/pages/638dfe/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-6fd27afe").then(t)}},{path:"/pages/638dfe/index.html",redirect:"/pages/638dfe/"},{path:"/02.学习笔记/21.系列笔记-Apollo自动驾驶/03.定位.html",redirect:"/pages/638dfe/"},{name:"v-38b4cc66",path:"/pages/38d46e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-38b4cc66").then(t)}},{path:"/pages/38d46e/index.html",redirect:"/pages/38d46e/"},{path:"/02.学习笔记/25.深度学习及机器学习理论知识学习笔记/03.矩估计.html",redirect:"/pages/38d46e/"},{name:"v-19930428",path:"/pages/e646c8/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-19930428").then(t)}},{path:"/pages/e646c8/index.html",redirect:"/pages/e646c8/"},{path:"/02.学习笔记/25.深度学习及机器学习理论知识学习笔记/04.损失函数的前置知识.html",redirect:"/pages/e646c8/"},{name:"v-06a7f062",path:"/pages/60e26f/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-06a7f062").then(t)}},{path:"/pages/60e26f/index.html",redirect:"/pages/60e26f/"},{path:"/02.学习笔记/26.PyTorch Tricks/00.PyTorch 常见代码片段.html",redirect:"/pages/60e26f/"},{name:"v-2df9e565",path:"/pages/967c74/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-2df9e565").then(t)}},{path:"/pages/967c74/index.html",redirect:"/pages/967c74/"},{path:"/03.生活杂谈/01.心情杂货/01.敬告青年.html",redirect:"/pages/967c74/"},{name:"v-1efee980",path:"/pages/21270d/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-1efee980").then(t)}},{path:"/pages/21270d/index.html",redirect:"/pages/21270d/"},{path:"/02.学习笔记/26.PyTorch Tricks/02.使用半精度训练.html",redirect:"/pages/21270d/"},{name:"v-4120d242",path:"/pages/15e77c/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-4120d242").then(t)}},{path:"/pages/15e77c/index.html",redirect:"/pages/15e77c/"},{path:"/02.学习笔记/26.PyTorch Tricks/03.数据增强.html",redirect:"/pages/15e77c/"},{name:"v-ae94161c",path:"/pages/53d32c/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-ae94161c").then(t)}},{path:"/pages/53d32c/index.html",redirect:"/pages/53d32c/"},{path:"/02.学习笔记/26.PyTorch Tricks/04.常见 Tricks 代码片段.html",redirect:"/pages/53d32c/"},{name:"v-9f230ad8",path:"/pages/fe5265/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-9f230ad8").then(t)}},{path:"/pages/fe5265/index.html",redirect:"/pages/fe5265/"},{path:"/03.生活杂谈/01.心情杂货/02.年度计划模板.html",redirect:"/pages/fe5265/"},{name:"v-6ed459d2",path:"/pages/32c4fa/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-6ed459d2").then(t)}},{path:"/pages/32c4fa/index.html",redirect:"/pages/32c4fa/"},{path:"/03.生活杂谈/01.心情杂货/03.年度总结模板.html",redirect:"/pages/32c4fa/"},{name:"v-49a90feb",path:"/pages/a32ea4/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-49a90feb").then(t)}},{path:"/pages/a32ea4/index.html",redirect:"/pages/a32ea4/"},{path:"/03.生活杂谈/01.心情杂货/04.2020年终总结备份.html",redirect:"/pages/a32ea4/"},{name:"v-61ae05f4",path:"/pages/daccdc/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-61ae05f4").then(t)}},{path:"/pages/daccdc/index.html",redirect:"/pages/daccdc/"},{path:"/03.生活杂谈/01.心情杂货/05.六月的离别让人忧伤.html",redirect:"/pages/daccdc/"},{name:"v-20dcdbb3",path:"/pages/aceb33/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-20dcdbb3").then(t)}},{path:"/pages/aceb33/index.html",redirect:"/pages/aceb33/"},{path:"/03.生活杂谈/01.心情杂货/06.2021 年终总结 | 时光飞逝的一年.html",redirect:"/pages/aceb33/"},{name:"v-44df5588",path:"/pages/de6315/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-44df5588").then(t)}},{path:"/pages/de6315/index.html",redirect:"/pages/de6315/"},{path:"/03.生活杂谈/01.心情杂货/07.年少可以听听李宗盛，只是容易上头.html",redirect:"/pages/de6315/"},{name:"v-644ecf6a",path:"/pages/4341a5/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-644ecf6a").then(t)}},{path:"/pages/4341a5/index.html",redirect:"/pages/4341a5/"},{path:"/03.生活杂谈/02.学术杂谈/00.Data-centric vs Model-centric 的个人拙见.html",redirect:"/pages/4341a5/"},{name:"v-34007dfb",path:"/pages/eb4db7/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-34007dfb").then(t)}},{path:"/pages/eb4db7/index.html",redirect:"/pages/eb4db7/"},{path:"/02.学习笔记/26.PyTorch Tricks/01.使用单机多卡分布式训练.html",redirect:"/pages/eb4db7/"},{name:"v-903271d4",path:"/pages/06b1ed/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-903271d4").then(t)}},{path:"/pages/06b1ed/index.html",redirect:"/pages/06b1ed/"},{path:"/03.生活杂谈/02.学术杂谈/01.重参数化宇宙的起源.html",redirect:"/pages/06b1ed/"},{name:"v-176eac29",path:"/pages/ba21cc/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-176eac29").then(t)}},{path:"/pages/ba21cc/index.html",redirect:"/pages/ba21cc/"},{path:"/03.生活杂谈/02.学术杂谈/02.动态卷积.html",redirect:"/pages/ba21cc/"},{name:"v-3a012f16",path:"/pages/4de976/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-3a012f16").then(t)}},{path:"/pages/4de976/index.html",redirect:"/pages/4de976/"},{path:"/03.生活杂谈/02.学术杂谈/03.从 Tesla AI Day 看自动驾驶的进展.html",redirect:"/pages/4de976/"},{name:"v-4582675f",path:"/pages/3379fb/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-4582675f").then(t)}},{path:"/pages/3379fb/index.html",redirect:"/pages/3379fb/"},{path:"/04.wiki搬运/01.常见 bug 修复/00.Vuepress deploy时的若干问题.html",redirect:"/pages/3379fb/"},{name:"v-53ea97c3",path:"/pages/b5ac46/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-53ea97c3").then(t)}},{path:"/pages/b5ac46/index.html",redirect:"/pages/b5ac46/"},{path:"/04.wiki搬运/01.常见 bug 修复/01.npm项目启动报错.html",redirect:"/pages/b5ac46/"},{name:"v-08d4a690",path:"/pages/df0682/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-08d4a690").then(t)}},{path:"/pages/df0682/index.html",redirect:"/pages/df0682/"},{path:"/04.wiki搬运/02.环境配置/01.Anaconda下载及配置.html",redirect:"/pages/df0682/"},{name:"v-38d61985",path:"/pages/63e1ef/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-38d61985").then(t)}},{path:"/pages/63e1ef/index.html",redirect:"/pages/63e1ef/"},{path:"/04.wiki搬运/01.常见 bug 修复/02.连接远程服务器显示Host key verification failed.html",redirect:"/pages/63e1ef/"},{name:"v-170f256a",path:"/pages/5628db/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-170f256a").then(t)}},{path:"/pages/5628db/index.html",redirect:"/pages/5628db/"},{path:"/04.wiki搬运/02.环境配置/00.GPU功率不一致.html",redirect:"/pages/5628db/"},{name:"v-b4b99e96",path:"/pages/b6a8b0/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-b4b99e96").then(t)}},{path:"/pages/b6a8b0/index.html",redirect:"/pages/b6a8b0/"},{path:"/04.wiki搬运/02.环境配置/02.从零开始配PyTorch GPU环境.html",redirect:"/pages/b6a8b0/"},{name:"v-22b71997",path:"/pages/cf9a89/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-22b71997").then(t)}},{path:"/pages/cf9a89/index.html",redirect:"/pages/cf9a89/"},{path:"/04.wiki搬运/02.环境配置/03.ubuntu 18-04 搭建 go 语言开发环境.html",redirect:"/pages/cf9a89/"},{name:"v-58bcda0a",path:"/pages/894290/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-58bcda0a").then(t)}},{path:"/pages/894290/index.html",redirect:"/pages/894290/"},{path:"/04.wiki搬运/02.环境配置/04.GPU速度太慢问题排查.html",redirect:"/pages/894290/"},{name:"v-4d3dd9d1",path:"/pages/0b52e1/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-4d3dd9d1").then(t)}},{path:"/pages/0b52e1/index.html",redirect:"/pages/0b52e1/"},{path:"/04.wiki搬运/02.环境配置/05.Ubuntu系统安装.html",redirect:"/pages/0b52e1/"},{name:"v-e762e6a2",path:"/pages/0d3088/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-e762e6a2").then(t)}},{path:"/pages/0d3088/index.html",redirect:"/pages/0d3088/"},{path:"/04.wiki搬运/02.环境配置/06.服务器重装系统.html",redirect:"/pages/0d3088/"},{name:"v-62fa926d",path:"/pages/715b93/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-62fa926d").then(t)}},{path:"/pages/715b93/index.html",redirect:"/pages/715b93/"},{path:"/04.wiki搬运/02.环境配置/07.Clash 配置.html",redirect:"/pages/715b93/"},{name:"v-4392326d",path:"/pages/95cffd/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-4392326d").then(t)}},{path:"/pages/95cffd/index.html",redirect:"/pages/95cffd/"},{path:"/04.wiki搬运/03.常用库的常见用法/00.pandas 库常用用法.html",redirect:"/pages/95cffd/"},{name:"v-7953ded5",path:"/pages/a5960e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-7953ded5").then(t)}},{path:"/pages/a5960e/index.html",redirect:"/pages/a5960e/"},{path:"/04.wiki搬运/03.常用库的常见用法/02.sklearn 库常用用法.html",redirect:"/pages/a5960e/"},{name:"v-5457056d",path:"/pages/52270e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-5457056d").then(t)}},{path:"/pages/52270e/index.html",redirect:"/pages/52270e/"},{path:"/04.wiki搬运/03.常用库的常见用法/01.glob 库常用用法.html",redirect:"/pages/52270e/"},{name:"v-59f183b7",path:"/pages/997b21/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-59f183b7").then(t)}},{path:"/pages/997b21/index.html",redirect:"/pages/997b21/"},{path:"/04.wiki搬运/03.常用库的常见用法/04.图像处理库的常用用法.html",redirect:"/pages/997b21/"},{name:"v-732238a5",path:"/pages/42f579/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-732238a5").then(t)}},{path:"/pages/42f579/index.html",redirect:"/pages/42f579/"},{path:"/04.wiki搬运/03.常用库的常见用法/03.matplotlib 库常用用法--散点图绘制.html",redirect:"/pages/42f579/"},{name:"v-1c2d5bbc",path:"/pages/e7ed74/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-1c2d5bbc").then(t)}},{path:"/pages/e7ed74/index.html",redirect:"/pages/e7ed74/"},{path:"/04.wiki搬运/03.常用库的常见用法/05.ubuntu 系统常用命令.html",redirect:"/pages/e7ed74/"},{name:"v-d9a68072",path:"/pages/37da51/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-d9a68072").then(t)}},{path:"/pages/37da51/index.html",redirect:"/pages/37da51/"},{path:"/04.wiki搬运/03.常用库的常见用法/06.numpy 库常用用法.html",redirect:"/pages/37da51/"},{name:"v-08f68526",path:"/pages/20455a/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-08f68526").then(t)}},{path:"/pages/20455a/index.html",redirect:"/pages/20455a/"},{path:"/04.wiki搬运/03.常用库的常见用法/07.matplotlib 库常用用法.html",redirect:"/pages/20455a/"},{name:"v-fcbfac08",path:"/pages/cdf916/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-fcbfac08").then(t)}},{path:"/pages/cdf916/index.html",redirect:"/pages/cdf916/"},{path:"/04.wiki搬运/03.常用库的常见用法/09.Docker 常用用法.html",redirect:"/pages/cdf916/"},{name:"v-82aa7594",path:"/pages/25adce/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-82aa7594").then(t)}},{path:"/pages/25adce/index.html",redirect:"/pages/25adce/"},{path:"/04.wiki搬运/03.常用库的常见用法/08.tmux 常用用法.html",redirect:"/pages/25adce/"},{name:"v-f453b4b0",path:"/pages/018ff0/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-f453b4b0").then(t)}},{path:"/pages/018ff0/index.html",redirect:"/pages/018ff0/"},{path:"/04.wiki搬运/03.常用库的常见用法/10.zsh 相关用法.html",redirect:"/pages/018ff0/"},{name:"v-4be9171a",path:"/pages/b96e43/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-4be9171a").then(t)}},{path:"/pages/b96e43/index.html",redirect:"/pages/b96e43/"},{path:"/04.wiki搬运/03.常用库的常见用法/11.常见的专有名词.html",redirect:"/pages/b96e43/"},{name:"v-0cefb3e1",path:"/pages/023d1e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-0cefb3e1").then(t)}},{path:"/pages/023d1e/index.html",redirect:"/pages/023d1e/"},{path:"/04.wiki搬运/03.常用库的常见用法/12.sharelatex 部署.html",redirect:"/pages/023d1e/"},{name:"v-203c8ca1",path:"/pages/8b291c/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-203c8ca1").then(t)}},{path:"/pages/8b291c/index.html",redirect:"/pages/8b291c/"},{path:"/04.wiki搬运/03.常用库的常见用法/14.cpp STL 常用用法.html",redirect:"/pages/8b291c/"},{name:"v-b5744878",path:"/pages/29671f/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-b5744878").then(t)}},{path:"/pages/29671f/index.html",redirect:"/pages/29671f/"},{path:"/04.wiki搬运/03.常用库的常见用法/15.Tensorboard 常用用法.html",redirect:"/pages/29671f/"},{name:"v-4401a9cd",path:"/pages/dfec85/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-4401a9cd").then(t)}},{path:"/pages/dfec85/index.html",redirect:"/pages/dfec85/"},{path:"/04.wiki搬运/03.常用库的常见用法/13.typecho 部署.html",redirect:"/pages/dfec85/"},{name:"v-7e1bdab1",path:"/pages/aea6571b7a8bae86/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-7e1bdab1").then(t)}},{path:"/pages/aea6571b7a8bae86/index.html",redirect:"/pages/aea6571b7a8bae86/"},{path:"/05.资源收藏/01.面试资料/00.面试问题集锦.html",redirect:"/pages/aea6571b7a8bae86/"},{name:"v-038a1890",path:"/pages/4f4178/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-038a1890").then(t)}},{path:"/pages/4f4178/index.html",redirect:"/pages/4f4178/"},{path:"/05.资源收藏/01.面试资料/01.面试可能会用到的知识点.html",redirect:"/pages/4f4178/"},{name:"v-60771a10",path:"/pages/85131e/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-60771a10").then(t)}},{path:"/pages/85131e/index.html",redirect:"/pages/85131e/"},{path:"/05.资源收藏/01.面试资料/02.Supermemo 面试知识点卡片-20210808.html",redirect:"/pages/85131e/"},{name:"v-1acf53d4",path:"/about/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-1acf53d4").then(t)}},{path:"/about/index.html",redirect:"/about/"},{path:"/06.关于/01.关于.html",redirect:"/about/"},{name:"v-02f11e84",path:"/pages/d22623/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-02f11e84").then(t)}},{path:"/pages/d22623/index.html",redirect:"/pages/d22623/"},{path:"/05.资源收藏/01.面试资料/03.LeetCode 面试算法题卡片-2021-0808.html",redirect:"/pages/d22623/"},{name:"v-04b09795",path:"/pages/32cc8f/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-04b09795").then(t)}},{path:"/pages/32cc8f/index.html",redirect:"/pages/32cc8f/"},{path:"/05.资源收藏/02.资料搜集/00.资料搜集.html",redirect:"/pages/32cc8f/"},{name:"v-3e931a1c",path:"/tags/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-3e931a1c").then(t)}},{path:"/tags/index.html",redirect:"/tags/"},{path:"/@pages/tagsPage.html",redirect:"/tags/"},{name:"v-171a9f6a",path:"/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-171a9f6a").then(t)}},{path:"/index.html",redirect:"/"},{name:"v-78788d52",path:"/categories/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-78788d52").then(t)}},{path:"/categories/index.html",redirect:"/categories/"},{path:"/@pages/categoriesPage.html",redirect:"/categories/"},{name:"v-4d695430",path:"/friends/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-4d695430").then(t)}},{path:"/friends/index.html",redirect:"/friends/"},{path:"/07.友情链接/01.友情链接.html",redirect:"/friends/"},{name:"v-20ca5f1c",path:"/archives/",component:gl,beforeEnter:function(n,e,t){Wo("Layout","v-20ca5f1c").then(t)}},{path:"/archives/index.html",redirect:"/archives/"},{path:"/@pages/archivesPage.html",redirect:"/archives/"},{path:"*",component:gl}],vl={title:"Muyun99's wiki",description:"",base:"/",headTags:[["link",{rel:"icon",href:"/img/favicon.ico"}],["meta",{name:"keywords",content:""}],["meta",{name:"baidu-site-verification",content:"7F55weZDDc"}],["meta",{name:"theme-color",content:"#11a8cd"}],["script",{"data-ad-client":"ca-pub-7828333725993554",async:"async",src:"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"}]],pages:[{title:"学术搬砖",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"01.学术搬砖",imgUrl:"/img/research.png",description:"论文阅读笔记"}},title:"学术搬砖",date:"2020-03-11T21:50:53.000Z",permalink:"/research",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96.html",relativePath:"00.目录页/01.学术搬砖.md",key:"v-1dc59a20",path:"/research/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/08/02, 21:04:52"},{title:"学习笔记",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"02.学习笔记",imgUrl:"/img/notes.png",description:"对于某个系列的课程或者读书笔记"}},title:"学习笔记",date:"2020-03-11T21:50:54.000Z",permalink:"/notes",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html",relativePath:"00.目录页/02.学习笔记.md",key:"v-212ad4b4",path:"/notes/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/08/02, 21:04:52"},{title:"生活杂谈",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"03.生活杂谈",imgUrl:"/img/life.png",description:"生活的一些杂谈"}},title:"生活杂谈",date:"2020-03-11T21:50:55.000Z",permalink:"/life",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/03.%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88.html",relativePath:"00.目录页/03.生活杂谈.md",key:"v-5d3b959a",path:"/life/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/08/02, 21:04:52"},{title:"wiki搬运",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"04.wiki搬运",imgUrl:"/img/notes.png",description:"包括 Bug 修复、环境配置"}},title:"wiki搬运",date:"2020-03-11T21:50:55.000Z",permalink:"/wiki",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/04.wiki%E6%90%AC%E8%BF%90.html",relativePath:"00.目录页/04.wiki搬运.md",key:"v-12ba5629",path:"/wiki/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/08/02, 21:04:52"},{title:"资源收藏",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"05.资源收藏",imgUrl:"/img/more.png",description:"学习、面试、在线工具等更多文章和页面"}},title:"资源收藏",date:"2020-03-11T21:50:56.000Z",permalink:"/resources",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/05.%E8%B5%84%E6%BA%90%E6%94%B6%E8%97%8F.html",relativePath:"00.目录页/05.资源收藏.md",key:"v-57cabcae",path:"/resources/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/08/02, 21:04:52"},{title:"论文中值得摘抄的句子",frontmatter:{title:"论文中值得摘抄的句子",date:"2021-04-19T21:25:28.000Z",permalink:"/pages/3fe31b/",categories:["论文阅读","论文摘抄"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/01.%E8%AE%BA%E6%96%87%E6%91%98%E6%8A%84/00.%E8%AE%BA%E6%96%87%E4%B8%AD%E5%80%BC%E5%BE%97%E6%91%98%E6%8A%84%E7%9A%84%E5%8F%A5%E5%AD%90.html",relativePath:"01.学术搬砖/01.论文摘抄/00.论文中值得摘抄的句子.md",key:"v-5aaf9a28",path:"/pages/3fe31b/",headers:[{level:3,title:"论文中值得摘抄的句子",slug:"论文中值得摘抄的句子",normalizedTitle:"论文中值得摘抄的句子",charIndex:2},{level:3,title:"每日一句",slug:"每日一句",normalizedTitle:"每日一句",charIndex:1554}],headersStr:"论文中值得摘抄的句子 每日一句",content:"# 论文中值得摘抄的句子\n\n * We abandon the memory queue, which we find has diminishing gain if the batch is sufficiently large：我们抛弃了内存队列机制，我们发现当批样本量相当大的时候，内存队列的性能增益会减小\n * which indicates the misuse of the weak labels：这说明弱标签的使用不当\n * the proposed dual branches separately handle full and weaksupervised learning and effectively eliminate their mutual interference：所提出的双分支独立的处理全监督和弱监督学习，能够有效消除他们的彼此干扰\n * Besides, separate training prohibits the exchange for supervision information：此外，单独的训练组织了监督信息的交换\n * Our DDF method facilitates the feature disentanglement at the global and local stages：我们的DDF方法促进了全局和局部阶段的特征解构\n * To address this dilemma：为了解决这一难题\n * To alleviate this issue：为了缓解这个问题\n * Motivated by the aforementioned observations：出于上述观察的动机\n * The deficiency of segmentation labels is one of the main obstacles to semantic segmentation：分割标签的不足是语义分割的主要障碍之一\n * The characterization of similarity and discrepancy is at thecore of all clustering methods. ：相似性和差异性的特征是所有聚类方法的核心\n * The basic idea of SPICE is to synergize the discrepancy among semantic clusters, the similarity among instance samples, and the semantic con-sistency of local samples in an embedding space to optimize the clustering network in a semantically-driven paradigm.：SPICE 的基本idea是同步：语义类之间的差异性，实例样本间的相似度以及嵌入空间的局部样本间的语义一致性，利用语义驱动的范式来优化聚类网络\n * However, CAM usually onlyidentifies the most discriminative object extents,which is attributed to the fact that the networkdoesn’t need to discover the integral object to recognize image-level labels.：然而，CAM通常只识别出最具鉴别力的物体范围，这是因为网络不需要发现物体全貌来识别图像级别的标签。\n\n\n# 每日一句\n\n * Dream big and dare to fail.\n   * 敢于梦想并敢于失败",normalizedContent:"# 论文中值得摘抄的句子\n\n * we abandon the memory queue, which we find has diminishing gain if the batch is sufficiently large：我们抛弃了内存队列机制，我们发现当批样本量相当大的时候，内存队列的性能增益会减小\n * which indicates the misuse of the weak labels：这说明弱标签的使用不当\n * the proposed dual branches separately handle full and weaksupervised learning and effectively eliminate their mutual interference：所提出的双分支独立的处理全监督和弱监督学习，能够有效消除他们的彼此干扰\n * besides, separate training prohibits the exchange for supervision information：此外，单独的训练组织了监督信息的交换\n * our ddf method facilitates the feature disentanglement at the global and local stages：我们的ddf方法促进了全局和局部阶段的特征解构\n * to address this dilemma：为了解决这一难题\n * to alleviate this issue：为了缓解这个问题\n * motivated by the aforementioned observations：出于上述观察的动机\n * the deficiency of segmentation labels is one of the main obstacles to semantic segmentation：分割标签的不足是语义分割的主要障碍之一\n * the characterization of similarity and discrepancy is at thecore of all clustering methods. ：相似性和差异性的特征是所有聚类方法的核心\n * the basic idea of spice is to synergize the discrepancy among semantic clusters, the similarity among instance samples, and the semantic con-sistency of local samples in an embedding space to optimize the clustering network in a semantically-driven paradigm.：spice 的基本idea是同步：语义类之间的差异性，实例样本间的相似度以及嵌入空间的局部样本间的语义一致性，利用语义驱动的范式来优化聚类网络\n * however, cam usually onlyidentifies the most discriminative object extents,which is attributed to the fact that the networkdoesn’t need to discover the integral object to recognize image-level labels.：然而，cam通常只识别出最具鉴别力的物体范围，这是因为网络不需要发现物体全貌来识别图像级别的标签。\n\n\n# 每日一句\n\n * dream big and dare to fail.\n   * 敢于梦想并敢于失败",charsets:{cjk:!0},lastUpdated:"2021/10/17, 16:23:18"},{title:"论文常用表达",frontmatter:{title:"论文常用表达",date:"2021-04-19T21:25:41.000Z",permalink:"/pages/7e3623/",categories:["论文阅读","论文摘抄"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/01.%E8%AE%BA%E6%96%87%E6%91%98%E6%8A%84/01.%E8%AE%BA%E6%96%87%E5%B8%B8%E7%94%A8%E8%A1%A8%E8%BE%BE.html",relativePath:"01.学术搬砖/01.论文摘抄/01.论文常用表达.md",key:"v-6efe7e76",path:"/pages/7e3623/",headers:[{level:3,title:"论文常用表达",slug:"论文常用表达",normalizedTitle:"论文常用表达",charIndex:2}],headersStr:"论文常用表达",content:"# 论文常用表达\n\n * diminish：减少\n * indicate：表明\n * eliminate：消除\n * For brevity：为了简便起见\n * facilitate：促进\n * discrepancy：差异，差距\n * inevitably：不可避免地\n * deficiency：缺乏，缺陷，不足\n * main obstacles：主要障碍\n * alleviate：缓解\n * issue：问题\n * roughly：大概地\n * leverage：利用（以小博大）",normalizedContent:"# 论文常用表达\n\n * diminish：减少\n * indicate：表明\n * eliminate：消除\n * for brevity：为了简便起见\n * facilitate：促进\n * discrepancy：差异，差距\n * inevitably：不可避免地\n * deficiency：缺乏，缺陷，不足\n * main obstacles：主要障碍\n * alleviate：缓解\n * issue：问题\n * roughly：大概地\n * leverage：利用（以小博大）",charsets:{cjk:!0},lastUpdated:"2021/09/12, 20:42:58"},{title:"LaTeX常用表达",frontmatter:{title:"LaTeX常用表达",date:"2021-05-31T21:04:21.000Z",permalink:"/pages/551fae/",categories:["论文阅读","论文摘抄"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/01.%E8%AE%BA%E6%96%87%E6%91%98%E6%8A%84/02.LaTeX%E5%B8%B8%E7%94%A8%E5%85%AC%E5%BC%8F.html",relativePath:"01.学术搬砖/01.论文摘抄/02.LaTeX常用公式.md",key:"v-86e9fd48",path:"/pages/551fae/",headers:[{level:3,title:"LaTeX 常用表达",slug:"latex-常用表达",normalizedTitle:"latex 常用表达",charIndex:2}],headersStr:"LaTeX 常用表达",content:"# LaTeX 常用表达\n\n\n\n * 叉乘（×\\times×）：\\times\n\n * 点乘（⋅\\cdot⋅）：\\cdot\n\n * 除法：\\frac{a}{b}\n\n * 求和符号：\\sum_{}^{}\n\n * 属于符号（∈\\in∈）：\\in\n\n * 字母空心化（E\\mathbb{E}E）：\\mathbb{E}\n\n * 向量与矩阵运算：符号 ⊙ ，表示方法：\\odot\n   \n   向量与向量运算：符号 ∘\\circ∘，表示方法：\\circ\n\n * ∇\\nabla∇\n\n * \n\n# 参考资料\n\n * http://mohu.org/info/symbols/symbols.htm",normalizedContent:"# latex 常用表达\n\n\n\n * 叉乘（×\\times×）：\\times\n\n * 点乘（⋅\\cdot⋅）：\\cdot\n\n * 除法：\\frac{a}{b}\n\n * 求和符号：\\sum_{}^{}\n\n * 属于符号（∈\\in∈）：\\in\n\n * 字母空心化（e\\mathbb{e}e）：\\mathbb{e}\n\n * 向量与矩阵运算：符号 ⊙ ，表示方法：\\odot\n   \n   向量与向量运算：符号 ∘\\circ∘，表示方法：\\circ\n\n * ∇\\nabla∇\n\n * \n\n# 参考资料\n\n * http://mohu.org/info/symbols/symbols.htm",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"论文阅读笔记范文",frontmatter:{title:"论文阅读笔记范文",date:"2021-07-20T20:36:47.000Z",permalink:"/pages/7e460e/",categories:["论文阅读","论文摘抄"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/01.%E8%AE%BA%E6%96%87%E6%91%98%E6%8A%84/03.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E8%8C%83%E6%96%87.html",relativePath:"01.学术搬砖/01.论文摘抄/03.论文阅读笔记范文.md",key:"v-4230263f",path:"/pages/7e460e/",headers:[{level:2,title:"Puzzle-CAM: Improved localization via matching partial and full features",slug:"puzzle-cam-improved-localization-via-matching-partial-and-full-features",normalizedTitle:"puzzle-cam: improved localization via matching partial and full features",charIndex:2}],headersStr:"Puzzle-CAM: Improved localization via matching partial and full features",content:"# Puzzle-CAM: Improved localization via matching partial and full features\n\n# 单位：KAIST\n\n# 作者：Sanghyun Jo, In-Jae Yu\n\n# 发表：Arxiv\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 总结\n\n# 论文的贡献\n\n论文主要是针对弱监督语义分割提出了 Puzzle 模块，将原始图像分块后再算一个CAMs，并与原始的 CAMs 做一个重建损失，三项损失联合优化分类网络，提升 CAMs 的精度。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料",normalizedContent:"# puzzle-cam: improved localization via matching partial and full features\n\n# 单位：kaist\n\n# 作者：sanghyun jo, in-jae yu\n\n# 发表：arxiv\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 总结\n\n# 论文的贡献\n\n论文主要是针对弱监督语义分割提出了 puzzle 模块，将原始图像分块后再算一个cams，并与原始的 cams 做一个重建损失，三项损失联合优化分类网络，提升 cams 的精度。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"可能会用到的表达",frontmatter:{title:"可能会用到的表达",date:"2021-09-19T16:11:33.000Z",permalink:"/pages/a2c095/",categories:["学术搬砖","论文摘抄"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/01.%E8%AE%BA%E6%96%87%E6%91%98%E6%8A%84/04.%E5%8F%AF%E8%83%BD%E4%BC%9A%E7%94%A8%E5%88%B0%E7%9A%84%E8%A1%A8%E8%BE%BE.html",relativePath:"01.学术搬砖/01.论文摘抄/04.可能会用到的表达.md",key:"v-0ee8bb4e",path:"/pages/a2c095/",headersStr:null,content:"在工程角度来讲，以数据为核心的方案可能比以模型为中心的方案能够取得更好的结果",normalizedContent:"在工程角度来讲，以数据为核心的方案可能比以模型为中心的方案能够取得更好的结果",charsets:{cjk:!0},lastUpdated:"2021/10/17, 16:23:18"},{title:"撰写论文工具",frontmatter:{title:"撰写论文工具",date:"2021-10-16T21:02:53.000Z",permalink:"/pages/268893/",categories:["学术搬砖","论文摘抄"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/01.%E8%AE%BA%E6%96%87%E6%91%98%E6%8A%84/05.%E6%92%B0%E5%86%99%E8%AE%BA%E6%96%87%E5%B7%A5%E5%85%B7.html",relativePath:"01.学术搬砖/01.论文摘抄/05.撰写论文工具.md",key:"v-42849146",path:"/pages/268893/",headersStr:null,content:"1、绘图颜色\n\n颜色选择：https://www.toptal.com/designers/colourcode/\n\n\n\n2、语法：Grammarly",normalizedContent:"1、绘图颜色\n\n颜色选择：https://www.toptal.com/designers/colourcode/\n\n\n\n2、语法：grammarly",charsets:{cjk:!0},lastUpdated:"2021/10/17, 16:23:18"},{title:"Query2Label A Simple Transformer Way to Multi-Label Classification",frontmatter:{title:"Query2Label A Simple Transformer Way to Multi-Label Classification",date:"2021-07-30T00:45:54.000Z",permalink:"/pages/80034b/",categories:["论文阅读","多标签分类"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/02.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/00.Query2Label%20A%20Simple%20Transformer%20Way%20to%20Multi-Label%20Classification.html",relativePath:"01.学术搬砖/02.论文阅读-图像分类/00.Query2Label A Simple Transformer Way to Multi-Label Classification.md",key:"v-62525ab4",path:"/pages/80034b/",headers:[{level:2,title:"Query2Label: A Simple Transformer Way to Multi-Label Classification",slug:"query2label-a-simple-transformer-way-to-multi-label-classification",normalizedTitle:"query2label: a simple transformer way to multi-label classification",charIndex:2},{level:3,title:"摘要阅读",slug:"摘要阅读",normalizedTitle:"摘要阅读",charIndex:154},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:222},{level:3,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:258}],headersStr:"Query2Label: A Simple Transformer Way to Multi-Label Classification 摘要阅读 总结 参考资料",content:"# Query2Label: A Simple Transformer Way to Multi-Label Classification\n\n# 单位：THU\n\n# 作者：Shilong Liu, Lei Zhang, Xiao Yang, Hang Su, Jun Zhu\n\n# 发表：ariXv\n\n\n# 摘要阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n\n\n\n\n\n\nCAM 也不是很完善\n\n\n\n# 论文的方法\n\n# 论文的背景\n\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n\n# 参考资料\n\n * https://arxiv.org/abs/2107.10834\n\n * https://github.com/SlongLiu/query2labels",normalizedContent:"# query2label: a simple transformer way to multi-label classification\n\n# 单位：thu\n\n# 作者：shilong liu, lei zhang, xiao yang, hang su, jun zhu\n\n# 发表：arixv\n\n\n# 摘要阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n\n\n\n\n\n\ncam 也不是很完善\n\n\n\n# 论文的方法\n\n# 论文的背景\n\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n\n# 参考资料\n\n * https://arxiv.org/abs/2107.10834\n\n * https://github.com/slongliu/query2labels",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"Contextual Transformer Networks for Visual Recognition",frontmatter:{title:"Contextual Transformer Networks for Visual Recognition",date:"2021-08-02T21:11:58.000Z",permalink:"/pages/2203ea/",categories:["学术搬砖","论文阅读-图像分类orBackbone"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/02.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/01.Contextual%20Transformer%20Networks%20for%20Visual%20Recognition.html",relativePath:"01.学术搬砖/02.论文阅读-图像分类/01.Contextual Transformer Networks for Visual Recognition.md",key:"v-13f035ec",path:"/pages/2203ea/",headers:[{level:2,title:"Contextual Transformer Networks for Visual Recognition",slug:"contextual-transformer-networks-for-visual-recognition",normalizedTitle:"contextual transformer networks for visual recognition",charIndex:2}],headersStr:"Contextual Transformer Networks for Visual Recognition",content:"# Contextual Transformer Networks for Visual Recognition\n\n# 单位：JD AI Research\n\n# 作者：Yehao Li, Ting Yao, Yingwei Pan, and Tao Mei\n\n# 发表：Arxiv ( Rank 1 in open-set image classification task of Open World Vision Challenge @ CVPR 2021)\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2107.12292\n\n * https://github.com/JDAI-CV/CoTNet",normalizedContent:"# contextual transformer networks for visual recognition\n\n# 单位：jd ai research\n\n# 作者：yehao li, ting yao, yingwei pan, and tao mei\n\n# 发表：arxiv ( rank 1 in open-set image classification task of open world vision challenge @ cvpr 2021)\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2107.12292\n\n * https://github.com/jdai-cv/cotnet",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"General Multi-label Image Classification with Transformers",frontmatter:{title:"General Multi-label Image Classification with Transformers",date:"2022-01-10T14:21:30.000Z",permalink:"/pages/499d0c/",categories:["学术搬砖","论文阅读-图像分类"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/02.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/02.General%20Multi-label%20Image%20Classification%20with%20Transformers.html",relativePath:"01.学术搬砖/02.论文阅读-图像分类/02.General Multi-label Image Classification with Transformers.md",key:"v-2ff958f8",path:"/pages/499d0c/",headersStr:null,content:"参考资料\n\n * https://arxiv.org/abs/2011.14027\n\n * https://github.com/QData/C-Tran",normalizedContent:"参考资料\n\n * https://arxiv.org/abs/2011.14027\n\n * https://github.com/qdata/c-tran",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"RepVGG",frontmatter:{title:"RepVGG",date:"2022-03-18T22:42:32.000Z",permalink:"/pages/a34c56/",categories:["学术搬砖","论文阅读-图像分类"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/02.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/03.RepVGG.html",relativePath:"01.学术搬砖/02.论文阅读-图像分类/03.RepVGG.md",key:"v-141ae27f",path:"/pages/a34c56/",headers:[{level:2,title:"RepVGG: Making VGG-style ConvNets Great Again",slug:"repvgg-making-vgg-style-convnets-great-again",normalizedTitle:"repvgg: making vgg-style convnets great again",charIndex:2}],headersStr:"RepVGG: Making VGG-style ConvNets Great Again",content:"# RepVGG: Making VGG-style ConvNets Great Again\n\n# 单位：THU, MegVII\n\n# 作者：Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, Jian Sun\n\n# 发表：CVPR 2021\n\n# Motivation\n\n * 多分支训练的优点：隐式集成，往往性能更高；\n * 多分支训练的缺点：每个分支结果需要保存，直到最后一步融合才释放掉，比较消耗显存\n * 单分支结构：速度快，省内存，但其性能较低\n\n是否能融合多分支和单分支的优点呢：让单分支网络也有隐式的集成所带来的性能提升\n\nRepVGG就做了这样的事情\n\n# Method\n\n先介绍一下做法：在训练的时候加入多分支结构进行训练，在推理的时候重参数化为单分支结构加速推理\n\nReparam(3x3) = 3x3-BN + 1x1-BN + BN。对每个3x3卷积，在训练时给它构造并行的恒等和1x1卷积分支，并各自过BN后相加。\n\n# Problem\n\n简单看完做法之后，提一些问题：\n\n为什么去掉分支和多的 BN 不会对性能造成影响呢\n\n * 去掉分支是否有等价性的证明？\n   * 作者将训练得到的三个卷积都看作是 3x3 卷积，其参数相加是等价于修改前的多分支的效果\n * 去掉多的 BN 会不会对性能有所影响？\n   * BN 和 Conv 可以融合到一起形成 Fused BN，是加速的常用操作\n * 但是积累的均值和方差会不一样？\n   * 在推理的时候 BN 的均值和方差都是采用训练阶段的\n\n# Details\n\n详细讲下设计到的两个核心部件\n\n1、卷积等价性\n\n参考资料\n\n * https://zhuanlan.zhihu.com/p/344324470\n\n * https://zhuanlan.zhihu.com/p/352239591\n\n * 因为RepVGG Block中的1x1卷积是相当于一个特殊（卷积核中有很多0）的3x3卷积\n\n * 而恒等映射是一个特殊（以单位矩阵为卷积核）的1x1卷积，因此也是一个特殊的3x3卷积！\n\n操作方式\n\n * 1. 把identity转换为1x1卷积，只要构造出一个以单位矩阵为卷积核的1x1卷积即可；\n   2. 把1x1卷积等价转换为3x3卷积，只要用0填充即可。\n\n2、卷积和 BN 进行融合\n\n参考资料：https://zhuanlan.zhihu.com/p/352239591\n\n * Conv(x)=W(x)+bConv(x) = W(x) + bConv(x)=W(x)+b\n\n * BN(x)=γ∗x−meanvar+ϵ+βBN(x) = \\gamma*\\frac{x-mean}{\\sqrt{var} + \\epsilon} + \\betaBN(x)=γ∗var +ϵx−mean +β\n\n * BN(Conv(x))=γ∗W(x)+b−meanvar+ϵ+βBN(Conv(x)) = \\gamma*\\frac{W(x) + b - mean}{\\sqrt{var} + \\epsilon}+ \\betaBN(Conv(x))=γ∗var +ϵW(x)+b−mean +β\n\n将其变形为\n\n * BN(Conv(x))=γ∗W(x)var+ϵ+γ∗(b−mean)var+ϵ+βBN(Conv(x)) = \\gamma*\\frac{W(x) }{\\sqrt{var} + \\epsilon} + \\frac{\\gamma*( b - mean)}{\\sqrt{var} + \\epsilon}+ \\betaBN(Conv(x))=γ∗var +ϵW(x) +var +ϵγ∗(b−mean) +β\n\n那么可以得到一个新卷积层，其参数为\n\n * Wfused=γ∗W(x)var+ϵW_{fused} = \\gamma*\\frac{W(x) }{\\sqrt{var} + \\epsilon}Wfused =γ∗var +ϵW(x)\n * Bfused=γ∗(b−mean)var+ϵ+βB_{fused} = \\gamma * \\frac{( b - mean)}{\\sqrt{var} + \\epsilon}+ \\betaBfused =γ∗var +ϵ(b−mean) +β\n\n最终融合的效果即为\n\n * BN(Conv(x))=Wfused(x)+BfusedBN(Conv(x))=W_{fused}(x) + B_{fused}BN(Conv(x))=Wfused (x)+Bfused\n\n代码参考，这份代码里没有处理原卷积的 bias，因为 RepVGG 的卷积层中没有使用 Bias\n\ndef _fuse_bn_tensor(self, branch):\n    if branch is None:\n        return 0, 0\n    if isinstance(branch, nn.Sequential):\n        kernel = branch.conv.weight\n        running_mean = branch.bn.running_mean\n        running_var = branch.bn.running_var\n        gamma = branch.bn.weight\n        beta = branch.bn.bias\n        eps = branch.bn.eps\n    else:\n        ...\n        std = (running_var + eps).sqrt()\n        t = (gamma / std).reshape(-1, 1, 1, 1)\n        return kernel * t, beta - running_mean * gamma / std\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n# 参考资料\n\n * https://arxiv.org/abs/2101.03697\n\n * https://github.com/DingXiaoH/RepVGG",normalizedContent:"# repvgg: making vgg-style convnets great again\n\n# 单位：thu, megvii\n\n# 作者：xiaohan ding, xiangyu zhang, ningning ma, jungong han, guiguang ding, jian sun\n\n# 发表：cvpr 2021\n\n# motivation\n\n * 多分支训练的优点：隐式集成，往往性能更高；\n * 多分支训练的缺点：每个分支结果需要保存，直到最后一步融合才释放掉，比较消耗显存\n * 单分支结构：速度快，省内存，但其性能较低\n\n是否能融合多分支和单分支的优点呢：让单分支网络也有隐式的集成所带来的性能提升\n\nrepvgg就做了这样的事情\n\n# method\n\n先介绍一下做法：在训练的时候加入多分支结构进行训练，在推理的时候重参数化为单分支结构加速推理\n\nreparam(3x3) = 3x3-bn + 1x1-bn + bn。对每个3x3卷积，在训练时给它构造并行的恒等和1x1卷积分支，并各自过bn后相加。\n\n# problem\n\n简单看完做法之后，提一些问题：\n\n为什么去掉分支和多的 bn 不会对性能造成影响呢\n\n * 去掉分支是否有等价性的证明？\n   * 作者将训练得到的三个卷积都看作是 3x3 卷积，其参数相加是等价于修改前的多分支的效果\n * 去掉多的 bn 会不会对性能有所影响？\n   * bn 和 conv 可以融合到一起形成 fused bn，是加速的常用操作\n * 但是积累的均值和方差会不一样？\n   * 在推理的时候 bn 的均值和方差都是采用训练阶段的\n\n# details\n\n详细讲下设计到的两个核心部件\n\n1、卷积等价性\n\n参考资料\n\n * https://zhuanlan.zhihu.com/p/344324470\n\n * https://zhuanlan.zhihu.com/p/352239591\n\n * 因为repvgg block中的1x1卷积是相当于一个特殊（卷积核中有很多0）的3x3卷积\n\n * 而恒等映射是一个特殊（以单位矩阵为卷积核）的1x1卷积，因此也是一个特殊的3x3卷积！\n\n操作方式\n\n * 1. 把identity转换为1x1卷积，只要构造出一个以单位矩阵为卷积核的1x1卷积即可；\n   2. 把1x1卷积等价转换为3x3卷积，只要用0填充即可。\n\n2、卷积和 bn 进行融合\n\n参考资料：https://zhuanlan.zhihu.com/p/352239591\n\n * conv(x)=w(x)+bconv(x) = w(x) + bconv(x)=w(x)+b\n\n * bn(x)=γ∗x−meanvar+ϵ+βbn(x) = \\gamma*\\frac{x-mean}{\\sqrt{var} + \\epsilon} + \\betabn(x)=γ∗var +ϵx−mean +β\n\n * bn(conv(x))=γ∗w(x)+b−meanvar+ϵ+βbn(conv(x)) = \\gamma*\\frac{w(x) + b - mean}{\\sqrt{var} + \\epsilon}+ \\betabn(conv(x))=γ∗var +ϵw(x)+b−mean +β\n\n将其变形为\n\n * bn(conv(x))=γ∗w(x)var+ϵ+γ∗(b−mean)var+ϵ+βbn(conv(x)) = \\gamma*\\frac{w(x) }{\\sqrt{var} + \\epsilon} + \\frac{\\gamma*( b - mean)}{\\sqrt{var} + \\epsilon}+ \\betabn(conv(x))=γ∗var +ϵw(x) +var +ϵγ∗(b−mean) +β\n\n那么可以得到一个新卷积层，其参数为\n\n * wfused=γ∗w(x)var+ϵw_{fused} = \\gamma*\\frac{w(x) }{\\sqrt{var} + \\epsilon}wfused =γ∗var +ϵw(x)\n * bfused=γ∗(b−mean)var+ϵ+βb_{fused} = \\gamma * \\frac{( b - mean)}{\\sqrt{var} + \\epsilon}+ \\betabfused =γ∗var +ϵ(b−mean) +β\n\n最终融合的效果即为\n\n * bn(conv(x))=wfused(x)+bfusedbn(conv(x))=w_{fused}(x) + b_{fused}bn(conv(x))=wfused (x)+bfused\n\n代码参考，这份代码里没有处理原卷积的 bias，因为 repvgg 的卷积层中没有使用 bias\n\ndef _fuse_bn_tensor(self, branch):\n    if branch is none:\n        return 0, 0\n    if isinstance(branch, nn.sequential):\n        kernel = branch.conv.weight\n        running_mean = branch.bn.running_mean\n        running_var = branch.bn.running_var\n        gamma = branch.bn.weight\n        beta = branch.bn.bias\n        eps = branch.bn.eps\n    else:\n        ...\n        std = (running_var + eps).sqrt()\n        t = (gamma / std).reshape(-1, 1, 1, 1)\n        return kernel * t, beta - running_mean * gamma / std\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n# 参考资料\n\n * https://arxiv.org/abs/2101.03697\n\n * https://github.com/dingxiaoh/repvgg",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"Awesome-Knowledge-distillation",frontmatter:{title:"Awesome-Knowledge-distillation",date:"2021-10-24T15:47:42.000Z",permalink:"/pages/885a91/",categories:["学术搬砖","论文阅读-知识蒸馏"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/04.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/00.Awesome-Knowledge-distillation.html",relativePath:"01.学术搬砖/04.论文阅读-知识蒸馏/00.Awesome-Knowledge-distillation.md",key:"v-0de9dd06",path:"/pages/885a91/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/11/03, 23:35:28"},{title:"Transformer系列代码",frontmatter:{title:"Transformer系列代码",date:"2021-10-16T20:57:31.000Z",permalink:"/pages/b34b2b/",categories:["学习笔记","代码实践-图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/05.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Transformer/00.Awesome-Visual-Transformer.html",relativePath:"01.学术搬砖/05.论文阅读-Transformer/00.Awesome-Visual-Transformer.md",key:"v-3beab5b9",path:"/pages/b34b2b/",headers:[{level:2,title:"CrossFormer: https://github.com/cheerss/CrossFormer",slug:"crossformer-https-github-com-cheerss-crossformer",normalizedTitle:"crossformer: https://github.com/cheerss/crossformer",charIndex:67},{level:2,title:"Swin-Transformer: https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation",slug:"swin-transformer-https-github-com-swintransformer-swin-transformer-semantic-segmentation",normalizedTitle:"swin-transformer: https://github.com/swintransformer/swin-transformer-semantic-segmentation",charIndex:764},{level:2,title:"Residual Attention: A Simple but Effective Method for Multi-Label Recognition: https://github.com/Kevinz-code/CSRA",slug:"residual-attention-a-simple-but-effective-method-for-multi-label-recognition-https-github-com-kevinz-code-csra",normalizedTitle:"residual attention: a simple but effective method for multi-label recognition: https://github.com/kevinz-code/csra",charIndex:1206},{level:2,title:"CMT: Convolutional Neural Networks Meet Vision Transformers",slug:"cmt-convolutional-neural-networks-meet-vision-transformers",normalizedTitle:"cmt: convolutional neural networks meet vision transformers",charIndex:1795},{level:2,title:"Pre-Trained Image Processing Transformer (IPT)",slug:"pre-trained-image-processing-transformer-ipt",normalizedTitle:"pre-trained image processing transformer (ipt)",charIndex:1859},{level:2,title:"HRFormer: High-Resolution Transformer for Dense Prediction, NeurIPS 2021",slug:"hrformer-high-resolution-transformer-for-dense-prediction-neurips-2021",normalizedTitle:"hrformer: high-resolution transformer for dense prediction, neurips 2021",charIndex:1957},{level:2,title:"DeiT: Data-efficient Image Transformers",slug:"deit-data-efficient-image-transformers",normalizedTitle:"deit: data-efficient image transformers",charIndex:2780},{level:2,title:"Efficient Vision Transformers via Fine-Grained Manifold Distillation",slug:"efficient-vision-transformers-via-fine-grained-manifold-distillation",normalizedTitle:"efficient vision transformers via fine-grained manifold distillation",charIndex:3732},{level:2,title:"Augmented Shortcuts for Vision Transformers",slug:"augmented-shortcuts-for-vision-transformers",normalizedTitle:"augmented shortcuts for vision transformers",charIndex:3839},{level:2,title:"SOFT: Softmax-free Transformer with Linear Complexity",slug:"soft-softmax-free-transformer-with-linear-complexity",normalizedTitle:"soft: softmax-free transformer with linear complexity",charIndex:3948},{level:3,title:"Image Classification",slug:"image-classification",normalizedTitle:"image classification",charIndex:4041}],headersStr:"CrossFormer: https://github.com/cheerss/CrossFormer Swin-Transformer: https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation Residual Attention: A Simple but Effective Method for Multi-Label Recognition: https://github.com/Kevinz-code/CSRA CMT: Convolutional Neural Networks Meet Vision Transformers Pre-Trained Image Processing Transformer (IPT) HRFormer: High-Resolution Transformer for Dense Prediction, NeurIPS 2021 DeiT: Data-efficient Image Transformers Efficient Vision Transformers via Fine-Grained Manifold Distillation Augmented Shortcuts for Vision Transformers SOFT: Softmax-free Transformer with Linear Complexity Image Classification",content:"可解释性：https://github.com/hila-chefer/Transformer-Explainability\n\n\n# CrossFormer: https://github.com/cheerss/CrossFormer\n\n# ADE20K\n\nBACKBONE        SEGMENTATION HEAD   ITERATIONS   PARAMS   FLOPS     IOU    MS IOU\nCrossFormer-S   FPN                 80K          34.3M    209.8G    46.4   -\nCrossFormer-B   FPN                 80K          55.6M    320.1G    48.0   -\nCrossFormer-L   FPN                 80K          95.4M    482.7G    49.1   -\nResNet-101      UPerNet             160K         86.0M    1029.G    44.9   -\nCrossFormer-S   UPerNet             160K         62.3M    979.5G    47.6   48.4\nCrossFormer-B   UPerNet             160K         83.6M    1089.7G   49.7   50.6\nCrossFormer-L   UPerNet             160K         125.5M   1257.8G   50.4   51.4\n\n\n# Swin-Transformer: https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation\n\n# ADE20K\n\nBACKBONE   METHOD    CROP SIZE   LR SCHD   MIOU    MIOU (MS+FLIP)   #PARAMS   FLOPS\nSwin-T     UPerNet   512x512     160K      44.51   45.81            60M       945G\nSwin-S     UperNet   512x512     160K      47.64   49.47            81M       1038G\nSwin-B     UperNet   512x512     160K      48.13   49.72            121M      1188G\n\n\n# Residual Attention: A Simple but Effective Method for Multi-Label Recognition: https://github.com/Kevinz-code/CSRA\n\nDATASET   BACKBONE      HEAD NUMS   MAP(%)   RESOLUTION   DOWNLOAD\nVOC2007   ResNet-101    1           94.7     448x448      download\nVOC2007   ResNet-cut    1           95.2     448x448      download\nCOCO      ResNet-101    4           83.3     448x448      download\nCOCO      ResNet-cut    6           85.6     448x448      download\nWider     VIT_B16_224   1           89.0     224x224      download\nWider     VIT_L16_224   1           90.2     224x224      download\n\n\n# CMT: Convolutional Neural Networks Meet Vision Transformers\n\n\n# Pre-Trained Image Processing Transformer (IPT)\n\nhttps://github.com/huawei-noah/Pretrained-IPT\n\n\n# HRFormer: High-Resolution Transformer for Dense Prediction, NeurIPS 2021\n\nhttps://github.com/HRNet/HRFormer\n\n# ADE20K\n\nMETHODS   BACKBONE     WINDOW SIZE   TRAIN SET   TEST SET   ITERATIONS   BATCH SIZE   OHEM   MIOU   MIOU (MULTI-SCALE)   LOG   CKPT   SCRIPT\nOCRNet    HRFormer-S   7x7           Train       Val        150000       8            Yes    44.0   45.1                 log   ckpt   script\nOCRNet    HRFormer-B   7x7           Train       Val        150000       8            Yes    46.3   47.6                 log   ckpt   script\nOCRNet    HRFormer-B   13x13         Train       Val        150000       8            Yes    48.7   50.0                 log   ckpt   script\nOCRNet    HRFormer-B   15x15         Train       Val        150000       8            Yes    -      -                    -     -      -\n\n\n# DeiT: Data-efficient Image Transformers\n\nhttps://github.com/facebookresearch/deit\n\n# Model Zoo\n\nWe provide baseline DeiT models pretrained on ImageNet 2012.\n\nNAME                                    ACC@1   ACC@5   #PARAMS   URL\nDeiT-tiny                               72.2    91.1    5M        model\nDeiT-small                              79.9    95.0    22M       model\nDeiT-base                               81.8    95.6    86M       model\nDeiT-tiny distilled                     74.5    91.9    6M        model\nDeiT-small distilled                    81.2    95.4    22M       model\nDeiT-base distilled                     83.4    96.5    87M       model\nDeiT-base 384                           82.9    96.2    87M       model\nDeiT-base distilled 384 (1000 epochs)   85.2    97.2    88M       model\nCaiT-S24 distilled 384                  85.1    97.3    47M       model\nCaiT-M48 distilled 448                  86.5    97.7    356M      model\n\n\n# Efficient Vision Transformers via Fine-Grained Manifold Distillation\n\nhttps://arxiv.org/abs/2107.01378\n\n\n# Augmented Shortcuts for Vision Transformers\n\nhttps://arxiv.org/abs/2106.15941\n\nAttention Map 的 rank 和多样性\n\n\n# SOFT: Softmax-free Transformer with Linear Complexity\n\nhttps://github.com/fudan-zvg/SOFT\n\n\n# Image Classification\n\n# ImageNet-1K\n\nMODEL         RESOLUTION   PARAMS   FLOPS   TOP-1 %   CONFIG\nSOFT-Tiny     224          13M      1.9G    79.3      SOFT_Tiny.yaml, SOFT_Tiny_cuda.yaml\nSOFT-Small    224          24M      3.3G    82.2      SOFT_Small.yaml, SOFT_Small_cuda.yaml\nSOFT-Medium   224          45M      7.2G    82.9      SOFT_Meidum.yaml, SOFT_Meidum_cuda.yaml\nSOFT-Large    224          64M      11.0G   83.1      SOFT_Large.yaml, SOFT_Large_cuda.yaml\nSOFT-Huge     224          87M      16.3G   83.3      SOFT_Huge.yaml, SOFT_Huge_cuda.yaml",normalizedContent:"可解释性：https://github.com/hila-chefer/transformer-explainability\n\n\n# crossformer: https://github.com/cheerss/crossformer\n\n# ade20k\n\nbackbone        segmentation head   iterations   params   flops     iou    ms iou\ncrossformer-s   fpn                 80k          34.3m    209.8g    46.4   -\ncrossformer-b   fpn                 80k          55.6m    320.1g    48.0   -\ncrossformer-l   fpn                 80k          95.4m    482.7g    49.1   -\nresnet-101      upernet             160k         86.0m    1029.g    44.9   -\ncrossformer-s   upernet             160k         62.3m    979.5g    47.6   48.4\ncrossformer-b   upernet             160k         83.6m    1089.7g   49.7   50.6\ncrossformer-l   upernet             160k         125.5m   1257.8g   50.4   51.4\n\n\n# swin-transformer: https://github.com/swintransformer/swin-transformer-semantic-segmentation\n\n# ade20k\n\nbackbone   method    crop size   lr schd   miou    miou (ms+flip)   #params   flops\nswin-t     upernet   512x512     160k      44.51   45.81            60m       945g\nswin-s     upernet   512x512     160k      47.64   49.47            81m       1038g\nswin-b     upernet   512x512     160k      48.13   49.72            121m      1188g\n\n\n# residual attention: a simple but effective method for multi-label recognition: https://github.com/kevinz-code/csra\n\ndataset   backbone      head nums   map(%)   resolution   download\nvoc2007   resnet-101    1           94.7     448x448      download\nvoc2007   resnet-cut    1           95.2     448x448      download\ncoco      resnet-101    4           83.3     448x448      download\ncoco      resnet-cut    6           85.6     448x448      download\nwider     vit_b16_224   1           89.0     224x224      download\nwider     vit_l16_224   1           90.2     224x224      download\n\n\n# cmt: convolutional neural networks meet vision transformers\n\n\n# pre-trained image processing transformer (ipt)\n\nhttps://github.com/huawei-noah/pretrained-ipt\n\n\n# hrformer: high-resolution transformer for dense prediction, neurips 2021\n\nhttps://github.com/hrnet/hrformer\n\n# ade20k\n\nmethods   backbone     window size   train set   test set   iterations   batch size   ohem   miou   miou (multi-scale)   log   ckpt   script\nocrnet    hrformer-s   7x7           train       val        150000       8            yes    44.0   45.1                 log   ckpt   script\nocrnet    hrformer-b   7x7           train       val        150000       8            yes    46.3   47.6                 log   ckpt   script\nocrnet    hrformer-b   13x13         train       val        150000       8            yes    48.7   50.0                 log   ckpt   script\nocrnet    hrformer-b   15x15         train       val        150000       8            yes    -      -                    -     -      -\n\n\n# deit: data-efficient image transformers\n\nhttps://github.com/facebookresearch/deit\n\n# model zoo\n\nwe provide baseline deit models pretrained on imagenet 2012.\n\nname                                    acc@1   acc@5   #params   url\ndeit-tiny                               72.2    91.1    5m        model\ndeit-small                              79.9    95.0    22m       model\ndeit-base                               81.8    95.6    86m       model\ndeit-tiny distilled                     74.5    91.9    6m        model\ndeit-small distilled                    81.2    95.4    22m       model\ndeit-base distilled                     83.4    96.5    87m       model\ndeit-base 384                           82.9    96.2    87m       model\ndeit-base distilled 384 (1000 epochs)   85.2    97.2    88m       model\ncait-s24 distilled 384                  85.1    97.3    47m       model\ncait-m48 distilled 448                  86.5    97.7    356m      model\n\n\n# efficient vision transformers via fine-grained manifold distillation\n\nhttps://arxiv.org/abs/2107.01378\n\n\n# augmented shortcuts for vision transformers\n\nhttps://arxiv.org/abs/2106.15941\n\nattention map 的 rank 和多样性\n\n\n# soft: softmax-free transformer with linear complexity\n\nhttps://github.com/fudan-zvg/soft\n\n\n# image classification\n\n# imagenet-1k\n\nmodel         resolution   params   flops   top-1 %   config\nsoft-tiny     224          13m      1.9g    79.3      soft_tiny.yaml, soft_tiny_cuda.yaml\nsoft-small    224          24m      3.3g    82.2      soft_small.yaml, soft_small_cuda.yaml\nsoft-medium   224          45m      7.2g    82.9      soft_meidum.yaml, soft_meidum_cuda.yaml\nsoft-large    224          64m      11.0g   83.1      soft_large.yaml, soft_large_cuda.yaml\nsoft-huge     224          87m      16.3g   83.3      soft_huge.yaml, soft_huge_cuda.yaml",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"An Image is Worth 16x16 Words Transformers for Image Recognition at Scale",frontmatter:{title:"An Image is Worth 16x16 Words Transformers for Image Recognition at Scale",date:"2022-01-04T15:02:17.000Z",permalink:"/pages/a96d59/",categories:["学术搬砖","论文阅读-Transformer"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/05.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Transformer/01.An%20Image%20is%20Worth%2016x16%20Words%20Transformers%20for%20Image%20Recognition%20at%20Scale.html",relativePath:"01.学术搬砖/05.论文阅读-Transformer/01.An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.md",key:"v-2bd63fd0",path:"/pages/a96d59/",headers:[{level:2,title:"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.",slug:"an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale",normalizedTitle:"an image is worth 16x16 words: transformers for image recognition at scale.",charIndex:2}],headersStr:"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.",content:"# An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\n\n# 单位：Google Research, Brain Team\n\n# 作者：Alexey Dosovitskiy, Neil Houlsby\n\n# 发表：ICLR 2020\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 总结\n\n# 论文的贡献\n\n论文主要是针对弱监督语义分割提出了 Puzzle 模块，将原始图像分块后再算一个CAMs，并与原始的 CAMs 做一个重建损失，三项损失联合优化分类网络，提升 CAMs 的精度。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料",normalizedContent:"# an image is worth 16x16 words: transformers for image recognition at scale.\n\n# 单位：google research, brain team\n\n# 作者：alexey dosovitskiy, neil houlsby\n\n# 发表：iclr 2020\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 总结\n\n# 论文的贡献\n\n论文主要是针对弱监督语义分割提出了 puzzle 模块，将原始图像分块后再算一个cams，并与原始的 cams 做一个重建损失，三项损失联合优化分类网络，提升 cams 的精度。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"(DeepLabv1) Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs",frontmatter:{title:"(DeepLabv1) Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs",date:"2021-07-20T19:52:04.000Z",permalink:"/pages/8d4552/",categories:["论文阅读","语义分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/03.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/00.(DeepLabv1)%20Semantic%20Image%20Segmentation%20with%20Deep%20Convolutional%20Nets%20and%20Fully%20Connected%20CRFs.html",relativePath:"01.学术搬砖/03.论文阅读-语义分割/00.(DeepLabv1) Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs.md",key:"v-6d38f8e1",path:"/pages/8d4552/",headers:[{level:2,title:"(DeepLabv1) Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs",slug:"deeplabv1-semantic-image-segmentation-with-deep-convolutional-nets-and-fully-connected-crfs",normalizedTitle:"(deeplabv1) semantic image segmentation with deep convolutional nets and fully connected crfs",charIndex:2},{level:3,title:"摘要",slug:"摘要",normalizedTitle:"摘要",charIndex:218},{level:3,title:"阅读",slug:"阅读",normalizedTitle:"阅读",charIndex:498},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:1766},{level:3,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:1802}],headersStr:"(DeepLabv1) Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs 摘要 阅读 总结 参考资料",content:"# (DeepLabv1) Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs\n\n# 单位：KAIST\n\n# 作者：Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille\n\n# 发表：ICLR 2015\n\n\n# 摘要\n\n该工作将深度卷积神经网络和概率图模型结合来解决像素级分类问题（又称语义分割）。发现 DCNN 最后一层的响应不足以定位精确的物体边缘，这是因为不变性的性质使得 DCNN 对 high-level 的任务很有效。作者将 DCNN和 CRF 结合起来，以克服深度网络的这种不良定位特性。量化结果表明，DeepLab 系统可以定位很精确的目标边缘，在 PASCAL VOC-2012 数据集上达到了新的 SOTA 结果，测试集上的性能为 71.6% 的 mIoU。作者分析了性能提升的主要两点原因：预训练网络（VGG-16）的利用以及空洞卷积的应用\n\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n\n\n# 论文的方法\n\n# 3 用于密集图像标记的卷积神经网络\n\n# 3.1 使用空洞卷积进行高效的密集滑动窗口的特征提取\n\n空洞卷积是从小波变换的论文中得来的，使用空洞卷积算法可以使8倍的下采样达到32倍下采样的效果？\n\nVGG-16 网络在 ImageNet 数据集上预训练，将1000路的分类器变为了21类，\n\n# 3.2 使用卷积网络以控制感受野尺寸以及加速密集计算\n\n在 VGG-16 网络中，图像的输入尺寸为 224×224224 \\times 224224×224，其全连接层有 4096 个 7×77\\times77×7 的卷积层，是计算瓶颈。作者降低了输入尺寸，并且减少了全连接层的个数，以加速密集计算\n\n# 4 详细的边缘恢复：全连接条件随机场以及多尺度预测\n\n# 4.1 深度卷积网络以及其定位挑战\n\n\n\n如上图所示，DCNN 的 score map 可以可靠地预测图像中对象的大致位置，但不太适合精确指出它们的确切轮廓。最大池化层被证明对图像分类任务来讲十分有效，但其会增加不变性以及大感受野，使分割问题更具挑战\n\n# 4.2 用于精确定位的全连接条件随机场（后续需要专门分析一波代码）\n\n\n\n在传统方法中，条件随机场被用作平滑的带噪分割图的预测图。通常这些模型包含一个能量函数使得相邻的节点耦合，有利于空间上相似像素的预测标签分配，可以减少错误预测\n\n使用了全连接条件随机场，能量函数是：\n\nE(x)=\\sum_\\limits{i}\\theta_i(x_i)+\\sum_\\limits{ij}\\theta_{ij}(x_i,x_j)\n\nxxx 是像素级别的标签，使用单元势能 θi(xi)=−logP(xi)\\theta_i(x_i) = -logP(x_i)θi (xi )=−logP(xi ) ，P(xi)P(x_i)P(xi ) 是像素 iii 的标签分配概率。成对势能是θij(xij,yij)=μ(xi,yi)∑m=1Kwm⋅km(fi,fj)\\theta_{ij}(x_{ij},y_{ij}) = \\mu(x_i,y_i)\\sum_{m=1}^Kw_m\\cdot k^m(f_i,f_j)θij (xij ,yij )=μ(xi ,yi )∑m=1K wm ⋅km(fi ,fj )\n\n如果xi≠xjx_i \\neq x_jxi =xj 则 μ(xi,yi)=1\\mu(x_i,y_i) = 1μ(xi ,yi )=1 ，否则则为 0\n\n每个 kmk^mkm 都是的高斯核，其由 i,ji,ji,j 像素提取的特征决定，并且由参数 wmw_mwm 来加权，第一个核取决于像素位置和像素颜色强度，而第二个核只取决于像素位置，几个参数控制着高斯核的尺度\n\n# 4.3 多尺度预测\n\n使用多尺度预测的方法来提升边缘定位的精度，我们在输入图像和前四个最大池化层每个输出上附加两层MLP，其特征图与主网络的最后一层特征图相连接\n\n# 论文的背景\n\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n\n# 参考资料",normalizedContent:"# (deeplabv1) semantic image segmentation with deep convolutional nets and fully connected crfs\n\n# 单位：kaist\n\n# 作者：liang-chieh chen, george papandreou, iasonas kokkinos, kevin murphy, alan l. yuille\n\n# 发表：iclr 2015\n\n\n# 摘要\n\n该工作将深度卷积神经网络和概率图模型结合来解决像素级分类问题（又称语义分割）。发现 dcnn 最后一层的响应不足以定位精确的物体边缘，这是因为不变性的性质使得 dcnn 对 high-level 的任务很有效。作者将 dcnn和 crf 结合起来，以克服深度网络的这种不良定位特性。量化结果表明，deeplab 系统可以定位很精确的目标边缘，在 pascal voc-2012 数据集上达到了新的 sota 结果，测试集上的性能为 71.6% 的 miou。作者分析了性能提升的主要两点原因：预训练网络（vgg-16）的利用以及空洞卷积的应用\n\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n\n\n# 论文的方法\n\n# 3 用于密集图像标记的卷积神经网络\n\n# 3.1 使用空洞卷积进行高效的密集滑动窗口的特征提取\n\n空洞卷积是从小波变换的论文中得来的，使用空洞卷积算法可以使8倍的下采样达到32倍下采样的效果？\n\nvgg-16 网络在 imagenet 数据集上预训练，将1000路的分类器变为了21类，\n\n# 3.2 使用卷积网络以控制感受野尺寸以及加速密集计算\n\n在 vgg-16 网络中，图像的输入尺寸为 224×224224 \\times 224224×224，其全连接层有 4096 个 7×77\\times77×7 的卷积层，是计算瓶颈。作者降低了输入尺寸，并且减少了全连接层的个数，以加速密集计算\n\n# 4 详细的边缘恢复：全连接条件随机场以及多尺度预测\n\n# 4.1 深度卷积网络以及其定位挑战\n\n\n\n如上图所示，dcnn 的 score map 可以可靠地预测图像中对象的大致位置，但不太适合精确指出它们的确切轮廓。最大池化层被证明对图像分类任务来讲十分有效，但其会增加不变性以及大感受野，使分割问题更具挑战\n\n# 4.2 用于精确定位的全连接条件随机场（后续需要专门分析一波代码）\n\n\n\n在传统方法中，条件随机场被用作平滑的带噪分割图的预测图。通常这些模型包含一个能量函数使得相邻的节点耦合，有利于空间上相似像素的预测标签分配，可以减少错误预测\n\n使用了全连接条件随机场，能量函数是：\n\ne(x)=\\sum_\\limits{i}\\theta_i(x_i)+\\sum_\\limits{ij}\\theta_{ij}(x_i,x_j)\n\nxxx 是像素级别的标签，使用单元势能 θi(xi)=−logp(xi)\\theta_i(x_i) = -logp(x_i)θi (xi )=−logp(xi ) ，p(xi)p(x_i)p(xi ) 是像素 iii 的标签分配概率。成对势能是θij(xij,yij)=μ(xi,yi)∑m=1kwm⋅km(fi,fj)\\theta_{ij}(x_{ij},y_{ij}) = \\mu(x_i,y_i)\\sum_{m=1}^kw_m\\cdot k^m(f_i,f_j)θij (xij ,yij )=μ(xi ,yi )∑m=1k wm ⋅km(fi ,fj )\n\n如果xi=xjx_i \\neq x_jxi =xj 则 μ(xi,yi)=1\\mu(x_i,y_i) = 1μ(xi ,yi )=1 ，否则则为 0\n\n每个 kmk^mkm 都是的高斯核，其由 i,ji,ji,j 像素提取的特征决定，并且由参数 wmw_mwm 来加权，第一个核取决于像素位置和像素颜色强度，而第二个核只取决于像素位置，几个参数控制着高斯核的尺度\n\n# 4.3 多尺度预测\n\n使用多尺度预测的方法来提升边缘定位的精度，我们在输入图像和前四个最大池化层每个输出上附加两层mlp，其特征图与主网络的最后一层特征图相连接\n\n# 论文的背景\n\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n\n# 参考资料",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Do Vision Transformers See Like Convolutional Neural Networks",frontmatter:{title:"Do Vision Transformers See Like Convolutional Neural Networks",date:"2022-01-04T15:10:05.000Z",permalink:"/pages/7cfb60/",categories:["学术搬砖","论文阅读-Transformer"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/05.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Transformer/02.Do%20Vision%20Transformers%20See%20Like%20Convolutional%20Neural%20Networks.html",relativePath:"01.学术搬砖/05.论文阅读-Transformer/02.Do Vision Transformers See Like Convolutional Neural Networks.md",key:"v-33e2c2c0",path:"/pages/7cfb60/",headers:[{level:2,title:"Do Vision Transformers See Like Convolutional Neural Networks?",slug:"do-vision-transformers-see-like-convolutional-neural-networks",normalizedTitle:"do vision transformers see like convolutional neural networks?",charIndex:2}],headersStr:"Do Vision Transformers See Like Convolutional Neural Networks?",content:"# Do Vision Transformers See Like Convolutional Neural Networks?\n\n# 单位：KAIST\n\n# 作者：Sanghyun Jo, In-Jae Yu\n\n# 发表：NeurIPS 2021\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 总结\n\n# 论文的贡献\n\n论文主要是针对弱监督语义分割提出了 Puzzle 模块，将原始图像分块后再算一个CAMs，并与原始的 CAMs 做一个重建损失，三项损失联合优化分类网络，提升 CAMs 的精度。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料",normalizedContent:"# do vision transformers see like convolutional neural networks?\n\n# 单位：kaist\n\n# 作者：sanghyun jo, in-jae yu\n\n# 发表：neurips 2021\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 总结\n\n# 论文的贡献\n\n论文主要是针对弱监督语义分割提出了 puzzle 模块，将原始图像分块后再算一个cams，并与原始的 cams 做一个重建损失，三项损失联合优化分类网络，提升 cams 的精度。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"Awesome-Graph-Neural-Network",frontmatter:{title:"Awesome-Graph-Neural-Network",date:"2021-10-24T15:47:53.000Z",permalink:"/pages/2a87f2/",categories:["学术搬砖","论文阅读-图卷积网络"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/06.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/00.Awesome-Graph-Neural-Network.html",relativePath:"01.学术搬砖/06.论文阅读-图卷积网络/00.Awesome-Graph-Neural-Network.md",key:"v-88f16062",path:"/pages/2a87f2/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/11/03, 23:35:28"},{title:"Awesome weakly supervised semantic segmentation",frontmatter:{title:"Awesome weakly supervised semantic segmentation",date:"2021-10-14T13:24:30.000Z",permalink:"/pages/6d34a8/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/00.Awesome%20weakly%20supervised%20semantic%20segmentation.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/00.Awesome weakly supervised semantic segmentation.md",key:"v-04892c9a",path:"/pages/6d34a8/",headers:[{level:2,title:"0 参考资料",slug:"_0-参考资料",normalizedTitle:"0 参考资料",charIndex:2},{level:2,title:"1.1. Results on PASCAL VOC 2012 dataset",slug:"_1-1-results-on-pascal-voc-2012-dataset",normalizedTitle:"1.1. results on pascal voc 2012 dataset",charIndex:247},{level:2,title:"2.1. Supervised by image tags without extra data.(such as saliency)",slug:"_2-1-supervised-by-image-tags-without-extra-data-such-as-saliency",normalizedTitle:"2.1. supervised by image tags without extra data.(such as saliency)",charIndex:3572},{level:2,title:"2.2. Supervised by image tags with extra data.(such as saliency)",slug:"_2-2-supervised-by-image-tags-with-extra-data-such-as-saliency",normalizedTitle:"2.2. supervised by image tags with extra data.(such as saliency)",charIndex:7124}],headersStr:"0 参考资料 1.1. Results on PASCAL VOC 2012 dataset 2.1. Supervised by image tags without extra data.(such as saliency) 2.2. Supervised by image tags with extra data.(such as saliency)",content:'# 0 参考资料\n\n * https://course.zhidx.com/c/MjUxYzUxODYzMDkwYWViNjBjZjM=\n * https://zhuanlan.zhihu.com/p/42058498\n * https://www.bilibili.com/video/BV1S5411w7Yo?p=5\n * https://github.com/gyguo/awesome-weakly-supervised-semantic-segmentation-image\n\n\n# 1.1. Results on PASCAL VOC 2012 dataset\n\n * For each method, I will provide the name of baseline in brackets if it has.\n * Sup.:\n   * I-image-level class label\n   * B-bounding box label\n   * S-scribble label\n   * P-point label\n * Bac. C: Method for generating pseudo label, or backbone of the classification network.\n * Arc. S: backbone and method of the segmentation network.\n * Pre.s : The dataset used to pre-train the segmentation network,\n   * "I" denotes ImageNet,\n   * "C" denotes COCO.\n   * Note that many works use COCO pre-trained DeepLab model but not mentioned in the paper.\n * For methods that use multiple backbones, I only reports the results of ResNet101.\n * "-" indicates no fully-supervised model is utilized, "?" indicates the corresponding item is not mentioned in the paper.\n\nMETHOD          PUB.        BAC. C        ARC. S                   SUP.   EXTRA DATA   PRE.S   VAL    TEST   CODE   PAPER   NOTE\nAffinityNet     CVPR18      ResNet38      ResNet38                 I      -            ?       61.7   63.7   code   paper   \nICD             CVPR20      VGG16         ResNet101 DeepLabv1      I      -            ?       64.1   64.3   code   paper   \nIRN             CVPR19      ResNet50      ResNet50 DeepLabv2       I      -            I       63.5   64.8   code   paper   \nIAL             IJCV20      ResNet?       ResNet?                  I      -            I       64.3   65.4   -      paper   \nSSDD (PSA)      ICCV19      ResNet38      ResNet38                 I      -            I       64.9   65.5   code   paper   \nSEAM            CVPR20      ResNet38      ResNet38 DeepLabv2       I      -            I       64.5   65.7   code   paper   \nSC-CAM          CVPR20      ResNet38      ResNet101 DeepLabv2      I      -            ?       66.1   65.9   code   paper   \nRRM             AAAI20      ResNet38      ResNet101 DeepLabv2      I      -            ?       66.3   66.5   code   paper   \nBES             ECCV20      ResNet50      ResNet101 DeepLabv2      I      -            ?       65.7   66.6   code   paper   \nCONTA (+SEAM)   NeurIPS20   ResNet38      ResNet101 DeepLabv2      I      -            ?       66.1   66.7   code   paper   \nPuzzle-CAM      ICIP21      ResNeSt-101   ResNeSt-101 DeepLabv3+   I      -            ?       66.9   67.7   code   paper   Note\nVWE             IJCAI21     ResNet101     ResNet101 DeepLabv2      I      -            ?       67.2   67.3   code   paper   Note\nWSGCN (IRN)     ICME21      ResNet50      ResNet101 DeepLabv2      I      -            I       66.7   68.8   code   paper   Note\nCPN             ICCV21      ResNet38      ResNet38 DeepLabv1       I      -            ?       67.8   68.5   code   paper   \nRPNet           arxiv21     ResNet101     ResNet50 DeepLabv2       I      -            I       68.0   68.2   code   paper   \nAdvCAM          CVPR21      ResNet50      ResNet101 DeepLabv2      I      -            I       68.1   68.0   code   paper   \nWSGCN (IRN)     ICME21      ResNet50      ResNet101 DeepLabv2      I      -            I+C     68.7   69.3   code   paper   Note\nPuzzle-CAM      ICIP21      ResNeSt-101   ResNeSt-269 DeepLabv3+   I      -            ?       71.9   72.2   code   paper   Note\nCDA                                                                                                          code   paper   \n\n\n# 2.1. Supervised by image tags without extra data.(such as saliency)\n\n2021\n\n * AdvCAM: " Anti-Adversarially Manipulated Attributions for Weakly and Semi-Supervised Semantic Segmentation" CVPR2021\n * WSGCN: "Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks" ICME2021\n * PuzzleCAM: "Puzzle-CAM Improved localization via matching partial and full features" ICIP2021\n * CDA: "Context Decoupling Augmentation for Weakly Supervised Semantic Segmentation" 2021arXiv\n * VWE: "Learning Visual Words for Weakly-Supervised Semantic Segmentation" IJCAI2021\n * CPN: "Complementary Patch for Weakly Supervised Semantic Segmentation" ICCV2021\n * RPNet: "Cross-Image Region Mining with Region Prototypical Network for Weakly Supervised Segmentation" arxiv2021\n * Method: "" 2021\n\n2020\n\n * RRM: "Reliability Does Matter An End-to-End Weakly Supervised Semantic Segmentation Approach" AAAI2020\n * IAL: "Weakly-Supervised Semantic Segmentation by Iterative Affinity Learning" IJCV2020\n * SEAM: "Self-supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation" CVPR2020\n * SC-CAM: "Weakly-Supervised Semantic Segmentation via Sub-category Exploration" CVPR2020\n * ICD: "Learning Integral Objects with Intra-Class Discriminator for Weakly-Supervised Semantic Segmentation" CVPR2020\n * Fan et al.: "Employing multi-estimations for weakly-supervised semantic segmentation" ECCV2020\n * MCIS: "Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation" 2020\n * BES: "Weakly Supervised Semantic Segmentation with Boundary Exploration" ECCV2020\n * CONTA: "Causal intervention for weakly-supervised semantic segmentation" NeurIPS2020\n * Method: "Find it if You Can: End-to-End Adversarial Erasing for Weakly-Supervised Semantic Segmentation" 2020arXiv\n * Zhang et al.: "Splitting vs. Merging: Mining Object Regions with Discrepancy and Intersection Loss for Weakly Supervised Semantic Segmentation" ECCV2020\n\n2019\n\n * IRN: "Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations" CVPR2019\n * Ficklenet: " Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference" CVPR2019\n * Lee *et al.*: "Frame-to-Frame Aggregation of Active Regions in Web Videos for Weakly Supervised Semantic Segmentation" ICCV2019\n * OAA: "Integral Object Mining via Online Attention Accumulation" ICCV2019\n * SSDD: "Self-supervised difference detection for weakly-supervised semantic segmentation" ICCV2019\n\n2018\n\n * DSRG: "Weakly-supervised semantic segmentation network with deep seeded region growing" CVPR2018\n * AffinityNet: "Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation" CVPR2018\n * GAIN: " Tell me where to look: Guided attention inference network" CVPR2018\n * AISI: "Associating inter-image salient instances for weakly supervised semantic segmentation" ECCV2018\n * SeeNet: "Self-Erasing Network for Integral Object Attention" NeurIPS2018\n * Method: "" 2018\n\n2017\n\n * CrawlSeg: "Weakly Supervised Semantic Segmentation using Web-Crawled Videos" CVPR2017\n * WebS-i2: "Webly supervised semantic segmentation" CVPR2017\n * Oh *et al*.: "Exploiting saliency for object segmentation from image level labels" CVPR2017\n * TPL: "Two-phase learning for weakly supervised object localization" ICCV2017\n\n2016\n\n * SEC: "Seed, expand and constrain: Three principles for weakly-supervised image segmentation" ECCV2016\n * AF-SS: "Augmented Feedback in Semantic Segmentation under Image Level Supervision" 2016\n\n\n# 2.2. Supervised by image tags with extra data.(such as saliency)\n\n2021\n\n * SPML: "Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning" ICLR2021\n\n * Li et al.: "Group-Wise Semantic Mining for Weakly Supervised Semantic Segmentation" AAAI2021\n\n * DRS: "Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation" AAAI2021\n\n * **Yao et al. **: "Non-Salient Region Object Mining for Weakly Supervised Semantic Segmentation" CVPR2021\n\n * EDAM: "Embedded Discriminative Attention Mechanism for Weakly Supervised Semantic Segmentation" CVPR2021\n\n * AuxSegNet: "Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation" ICCV2021',normalizedContent:'# 0 参考资料\n\n * https://course.zhidx.com/c/mjuxyzuxodyzmdkwywvinjbjzjm=\n * https://zhuanlan.zhihu.com/p/42058498\n * https://www.bilibili.com/video/bv1s5411w7yo?p=5\n * https://github.com/gyguo/awesome-weakly-supervised-semantic-segmentation-image\n\n\n# 1.1. results on pascal voc 2012 dataset\n\n * for each method, i will provide the name of baseline in brackets if it has.\n * sup.:\n   * i-image-level class label\n   * b-bounding box label\n   * s-scribble label\n   * p-point label\n * bac. c: method for generating pseudo label, or backbone of the classification network.\n * arc. s: backbone and method of the segmentation network.\n * pre.s : the dataset used to pre-train the segmentation network,\n   * "i" denotes imagenet,\n   * "c" denotes coco.\n   * note that many works use coco pre-trained deeplab model but not mentioned in the paper.\n * for methods that use multiple backbones, i only reports the results of resnet101.\n * "-" indicates no fully-supervised model is utilized, "?" indicates the corresponding item is not mentioned in the paper.\n\nmethod          pub.        bac. c        arc. s                   sup.   extra data   pre.s   val    test   code   paper   note\naffinitynet     cvpr18      resnet38      resnet38                 i      -            ?       61.7   63.7   code   paper   \nicd             cvpr20      vgg16         resnet101 deeplabv1      i      -            ?       64.1   64.3   code   paper   \nirn             cvpr19      resnet50      resnet50 deeplabv2       i      -            i       63.5   64.8   code   paper   \nial             ijcv20      resnet?       resnet?                  i      -            i       64.3   65.4   -      paper   \nssdd (psa)      iccv19      resnet38      resnet38                 i      -            i       64.9   65.5   code   paper   \nseam            cvpr20      resnet38      resnet38 deeplabv2       i      -            i       64.5   65.7   code   paper   \nsc-cam          cvpr20      resnet38      resnet101 deeplabv2      i      -            ?       66.1   65.9   code   paper   \nrrm             aaai20      resnet38      resnet101 deeplabv2      i      -            ?       66.3   66.5   code   paper   \nbes             eccv20      resnet50      resnet101 deeplabv2      i      -            ?       65.7   66.6   code   paper   \nconta (+seam)   neurips20   resnet38      resnet101 deeplabv2      i      -            ?       66.1   66.7   code   paper   \npuzzle-cam      icip21      resnest-101   resnest-101 deeplabv3+   i      -            ?       66.9   67.7   code   paper   note\nvwe             ijcai21     resnet101     resnet101 deeplabv2      i      -            ?       67.2   67.3   code   paper   note\nwsgcn (irn)     icme21      resnet50      resnet101 deeplabv2      i      -            i       66.7   68.8   code   paper   note\ncpn             iccv21      resnet38      resnet38 deeplabv1       i      -            ?       67.8   68.5   code   paper   \nrpnet           arxiv21     resnet101     resnet50 deeplabv2       i      -            i       68.0   68.2   code   paper   \nadvcam          cvpr21      resnet50      resnet101 deeplabv2      i      -            i       68.1   68.0   code   paper   \nwsgcn (irn)     icme21      resnet50      resnet101 deeplabv2      i      -            i+c     68.7   69.3   code   paper   note\npuzzle-cam      icip21      resnest-101   resnest-269 deeplabv3+   i      -            ?       71.9   72.2   code   paper   note\ncda                                                                                                          code   paper   \n\n\n# 2.1. supervised by image tags without extra data.(such as saliency)\n\n2021\n\n * advcam: " anti-adversarially manipulated attributions for weakly and semi-supervised semantic segmentation" cvpr2021\n * wsgcn: "weakly-supervised image semantic segmentation using graph convolutional networks" icme2021\n * puzzlecam: "puzzle-cam improved localization via matching partial and full features" icip2021\n * cda: "context decoupling augmentation for weakly supervised semantic segmentation" 2021arxiv\n * vwe: "learning visual words for weakly-supervised semantic segmentation" ijcai2021\n * cpn: "complementary patch for weakly supervised semantic segmentation" iccv2021\n * rpnet: "cross-image region mining with region prototypical network for weakly supervised segmentation" arxiv2021\n * method: "" 2021\n\n2020\n\n * rrm: "reliability does matter an end-to-end weakly supervised semantic segmentation approach" aaai2020\n * ial: "weakly-supervised semantic segmentation by iterative affinity learning" ijcv2020\n * seam: "self-supervised equivariant attention mechanism for weakly supervised semantic segmentation" cvpr2020\n * sc-cam: "weakly-supervised semantic segmentation via sub-category exploration" cvpr2020\n * icd: "learning integral objects with intra-class discriminator for weakly-supervised semantic segmentation" cvpr2020\n * fan et al.: "employing multi-estimations for weakly-supervised semantic segmentation" eccv2020\n * mcis: "mining cross-image semantics for weakly supervised semantic segmentation" 2020\n * bes: "weakly supervised semantic segmentation with boundary exploration" eccv2020\n * conta: "causal intervention for weakly-supervised semantic segmentation" neurips2020\n * method: "find it if you can: end-to-end adversarial erasing for weakly-supervised semantic segmentation" 2020arxiv\n * zhang et al.: "splitting vs. merging: mining object regions with discrepancy and intersection loss for weakly supervised semantic segmentation" eccv2020\n\n2019\n\n * irn: "weakly supervised learning of instance segmentation with inter-pixel relations" cvpr2019\n * ficklenet: " ficklenet: weakly and semi-supervised semantic image segmentation using stochastic inference" cvpr2019\n * lee *et al.*: "frame-to-frame aggregation of active regions in web videos for weakly supervised semantic segmentation" iccv2019\n * oaa: "integral object mining via online attention accumulation" iccv2019\n * ssdd: "self-supervised difference detection for weakly-supervised semantic segmentation" iccv2019\n\n2018\n\n * dsrg: "weakly-supervised semantic segmentation network with deep seeded region growing" cvpr2018\n * affinitynet: "learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation" cvpr2018\n * gain: " tell me where to look: guided attention inference network" cvpr2018\n * aisi: "associating inter-image salient instances for weakly supervised semantic segmentation" eccv2018\n * seenet: "self-erasing network for integral object attention" neurips2018\n * method: "" 2018\n\n2017\n\n * crawlseg: "weakly supervised semantic segmentation using web-crawled videos" cvpr2017\n * webs-i2: "webly supervised semantic segmentation" cvpr2017\n * oh *et al*.: "exploiting saliency for object segmentation from image level labels" cvpr2017\n * tpl: "two-phase learning for weakly supervised object localization" iccv2017\n\n2016\n\n * sec: "seed, expand and constrain: three principles for weakly-supervised image segmentation" eccv2016\n * af-ss: "augmented feedback in semantic segmentation under image level supervision" 2016\n\n\n# 2.2. supervised by image tags with extra data.(such as saliency)\n\n2021\n\n * spml: "universal weakly supervised segmentation by pixel-to-segment contrastive learning" iclr2021\n\n * li et al.: "group-wise semantic mining for weakly supervised semantic segmentation" aaai2021\n\n * drs: "discriminative region suppression for weakly-supervised semantic segmentation" aaai2021\n\n * **yao et al. **: "non-salient region object mining for weakly supervised semantic segmentation" cvpr2021\n\n * edam: "embedded discriminative attention mechanism for weakly supervised semantic segmentation" cvpr2021\n\n * auxsegnet: "leveraging auxiliary tasks with affinity learning for weakly supervised semantic segmentation" iccv2021',charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Self-supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation",frontmatter:{title:"Self-supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation",date:"2021-04-20T10:17:45.000Z",permalink:"/pages/abd9df/",categories:["论文阅读","弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/01.Self-supervised%20Equivariant%20Attention%20Mechanism%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/01.Self-supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation.md",key:"v-6044af56",path:"/pages/abd9df/",headersStr:null,content:"# Self-supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation\n\n# 单位：中科院计算所-山世光组\n\n# 作者：Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, Xilin Chen\n\n# 发表：CVPR 2020 Oral\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2004.04581\n\n * https://github.com/YudeWang/SEAM",normalizedContent:"# self-supervised equivariant attention mechanism for weakly supervised semantic segmentation\n\n# 单位：中科院计算所-山世光组\n\n# 作者：yude wang, jie zhang, meina kan, shiguang shan, xilin chen\n\n# 发表：cvpr 2020 oral\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2004.04581\n\n * https://github.com/yudewang/seam",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks",frontmatter:{title:"Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks",date:"2021-05-11T11:16:24.000Z",permalink:"/pages/a03587/",categories:["论文阅读","弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/02.Weakly-Supervised%20Image%20Semantic%20Segmentation%20Using%20Graph%20Convolutional%20Networks.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/02.Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks.md",key:"v-79125705",path:"/pages/a03587/",headers:[{level:2,title:"Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks",slug:"weakly-supervised-image-semantic-segmentation-using-graph-convolutional-networks",normalizedTitle:"weakly-supervised image semantic segmentation using graph convolutional networks",charIndex:2},{level:3,title:"01 摘要",slug:"_01-摘要",normalizedTitle:"01 摘要",charIndex:180},{level:3,title:"02 论文的目的及结论",slug:"_02-论文的目的及结论",normalizedTitle:"02 论文的目的及结论",charIndex:404},{level:3,title:"03 论文的实验",slug:"_03-论文的实验",normalizedTitle:"03 论文的实验",charIndex:649},{level:3,title:"04 论文的方法",slug:"_04-论文的方法",normalizedTitle:"04 论文的方法",charIndex:880},{level:3,title:"论文的背景",slug:"论文的背景",normalizedTitle:"论文的背景",charIndex:3423},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:3433}],headersStr:"Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks 01 摘要 02 论文的目的及结论 03 论文的实验 04 论文的方法 论文的背景 总结",content:"# Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks\n\n# 单位：國立陽明交通大學\n\n# 作者： Shun-Yi Pan, Cheng-You Lu, Shih-Po Lee, Wen-Hsiao Peng\n\n# 发表：ICME 2021\n\n\n# 01 摘要\n\n基于 image-level 类别标签来做图像的语义分割，常用的方式是使用 Random Walk 来传播 CAM 的分数，用全监督的方式来训语义分割网络。但是 Random Walk 的前馈机制没有添加任何的正则化，本论文提出了 GCN-based 框架。将完整伪标签的生成 formulate 为一个半监督的学习任务，为每张训练图像都去学习 2 层的 GCN，用Laplacioan 以及 Entropy 正则化的损失。\n\n\n# 02 论文的目的及结论\n\n该论文希望用 GCN-based 特征传播范式来替代 Random Walk.\n\nRandom Walk 主要依赖特征域上像素间的亲和度，来传播 CAM 的激活分数。论文提出的范式学习 GCN 来正则化特征传播，不仅有上述的特征亲和力信息，还利用了输入图像的颜色信息。\n\n此外，本论文认为伪标签的生成是一个离线过程，所以训练了一个单独的GCN来优化每个训练图像的特征传播。 选择 GCN 而不是 CNN 的目的是因为它们在特征样本之间有不规则的亲和关系。\n\n\n# 03 论文的实验\n\nWSGCN-I 使用 IR-Net 生成的亲和度矩阵 AAA 以及 节点特征 VVV\n\nWSGCN-P 使用 PSA 生成的亲和度矩阵 AAA 以及 节点特征 VVV\n\nWSGCN-I使用边界检测网络[4]构建亲和矩阵，并将位于边界检测网络最后一层和1×1卷积层之前的特征作为节点特征。WSGCN-P 在指定亲和度矩阵 A 时遵循 AffinityNet [3]，并使用语义特征作为节点特征 V 进行亲和度评估。\n\n\n\n\n\n\n\n\n\n\n\n\n# 04 论文的方法\n\n# 4.1 Framework Overview\n\n\n\nAffinity Network 来自 PSA 和 IR-Net，\n\nFollow [3]和[10]来生成partial pseudo labels\n\n在位置 (x,y) 上，伪标签 P(x,y) 会 assign 一个类别标签，C 是前景类别，cbgc^{bg}cbg 代表背景类别，还有 ignored 的标签\n\n给定部分伪标签 P，我们认为生成完全伪标签 P 是 Graph 上的半监督学习问题。然后第一阶段的输出包括图像I的完整伪标签，在第二阶段用作训练语义分割网络的真实标签。下面详细介绍每个组件的操作。\n\n# 4.2 Inference of Complete Pseudo Labels on a Graph\n\nG=(V,E)G = (V,E)G=(V,E)，GGG 是一个 图，由点集 VVV 和 边集 EEE 组成，一个边会包含两个点以及边的权重。\n\n * 节点总数 N=H/S∗W/SN = H/S * W/SN=H/S∗W/S\n * SSS 代表下采样的因子\n * 节点特征的选择在 4.1 小节有详细说明\n * viv_ivi 的伪标签表示为 pi=P(xi,yi)p_i =P(x_i,y_i)pi =P(xi ,yi )\n * 边的权重 AijA_{ij}Aij 度量节点 viv_ivi 和 vjv_jvj 的亲和度\n * 由于 GCN 可以选择广泛的亲和力措施，所以在实验中测试了两种不同的措施 [3,4]\n\n\n\n为了生产完整的Pseudo Label，论文提出了利用图 GGG 以及 2 层的 GCN 进行特征传播以及推理，推理的方式如上\n\n点集 VVV 是节点特征的 D 维向量组成的矩阵 RN×DR^{N\\times D}RN×D，\n\nW1W_1W1 是 RD×16R^{D\\times 16}RD×16, W2W_2W2 是 R16×(∣C∣+1)R^{16 \\times (|C| + 1)}R16×(∣C∣+1)，是两个可学习的网络参数\n\nσr()\\sigma_r()σr () 和 σs()\\sigma_s()σs () 分别是 ReLU 和 softmax 的激活函数\n\nAundefined=A+IN\\widetilde{A} = A + I_NA=A+IN ，其中 INI_NIN 表示identity matrix， A∈RN×NA\\in R^{N \\times N}A∈RN×N 是亲和度矩阵\n\n背景类也要算上，所以是 ∣C∣+1|C| + 1∣C∣+1，\n\nQ=[q1,q2,...,qN]T∈RN×(∣C∣+1)Q= [q_1,q_2,...,q_N]^T \\in R^{N×(|C|+1)}Q=[q1 ,q2 ,...,qN ]T∈RN×(∣C∣+1)，每一行表示像素 (xi,yi)(x_i,y_i)(xi ,yi ) 在特征域上语义类别的概率分布，这些概率分布会进行插值（使用双线性插值）后恢复到全分辨率，然后以通道方式应用 dCRF [13] 并在每个像素处取最大跨通道以获得完整的伪标签。\n\n# 4.3 Training a GCN for Feature Propagation\n\n论文将标签的细化建模为 Graph 的半监督问题，设计了四个损失。\n\n * foreground loss lfgl_{fg}lfg\n * background loss lbgl_{bg}lbg\n * entropy loss lentl_{ent}lent\n * Laplacian loss llpl_{lp}llp\n\n总损失 l=lfg+lbg+β1lent+β2llpl = l_{fg} + l_{bg} +\\beta_1l_{ent} + \\beta_2l_{lp}l=lfg +lbg +β1 lent +β2 llp\n\nβ1\\beta_1β1 和 β2\\beta_2β2 就是超参数，前面两个就是在特征域上前景和背景像素的交叉熵。前景部分有partial pseudo labels P(x,y)∈CP(x,y)\\in CP(x,y)∈C，背景像素为 P(x,y)=cbgP(x,y)=c^{bg}P(x,y)=cbg。将交叉熵分成前景组和背景组背后的基本原理是解决这两类像素之间的不平衡问题。\n\n\n\n对于特征域中那些伪标签 P(x,y)P(x,y)P(x,y) 被标记为 ignored 的像素，例如未标记的像素，我们施加以下熵损失，要求对其类别预测的不确定性应最小化。 换句话说，它鼓励那些未标记的像素的类别预测 qiq_iqi 近似于 one-hot vectors。\n\n\n\n其中̄ Cundefined=C∪{cbg}\\widetilde{C}=C∪{\\{c^{bg}\\}}C=C∪{cbg}，VigV_{ig}Vig 指的是未标记的像素。 此外，由于观察到具有相似颜色值的相邻像素通常具有相同的语义类别，我们引入了 Laplacian loss 以确保类别预测与图像内容的一致性。 这种先验知识以拉普拉斯损失的形式被纳入GCN的训练中。\n\n\n\n该 loss 旨在根据反映像素和像素的颜色值和位置的相似性的权重 ΦiΦ_iΦi ，最小化像素 iii 和周围 NiN_iNi 像素的类别预测之间的差异（在 l2l_2l2 正则化来度量）\n\n\n\nΦi,jΦ_{i,j}Φi,j 定义在上图，\n\n * fi=(xi,yi)f_i= (x_i,y_i)fi =(xi ,yi ) 表示点的坐标\n\n * IiI_iIi 指的是像素 (xi,yi)(x_i,y_i)(xi ,yi )处的颜色值\n\n * σ1=√3, σ2= 10 是超参数\n\n * NiN_iNi 定义为 5×55 \\times 55×5 的窗口\n\nΦ 依赖于 low-level 的颜色和空间信息来正则化 GCN 输出，要与亲和度矩阵 AAA 区分开来。亲和度矩阵 AAA 使用高级语义信息 [3, 4] 来指定用于特征传播的 GCN 的图结构，接下来将详细介绍\n\n\n# 论文的背景\n\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2103.16762\n\n * https://github.com/Xavier-Pan/WSGCN",normalizedContent:"# weakly-supervised image semantic segmentation using graph convolutional networks\n\n# 单位：國立陽明交通大學\n\n# 作者： shun-yi pan, cheng-you lu, shih-po lee, wen-hsiao peng\n\n# 发表：icme 2021\n\n\n# 01 摘要\n\n基于 image-level 类别标签来做图像的语义分割，常用的方式是使用 random walk 来传播 cam 的分数，用全监督的方式来训语义分割网络。但是 random walk 的前馈机制没有添加任何的正则化，本论文提出了 gcn-based 框架。将完整伪标签的生成 formulate 为一个半监督的学习任务，为每张训练图像都去学习 2 层的 gcn，用laplacioan 以及 entropy 正则化的损失。\n\n\n# 02 论文的目的及结论\n\n该论文希望用 gcn-based 特征传播范式来替代 random walk.\n\nrandom walk 主要依赖特征域上像素间的亲和度，来传播 cam 的激活分数。论文提出的范式学习 gcn 来正则化特征传播，不仅有上述的特征亲和力信息，还利用了输入图像的颜色信息。\n\n此外，本论文认为伪标签的生成是一个离线过程，所以训练了一个单独的gcn来优化每个训练图像的特征传播。 选择 gcn 而不是 cnn 的目的是因为它们在特征样本之间有不规则的亲和关系。\n\n\n# 03 论文的实验\n\nwsgcn-i 使用 ir-net 生成的亲和度矩阵 aaa 以及 节点特征 vvv\n\nwsgcn-p 使用 psa 生成的亲和度矩阵 aaa 以及 节点特征 vvv\n\nwsgcn-i使用边界检测网络[4]构建亲和矩阵，并将位于边界检测网络最后一层和1×1卷积层之前的特征作为节点特征。wsgcn-p 在指定亲和度矩阵 a 时遵循 affinitynet [3]，并使用语义特征作为节点特征 v 进行亲和度评估。\n\n\n\n\n\n\n\n\n\n\n\n\n# 04 论文的方法\n\n# 4.1 framework overview\n\n\n\naffinity network 来自 psa 和 ir-net，\n\nfollow [3]和[10]来生成partial pseudo labels\n\n在位置 (x,y) 上，伪标签 p(x,y) 会 assign 一个类别标签，c 是前景类别，cbgc^{bg}cbg 代表背景类别，还有 ignored 的标签\n\n给定部分伪标签 p，我们认为生成完全伪标签 p 是 graph 上的半监督学习问题。然后第一阶段的输出包括图像i的完整伪标签，在第二阶段用作训练语义分割网络的真实标签。下面详细介绍每个组件的操作。\n\n# 4.2 inference of complete pseudo labels on a graph\n\ng=(v,e)g = (v,e)g=(v,e)，ggg 是一个 图，由点集 vvv 和 边集 eee 组成，一个边会包含两个点以及边的权重。\n\n * 节点总数 n=h/s∗w/sn = h/s * w/sn=h/s∗w/s\n * sss 代表下采样的因子\n * 节点特征的选择在 4.1 小节有详细说明\n * viv_ivi 的伪标签表示为 pi=p(xi,yi)p_i =p(x_i,y_i)pi =p(xi ,yi )\n * 边的权重 aija_{ij}aij 度量节点 viv_ivi 和 vjv_jvj 的亲和度\n * 由于 gcn 可以选择广泛的亲和力措施，所以在实验中测试了两种不同的措施 [3,4]\n\n\n\n为了生产完整的pseudo label，论文提出了利用图 ggg 以及 2 层的 gcn 进行特征传播以及推理，推理的方式如上\n\n点集 vvv 是节点特征的 d 维向量组成的矩阵 rn×dr^{n\\times d}rn×d，\n\nw1w_1w1 是 rd×16r^{d\\times 16}rd×16, w2w_2w2 是 r16×(∣c∣+1)r^{16 \\times (|c| + 1)}r16×(∣c∣+1)，是两个可学习的网络参数\n\nσr()\\sigma_r()σr () 和 σs()\\sigma_s()σs () 分别是 relu 和 softmax 的激活函数\n\naundefined=a+in\\widetilde{a} = a + i_na=a+in ，其中 ini_nin 表示identity matrix， a∈rn×na\\in r^{n \\times n}a∈rn×n 是亲和度矩阵\n\n背景类也要算上，所以是 ∣c∣+1|c| + 1∣c∣+1，\n\nq=[q1,q2,...,qn]t∈rn×(∣c∣+1)q= [q_1,q_2,...,q_n]^t \\in r^{n×(|c|+1)}q=[q1 ,q2 ,...,qn ]t∈rn×(∣c∣+1)，每一行表示像素 (xi,yi)(x_i,y_i)(xi ,yi ) 在特征域上语义类别的概率分布，这些概率分布会进行插值（使用双线性插值）后恢复到全分辨率，然后以通道方式应用 dcrf [13] 并在每个像素处取最大跨通道以获得完整的伪标签。\n\n# 4.3 training a gcn for feature propagation\n\n论文将标签的细化建模为 graph 的半监督问题，设计了四个损失。\n\n * foreground loss lfgl_{fg}lfg\n * background loss lbgl_{bg}lbg\n * entropy loss lentl_{ent}lent\n * laplacian loss llpl_{lp}llp\n\n总损失 l=lfg+lbg+β1lent+β2llpl = l_{fg} + l_{bg} +\\beta_1l_{ent} + \\beta_2l_{lp}l=lfg +lbg +β1 lent +β2 llp\n\nβ1\\beta_1β1 和 β2\\beta_2β2 就是超参数，前面两个就是在特征域上前景和背景像素的交叉熵。前景部分有partial pseudo labels p(x,y)∈cp(x,y)\\in cp(x,y)∈c，背景像素为 p(x,y)=cbgp(x,y)=c^{bg}p(x,y)=cbg。将交叉熵分成前景组和背景组背后的基本原理是解决这两类像素之间的不平衡问题。\n\n\n\n对于特征域中那些伪标签 p(x,y)p(x,y)p(x,y) 被标记为 ignored 的像素，例如未标记的像素，我们施加以下熵损失，要求对其类别预测的不确定性应最小化。 换句话说，它鼓励那些未标记的像素的类别预测 qiq_iqi 近似于 one-hot vectors。\n\n\n\n其中 cundefined=c∪{cbg}\\widetilde{c}=c∪{\\{c^{bg}\\}}c=c∪{cbg}，vigv_{ig}vig 指的是未标记的像素。 此外，由于观察到具有相似颜色值的相邻像素通常具有相同的语义类别，我们引入了 laplacian loss 以确保类别预测与图像内容的一致性。 这种先验知识以拉普拉斯损失的形式被纳入gcn的训练中。\n\n\n\n该 loss 旨在根据反映像素和像素的颜色值和位置的相似性的权重 φiφ_iφi ，最小化像素 iii 和周围 nin_ini 像素的类别预测之间的差异（在 l2l_2l2 正则化来度量）\n\n\n\nφi,jφ_{i,j}φi,j 定义在上图，\n\n * fi=(xi,yi)f_i= (x_i,y_i)fi =(xi ,yi ) 表示点的坐标\n\n * iii_iii 指的是像素 (xi,yi)(x_i,y_i)(xi ,yi )处的颜色值\n\n * σ1=√3, σ2= 10 是超参数\n\n * nin_ini 定义为 5×55 \\times 55×5 的窗口\n\nφ 依赖于 low-level 的颜色和空间信息来正则化 gcn 输出，要与亲和度矩阵 aaa 区分开来。亲和度矩阵 aaa 使用高级语义信息 [3, 4] 来指定用于特征传播的 gcn 的图结构，接下来将详细介绍\n\n\n# 论文的背景\n\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2103.16762\n\n * https://github.com/xavier-pan/wsgcn",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation",frontmatter:{title:"Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation",date:"2021-05-11T11:19:52.000Z",permalink:"/pages/7232a7/",categories:["论文阅读","弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/03.Discriminative%20Region%20Suppression%20for%20Weakly-Supervised%20Semantic%20Segmentation.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/03.Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation.md",key:"v-91bf2bb8",path:"/pages/7232a7/",headers:[{level:2,title:"Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation",slug:"discriminative-region-suppression-for-weakly-supervised-semantic-segmentation",normalizedTitle:"discriminative region suppression for weakly-supervised semantic segmentation",charIndex:2}],headersStr:"Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation",content:"# Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation\n\n# 单位：KAIST\n\n# 作者：Beomyoung Kim, Sangeun Han, Junmo Kim\n\n# 发表：AAAI 2021\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2103.07246\n\n * https://github.com/qjadud1994/DRS",normalizedContent:"# discriminative region suppression for weakly-supervised semantic segmentation\n\n# 单位：kaist\n\n# 作者：beomyoung kim, sangeun han, junmo kim\n\n# 发表：aaai 2021\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2103.07246\n\n * https://github.com/qjadud1994/drs",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Weakly-Supervised Semantic Segmentation via Sub-category Exploration",frontmatter:{title:"Weakly-Supervised Semantic Segmentation via Sub-category Exploration",date:"2021-05-11T15:31:15.000Z",permalink:"/pages/60761b/",categories:["论文阅读","弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/04.Weakly-Supervised%20Semantic%20Segmentation%20via%20Sub-category%20Exploration.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/04.Weakly-Supervised Semantic Segmentation via Sub-category Exploration.md",key:"v-4be82551",path:"/pages/60761b/",headers:[{level:2,title:"Weakly-Supervised Semantic Segmentation via Sub-category Exploration",slug:"weakly-supervised-semantic-segmentation-via-sub-category-exploration",normalizedTitle:"weakly-supervised semantic segmentation via sub-category exploration",charIndex:2}],headersStr:"Weakly-Supervised Semantic Segmentation via Sub-category Exploration",content:"# Weakly-Supervised Semantic Segmentation via Sub-category Exploration\n\n# 单位：UC Merced, eBay, NEC Labs Ameriva, Google Research.\n\n# 作者： Yu-Ting Chang, Qiaosong Wang, Wei-Chih Hung, Robinson Piramuthu, Yi-Hsuan Tsai, Ming-Hsuan Yang\n\n# 发表：CVPR 2020\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n\n\n\n\n\n\n# 实验性能（3张表）\n\n * Tab.1 是和 AffinityNet 的比较\n * Tab.3 是和其他方法在每个类别上的性能比较\n * Tab.4 是和 SOTA 方法的性能比较\n\n\n\n\n\n# 消融实验（2个表）\n\n上面两张表是对子类数量 KKK 以及迭代次数的消融实验，最终K 取值为 10，round取值为3\n\n\n\n\n\n\n\n# 可视化结果（4张图）\n\n * Fig.3 展示了CAM的可视化结果\n * Fig.5 展示了聚类的结果\n * Fig.6 展示了特征空间上子类和父类的 t-SNE 结果，表明人这个子类别通常和其他的父类靠的很近，因为他们在同一张图里共同出现的概率很高\n * Fig.7 是量化结果\n * \n\n# 论文的方法\n\n\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n将原始类别称为父类，在特征空间上对同父类样本的特征聚类后生成 KKK 个子类，聚类的结果作为子类的groundtruth，用于计算子类的loss。相当于做了一个额外的任务约束，来激励模型学到更多的语义信息。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2008.01183\n\n * https://github.com/Juliachang/SC-CAM\n\n * Refinement: We adopt the random walk method via affinity to refine the map as pixel-wise pseudo ground truths for semantic segmentation. Please refer to the repo of AffinityNet [1].\n\n * Segmentation network: We utilize the Deeplab-v2 framework [2] with the ResNet-101 architecture [3] as the backbone model to train the segmentation network. Please refer to the repo.",normalizedContent:"# weakly-supervised semantic segmentation via sub-category exploration\n\n# 单位：uc merced, ebay, nec labs ameriva, google research.\n\n# 作者： yu-ting chang, qiaosong wang, wei-chih hung, robinson piramuthu, yi-hsuan tsai, ming-hsuan yang\n\n# 发表：cvpr 2020\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n\n\n\n\n\n\n# 实验性能（3张表）\n\n * tab.1 是和 affinitynet 的比较\n * tab.3 是和其他方法在每个类别上的性能比较\n * tab.4 是和 sota 方法的性能比较\n\n\n\n\n\n# 消融实验（2个表）\n\n上面两张表是对子类数量 kkk 以及迭代次数的消融实验，最终k 取值为 10，round取值为3\n\n\n\n\n\n\n\n# 可视化结果（4张图）\n\n * fig.3 展示了cam的可视化结果\n * fig.5 展示了聚类的结果\n * fig.6 展示了特征空间上子类和父类的 t-sne 结果，表明人这个子类别通常和其他的父类靠的很近，因为他们在同一张图里共同出现的概率很高\n * fig.7 是量化结果\n * \n\n# 论文的方法\n\n\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n将原始类别称为父类，在特征空间上对同父类样本的特征聚类后生成 kkk 个子类，聚类的结果作为子类的groundtruth，用于计算子类的loss。相当于做了一个额外的任务约束，来激励模型学到更多的语义信息。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2008.01183\n\n * https://github.com/juliachang/sc-cam\n\n * refinement: we adopt the random walk method via affinity to refine the map as pixel-wise pseudo ground truths for semantic segmentation. please refer to the repo of affinitynet [1].\n\n * segmentation network: we utilize the deeplab-v2 framework [2] with the resnet-101 architecture [3] as the backbone model to train the segmentation network. please refer to the repo.",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"AffinityNet Learning Pixel level Semantic Affinity with Image level Supervision for Weakly Supervised Semantic Segmentation",frontmatter:{title:"AffinityNet Learning Pixel level Semantic Affinity with Image level Supervision for Weakly Supervised Semantic Segmentation",date:"2021-05-11T15:42:47.000Z",permalink:"/pages/35ccd6/",categories:["论文阅读","弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/05.Learning%20Pixel%20level%20Semantic%20Affinity%20with%20Image%20level%20Supervision%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/05.Learning Pixel level Semantic Affinity with Image level Supervision for Weakly Supervised Semantic Segmentation.md",key:"v-456339ea",path:"/pages/35ccd6/",headers:[{level:2,title:"Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation",slug:"learning-pixel-level-semantic-affinity-with-image-level-supervision-for-weakly-supervised-semantic-segmentation",normalizedTitle:"learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation",charIndex:2}],headersStr:"Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation",content:"# Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation\n\n# 单位：DGIST, POSTECH\n\n# 作者：Jiwoon Ahn, Suha Kwak\n\n# 发表：CVPR 2018\n\n# 摘要\n\n分割标签的不足是语义分割的主要问题，为了缓解这一问题，我们提出了一个全新的框架，给定image-level 类别标签即可生成分割标签。在这种弱监督的设置下，众所周知的是训练模型是对局部辨别的部分而不是整个物体区域进行判别。我们的解决方案是将这种局部反应传播到属于同一语义实体的附近区域。命名为 AffinityNet ，用来预测一对相邻图像坐标之间的语义亲和性，语义传播则利用AffinityNet 预测得到的 affinity 进行 Random Walk操作来实现。\n\n更重要的是，用于训练AffinityNet的监督是由局部判别性部分分割提供的，它作为分割注释是不完整的，但对于学习小图像区域内的语义亲和力是足够的。\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n方法分成两部分\n\n * 利用给定的image-level 的标签合成pixel-level的标签\n * 利用生成的分割标签训练分割模型\n\n整个框架分为三个网络：一个网络用于计算CAMs，AffinityNet，以及一个分割模型，前两个用于生成训练图像的分割标签，最后一个利用生成的分割标签来完成语义分割任务。\n\n# 3.1 计算CAMs\n\n在其他弱监督方法中，CAMs 会被当做分割种子，局部显著区域，然后传播以覆盖到整个目标区域，在本论文中，CAMs 还被用作 AffinityNet 的监督信号。Follow 了 [40] 的方法来计算CAMs，该架构是一个典型的分类网络，具有全局平均池（GAP），然后是完全连接层，并通过一个具有图像级别标签的分类标准进行训练。\n\n给定训练好的网络，groundtruth 的类别 ccc 的 CAMs 表示为 McM_cMc ，计算公式为 Mc(x,y)=wcTfcam(x,y)M_c(x,y)=w_c^Tf^{cam}(x,y)Mc (x,y)=wcT fcam(x,y)\n\nwcw_cwc 是与类别 ccc 有关的分类权重，fcam(x,y)f^{cam}(x,y)fcam(x,y) 是指 (x,y) 处的 GAP 层前的特征向量。McM_cMc 被进一步归一化，使得最大响应等于 1：Mc(x,y)→Mc(x,y)/maxx,yMc(x,y)M_c(x,y)→M_c(x,y)/max_{x,y}M_c(x,y)Mc (x,y)→Mc (x,y)/maxx,y Mc (x,y)。对于和groundtruth 无关的类别来讲，我们都将其 McM_cMc 置为0。该文章也计算了一个背景响应图，计算公式是\n\n\n\n\n\n# 3.2 学习 AffinityNet\n\nAffinityNet 旨在预测训练图像上一对相邻坐标之间的类不可知的语义亲和力。预测得到的亲和力被用于random walk 的转移概率，random walk 将 CAMs 的激活分数传播到具有相同语义的附近区域，从而显著提升了 CAMs 的质量。\n\n为了计算效率，AffinityNet 预测一个卷积的特征图 fafff^{aff}faff ，一对特征向量的语义亲和度由他们之间的 L1L_1L1 距离来定义，feature iii 和 feature jjj 的距离 WijW_{ij}Wij 公式如下，(xi,yi)(x_i,y_i)(xi ,yi ) 表示 ithi^{th}ith feature 在 feature map fafff^{aff}faff 上的坐标：\n\nWij=exp(−∣∣faff(xi,yi)−faff(xj,yj)∣∣)W_{ij} = exp(-||f^{aff}(x_i,y_i) - f^{aff}(x_j,y_j)||)Wij =exp(−∣∣faff(xi ,yi )−faff(xj ,yj )∣∣)\n\n\n\n上图是AffinityNet 的整体架构，Backbone 后不同stage 的特征经过 1x1 卷积降维后 concat 到一起得到 fafff^{aff}faff\n\n# 3.2.1 生成语义亲和度标签\n\n为了用 image-level 的标签训练 AffinityNet，我们利用CAMs 作为监督信号。虽然 CAMs 通常来说都不是很精确，我们发现仔细处理他们也能获得对语义相似性的可靠监督信号\n\n\n\n基础的 idea 就是判别出那些可靠性较高的目标区域以及背景，从这些区域中采样训练样本。通过这种方式，一堆采样坐标间的语义相似性可以被较为可靠的确定。为了估计物体的置信区域，我们首先通过减少公式（2）中的 α来放大MbgM_{bg}Mbg ，使背景分数支配 CAMs 中物体的不重要的激活分数。在经过 dCRF 对 CAMs 做细化之后，通过收集那些目标类别的分数比其他类别及背景分数大的坐标来作为置信区域。在相反的实验设置（增大α\\alphaα，减弱 MbgM_{bg}Mbg ） ，可靠的背景区域会以同样的方式识别出来，图像剩余的区域则被认为是中立区域（neutral），结果如上图所示\n\n现在即生成了二类的affinity label，对于每一对像素的坐标(xi,yi)(x_i,y_i)(xi ,yi ) 以及 (xj,yj)(x_j,y_j)(xj ,yj ) 来讲，如果他们的类别相同，其affinity label 即为1，如果类别不同则为0。如果其中一个坐标是中立区域，我们就可以简单的在训练过程中忽略掉这一对点。利用这种方案使得我们能够收集相当多的成对亲和力标签，这些标签也足够可靠\n\n# 3.2.2 AffinityNet Training\n\nAffinityNet 通过近似二类的affinity label来训练。在训练期间由于以下两个原因只需要考虑足够相邻坐标的亲和力即可\n\n * 由于上下文的限制，预测两个非常远坐标的语义相似度是很难的\n * 为了只解决相邻的成对坐标，我们可以大大减少计算开销\n\n所以用作训练的坐标对叫做PPP ,P={(i,j)∣d((xi,yi),(xj,yj))<λ,∀i≠j}P=\\{(i,j)|d((x_i,y_i),(x_j,y_j)) < \\lambda,\\forall i \\neq j\\}P={(i,j)∣d((xi ,yi ),(xj ,yj ))<λ,∀i=j}\n\n也就是说在半径范围内的点才被用作训练，但是这还是会导致类别不平衡问题，PPP 中类别分布明显偏向于 Positive类，而 Negative 类仅仅在对象边界附近采样，在 Positive 类别中，背景类别对的数量也明显大于物体对的数量，为了解决这个问题，文章将 PPP 分成了三个子集，将三个子集上的损失汇总起来。\n\n\n\n如上图所示，先将 PPP 分为 P+P^+P+ 和 P−P^-P−, 再将 P+P^+P+ 分为 Pfg+P_{fg}^+Pfg+ 和 Pbg+P_{bg}^+Pbg+\n\n * P+P^+P+ 代表 WijW_{ij}Wij 为 1 的 pair，Pfg+P_{fg}^+Pfg+ 代表 object 的 Positive pair，Pfg−P_{fg}^-Pfg− 代表背景的 Positive pair\n * P−P^-P− 代表 WijW_{ij}Wij 为 0 的 pair\n\n最终的 Loss 也分为三块，分别是 Lfg+L_{fg}^+Lfg+ ，Lbg+L_{bg}^+Lbg+ ，L−L^-L−，总损失为 L=Lfg++Lfg−+2L−L = L_{fg}^+ + L_{fg}^- + 2L^-L=Lfg+ +Lfg− +2L−，值得注意的是，该损失是类别无关的，这可以使得 AffinityNet 学到更加 general 的表示用于区分目标和背景。\n\n# 3.3 使用 AffinityNet 对 CAMs 做细化\n\n用训好的 AffinityNet 对CAMs 做细化，AffinityNet 预测得到的局部语义相似度被转化为一个转移概率矩阵，确保random walk 可以了解图像中的语义边界，并鼓励其在这些边界内扩散激活分数。\n\n对于输入图像，AffinityNet 生成一个卷积特征图以及式（3）所描述的语义相似度。经过计算的相似度形成一个相似度矩阵W，其对角线元素为1。 从相似度矩阵中得出的Random walk的过渡概率矩阵T如下所示:\n\n\n\nβ\\betaβ 是一个比 1 大的超参数，得到原始相似度矩阵 W 的Hadamard幂，忽略掉一些相似度，得到 WoβW^{o\\beta}Woβ 矩阵\n\n对角矩阵 D 是对 WoβW^{o\\beta}Woβ 矩阵 做行归一化得到的\n\n通过矩阵 T 的Random walk，语义传播的操作是通过将 T 与 CAMs 相乘来实现的，我们迭代地进行这种传播，直到达到预定的迭代次数。\n\n\n\nvec(⋅)vec(·)vec(⋅) 代表矩阵向量化，ttt 代表迭代的次数，\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1803.10464\n\n * https://github.com/jiwoon-ahn/psa",normalizedContent:"# learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation\n\n# 单位：dgist, postech\n\n# 作者：jiwoon ahn, suha kwak\n\n# 发表：cvpr 2018\n\n# 摘要\n\n分割标签的不足是语义分割的主要问题，为了缓解这一问题，我们提出了一个全新的框架，给定image-level 类别标签即可生成分割标签。在这种弱监督的设置下，众所周知的是训练模型是对局部辨别的部分而不是整个物体区域进行判别。我们的解决方案是将这种局部反应传播到属于同一语义实体的附近区域。命名为 affinitynet ，用来预测一对相邻图像坐标之间的语义亲和性，语义传播则利用affinitynet 预测得到的 affinity 进行 random walk操作来实现。\n\n更重要的是，用于训练affinitynet的监督是由局部判别性部分分割提供的，它作为分割注释是不完整的，但对于学习小图像区域内的语义亲和力是足够的。\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n方法分成两部分\n\n * 利用给定的image-level 的标签合成pixel-level的标签\n * 利用生成的分割标签训练分割模型\n\n整个框架分为三个网络：一个网络用于计算cams，affinitynet，以及一个分割模型，前两个用于生成训练图像的分割标签，最后一个利用生成的分割标签来完成语义分割任务。\n\n# 3.1 计算cams\n\n在其他弱监督方法中，cams 会被当做分割种子，局部显著区域，然后传播以覆盖到整个目标区域，在本论文中，cams 还被用作 affinitynet 的监督信号。follow 了 [40] 的方法来计算cams，该架构是一个典型的分类网络，具有全局平均池（gap），然后是完全连接层，并通过一个具有图像级别标签的分类标准进行训练。\n\n给定训练好的网络，groundtruth 的类别 ccc 的 cams 表示为 mcm_cmc ，计算公式为 mc(x,y)=wctfcam(x,y)m_c(x,y)=w_c^tf^{cam}(x,y)mc (x,y)=wct fcam(x,y)\n\nwcw_cwc 是与类别 ccc 有关的分类权重，fcam(x,y)f^{cam}(x,y)fcam(x,y) 是指 (x,y) 处的 gap 层前的特征向量。mcm_cmc 被进一步归一化，使得最大响应等于 1：mc(x,y)→mc(x,y)/maxx,ymc(x,y)m_c(x,y)→m_c(x,y)/max_{x,y}m_c(x,y)mc (x,y)→mc (x,y)/maxx,y mc (x,y)。对于和groundtruth 无关的类别来讲，我们都将其 mcm_cmc 置为0。该文章也计算了一个背景响应图，计算公式是\n\n\n\n\n\n# 3.2 学习 affinitynet\n\naffinitynet 旨在预测训练图像上一对相邻坐标之间的类不可知的语义亲和力。预测得到的亲和力被用于random walk 的转移概率，random walk 将 cams 的激活分数传播到具有相同语义的附近区域，从而显著提升了 cams 的质量。\n\n为了计算效率，affinitynet 预测一个卷积的特征图 fafff^{aff}faff ，一对特征向量的语义亲和度由他们之间的 l1l_1l1 距离来定义，feature iii 和 feature jjj 的距离 wijw_{ij}wij 公式如下，(xi,yi)(x_i,y_i)(xi ,yi ) 表示 ithi^{th}ith feature 在 feature map fafff^{aff}faff 上的坐标：\n\nwij=exp(−∣∣faff(xi,yi)−faff(xj,yj)∣∣)w_{ij} = exp(-||f^{aff}(x_i,y_i) - f^{aff}(x_j,y_j)||)wij =exp(−∣∣faff(xi ,yi )−faff(xj ,yj )∣∣)\n\n\n\n上图是affinitynet 的整体架构，backbone 后不同stage 的特征经过 1x1 卷积降维后 concat 到一起得到 fafff^{aff}faff\n\n# 3.2.1 生成语义亲和度标签\n\n为了用 image-level 的标签训练 affinitynet，我们利用cams 作为监督信号。虽然 cams 通常来说都不是很精确，我们发现仔细处理他们也能获得对语义相似性的可靠监督信号\n\n\n\n基础的 idea 就是判别出那些可靠性较高的目标区域以及背景，从这些区域中采样训练样本。通过这种方式，一堆采样坐标间的语义相似性可以被较为可靠的确定。为了估计物体的置信区域，我们首先通过减少公式（2）中的 α来放大mbgm_{bg}mbg ，使背景分数支配 cams 中物体的不重要的激活分数。在经过 dcrf 对 cams 做细化之后，通过收集那些目标类别的分数比其他类别及背景分数大的坐标来作为置信区域。在相反的实验设置（增大α\\alphaα，减弱 mbgm_{bg}mbg ） ，可靠的背景区域会以同样的方式识别出来，图像剩余的区域则被认为是中立区域（neutral），结果如上图所示\n\n现在即生成了二类的affinity label，对于每一对像素的坐标(xi,yi)(x_i,y_i)(xi ,yi ) 以及 (xj,yj)(x_j,y_j)(xj ,yj ) 来讲，如果他们的类别相同，其affinity label 即为1，如果类别不同则为0。如果其中一个坐标是中立区域，我们就可以简单的在训练过程中忽略掉这一对点。利用这种方案使得我们能够收集相当多的成对亲和力标签，这些标签也足够可靠\n\n# 3.2.2 affinitynet training\n\naffinitynet 通过近似二类的affinity label来训练。在训练期间由于以下两个原因只需要考虑足够相邻坐标的亲和力即可\n\n * 由于上下文的限制，预测两个非常远坐标的语义相似度是很难的\n * 为了只解决相邻的成对坐标，我们可以大大减少计算开销\n\n所以用作训练的坐标对叫做ppp ,p={(i,j)∣d((xi,yi),(xj,yj))<λ,∀i=j}p=\\{(i,j)|d((x_i,y_i),(x_j,y_j)) < \\lambda,\\forall i \\neq j\\}p={(i,j)∣d((xi ,yi ),(xj ,yj ))<λ,∀i=j}\n\n也就是说在半径范围内的点才被用作训练，但是这还是会导致类别不平衡问题，ppp 中类别分布明显偏向于 positive类，而 negative 类仅仅在对象边界附近采样，在 positive 类别中，背景类别对的数量也明显大于物体对的数量，为了解决这个问题，文章将 ppp 分成了三个子集，将三个子集上的损失汇总起来。\n\n\n\n如上图所示，先将 ppp 分为 p+p^+p+ 和 p−p^-p−, 再将 p+p^+p+ 分为 pfg+p_{fg}^+pfg+ 和 pbg+p_{bg}^+pbg+\n\n * p+p^+p+ 代表 wijw_{ij}wij 为 1 的 pair，pfg+p_{fg}^+pfg+ 代表 object 的 positive pair，pfg−p_{fg}^-pfg− 代表背景的 positive pair\n * p−p^-p− 代表 wijw_{ij}wij 为 0 的 pair\n\n最终的 loss 也分为三块，分别是 lfg+l_{fg}^+lfg+ ，lbg+l_{bg}^+lbg+ ，l−l^-l−，总损失为 l=lfg++lfg−+2l−l = l_{fg}^+ + l_{fg}^- + 2l^-l=lfg+ +lfg− +2l−，值得注意的是，该损失是类别无关的，这可以使得 affinitynet 学到更加 general 的表示用于区分目标和背景。\n\n# 3.3 使用 affinitynet 对 cams 做细化\n\n用训好的 affinitynet 对cams 做细化，affinitynet 预测得到的局部语义相似度被转化为一个转移概率矩阵，确保random walk 可以了解图像中的语义边界，并鼓励其在这些边界内扩散激活分数。\n\n对于输入图像，affinitynet 生成一个卷积特征图以及式（3）所描述的语义相似度。经过计算的相似度形成一个相似度矩阵w，其对角线元素为1。 从相似度矩阵中得出的random walk的过渡概率矩阵t如下所示:\n\n\n\nβ\\betaβ 是一个比 1 大的超参数，得到原始相似度矩阵 w 的hadamard幂，忽略掉一些相似度，得到 woβw^{o\\beta}woβ 矩阵\n\n对角矩阵 d 是对 woβw^{o\\beta}woβ 矩阵 做行归一化得到的\n\n通过矩阵 t 的random walk，语义传播的操作是通过将 t 与 cams 相乘来实现的，我们迭代地进行这种传播，直到达到预定的迭代次数。\n\n\n\nvec(⋅)vec(·)vec(⋅) 代表矩阵向量化，ttt 代表迭代的次数，\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1803.10464\n\n * https://github.com/jiwoon-ahn/psa",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Grad-CAM Visual Explanations from Deep Networks via Gradient-based Localization",frontmatter:{title:"Grad-CAM Visual Explanations from Deep Networks via Gradient-based Localization",date:"2021-05-13T16:02:48.000Z",permalink:"/pages/cf4b85/",categories:["论文阅读","弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/06.Grad-CAM%20Visual%20Explanations%20from%20Deep%20Networks%20via%20Gradient-based%20Localization.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/06.Grad-CAM Visual Explanations from Deep Networks via Gradient-based Localization.md",key:"v-2ecd8cf6",path:"/pages/cf4b85/",headers:[{level:2,title:"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",slug:"grad-cam-visual-explanations-from-deep-networks-via-gradient-based-localization",normalizedTitle:"grad-cam: visual explanations from deep networks via gradient-based localization",charIndex:2}],headersStr:"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",content:"# Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\n\n# 作者：Georgia Tech\n\n# 发表：ICCV 2017\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1610.02391\n\n * https://github.com/ramprs/grad-cam",normalizedContent:"# grad-cam: visual explanations from deep networks via gradient-based localization\n\n# 作者：georgia tech\n\n# 发表：iccv 2017\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1610.02391\n\n * https://github.com/ramprs/grad-cam",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Grad-CAM++ Improved Visual Explanations for Deep Convolutional Networks",frontmatter:{title:"Grad-CAM++ Improved Visual Explanations for Deep Convolutional Networks",date:"2021-05-13T16:02:56.000Z",permalink:"/pages/fa177d/",categories:["论文阅读","弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/07.Grad-CAM++%20Improved%20Visual%20Explanations%20for%20Deep%20Convolutional%20Networks.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/07.Grad-CAM++ Improved Visual Explanations for Deep Convolutional Networks.md",key:"v-04d06b5b",path:"/pages/fa177d/",headers:[{level:2,title:"Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks",slug:"grad-cam-improved-visual-explanations-for-deep-convolutional-networks",normalizedTitle:"grad-cam++: improved visual explanations for deep convolutional networks",charIndex:2}],headersStr:"Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks",content:"# Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks\n\n# 作者：Indian Institute of Technology Hyderabad\n\n# 发表：WACV 2018\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1710.11063\n\n * https://github.com/adityac94/Grad_CAM_plus_plus",normalizedContent:"# grad-cam++: improved visual explanations for deep convolutional networks\n\n# 作者：indian institute of technology hyderabad\n\n# 发表：wacv 2018\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1710.11063\n\n * https://github.com/adityac94/grad_cam_plus_plus",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation",frontmatter:{title:"Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation",date:"2021-07-29T17:30:48.000Z",permalink:"/pages/1e1493/",categories:["论文阅读","弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/08.Leveraging%20Auxiliary%20Tasks%20with%20Affinity%20Learning%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/08.Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation.md",key:"v-0e6fb88c",path:"/pages/1e1493/",headersStr:null,content:"# Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation\n\n# 作者：Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid Boussaid, Ferdous Sohel, and Dan Xu\n\n# 单位：UWA, USYD, MURS, HKUST\n\n# 发表：ICCV 2021\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2107.11787\n\n * https://github.com/xulianuwa/AuxSegNet",normalizedContent:"# leveraging auxiliary tasks with affinity learning for weakly supervised semantic segmentation\n\n# 作者：lian xu, wanli ouyang, mohammed bennamoun, farid boussaid, ferdous sohel, and dan xu\n\n# 单位：uwa, usyd, murs, hkust\n\n# 发表：iccv 2021\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2107.11787\n\n * https://github.com/xulianuwa/auxsegnet",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Embedded Discriminative Attention Mechanism for Weakly Supervised Semantic Segmentation",frontmatter:{title:"Embedded Discriminative Attention Mechanism for Weakly Supervised Semantic Segmentation",date:"2021-10-12T23:57:59.000Z",permalink:"/pages/6947ae/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/09.Embedded%20Discriminative%20Attention%20Mechanism%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/09.Embedded Discriminative Attention Mechanism for Weakly Supervised Semantic Segmentation.md",key:"v-6d7c880a",path:"/pages/6947ae/",headersStr:null,content:"# Embedded Discriminative Attention Mechanism for Weakly Supervised Semantic Segmentation\n\n# 作者：Jiwoon Ahn, Sunghyun Cho, Suha Kwak\n\n# 单位：DGIST, POSTECH\n\n# 发表：CVPR 2019 (Oral)\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Embedded_Discriminative_Attention_Mechanism_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2021_paper.pdf\n * https://github.com/allenwu97/EDAM",normalizedContent:"# embedded discriminative attention mechanism for weakly supervised semantic segmentation\n\n# 作者：jiwoon ahn, sunghyun cho, suha kwak\n\n# 单位：dgist, postech\n\n# 发表：cvpr 2019 (oral)\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://openaccess.thecvf.com/content/cvpr2021/papers/wu_embedded_discriminative_attention_mechanism_for_weakly_supervised_semantic_segmentation_cvpr_2021_paper.pdf\n * https://github.com/allenwu97/edam",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Group-Wise Semantic Mining for Weakly Supervised Semantic Segmentation",frontmatter:{title:"Group-Wise Semantic Mining for Weakly Supervised Semantic Segmentation",date:"2021-10-13T00:51:57.000Z",permalink:"/pages/dd295d/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/10.Group-Wise%20Semantic%20Mining%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/10.Group-Wise Semantic Mining for Weakly Supervised Semantic Segmentation.md",key:"v-7fc45692",path:"/pages/dd295d/",headersStr:null,content:"# Group-Wise Semantic Mining for Weakly Supervised Semantic Segmentation.\n\n# 作者：Xueyi Li, Tianfei Zhou, Jianwu Li, Yi Zhou, Zhaoxiang Zhang\n\n# 单位：BIT, ETH Zurich, Southeast University, CASIA\n\n# 发表：AAAI 2021\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2012.05007\n\n * https://github.com/Lixy1997/Group-WSSS",normalizedContent:"# group-wise semantic mining for weakly supervised semantic segmentation.\n\n# 作者：xueyi li, tianfei zhou, jianwu li, yi zhou, zhaoxiang zhang\n\n# 单位：bit, eth zurich, southeast university, casia\n\n# 发表：aaai 2021\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2012.05007\n\n * https://github.com/lixy1997/group-wsss",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation",frontmatter:{title:"Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation",date:"2021-10-13T00:55:19.000Z",permalink:"/pages/27c6f8/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/11.Mining%20Cross-Image%20Semantics%20for%20Weakly%20Supervised%20Semantic%20Segmentation.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/11.Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation.md",key:"v-dce64a46",path:"/pages/27c6f8/",headers:[{level:2,title:"Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation",slug:"mining-cross-image-semantics-for-weakly-supervised-semantic-segmentation",normalizedTitle:"mining cross-image semantics for weakly supervised semantic segmentation",charIndex:2}],headersStr:"Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation",content:"# Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation\n\n# 作者：Guolei Sun, Wenguan Wang, Jifeng Dai, Luc Van Gool.\n\n# 单位：ETH Zurich, SenseTime, SJTU\n\n# 发表：ECCV 2020 Oral\n\n# 补充： Best Paper Award and winner solution in WSSS Track of CVPR2020 LID challenge\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470341.pdf\n\n * https://github.com/GuoleiSun/MCIS_wsss\n\n * https://zhuanlan.zhihu.com/p/161042291\n\n * https://lidchallenge.github.io/",normalizedContent:"# mining cross-image semantics for weakly supervised semantic segmentation\n\n# 作者：guolei sun, wenguan wang, jifeng dai, luc van gool.\n\n# 单位：eth zurich, sensetime, sjtu\n\n# 发表：eccv 2020 oral\n\n# 补充： best paper award and winner solution in wsss track of cvpr2020 lid challenge\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://www.ecva.net/papers/eccv_2020/papers_eccv/papers/123470341.pdf\n\n * https://github.com/guoleisun/mcis_wsss\n\n * https://zhuanlan.zhihu.com/p/161042291\n\n * https://lidchallenge.github.io/",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"NoPeopleAllowed The Three-Step Approach to Weakly Supervised SemanticSegmentation",frontmatter:{title:"NoPeopleAllowed The Three-Step Approach to Weakly Supervised SemanticSegmentation",date:"2021-10-13T00:58:32.000Z",permalink:"/pages/611606/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/12.NoPeopleAllowed%20The%20Three-Step%20Approach%20to%20Weakly%20Supervised%20SemanticSegmentation.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/12.NoPeopleAllowed The Three-Step Approach to Weakly Supervised SemanticSegmentation.md",key:"v-a6e70b22",path:"/pages/611606/",headers:[{level:2,title:"NoPeopleAllowed: The Three-Step Approach to Weakly Supervised Semantic Segmentation",slug:"nopeopleallowed-the-three-step-approach-to-weakly-supervised-semantic-segmentation",normalizedTitle:"nopeopleallowed: the three-step approach to weakly supervised semantic segmentation",charIndex:2}],headersStr:"NoPeopleAllowed: The Three-Step Approach to Weakly Supervised Semantic Segmentation",content:"# NoPeopleAllowed: The Three-Step Approach to Weakly Supervised Semantic Segmentation\n\n# 作者：Mariia Dobko, Ostap Viniavskyi, Oles Dobosevych\n\n# 单位：ETH Zurich, SenseTime, SJTU\n\n# 补充：A 3rd place solution for LID Challenge at CVPR 2020 on Weakly Supervised Semantic Segmentation\n\n# 发表：CVPR 2020\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2006.07601\n\n * https://github.com/ucuapps/LIDChallenge2020-NoPeopleAllowed\n\n * https://lidchallenge.github.io/",normalizedContent:"# nopeopleallowed: the three-step approach to weakly supervised semantic segmentation\n\n# 作者：mariia dobko, ostap viniavskyi, oles dobosevych\n\n# 单位：eth zurich, sensetime, sjtu\n\n# 补充：a 3rd place solution for lid challenge at cvpr 2020 on weakly supervised semantic segmentation\n\n# 发表：cvpr 2020\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2006.07601\n\n * https://github.com/ucuapps/lidchallenge2020-nopeopleallowed\n\n * https://lidchallenge.github.io/",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations",frontmatter:{title:"Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations",date:"2021-10-13T02:32:42.000Z",permalink:"/pages/72359e/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/13.Weakly%20Supervised%20Learning%20of%20Instance%20Segmentation%20with%20Inter-pixel%20Relations.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/13.Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations.md",key:"v-30f9ac0c",path:"/pages/72359e/",headers:[{level:2,title:"Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations",slug:"weakly-supervised-learning-of-instance-segmentation-with-inter-pixel-relations",normalizedTitle:"weakly supervised learning of instance segmentation with inter-pixel relations",charIndex:2}],headersStr:"Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations",content:"# Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations\n\n# 作者：Jiwoon Ahn, Sunghyun Cho, Suha Kwak\n\n# 单位：DGIST, POSTECH\n\n# 发表：CVPR 2019 Oral\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1904.05044\n\n * https://github.com/jiwoon-ahn/irn\n\n * https://zhuanlan.zhihu.com/p/83197212",normalizedContent:"# weakly supervised learning of instance segmentation with inter-pixel relations\n\n# 作者：jiwoon ahn, sunghyun cho, suha kwak\n\n# 单位：dgist, postech\n\n# 发表：cvpr 2019 oral\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1904.05044\n\n * https://github.com/jiwoon-ahn/irn\n\n * https://zhuanlan.zhihu.com/p/83197212",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Learning Deep Features for Discriminative Localization",frontmatter:{title:"Learning Deep Features for Discriminative Localization",date:"2021-10-13T14:04:41.000Z",permalink:"/pages/632c01/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/14.Learning%20Deep%20Features%20for%20Discriminative%20Localization.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/14.Learning Deep Features for Discriminative Localization.md",key:"v-28c46957",path:"/pages/632c01/",headersStr:null,content:"# Learning Deep Features for Discriminative Localization\n\n# 作者：Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba\n\n# 单位：MIT\n\n# 发表：CVPR 2016\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n核心代码非常简单， 提取到特征图和目标类别全连接的权重，直接加权求和，再经过relu操作去除负值，最后归一化获取CAM，具体如下:\n\n# 获取全连接层的权重\nself._fc_weights = self.model._modules.get(fc_layer).weight.data\n# 获取目标类别的权重作为特征权重\nweights=self._fc_weights[class_idx, :]\n# 这里self.hook_a为最后一层特征图的输出\nbatch_cams = (weights.unsqueeze(-1).unsqueeze(-1) * self.hook_a.squeeze(0)).sum(dim=0)\n# relu操作,去除负值\nbatch_cams = F.relu(batch_cams, inplace=True)\n# 归一化操作\nbatch_cams = self._normalize(batch_cams)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n利用 GAP 获取 CAM 的开山之作\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1512.04150\n\n * https://github.com/zhoubolei/CAM\n   \n   * https://github.com/zhoubolei/CAM/blob/master/pytorch_CAM.py\n\n * https://cloud.tencent.com/developer/article/1674200\n\n * ",normalizedContent:"# learning deep features for discriminative localization\n\n# 作者：bolei zhou, aditya khosla, agata lapedriza, aude oliva, antonio torralba\n\n# 单位：mit\n\n# 发表：cvpr 2016\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n核心代码非常简单， 提取到特征图和目标类别全连接的权重，直接加权求和，再经过relu操作去除负值，最后归一化获取cam，具体如下:\n\n# 获取全连接层的权重\nself._fc_weights = self.model._modules.get(fc_layer).weight.data\n# 获取目标类别的权重作为特征权重\nweights=self._fc_weights[class_idx, :]\n# 这里self.hook_a为最后一层特征图的输出\nbatch_cams = (weights.unsqueeze(-1).unsqueeze(-1) * self.hook_a.squeeze(0)).sum(dim=0)\n# relu操作,去除负值\nbatch_cams = f.relu(batch_cams, inplace=true)\n# 归一化操作\nbatch_cams = self._normalize(batch_cams)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n利用 gap 获取 cam 的开山之作\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1512.04150\n\n * https://github.com/zhoubolei/cam\n   \n   * https://github.com/zhoubolei/cam/blob/master/pytorch_cam.py\n\n * https://cloud.tencent.com/developer/article/1674200\n\n * ",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Convolutional Random Walk Networks for Semantic Image Segmentation",frontmatter:{title:"Convolutional Random Walk Networks for Semantic Image Segmentation",date:"2021-10-13T15:51:48.000Z",permalink:"/pages/25f8e7/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/15.Convolutional%20Random%20Walk%20Networks%20for%20Semantic%20Image%20Segmentation.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/15.Convolutional Random Walk Networks for Semantic Image Segmentation.md",key:"v-0d3a7931",path:"/pages/25f8e7/",headers:[{level:2,title:"Convolutional Random Walk Networks for Semantic Image Segmentation",slug:"convolutional-random-walk-networks-for-semantic-image-segmentation",normalizedTitle:"convolutional random walk networks for semantic image segmentation",charIndex:2}],headersStr:"Convolutional Random Walk Networks for Semantic Image Segmentation",content:"# Convolutional Random Walk Networks for Semantic Image Segmentation\n\n# 作者：Gedas Bertasius, Lorenzo Torresani, Stella X. Yu, Jianbo Shi\n\n# 单位：UPenn, Dartmounth, Berkeley\n\n# 发表：CVPR 2017\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1605.07681",normalizedContent:"# convolutional random walk networks for semantic image segmentation\n\n# 作者：gedas bertasius, lorenzo torresani, stella x. yu, jianbo shi\n\n# 单位：upenn, dartmounth, berkeley\n\n# 发表：cvpr 2017\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1605.07681",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Learning random-walk label propagation for weakly-supervised semantic segmentation",frontmatter:{title:"Learning random-walk label propagation for weakly-supervised semantic segmentation",date:"2021-10-13T15:52:34.000Z",permalink:"/pages/fc1a12/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/16.Learning%20random-walk%20label%20propagation%20for%20weakly-supervised%20semantic%20segmentation.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/16.Learning random-walk label propagation for weakly-supervised semantic segmentation.md",key:"v-3706fd7a",path:"/pages/fc1a12/",headers:[{level:2,title:"Learning random-walk label propagation for weakly-supervised semantic segmentation",slug:"learning-random-walk-label-propagation-for-weakly-supervised-semantic-segmentation",normalizedTitle:"learning random-walk label propagation for weakly-supervised semantic segmentation",charIndex:2}],headersStr:"Learning random-walk label propagation for weakly-supervised semantic segmentation",content:"# Learning random-walk label propagation for weakly-supervised semantic segmentation\n\n# 作者：Paul Vernaza, Manmohan Chandraker\n\n# 单位：NEC Lab\n\n# 发表：CVPR 2017\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1802.00470",normalizedContent:"# learning random-walk label propagation for weakly-supervised semantic segmentation\n\n# 作者：paul vernaza, manmohan chandraker\n\n# 单位：nec lab\n\n# 发表：cvpr 2017\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1802.00470",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Puzzle-CAM Improved localization via matching partial and full features",frontmatter:{title:"Puzzle-CAM Improved localization via matching partial and full features",date:"2021-04-19T22:38:56.000Z",permalink:"/pages/25dbf3/",categories:["论文阅读","弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/17.Puzzle-CAM%20Improved%20localization%20via%20matching%20partial%20and%20full%20features.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/17.Puzzle-CAM Improved localization via matching partial and full features.md",key:"v-2e6c02ba",path:"/pages/25dbf3/",headers:[{level:2,title:"Puzzle-CAM: Improved localization via matching partial and full features",slug:"puzzle-cam-improved-localization-via-matching-partial-and-full-features",normalizedTitle:"puzzle-cam: improved localization via matching partial and full features",charIndex:2}],headersStr:"Puzzle-CAM: Improved localization via matching partial and full features",content:"# Puzzle-CAM: Improved localization via matching partial and full features\n\n# 单位：KAIST\n\n# 作者：Sanghyun Jo, In-Jae Yu\n\n# 发表：ICIP 2021\n\n# 摘要\n\n弱监督语义分割（WSSS）出于缩小像素级监督和图像级监督性能差距的目的而提出。许多方法都基于类激活图（CAMs）来生成伪标签以训练分割网络。WSSS 的主要缺陷是使用图像分类器的 CAMs 会主要关注目标最具判别力的地方。为了解决这个问题，作者提出了 Puzzle-CAM，用于最小化图像部分和整图的差异。我们的方法由一个 puzzle 模块以及两个正则化机制来找到物体较为完整的部分。Puzzle-CAM 能够基于图像级别的监督信号激活物体的整个区域，而不需要额外的参数。在实验中，Puzzle-CAM 在PASCAL VOC 2012 数据及上达到了 sota 的结果。\n\n# 阅读\n\n# 论文的目的及结论\n\n该篇论文希望改进 CAM 算法，让网络能够激活物体完整的部分，而不仅仅是较易判别的部分。最终可以达到PASCAL VOC 2012数据集上 sota 的结果。\n\n# 论文的实验\n\n4.1 实施细节\n\nPASCAL VOC 2012 数据集被划分为 1464 张图像作为训练集，1449张图像作为验证集，1456张图像作为测试集，follow 了[4,5,6] 中的实验设置，从 [15] 中建立了一个额外的包含 10582 张图像的训练集。图像被随机 Resize 到 [320,640] 的尺寸，然后 crop 到 512×512512\\times512512×512 用作网络输出。对于所有的实验，设置 α=4\\alpha=4α=4 作为最大值，并通过半个 epoch 将 α\\alphaα 线性地升至最大值。在推理阶段，利用不带 puzzle 模块的分类。因此作者作者用多尺度以及水平翻转来生成分割的伪标签。使用 TITAN-RTX GPU 来训练数据集。\n\n4.2 消融实验\n\n\n\n\n\n作者对提出的两个正则化loss 做了消融实验，发现在最终生成的CAMs 上都有不同程度的提升。Baseline 的 mIoUmIoUmIoU 是 47.82%，单独加入 LreL_{re}Lre 之后的 mIoUmIoUmIoU 是 49.21%（提升了1.39%），单独加入Lp−clsL_{p-cls}Lp−cls 的mIoUmIoUmIoU 和Baseline 相当，同时加入 LreL_{re}Lre 和 Lp−clsL_{p-cls}Lp−cls 之后 mIoUmIoUmIoU 能够提升 3.71%。图一和图三的可视化结果也能够说明这一点。\n\n4.3 与 SOTA 的方法比较\n\n\n\n作者的实验设置是基于 Puzzle-CAM 来训练 AffinityNet ，采用Resnest 架构来提升网络的特征提取能力。为了进一步改进伪分割标签的精度，follow了 [4] 中的实验设置来训练 AffinityNet。最终的伪标签能够在PASCAL VOC 2012 的训练集上达到 74.67% 的 mIoUmIoUmIoU，表 2 展示了原始 CAMs 和 Puzzle-CAM 的性能。\n\n\n\n使用 ResNeSt-269 Backbone 的 DeepLabv3+ 算法以全监督的方式得到最终的分割结果。表 3 展示了和其他方法的比较，III 表示 image-level 的标签，SSS 表示额外的显著性模型。\n\n# 论文的方法\n\n\n\n3.1 Puzzle 模块\n\nPuzzle 模块由分割模块（Tiling module）和合并模块（merging module）组成，对于尺寸为 W×HW\\times HW×H 的图像 III 而言，分割模块会生成四张无重复的尺寸为 W2×H2\\frac{W}{2} \\times \\frac{H}{2}2W ×2H 的图像块 {I1,1,I1,2,I2,1,I2,2I^{1,1},I^{1,2},I^{2,1},I^{2,2}I1,1,I1,2,I2,1,I2,2} ，然后对每个图像块都生成 CAMs，最后合并模块将四张图像块再拼到一起\n\n3.2 Puzzle-CAM 模块的损失函数\n\n\n\n第一部分对针对多标签图像分诶任务而言会有一个soft margin loss ℓcls\\ell_{cls}ℓcls ，公式如上。原始的 CAMs 定义为 AsA^sAs ，通过 Puzzle 模块生成的 CAMs 定义为 AreA_{re}Are ，使用 GAP 层转换得到 Y^s\\hat{Y}^sY^s 和 Y^re\\hat{Y}^{re}Y^re ，由这两个可以得到两个损失项，称为 Lcls=ℓcls(Y^s,Y)L_{cls} = \\ell_{cls}(\\hat{Y}^s, Y)Lcls =ℓcls (Y^s,Y) 以及 Lp−cls=ℓcls(Y^re,Y)L_{p-cls} = \\ell_{cls}(\\hat{Y}^{re}, Y)Lp−cls =ℓcls (Y^re,Y) 。这两个分类的损失用于改善图像分类的性能，为了原始图像的 CAMs，作者添加了重构正则化，原始 CAM 的重构损失被定义为 Lre=∣∣As−Are∣∣L_{re} = ||A^s-A^{re}||Lre =∣∣As−Are∣∣ 。\n\n\n\n最终将这三项损失结合到一起我们可以得到最终的 Puzzle-CAM 的损失函数：L=Lcls+Lp−cls+αLreL = L_{cls} + L_{p-cls} + \\alpha L_{re}L=Lcls +Lp−cls +αLre\n\nα\\alphaα 是损失平衡权重，分类损失 LclsL_{cls}Lcls 和 Lp−clsL_{p-cls}Lp−cls 被用于估计目标的区域，重建损失 LreL_{re}Lre 被用于缩小像素级和图像级的监督信号的差距，\n\n# 总结\n\n# 论文的贡献\n\n论文主要是针对弱监督语义分割提出了 Puzzle 模块，将原始图像分块后再算一个CAMs，并与原始的 CAMs 做一个重建损失，三项损失联合优化分类网络，提升 CAMs 的精度。\n\n该论文着眼于改进半监督语义分割的 Image-level 样本的利用方式，作者使用共享 Backbone 和 Neck的双分支网络，分为 Strong 分支以及 Weak 分支，将强监督样本送入 Strong 分支，将弱监督样本送入 Weak 分支，可以较好的消除监督不一致的现象，有助于缓解强弱样本不平衡问题。训练完成后，弱分支就不再需要了，其在训练过程中起到正则化的作用，加强了 Backbone 以及 Neck 网络的泛化能力\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2101.11253\n\n * https://github.com/OFRIN/PuzzleCAM",normalizedContent:"# puzzle-cam: improved localization via matching partial and full features\n\n# 单位：kaist\n\n# 作者：sanghyun jo, in-jae yu\n\n# 发表：icip 2021\n\n# 摘要\n\n弱监督语义分割（wsss）出于缩小像素级监督和图像级监督性能差距的目的而提出。许多方法都基于类激活图（cams）来生成伪标签以训练分割网络。wsss 的主要缺陷是使用图像分类器的 cams 会主要关注目标最具判别力的地方。为了解决这个问题，作者提出了 puzzle-cam，用于最小化图像部分和整图的差异。我们的方法由一个 puzzle 模块以及两个正则化机制来找到物体较为完整的部分。puzzle-cam 能够基于图像级别的监督信号激活物体的整个区域，而不需要额外的参数。在实验中，puzzle-cam 在pascal voc 2012 数据及上达到了 sota 的结果。\n\n# 阅读\n\n# 论文的目的及结论\n\n该篇论文希望改进 cam 算法，让网络能够激活物体完整的部分，而不仅仅是较易判别的部分。最终可以达到pascal voc 2012数据集上 sota 的结果。\n\n# 论文的实验\n\n4.1 实施细节\n\npascal voc 2012 数据集被划分为 1464 张图像作为训练集，1449张图像作为验证集，1456张图像作为测试集，follow 了[4,5,6] 中的实验设置，从 [15] 中建立了一个额外的包含 10582 张图像的训练集。图像被随机 resize 到 [320,640] 的尺寸，然后 crop 到 512×512512\\times512512×512 用作网络输出。对于所有的实验，设置 α=4\\alpha=4α=4 作为最大值，并通过半个 epoch 将 α\\alphaα 线性地升至最大值。在推理阶段，利用不带 puzzle 模块的分类。因此作者作者用多尺度以及水平翻转来生成分割的伪标签。使用 titan-rtx gpu 来训练数据集。\n\n4.2 消融实验\n\n\n\n\n\n作者对提出的两个正则化loss 做了消融实验，发现在最终生成的cams 上都有不同程度的提升。baseline 的 mioumioumiou 是 47.82%，单独加入 lrel_{re}lre 之后的 mioumioumiou 是 49.21%（提升了1.39%），单独加入lp−clsl_{p-cls}lp−cls 的mioumioumiou 和baseline 相当，同时加入 lrel_{re}lre 和 lp−clsl_{p-cls}lp−cls 之后 mioumioumiou 能够提升 3.71%。图一和图三的可视化结果也能够说明这一点。\n\n4.3 与 sota 的方法比较\n\n\n\n作者的实验设置是基于 puzzle-cam 来训练 affinitynet ，采用resnest 架构来提升网络的特征提取能力。为了进一步改进伪分割标签的精度，follow了 [4] 中的实验设置来训练 affinitynet。最终的伪标签能够在pascal voc 2012 的训练集上达到 74.67% 的 mioumioumiou，表 2 展示了原始 cams 和 puzzle-cam 的性能。\n\n\n\n使用 resnest-269 backbone 的 deeplabv3+ 算法以全监督的方式得到最终的分割结果。表 3 展示了和其他方法的比较，iii 表示 image-level 的标签，sss 表示额外的显著性模型。\n\n# 论文的方法\n\n\n\n3.1 puzzle 模块\n\npuzzle 模块由分割模块（tiling module）和合并模块（merging module）组成，对于尺寸为 w×hw\\times hw×h 的图像 iii 而言，分割模块会生成四张无重复的尺寸为 w2×h2\\frac{w}{2} \\times \\frac{h}{2}2w ×2h 的图像块 {i1,1,i1,2,i2,1,i2,2i^{1,1},i^{1,2},i^{2,1},i^{2,2}i1,1,i1,2,i2,1,i2,2} ，然后对每个图像块都生成 cams，最后合并模块将四张图像块再拼到一起\n\n3.2 puzzle-cam 模块的损失函数\n\n\n\n第一部分对针对多标签图像分诶任务而言会有一个soft margin loss ℓcls\\ell_{cls}ℓcls ，公式如上。原始的 cams 定义为 asa^sas ，通过 puzzle 模块生成的 cams 定义为 area_{re}are ，使用 gap 层转换得到 y^s\\hat{y}^sy^s 和 y^re\\hat{y}^{re}y^re ，由这两个可以得到两个损失项，称为 lcls=ℓcls(y^s,y)l_{cls} = \\ell_{cls}(\\hat{y}^s, y)lcls =ℓcls (y^s,y) 以及 lp−cls=ℓcls(y^re,y)l_{p-cls} = \\ell_{cls}(\\hat{y}^{re}, y)lp−cls =ℓcls (y^re,y) 。这两个分类的损失用于改善图像分类的性能，为了原始图像的 cams，作者添加了重构正则化，原始 cam 的重构损失被定义为 lre=∣∣as−are∣∣l_{re} = ||a^s-a^{re}||lre =∣∣as−are∣∣ 。\n\n\n\n最终将这三项损失结合到一起我们可以得到最终的 puzzle-cam 的损失函数：l=lcls+lp−cls+αlrel = l_{cls} + l_{p-cls} + \\alpha l_{re}l=lcls +lp−cls +αlre\n\nα\\alphaα 是损失平衡权重，分类损失 lclsl_{cls}lcls 和 lp−clsl_{p-cls}lp−cls 被用于估计目标的区域，重建损失 lrel_{re}lre 被用于缩小像素级和图像级的监督信号的差距，\n\n# 总结\n\n# 论文的贡献\n\n论文主要是针对弱监督语义分割提出了 puzzle 模块，将原始图像分块后再算一个cams，并与原始的 cams 做一个重建损失，三项损失联合优化分类网络，提升 cams 的精度。\n\n该论文着眼于改进半监督语义分割的 image-level 样本的利用方式，作者使用共享 backbone 和 neck的双分支网络，分为 strong 分支以及 weak 分支，将强监督样本送入 strong 分支，将弱监督样本送入 weak 分支，可以较好的消除监督不一致的现象，有助于缓解强弱样本不平衡问题。训练完成后，弱分支就不再需要了，其在训练过程中起到正则化的作用，加强了 backbone 以及 neck 网络的泛化能力\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2101.11253\n\n * https://github.com/ofrin/puzzlecam",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"区域擦除 | Object Region Mining with Adversarial Erasing A Simple Classification to Semantic Segmentation Approach",frontmatter:{title:"区域擦除 | Object Region Mining with Adversarial Erasing A Simple Classification to Semantic Segmentation Approach",date:"2021-11-01T15:39:42.000Z",permalink:"/pages/9bc70f/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/19.%E5%8C%BA%E5%9F%9F%E6%93%A6%E9%99%A4%20%7C%20Object%20Region%20Mining%20with%20Adversarial%20Erasing%20A%20Simple%20Classification%20to%20Semantic%20Segmentation%20Approach.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/19.区域擦除 | Object Region Mining with Adversarial Erasing A Simple Classification to Semantic Segmentation Approach.md",key:"v-b5ba63b2",path:"/pages/9bc70f/",headers:[{level:2,title:"Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach",slug:"object-region-mining-with-adversarial-erasing-a-simple-classification-to-semantic-segmentation-approach",normalizedTitle:"object region mining with adversarial erasing: a simple classification to semantic segmentation approach",charIndex:2}],headersStr:"Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach",content:"# Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach\n\n# 作者：Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan\n\n# 单位：NUS, CMU, NKU, BJTU, 360\n\n# 发表：CVPR 2017 Oral\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1703.08448",normalizedContent:"# object region mining with adversarial erasing: a simple classification to semantic segmentation approach\n\n# 作者：yunchao wei, jiashi feng, xiaodan liang, ming-ming cheng, yao zhao, shuicheng yan\n\n# 单位：nus, cmu, nku, bjtu, 360\n\n# 发表：cvpr 2017 oral\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1703.08448",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"CAM 扩散 | Tell Me Where to Look Guided Attention Inference Network",frontmatter:{title:"CAM 扩散 | Tell Me Where to Look Guided Attention Inference Network",date:"2021-11-01T15:45:04.000Z",permalink:"/pages/c691d0/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/20.CAM%20%E6%89%A9%E6%95%A3%20%7C%20Tell%20Me%20Where%20to%20Look%20Guided%20Attention%20Inference%20Network.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/20.CAM 扩散 | Tell Me Where to Look Guided Attention Inference Network.md",key:"v-a46f98a2",path:"/pages/c691d0/",headers:[{level:2,title:"Tell Me Where to Look: Guided Attention Inference Network",slug:"tell-me-where-to-look-guided-attention-inference-network",normalizedTitle:"tell me where to look: guided attention inference network",charIndex:2}],headersStr:"Tell Me Where to Look: Guided Attention Inference Network",content:"# Tell Me Where to Look: Guided Attention Inference Network\n\n# 作者：Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, Yun Fu\n\n# 单位：Northeastern University, Boston, MA\n\n# 发表：CVPR 2018\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1802.10171",normalizedContent:"# tell me where to look: guided attention inference network\n\n# 作者：kunpeng li, ziyan wu, kuan-chuan peng, jan ernst, yun fu\n\n# 单位：northeastern university, boston, ma\n\n# 发表：cvpr 2018\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1802.10171",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Self-Erasing Network for Integral Object Attention",frontmatter:{title:"Self-Erasing Network for Integral Object Attention",date:"2021-11-01T15:48:13.000Z",permalink:"/pages/742623/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/21.Self-Erasing%20Network%20for%20Integral%20Object%20Attention.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/21.Self-Erasing Network for Integral Object Attention.md",key:"v-6f1a4286",path:"/pages/742623/",headers:[{level:2,title:"Self-Erasing Network for Integral Object Attention",slug:"self-erasing-network-for-integral-object-attention",normalizedTitle:"self-erasing network for integral object attention",charIndex:2}],headersStr:"Self-Erasing Network for Integral Object Attention",content:"# Self-Erasing Network for Integral Object Attention\n\n# 作者：Qibin Hou, Peng-Tao Jiang, Yunchao Wei, Ming-Ming Cheng\n\n# 单位：NKU, UIUC\n\n# 发表：NeuIPS 2018\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1810.09821\n * http://mmcheng.net/SeeNet/",normalizedContent:"# self-erasing network for integral object attention\n\n# 作者：qibin hou, peng-tao jiang, yunchao wei, ming-ming cheng\n\n# 单位：nku, uiuc\n\n# 发表：neuips 2018\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1810.09821\n * http://mmcheng.net/seenet/",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Transformer CAM|Transformer Interpretability Beyond Attention Visualization",frontmatter:{title:"Transformer CAM|Transformer Interpretability Beyond Attention Visualization",date:"2021-11-02T13:53:52.000Z",permalink:"/pages/e9bd5f/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/22.Transformer%20CAM%7CTransformer%20Interpretability%20Beyond%20Attention%20Visualization.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/22.Transformer CAM|Transformer Interpretability Beyond Attention Visualization.md",key:"v-9dd185bc",path:"/pages/e9bd5f/",headers:[{level:2,title:"Transformer Interpretability Beyond Attention Visualization",slug:"transformer-interpretability-beyond-attention-visualization",normalizedTitle:"transformer interpretability beyond attention visualization",charIndex:2}],headersStr:"Transformer Interpretability Beyond Attention Visualization",content:"# Transformer Interpretability Beyond Attention Visualization\n\n# 作者：Hila Chefer, Shir Gur, Lior Wolf\n\n# 单位：TAU, FAIR\n\n# 发表：NeuIPS 2018\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2012.09838\n * https://github.com/hila-chefer/Transformer-Explainability",normalizedContent:"# transformer interpretability beyond attention visualization\n\n# 作者：hila chefer, shir gur, lior wolf\n\n# 单位：tau, fair\n\n# 发表：neuips 2018\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2012.09838\n * https://github.com/hila-chefer/transformer-explainability",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"GETAM Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation",frontmatter:{title:"GETAM Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation",date:"2022-01-04T15:36:51.000Z",permalink:"/pages/54a25d/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/23.GETAM%20Gradient-weighted%20Element-wise%20Transformer%20Attention%20Map%20for%20Weakly-supervised%20Semantic%20segmentation.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/23.GETAM Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation.md",key:"v-e3307746",path:"/pages/54a25d/",headers:[{level:2,title:"GETAM: Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation",slug:"getam-gradient-weighted-element-wise-transformer-attention-map-for-weakly-supervised-semantic-segmentation",normalizedTitle:"getam: gradient-weighted element-wise transformer attention map for weakly-supervised semantic segmentation",charIndex:2}],headersStr:"GETAM: Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation",content:"# GETAM: Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation\n\n# 作者：Weixuan Sun, Jing Zhang, Zheyuan Liu, Yiran Zhong, Nick Barnes\n\n# 单位：Australian National University, SenseTime\n\n# 发表：arXiv\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2112.02841\n * ",normalizedContent:"# getam: gradient-weighted element-wise transformer attention map for weakly-supervised semantic segmentation\n\n# 作者：weixuan sun, jing zhang, zheyuan liu, yiran zhong, nick barnes\n\n# 单位：australian national university, sensetime\n\n# 发表：arxiv\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2112.02841\n * ",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation",frontmatter:{title:"Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation",date:"2022-04-14T16:54:17.000Z",permalink:"/pages/fe1b38/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/24.Class%20Re-Activation%20Maps%20for%20Weakly-Supervised%20Semantic%20Segmentation.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/24.Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation.md",key:"v-40979294",path:"/pages/fe1b38/",headers:[{level:2,title:"Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation",slug:"class-re-activation-maps-for-weakly-supervised-semantic-segmentation",normalizedTitle:"class re-activation maps for weakly-supervised semantic segmentation",charIndex:2}],headersStr:"Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation",content:"# Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation\n\n# 作者：Zhaozheng Chen, Tan Wang, Xiongwei Wu, Xian-Sheng Hua, Hanwang Zhang, Qianru Sun\n\n# 单位：SMU、NTU、DAMO\n\n# 发表：CVPR 2022\n\n# 摘要\n\nBCE 是生成伪标签的关键，CAM 的一个像素可能会对应于原图的一个区域，所以很容易造成类别的误判。\n\n给定一张图像，使用 CAM 来提取特每个单独类别的特征像素，使用 SCE (Softmax Cross-Entropy) 去学习另一个全连接层。\n\n由于 SCE 的对比性质，像素相应被分解为不同的类别，因此预期的 mask 会更好。实验表明 ReCAM 不仅仅可以生成高质量的mask，还可以作为一种即插即用的组件到 CAM 的变体方法中去\n\n# 阅读\n\n# 论文的目的及结论\n\n作者观察到有两个常见的缺陷：\n\n * 被激活为 A 类的 False Positive 像素，其通常实际标签是类别 B ，而不是背景\n * 属于 A 类的 False Negative 的像素被错误地标记为背景\n\n关键现象\n\n这些现象当使用 BCE loss 的时候尤为明显，BCE loss 并不会惩罚分错的某一个类，\n\n# 论文的实验\n\nBCE 和 SCE 分类性能相似，但是 CAM 的质量不一致\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * 论文：https://arxiv.org/abs/2203.00962\n * 代码：https://github.com/zhaozhengChen/ReCAM",normalizedContent:"# class re-activation maps for weakly-supervised semantic segmentation\n\n# 作者：zhaozheng chen, tan wang, xiongwei wu, xian-sheng hua, hanwang zhang, qianru sun\n\n# 单位：smu、ntu、damo\n\n# 发表：cvpr 2022\n\n# 摘要\n\nbce 是生成伪标签的关键，cam 的一个像素可能会对应于原图的一个区域，所以很容易造成类别的误判。\n\n给定一张图像，使用 cam 来提取特每个单独类别的特征像素，使用 sce (softmax cross-entropy) 去学习另一个全连接层。\n\n由于 sce 的对比性质，像素相应被分解为不同的类别，因此预期的 mask 会更好。实验表明 recam 不仅仅可以生成高质量的mask，还可以作为一种即插即用的组件到 cam 的变体方法中去\n\n# 阅读\n\n# 论文的目的及结论\n\n作者观察到有两个常见的缺陷：\n\n * 被激活为 a 类的 false positive 像素，其通常实际标签是类别 b ，而不是背景\n * 属于 a 类的 false negative 的像素被错误地标记为背景\n\n关键现象\n\n这些现象当使用 bce loss 的时候尤为明显，bce loss 并不会惩罚分错的某一个类，\n\n# 论文的实验\n\nbce 和 sce 分类性能相似，但是 cam 的质量不一致\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * 论文：https://arxiv.org/abs/2203.00962\n * 代码：https://github.com/zhaozhengchen/recam",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"Semi-Supervised Learning by exploiting unlabeled data correlations in a dual-branch network",frontmatter:{title:"Semi-Supervised Learning by exploiting unlabeled data correlations in a dual-branch network",date:"2021-04-19T21:35:12.000Z",permalink:"/pages/8109a2/",categories:["论文阅读","半监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/08.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/02.Semi-Supervised%20Learning%20by%20exploiting%20unlabeled%20data%20correlations%20in%20a%20dual-branch%20network.html",relativePath:"01.学术搬砖/08.论文阅读-半监督图像分割/02.Semi-Supervised Learning by exploiting unlabeled data correlations in a dual-branch network.md",key:"v-456b02c8",path:"/pages/8109a2/",headers:[{level:2,title:"Semi-Supervised Learning by exploiting unlabeled data correlations in a dual-branch network",slug:"semi-supervised-learning-by-exploiting-unlabeled-data-correlations-in-a-dual-branch-network",normalizedTitle:"semi-supervised learning by exploiting unlabeled data correlations in a dual-branch network",charIndex:2}],headersStr:"Semi-Supervised Learning by exploiting unlabeled data correlations in a dual-branch network",content:"# Semi-Supervised Learning by exploiting unlabeled data correlations in a dual-branch network\n\n# 作者：Jie Ling, Meng Yang（ETH Zurich博士后）\n\n# 单位：中山大学\n\n# 发表：ICME 2021（论文还未放出）\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料",normalizedContent:"# semi-supervised learning by exploiting unlabeled data correlations in a dual-branch network\n\n# 作者：jie ling, meng yang（eth zurich博士后）\n\n# 单位：中山大学\n\n# 发表：icme 2021（论文还未放出）\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Semi-supervised semantic segmentation needs strong, varied perturbations",frontmatter:{title:"Semi-supervised semantic segmentation needs strong, varied perturbations",date:"2021-04-20T10:22:40.000Z",permalink:"/pages/c40a2c/",categories:["论文阅读","半监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/08.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/04.Semi-supervised%20semantic%20segmentation%20needs%20strong,%20varied%20perturbations.html",relativePath:"01.学术搬砖/08.论文阅读-半监督图像分割/04.Semi-supervised semantic segmentation needs strong, varied perturbations.md",key:"v-502d1918",path:"/pages/c40a2c/",headers:[{level:2,title:"Semi-supervised semantic segmentation needs strong, varied perturbations",slug:"semi-supervised-semantic-segmentation-needs-strong-varied-perturbations",normalizedTitle:"semi-supervised semantic segmentation needs strong, varied perturbations",charIndex:2}],headersStr:"Semi-supervised semantic segmentation needs strong, varied perturbations",content:"# Semi-supervised semantic segmentation needs strong, varied perturbations\n\n# 作者：UEA & NVIDIA\n\n# 发表：BMVC 2020\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1906.01916\n\n * https://github.com/Britefury/cutmix-semisup-seg",normalizedContent:"# semi-supervised semantic segmentation needs strong, varied perturbations\n\n# 作者：uea & nvidia\n\n# 发表：bmvc 2020\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1906.01916\n\n * https://github.com/britefury/cutmix-semisup-seg",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"DMT Dynamic Mutual Training for Semi-Supervised Learning",frontmatter:{title:"DMT Dynamic Mutual Training for Semi-Supervised Learning",date:"2021-05-07T16:46:58.000Z",permalink:"/pages/26f3ac/",categories:["论文阅读","半监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/08.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/03.DMT%20Dynamic%20Mutual%20Training%20for%20Semi-Supervised%20Learning.html",relativePath:"01.学术搬砖/08.论文阅读-半监督图像分割/03.DMT Dynamic Mutual Training for Semi-Supervised Learning.md",key:"v-343bb2dc",path:"/pages/26f3ac/",headersStr:null,content:"# DMT: Dynamic Mutual Training for Semi-Supervised Learning\n\n# 作者：商汤 Jianping Shi、上交 DMCV 实验室\n\n# 发表：arXiv 准备投 PR\n\n# 摘要\n\n近期的半监督学习方法将利用伪标签作为核心思想，但是伪标签并不可靠。自训练的方法依赖于单模型预测的置信度来过滤掉低置信度的伪标签，但是存在以下问题：有概率保留高置信度的带噪样本以及丢弃掉低置信度的正确样本。在这篇论文中，我们指出模型很难发现自己的错误，相反利用不同模型间的差异是定位伪标签错误的关键。从这个新角度出发，我们提出了在两个不同的模型中互训练，并利用一个动态重加权的损失函数，称作动态互训练（DMT），通过比较两种不同模型的预测以动态分配训练中的权重，来量化模型间的分歧，较大的分歧表示较高概率的错误，并对应较低的损失值，实验证明 DMT 能够在图像分类和图像分割上都能达到 SOTA 的结果。\n\n# 阅读\n\n# 论文的目的及结论\n\n想通过比较两种不同模型的预测来动态分配训练中样本的权重，最终能够在图像分类和图像分割上都能达到 SOTA 的结果。\n\n# 论文的实验\n\n对于图像分类任务，在 CIFAR-10 上做了实验。对于语义分割任务，在 PASCAL VOC 2012 和 Cityscapes 数据及上做了实验。由于 DMT 在迭代框架中能够有更好的性能以及快速的收敛，所有的 DMT 实验都默认 5 次迭代，在单张 RTX2080 Ti GPU 上做实验，全监督的学习结果被称作 Oracle，作为半监督学习结果的上限。然而由于全监督的标注中也有噪声，我们的 DMT 方法可以超过全监督方法得到的性能。\n\n# 1、超参数调整\n\n\n\n为了避免太多的超参数调整，作者设置 γ1=γ2\\gamma_1=\\gamma_2γ1 =γ2 ，我们有确定的比例（例如 labeled：unlabeled = 1：7），更多的细节在上述的表一中都列出来\n\n# 图像分类\n\n**训练阶段：**网络架构使用 MixMatch 中的设置 WideResNet-28-2 作为 backbone。每个 DMT 迭代是 750 epochs，学习率为 0.1，权重衰减为 5×10−45 \\times 10^{-4}5×10−4 ，动量设置为 0.9，余弦学习率调整器以及 512 的batch size，与 [26] 的课程学习实验设置相同，为了公平比较，作者并未使用 [32] 中提出的 SWA 技巧。数据增强是 带有 Cutout 的 RandAugment[33]，在每一个 step 中随机选择一种随机强度的增强操作以避免超参数的调整，并且使用 Mixup来应用动态权重\n\n测试阶段：五次测试的平均，指数移动平均网络（follow 了 MixMatch[11]）\n\n# 语义分割\n\n**训练阶段：**follow 了 [8, 12] 的方法使用 DeepLab-v2 ResNet-101 作为 backbone，没有多尺度融合以及 CRF 的后处理，作者的方法性能能够显著地超过以前方法的性能。由于使用了fine-tuning，分割任务的每个 DMT 迭代需要更少的训练 step，使用 SGD 优化器 （Momentum 为 0.9，lr schedule 为 poly, batch size 为 8），数据增强包括随机缩放，随机裁剪以及随机翻转，训练的尺度为 321×321321\\times321321×321 (PASCAL VOC 2012) 以及 256×512256\\times512256×512 (Cityscapes)\n\n测试阶段： 三次测试的平均\n\n# 2、性能的比较\n\n# 图像分类（CIFAR-10 数据集）\n\n\n\n和 Mean Teacher(MT) [9]，Curriculum Labeling(CL)[26]，Deep Co-Training (DCT)[15]，Dual Student(DS)[35]，MixMatch[11]，DAG[36] 方法进行比较，在 1k 和 4k 的标签划分下都进行了比较。带有 mixup 以及 其他数据增强的全监督性能作为Baseline，Baseline，CL，DMT 使用同一套 codebase 来实现，其他的方法都从原始论文中获取\n\n# 语义分割（PASCAL VOC 2012 以及 Cityscapes）\n\nPASCAL VOC 2012\n\n比较方法有：基于一致性的 MT-Seg[9]，Mean Teacher 使用了 CutMix 数据增强；基于特征层面一致性的 CCT 算法[37]；将孪生的学生用于语义分割的辅助缺陷检测器（GCT）[38]；基于 GAN 的方法[8]，与训练了一个分类器用来选择伪标签；以及混合方法 s4GAN + MLMT [12] 使用额外的分类分支为 [8] 添加了一致性的正则化约束。\n\n实验设置：将数据集做四个划分：1/106（100个标签）、1/50、1/20、1/8。我们不使用超过 1/8 的数据量对sota 的方法的性能超越不多。在监督子集上的有监督算法的性能称作 Baseline . MT-Seg、CCT、GCT的性能是在GCT 的 (codebase)[https://github.com/ZHKKKe/PixelSSL/tree/master/task/sseg] 上重新评估过。其他的性能使用原始论文中得到的。所有的方法都是使用与我们相同的网络架构进行评估的，CCT使用了略优秀的架构 PSPNet-ResNet-101 [39] 进行评估。\n\n\n\n在表 4 中，DMT 超过了现有的很多方法，然而由于有些方法有 GAN 网络以及额外的网络分支，其结果和Baseline 有所不同。因此作者展示了和 Oracal 之间的算法性能差异，我们的方法性能最优异，然后能够在不同的数据集上都有稳定的性能。\n\n与 human supervision 相比较，原始的PASCAL VOC 2012 有1464张训练图像，称作train set ，还有广泛使用的 10582 张训练集称作 trainaug (SBD)[30]。SBD数据集使用相同的图像集合，但其利用亚马逊的AMT提供了更多的标注信息。然而不专业的标注人员会造成带噪的边界，所以 trainaug 数据集会存在的很多 Coarse 的边界，存在更差的标签质量。所以我们使用 train 数据集作为已标注的子集，使用SBD 数据集中的 9118 张图像作为未标注的子集，然后利用 DMT 在其上做实验。\n\n\n\n在图五中展示的那样，DMT算法可以超过 Oracal 的性能，这个结果展示了 DMT 能够产生更好的标签质量，不过由于DMT 需要用到两个模型，所以其速度大约是一个模型训练代价的两倍。\n\nCityscapes\n\nCityscapes 具有复杂的街道场景，半监督的方法较少涉足。参考 PASCAL VOC 2012 数据集的设置，来评估每个方在 1/30（100 labels）、1/8 的splits，所有实验的性能数据都从原始论文中获得\n\n\n\n# 3、消融实验\n\n对五个方法做了消融实验：\n\n * Online ST：以固定的置信度阈值0.9 执行 online self-training 20个epochs\n * CBST：[13] 迭代式的类平衡的自训练算法，类似于CL [26] 算法的类平衡版本\n * DST：类似于DMT，单其只用一个模型来提供伪标签\n * DMT-Naive：直接对 loss 进行加权，而未对三类进行区分（未使用dynamic loss）\n * DMT-Flip：因为伪标签可能不正确，而不是直接把损失设置为0，我们将伪标签翻转为当前模型的预测，平切将损失重加权为 (1−cA)γ2(1-c_A)^{\\gamma_2}(1−cA )γ2 ，鉴于伪标签被翻转，作为对模型间分歧的估计\n\n\n\n * Online ST 在标签极度稀少的情况下，它的性能甚至比 Baseline 更差，\n * CBST 只进行自训练，但没有考虑伪标签的噪声，所以其在iteration = 3的时候性能增长就停止了，伪标签噪声组织了 CBST 的性能进一步提升\n * DST 缺乏动态的加权，其性能会较弱\n * DMT-Naive 较为简单，其可以整合模型间的分歧，当标签噪声严重时（1/50），其性能会大大的降低\n * DMT-Flip 比 DMT 更加复杂，但其性能与 DMT 类似。我们认为通过翻转标签，有些类似于在线自训练的过程，\n\n# 论文的方法\n\n# 4.1 Dynamic Mutual Training（动态互学习）\n\n\n\n我们提出了动态互学习方法，来量化模型的歧义并且确保噪声鲁棒的训练，上图展示了整体框架。首先，在已标注的子集上训练两个不同的模型 FAF_AFA 以及 FBF_BFB ，利用两种不同的初始化或者样本。然后一个模型，例如 FAF_AFA 固定并且产生未标注子集上的伪标签以及置信度，对于另一个模型 FBF_BFB 来讲，在所有的数据（使用已标注数据集以及未标注数据的伪标签）上使用动态加权的交叉熵进行 finetune，FBF_BFB 可以以同一种方式训练 FAF_AFA\n\n4.1.1 动态损失\n\n\n\n以图像分类问题为例，用XXX 以及 UUU 代表已标注以及未标注样本，定义 FAF_AFA 生成的伪标签为 yAy_AyA ，带有置信度为 cAc_AcA ，定义 FBF_BFB 生成的伪标签为 yBy_ByB ，带有置信度为 cBc_BcB 。以上是生成权重以及计算损失的公式，\n\n * 当 yAy_AyA 等于 yBy_ByB 时，权重为PBγ1P_B^{\\gamma_1}PBγ1 ，满足该条件的样本称作 agreement\n * 当 yAy_AyA 不等于 yBy_ByB 且cA≥cBc_A \\geq c_BcA ≥cB 时，权重为PBγ2P_B^{\\gamma_2}PBγ2 ，满足该条件的样本被称作Negetive disagreement，即是 FBF_BFB 与 FAF_AFA 的预测有分歧，但是 FBF_BFB 的置信度会高一些\n * 当 yAy_AyA 不等于 yBy_ByB 且cA<cBc_A < c_BcA <cB 时，权重为 0，满足该条件的样本被称作Positive disagreement，即是 FBF_BFB 与 FAF_AFA 的预测有分歧，但是 FAF_AFA 的置信度会高一些\n\n在 case1 和 case2情况下，使用当前模型预测的概率 PBP_BPB 作为权重，来量化模型的分歧。例如，一个高的概率意味着 FBF_BFB 和 FAF_AFA 拥有更强的共识，在 case3 的情况下，我们将动态权重设置为 0 因为其伪标签可能完全不正确。\n\n动态权重有两个超参数 γ1\\gamma_1γ1 以及 γ2\\gamma_2γ2 ，更高的 γ\\gammaγ 值会放大置信度的影响，更高的 γ1\\gamma_1γ1 值强调了熵的最小化，更高的 γ2\\gamma_2γ2 值强调了更多的互学习。高 γ\\gammaγ 值通常有利于高噪声的场景中，或者是为了保持更大的模型间分歧。对于语义分割问题来讲，wuH×Ww_u^{H\\times W}wuH×W 代表了一个像素级别的权重图，加权策略保持一直并且应用到每个像素上。\n\n作者解释了为什么要分情况来设置权重，其实直接使用 PBP_BPB 作为权重也是可以的，但是作者观察到，再严重的伪标签噪声下，这种过于简单的方法效果并不好，另外动态损失的其他设计也为带来明显的性能改善（可以看实验部分的消融实验）\n\n4.1.2 初始化分歧\n\nCIFAR-10 这种简单任务上，直接使用不同的随机初始化参数即可引入模型分歧。然而，对于需要使用预训练模型的模型（例如语义分割任务）来讲，会通过差异最大化采样来引入模型分歧\n\n# 4.2 迭代框架\n\n受到课程学习、无监督域适应语义分割等方法的启发，DMT 是一个迭代式的框架，来获得更好的性能\n\n4.2.1 图像分类\n\n\n\n我们首先在已标注子集上训练，每次从未标注子集上选择最高置信度的一批伪标签，然后重新训练一个随机初始化的模型，与 Curriculum Labeling[26] 论文类似。上图是该算法的伪代码，在模型从零重新训练的早期阶段会提供一些有用的信息，因此我们受到[9]的启发使用了一个类似 sigmoid 函数，总的来说，总的训练step是 tmaxt_{max}tmax ，在第 ttt 个 step，γ=γmaxe5(1−ttmax)\\gamma=\\gamma_{max}e^{5(1-\\frac{t}{t_{max}})}γ=γmax e5(1−tmax t )\n\n4.2.2 语义分割\n\n\n\n在语义分割任务中，有一些类别是比另外一些类别容易学的，CBST[13] 提出了一个迭代式自训练框架，在每一轮使用每个类别 最高置信度的伪标签。并且与图像分类任务不同的是，图像分割依赖预训练权重来加速拟合，受到CBST的启发，使用两个模型。\n\n\n\n在这种设置中，两个模型互相平等地进行训练，模型间的分歧被更直接地利用，上图是DMT在PASCAL VOC 2012数据集上的动态权重图。CBST没有靠排序来选择最可靠的伪标签，其按照类别定义阈值，将置信度超过1的像素被选中训练。大多数场景中，和直接排序相似，但是在极端情况下，预测的类别可能会改变。\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\nDMT 提出了一种方法，使用两个模型对伪标签生成动态权重。对样本划分了三种情况给予不同的权重，以迭代式的方法将伪标签 Refine 到越来越好，并且生成像素级的权重促进模型学习。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2004.08514\n\n * https://github.com/voldemortX/DST-CBC",normalizedContent:"# dmt: dynamic mutual training for semi-supervised learning\n\n# 作者：商汤 jianping shi、上交 dmcv 实验室\n\n# 发表：arxiv 准备投 pr\n\n# 摘要\n\n近期的半监督学习方法将利用伪标签作为核心思想，但是伪标签并不可靠。自训练的方法依赖于单模型预测的置信度来过滤掉低置信度的伪标签，但是存在以下问题：有概率保留高置信度的带噪样本以及丢弃掉低置信度的正确样本。在这篇论文中，我们指出模型很难发现自己的错误，相反利用不同模型间的差异是定位伪标签错误的关键。从这个新角度出发，我们提出了在两个不同的模型中互训练，并利用一个动态重加权的损失函数，称作动态互训练（dmt），通过比较两种不同模型的预测以动态分配训练中的权重，来量化模型间的分歧，较大的分歧表示较高概率的错误，并对应较低的损失值，实验证明 dmt 能够在图像分类和图像分割上都能达到 sota 的结果。\n\n# 阅读\n\n# 论文的目的及结论\n\n想通过比较两种不同模型的预测来动态分配训练中样本的权重，最终能够在图像分类和图像分割上都能达到 sota 的结果。\n\n# 论文的实验\n\n对于图像分类任务，在 cifar-10 上做了实验。对于语义分割任务，在 pascal voc 2012 和 cityscapes 数据及上做了实验。由于 dmt 在迭代框架中能够有更好的性能以及快速的收敛，所有的 dmt 实验都默认 5 次迭代，在单张 rtx2080 ti gpu 上做实验，全监督的学习结果被称作 oracle，作为半监督学习结果的上限。然而由于全监督的标注中也有噪声，我们的 dmt 方法可以超过全监督方法得到的性能。\n\n# 1、超参数调整\n\n\n\n为了避免太多的超参数调整，作者设置 γ1=γ2\\gamma_1=\\gamma_2γ1 =γ2 ，我们有确定的比例（例如 labeled：unlabeled = 1：7），更多的细节在上述的表一中都列出来\n\n# 图像分类\n\n**训练阶段：**网络架构使用 mixmatch 中的设置 wideresnet-28-2 作为 backbone。每个 dmt 迭代是 750 epochs，学习率为 0.1，权重衰减为 5×10−45 \\times 10^{-4}5×10−4 ，动量设置为 0.9，余弦学习率调整器以及 512 的batch size，与 [26] 的课程学习实验设置相同，为了公平比较，作者并未使用 [32] 中提出的 swa 技巧。数据增强是 带有 cutout 的 randaugment[33]，在每一个 step 中随机选择一种随机强度的增强操作以避免超参数的调整，并且使用 mixup来应用动态权重\n\n测试阶段：五次测试的平均，指数移动平均网络（follow 了 mixmatch[11]）\n\n# 语义分割\n\n**训练阶段：**follow 了 [8, 12] 的方法使用 deeplab-v2 resnet-101 作为 backbone，没有多尺度融合以及 crf 的后处理，作者的方法性能能够显著地超过以前方法的性能。由于使用了fine-tuning，分割任务的每个 dmt 迭代需要更少的训练 step，使用 sgd 优化器 （momentum 为 0.9，lr schedule 为 poly, batch size 为 8），数据增强包括随机缩放，随机裁剪以及随机翻转，训练的尺度为 321×321321\\times321321×321 (pascal voc 2012) 以及 256×512256\\times512256×512 (cityscapes)\n\n测试阶段： 三次测试的平均\n\n# 2、性能的比较\n\n# 图像分类（cifar-10 数据集）\n\n\n\n和 mean teacher(mt) [9]，curriculum labeling(cl)[26]，deep co-training (dct)[15]，dual student(ds)[35]，mixmatch[11]，dag[36] 方法进行比较，在 1k 和 4k 的标签划分下都进行了比较。带有 mixup 以及 其他数据增强的全监督性能作为baseline，baseline，cl，dmt 使用同一套 codebase 来实现，其他的方法都从原始论文中获取\n\n# 语义分割（pascal voc 2012 以及 cityscapes）\n\npascal voc 2012\n\n比较方法有：基于一致性的 mt-seg[9]，mean teacher 使用了 cutmix 数据增强；基于特征层面一致性的 cct 算法[37]；将孪生的学生用于语义分割的辅助缺陷检测器（gct）[38]；基于 gan 的方法[8]，与训练了一个分类器用来选择伪标签；以及混合方法 s4gan + mlmt [12] 使用额外的分类分支为 [8] 添加了一致性的正则化约束。\n\n实验设置：将数据集做四个划分：1/106（100个标签）、1/50、1/20、1/8。我们不使用超过 1/8 的数据量对sota 的方法的性能超越不多。在监督子集上的有监督算法的性能称作 baseline . mt-seg、cct、gct的性能是在gct 的 (codebase)[https://github.com/zhkkke/pixelssl/tree/master/task/sseg] 上重新评估过。其他的性能使用原始论文中得到的。所有的方法都是使用与我们相同的网络架构进行评估的，cct使用了略优秀的架构 pspnet-resnet-101 [39] 进行评估。\n\n\n\n在表 4 中，dmt 超过了现有的很多方法，然而由于有些方法有 gan 网络以及额外的网络分支，其结果和baseline 有所不同。因此作者展示了和 oracal 之间的算法性能差异，我们的方法性能最优异，然后能够在不同的数据集上都有稳定的性能。\n\n与 human supervision 相比较，原始的pascal voc 2012 有1464张训练图像，称作train set ，还有广泛使用的 10582 张训练集称作 trainaug (sbd)[30]。sbd数据集使用相同的图像集合，但其利用亚马逊的amt提供了更多的标注信息。然而不专业的标注人员会造成带噪的边界，所以 trainaug 数据集会存在的很多 coarse 的边界，存在更差的标签质量。所以我们使用 train 数据集作为已标注的子集，使用sbd 数据集中的 9118 张图像作为未标注的子集，然后利用 dmt 在其上做实验。\n\n\n\n在图五中展示的那样，dmt算法可以超过 oracal 的性能，这个结果展示了 dmt 能够产生更好的标签质量，不过由于dmt 需要用到两个模型，所以其速度大约是一个模型训练代价的两倍。\n\ncityscapes\n\ncityscapes 具有复杂的街道场景，半监督的方法较少涉足。参考 pascal voc 2012 数据集的设置，来评估每个方在 1/30（100 labels）、1/8 的splits，所有实验的性能数据都从原始论文中获得\n\n\n\n# 3、消融实验\n\n对五个方法做了消融实验：\n\n * online st：以固定的置信度阈值0.9 执行 online self-training 20个epochs\n * cbst：[13] 迭代式的类平衡的自训练算法，类似于cl [26] 算法的类平衡版本\n * dst：类似于dmt，单其只用一个模型来提供伪标签\n * dmt-naive：直接对 loss 进行加权，而未对三类进行区分（未使用dynamic loss）\n * dmt-flip：因为伪标签可能不正确，而不是直接把损失设置为0，我们将伪标签翻转为当前模型的预测，平切将损失重加权为 (1−ca)γ2(1-c_a)^{\\gamma_2}(1−ca )γ2 ，鉴于伪标签被翻转，作为对模型间分歧的估计\n\n\n\n * online st 在标签极度稀少的情况下，它的性能甚至比 baseline 更差，\n * cbst 只进行自训练，但没有考虑伪标签的噪声，所以其在iteration = 3的时候性能增长就停止了，伪标签噪声组织了 cbst 的性能进一步提升\n * dst 缺乏动态的加权，其性能会较弱\n * dmt-naive 较为简单，其可以整合模型间的分歧，当标签噪声严重时（1/50），其性能会大大的降低\n * dmt-flip 比 dmt 更加复杂，但其性能与 dmt 类似。我们认为通过翻转标签，有些类似于在线自训练的过程，\n\n# 论文的方法\n\n# 4.1 dynamic mutual training（动态互学习）\n\n\n\n我们提出了动态互学习方法，来量化模型的歧义并且确保噪声鲁棒的训练，上图展示了整体框架。首先，在已标注的子集上训练两个不同的模型 faf_afa 以及 fbf_bfb ，利用两种不同的初始化或者样本。然后一个模型，例如 faf_afa 固定并且产生未标注子集上的伪标签以及置信度，对于另一个模型 fbf_bfb 来讲，在所有的数据（使用已标注数据集以及未标注数据的伪标签）上使用动态加权的交叉熵进行 finetune，fbf_bfb 可以以同一种方式训练 faf_afa\n\n4.1.1 动态损失\n\n\n\n以图像分类问题为例，用xxx 以及 uuu 代表已标注以及未标注样本，定义 faf_afa 生成的伪标签为 yay_aya ，带有置信度为 cac_aca ，定义 fbf_bfb 生成的伪标签为 yby_byb ，带有置信度为 cbc_bcb 。以上是生成权重以及计算损失的公式，\n\n * 当 yay_aya 等于 yby_byb 时，权重为pbγ1p_b^{\\gamma_1}pbγ1 ，满足该条件的样本称作 agreement\n * 当 yay_aya 不等于 yby_byb 且ca≥cbc_a \\geq c_bca ≥cb 时，权重为pbγ2p_b^{\\gamma_2}pbγ2 ，满足该条件的样本被称作negetive disagreement，即是 fbf_bfb 与 faf_afa 的预测有分歧，但是 fbf_bfb 的置信度会高一些\n * 当 yay_aya 不等于 yby_byb 且ca<cbc_a < c_bca <cb 时，权重为 0，满足该条件的样本被称作positive disagreement，即是 fbf_bfb 与 faf_afa 的预测有分歧，但是 faf_afa 的置信度会高一些\n\n在 case1 和 case2情况下，使用当前模型预测的概率 pbp_bpb 作为权重，来量化模型的分歧。例如，一个高的概率意味着 fbf_bfb 和 faf_afa 拥有更强的共识，在 case3 的情况下，我们将动态权重设置为 0 因为其伪标签可能完全不正确。\n\n动态权重有两个超参数 γ1\\gamma_1γ1 以及 γ2\\gamma_2γ2 ，更高的 γ\\gammaγ 值会放大置信度的影响，更高的 γ1\\gamma_1γ1 值强调了熵的最小化，更高的 γ2\\gamma_2γ2 值强调了更多的互学习。高 γ\\gammaγ 值通常有利于高噪声的场景中，或者是为了保持更大的模型间分歧。对于语义分割问题来讲，wuh×ww_u^{h\\times w}wuh×w 代表了一个像素级别的权重图，加权策略保持一直并且应用到每个像素上。\n\n作者解释了为什么要分情况来设置权重，其实直接使用 pbp_bpb 作为权重也是可以的，但是作者观察到，再严重的伪标签噪声下，这种过于简单的方法效果并不好，另外动态损失的其他设计也为带来明显的性能改善（可以看实验部分的消融实验）\n\n4.1.2 初始化分歧\n\ncifar-10 这种简单任务上，直接使用不同的随机初始化参数即可引入模型分歧。然而，对于需要使用预训练模型的模型（例如语义分割任务）来讲，会通过差异最大化采样来引入模型分歧\n\n# 4.2 迭代框架\n\n受到课程学习、无监督域适应语义分割等方法的启发，dmt 是一个迭代式的框架，来获得更好的性能\n\n4.2.1 图像分类\n\n\n\n我们首先在已标注子集上训练，每次从未标注子集上选择最高置信度的一批伪标签，然后重新训练一个随机初始化的模型，与 curriculum labeling[26] 论文类似。上图是该算法的伪代码，在模型从零重新训练的早期阶段会提供一些有用的信息，因此我们受到[9]的启发使用了一个类似 sigmoid 函数，总的来说，总的训练step是 tmaxt_{max}tmax ，在第 ttt 个 step，γ=γmaxe5(1−ttmax)\\gamma=\\gamma_{max}e^{5(1-\\frac{t}{t_{max}})}γ=γmax e5(1−tmax t )\n\n4.2.2 语义分割\n\n\n\n在语义分割任务中，有一些类别是比另外一些类别容易学的，cbst[13] 提出了一个迭代式自训练框架，在每一轮使用每个类别 最高置信度的伪标签。并且与图像分类任务不同的是，图像分割依赖预训练权重来加速拟合，受到cbst的启发，使用两个模型。\n\n\n\n在这种设置中，两个模型互相平等地进行训练，模型间的分歧被更直接地利用，上图是dmt在pascal voc 2012数据集上的动态权重图。cbst没有靠排序来选择最可靠的伪标签，其按照类别定义阈值，将置信度超过1的像素被选中训练。大多数场景中，和直接排序相似，但是在极端情况下，预测的类别可能会改变。\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\ndmt 提出了一种方法，使用两个模型对伪标签生成动态权重。对样本划分了三种情况给予不同的权重，以迭代式的方法将伪标签 refine 到越来越好，并且生成像素级的权重促进模型学习。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2004.08514\n\n * https://github.com/voldemortx/dst-cbc",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Learning Visual Words for Weakly-Supervised Semantic Segmentation",frontmatter:{title:"Learning Visual Words for Weakly-Supervised Semantic Segmentation",date:"2021-10-16T14:19:29.000Z",permalink:"/pages/62e38a/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/07.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/18.Learning%20Visual%20Words%20for%20Weakly-Supervised%20Semantic%20Segmentation.html",relativePath:"01.学术搬砖/07.论文阅读-弱监督图像分割/18.Learning Visual Words for Weakly-Supervised Semantic Segmentation.md",key:"v-1c426e16",path:"/pages/62e38a/",headers:[{level:2,title:"Learning Visual Words for Weakly-Supervised Semantic Segmentation",slug:"learning-visual-words-for-weakly-supervised-semantic-segmentation",normalizedTitle:"learning visual words for weakly-supervised semantic segmentation",charIndex:2},{level:3,title:"01 摘要",slug:"_01-摘要",normalizedTitle:"01 摘要",charIndex:132},{level:3,title:"02 论文的目的及结论",slug:"_02-论文的目的及结论",normalizedTitle:"02 论文的目的及结论",charIndex:495},{level:3,title:"03 论文的方法",slug:"_03-论文的方法",normalizedTitle:"03 论文的方法",charIndex:579},{level:3,title:"04 论文的实验",slug:"_04-论文的实验",normalizedTitle:"04 论文的实验",charIndex:1572}],headersStr:"Learning Visual Words for Weakly-Supervised Semantic Segmentation 01 摘要 02 论文的目的及结论 03 论文的方法 04 论文的实验",content:'# Learning Visual Words for Weakly-Supervised Semantic Segmentation\n\n# 作者：Lixiang Ru, Bo Du, Chen Wu\n\n# 单位：WHU\n\n# 发表：IJCAI 2021\n\n\n# 01 摘要\n\nCAM通常只识别出最具鉴别力的物体范围，这是因为网络不需要发现物体全貌来识别图像级别的标签。论文提出同时学习图像级标签以及本地的visual word 标签来处理这个问题。\n\n用一个可学习的 codebook 来编码输入图像的feature map，为了网络能够分类编码的细粒度的visual words，生成的 CAM 应当需要覆盖更多的语义区域，除此之外，提出了混合空间金字塔池化模块（hybrid spatial pyramid pooling module），能够保留 feature map 上的局部最大值以及全局平均值，可以捕获更多的目标细节以及更少的背景，在PASCAL VOC 2012 的val set上可以达到 67.2% mIoU，test set上可以达到 67.3% mIoU\n\n\n# 02 论文的目的及结论\n\n * 用特征图上的 Visual Word 的特征来约束模型学到更多的语义区域\n * 提出 HSPP 结合 GAP 和 GMP 优点\n\n\n# 03 论文的方法\n\n\n\n主要提出两个模块\n\n * VWE: Visual Word Encoder，编码 local visual words\n\n * HSPP: Hybrid spatial pyramid pooling layer，更好地聚合信息\n\n# 3.1 VWE 模块\n\ncodebook 是一个 Matrix C∈Rk×dC\\in R^{k\\times d}C∈Rk×d\n\n * ddd 是 feature map 的维度\n * kkk 是词的数量\n\n\n\nSijS_{ij}Sij 是余弦距离，代表feature map 上 iii 位置上与 CCC 矩阵中 第 jjj 个词的相似度\n\n\n\n通过 softmax 做行上的归一化，来计算第 iii 个像素属于第 jjj 个词的概率，概率最大的词即作为 FiF_iFi 的 visual word label，对于输入图像来讲，visual word label 是一个 kkk 维的向量，\n\n\n\n在BoVW 模型中，每个visual word 的直方图分布通过出现的频次来衡量，然而，hard quantization 的方法会引入非连续性，并被证明会使训练过程难以完成。本篇论文通过累积 PPP 上的概率来计算每个词的频率，因此，第 jjj 个词的"软频率"就是如上式所示\n\n传统的 BoVW 模型中，codebook 通常认为是所有 visual word 的聚类中心，但在本文的模型中，visual word 的特征表示是在训练过程中在线更新的。因此，码本 CCC 也是会在线更新的。\n\n# 3.2 HSPP 模块\n\n为了克服 GAP 和 GMP 的缺点，提出 HSPP 聚合多尺度的局部最大值以及全局平均值\n\n\n\n假设特征图维度是 h×w×dh\\times w\\times dh×w×d ，按照缩放因子 rrr 分成多尺度，每一份的维度为 h/r×w/r×dh/r \\times w/r \\times dh/r×w/r×d，缩放因子的取值为1，2，4.\n\n\n\n\n\n\n\n式五只能提取局部的最大响应，可能会造成目标区域的不完整，\n\n# 3.3 损失设计\n\n\n\n\n\n# 3.4 生成CAM\n\n\n\n\n\n\n\n\n\n走两个分支，一个conv feature，一个word feature，最后取 max 共同生成CAM\n\n\n# 04 论文的实验\n\n将 ResNet50 用来提特征，使用 IRNet 做 CAM 的Refine，使用ResNet101 作为 backbone 的 DeepLabv2 产出最后的实验结果\n\n\n\n\n\n\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://www.ijcai.org/proceedings/2021/0136.pdf\n * https://github.com/rulixiang/vwe\n * https://lixiangru.cn/assets/files/VWE_Poster.pd',normalizedContent:'# learning visual words for weakly-supervised semantic segmentation\n\n# 作者：lixiang ru, bo du, chen wu\n\n# 单位：whu\n\n# 发表：ijcai 2021\n\n\n# 01 摘要\n\ncam通常只识别出最具鉴别力的物体范围，这是因为网络不需要发现物体全貌来识别图像级别的标签。论文提出同时学习图像级标签以及本地的visual word 标签来处理这个问题。\n\n用一个可学习的 codebook 来编码输入图像的feature map，为了网络能够分类编码的细粒度的visual words，生成的 cam 应当需要覆盖更多的语义区域，除此之外，提出了混合空间金字塔池化模块（hybrid spatial pyramid pooling module），能够保留 feature map 上的局部最大值以及全局平均值，可以捕获更多的目标细节以及更少的背景，在pascal voc 2012 的val set上可以达到 67.2% miou，test set上可以达到 67.3% miou\n\n\n# 02 论文的目的及结论\n\n * 用特征图上的 visual word 的特征来约束模型学到更多的语义区域\n * 提出 hspp 结合 gap 和 gmp 优点\n\n\n# 03 论文的方法\n\n\n\n主要提出两个模块\n\n * vwe: visual word encoder，编码 local visual words\n\n * hspp: hybrid spatial pyramid pooling layer，更好地聚合信息\n\n# 3.1 vwe 模块\n\ncodebook 是一个 matrix c∈rk×dc\\in r^{k\\times d}c∈rk×d\n\n * ddd 是 feature map 的维度\n * kkk 是词的数量\n\n\n\nsijs_{ij}sij 是余弦距离，代表feature map 上 iii 位置上与 ccc 矩阵中 第 jjj 个词的相似度\n\n\n\n通过 softmax 做行上的归一化，来计算第 iii 个像素属于第 jjj 个词的概率，概率最大的词即作为 fif_ifi 的 visual word label，对于输入图像来讲，visual word label 是一个 kkk 维的向量，\n\n\n\n在bovw 模型中，每个visual word 的直方图分布通过出现的频次来衡量，然而，hard quantization 的方法会引入非连续性，并被证明会使训练过程难以完成。本篇论文通过累积 ppp 上的概率来计算每个词的频率，因此，第 jjj 个词的"软频率"就是如上式所示\n\n传统的 bovw 模型中，codebook 通常认为是所有 visual word 的聚类中心，但在本文的模型中，visual word 的特征表示是在训练过程中在线更新的。因此，码本 ccc 也是会在线更新的。\n\n# 3.2 hspp 模块\n\n为了克服 gap 和 gmp 的缺点，提出 hspp 聚合多尺度的局部最大值以及全局平均值\n\n\n\n假设特征图维度是 h×w×dh\\times w\\times dh×w×d ，按照缩放因子 rrr 分成多尺度，每一份的维度为 h/r×w/r×dh/r \\times w/r \\times dh/r×w/r×d，缩放因子的取值为1，2，4.\n\n\n\n\n\n\n\n式五只能提取局部的最大响应，可能会造成目标区域的不完整，\n\n# 3.3 损失设计\n\n\n\n\n\n# 3.4 生成cam\n\n\n\n\n\n\n\n\n\n走两个分支，一个conv feature，一个word feature，最后取 max 共同生成cam\n\n\n# 04 论文的实验\n\n将 resnet50 用来提特征，使用 irnet 做 cam 的refine，使用resnet101 作为 backbone 的 deeplabv2 产出最后的实验结果\n\n\n\n\n\n\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://www.ijcai.org/proceedings/2021/0136.pdf\n * https://github.com/rulixiang/vwe\n * https://lixiangru.cn/assets/files/vwe_poster.pd',charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"ClassMix Segmentation-Based Data Augmentation for Semi-Supervised Learning",frontmatter:{title:"ClassMix Segmentation-Based Data Augmentation for Semi-Supervised Learning",date:"2021-04-20T10:22:52.000Z",permalink:"/pages/247800/",categories:["论文阅读","半监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/08.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/05.ClassMix%20Segmentation-Based%20Data%20Augmentation%20for%20Semi-Supervised%20Learning.html",relativePath:"01.学术搬砖/08.论文阅读-半监督图像分割/05.ClassMix Segmentation-Based Data Augmentation for Semi-Supervised Learning.md",key:"v-05671b78",path:"/pages/247800/",headers:[{level:2,title:"ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning",slug:"classmix-segmentation-based-data-augmentation-for-semi-supervised-learning",normalizedTitle:"classmix: segmentation-based data augmentation for semi-supervised learning",charIndex:2}],headersStr:"ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning",content:"# ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning\n\n# 作者：Chalmers University of Technology & Volvo Cars\n\n# 发表：WACV 2021\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2007.07936\n\n * https://github.com/WilhelmT/ClassMix",normalizedContent:"# classmix: segmentation-based data augmentation for semi-supervised learning\n\n# 作者：chalmers university of technology & volvo cars\n\n# 发表：wacv 2021\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2007.07936\n\n * https://github.com/wilhelmt/classmix",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Semi-supevised Semantic Segmentation with High- and Low-level Consistency",frontmatter:{title:"Semi-supevised Semantic Segmentation with High- and Low-level Consistency",date:"2021-05-11T13:38:14.000Z",permalink:"/pages/3f8f22/",categories:["论文阅读","半监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/08.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/07.Semi-supevised%20Semantic%20Segmentation%20with%20High-%20and%20Low-level%20Consistency.html",relativePath:"01.学术搬砖/08.论文阅读-半监督图像分割/07.Semi-supevised Semantic Segmentation with High- and Low-level Consistency.md",key:"v-5a7f928b",path:"/pages/3f8f22/",headers:[{level:2,title:"Semi-supevised Semantic Segmentation with High- and Low-level Consistency",slug:"semi-supevised-semantic-segmentation-with-high-and-low-level-consistency",normalizedTitle:"semi-supevised semantic segmentation with high- and low-level consistency",charIndex:2}],headersStr:"Semi-supevised Semantic Segmentation with High- and Low-level Consistency",content:"# Semi-supevised Semantic Segmentation with High- and Low-level Consistency\n\n# 作者：\n\n# 发表：TPAMI 2021\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1908.05724v1\n\n * https://github.com/sud0301/semisup-semseg",normalizedContent:"# semi-supevised semantic segmentation with high- and low-level consistency\n\n# 作者：\n\n# 发表：tpami 2021\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1908.05724v1\n\n * https://github.com/sud0301/semisup-semseg",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Social-STGCNN A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction",frontmatter:{title:"Social-STGCNN A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction",date:"2021-05-01T14:02:31.000Z",permalink:"/pages/e8a5ee/",categories:["论文阅读","半监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/08.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/06.Social-STGCNN%20A%20Social%20Spatio-Temporal%20Graph%20Convolutional%20Neural%20Network%20for%20Human%20Trajectory%20Prediction.html",relativePath:"01.学术搬砖/08.论文阅读-半监督图像分割/06.Social-STGCNN A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction.md",key:"v-ac5cebe2",path:"/pages/e8a5ee/",headers:[{level:2,title:"Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction",slug:"social-stgcnn-a-social-spatio-temporal-graph-convolutional-neural-network-for-human-trajectory-prediction",normalizedTitle:"social-stgcnn: a social spatio-temporal graph convolutional neural network for human trajectory prediction",charIndex:2}],headersStr:"Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction",content:"# Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction\n\n# 作者：\n\n# 发表：CVPR 2020\n\n# 摘要\n\n对行人行为有更好的机器理解，可以使自主车辆和人类等代理人之间的互动建模取得更快的进展。行人的轨迹不仅受到行人本身的影响，而且还受到与周围物体互动的影响。以前的方法通过使用各种整合不同的行人状态的聚合方法对这些互动进行建模。我们提出了社会时空图卷积神经网络（Social-STGCNN），它通过将交互建模为一个图来代替聚合方法的需要。我们的结果显示，最终位移误差(FDE)比现有技术水平提高了20%，平均位移误差(ADE)提高了8.5倍，推理速度比以前报道的方法快48倍。此外，我们的模型数据效率高，仅用20%的训练数据就超过了以往的ADE指标。我们提出了一个核函数，将行人之间的社会互动嵌入到邻接矩阵中。通过定性分析，我们表明我们的模型继承了行人轨迹之间可以预期的社会行为。\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n该模型是在两个人类轨迹预测数据集上训练的。ETH[21]和UCY[11]。ETH包含名为ETH和HOTEL的两个场景，而UCY包含名为ZARA1、ZARA2和UNIV的三个场景。每0.4秒对数据集中的轨迹进行一次采样，实验设置follow了Social-LSTM论文，在Social-LSTM中，模型是在特定数据集的一部分上训练的，并针对其余数据集进行了测试，并与其他四个数据集进行了验证。在进行评估时，模型会观察到与8帧相对应的3.2秒轨迹，并预测接下来的4.8秒（即12帧）的轨迹。\n\n两个指标被用来评估模型性能：方程6中定义的平均位移误差（ADE）[21]和方程7中定义的最终位移误差（FDE）[1]。 由于Social-STGCNN生成了一个双变量的高斯分布作为预测，为了比较一个分布与特定的目标值，我们遵循Social-LSTM[1]中使用的评估方法，即根据预测的分布生成20个样本。 然后用最接近地面真相的样本来计算ADE和FDE。这种评估方法被一些作品所采用，如Social-GAN[6]和其他许多作品。\n\n# 论文的方法\n\n\n\n\n\n由两部分组成，时空图卷积网络 ST-GCNN 以及 时间外推卷积神经网络 TXP-CNN。ST-GCNN 对行人轨迹的图形表示进行空间-时间卷积运算以提取特征，这些特征是观察到的行人轨迹历史的紧凑表示。TXP-CNN将这些特征作为输入，并预测所有行人的未来轨迹。我们使用时间外推器这个名字，是因为TXP-CNN有望通过卷积操作来外推未来轨迹。图2展示了该模型的概况。\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2002.11927\n\n * https://github.com/abduallahmohamed/Social-STGCNN\n\n# 代码释义\n\nmodel.py 查看 forward 函数：主要是传入v,a，然后顺序是\n\nst_gcns -> tpcnns[0] + prelus[0] -> tpcnns[1:n_txpcnn-1] + prelus[1:n_txpcnn-1] -> tpcnn_ouput\n\n\n1\n\n\n分模块解析\n\nCUDA_VISIBLE_DEVICES=0 python3 train.py --lr 0.01 --n_stgcnn 1 --n_txpcnn 5  --dataset eth --tag social-stgcnn-eth --use_lrschd --num_epochs 250\n\n\n1\n\n\n * st_gcns: st_gcn模型，原版的 n_stgcnn 为1，是 1 层 st_gcn\n\n * input_feat 为 2，output_feat 为 5\n\n * tp_cnns: tp_cnn模型，其实也就是几层卷积层，原版的 n_txpcnn 为5，就是5层卷积层\n\n * seq_len 为 8，pred_seq_len 为 12\n\n * prelus 就是 一些 PReLU() 函数\n\n改模型的话\n\n1、重点解析 st_gcns 模型\n\n2、tp_cnns 没有加残差结构\n\n3、tp_cnns 没有加注意力机制\n\n4、没有加环境输入",normalizedContent:"# social-stgcnn: a social spatio-temporal graph convolutional neural network for human trajectory prediction\n\n# 作者：\n\n# 发表：cvpr 2020\n\n# 摘要\n\n对行人行为有更好的机器理解，可以使自主车辆和人类等代理人之间的互动建模取得更快的进展。行人的轨迹不仅受到行人本身的影响，而且还受到与周围物体互动的影响。以前的方法通过使用各种整合不同的行人状态的聚合方法对这些互动进行建模。我们提出了社会时空图卷积神经网络（social-stgcnn），它通过将交互建模为一个图来代替聚合方法的需要。我们的结果显示，最终位移误差(fde)比现有技术水平提高了20%，平均位移误差(ade)提高了8.5倍，推理速度比以前报道的方法快48倍。此外，我们的模型数据效率高，仅用20%的训练数据就超过了以往的ade指标。我们提出了一个核函数，将行人之间的社会互动嵌入到邻接矩阵中。通过定性分析，我们表明我们的模型继承了行人轨迹之间可以预期的社会行为。\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n该模型是在两个人类轨迹预测数据集上训练的。eth[21]和ucy[11]。eth包含名为eth和hotel的两个场景，而ucy包含名为zara1、zara2和univ的三个场景。每0.4秒对数据集中的轨迹进行一次采样，实验设置follow了social-lstm论文，在social-lstm中，模型是在特定数据集的一部分上训练的，并针对其余数据集进行了测试，并与其他四个数据集进行了验证。在进行评估时，模型会观察到与8帧相对应的3.2秒轨迹，并预测接下来的4.8秒（即12帧）的轨迹。\n\n两个指标被用来评估模型性能：方程6中定义的平均位移误差（ade）[21]和方程7中定义的最终位移误差（fde）[1]。 由于social-stgcnn生成了一个双变量的高斯分布作为预测，为了比较一个分布与特定的目标值，我们遵循social-lstm[1]中使用的评估方法，即根据预测的分布生成20个样本。 然后用最接近地面真相的样本来计算ade和fde。这种评估方法被一些作品所采用，如social-gan[6]和其他许多作品。\n\n# 论文的方法\n\n\n\n\n\n由两部分组成，时空图卷积网络 st-gcnn 以及 时间外推卷积神经网络 txp-cnn。st-gcnn 对行人轨迹的图形表示进行空间-时间卷积运算以提取特征，这些特征是观察到的行人轨迹历史的紧凑表示。txp-cnn将这些特征作为输入，并预测所有行人的未来轨迹。我们使用时间外推器这个名字，是因为txp-cnn有望通过卷积操作来外推未来轨迹。图2展示了该模型的概况。\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2002.11927\n\n * https://github.com/abduallahmohamed/social-stgcnn\n\n# 代码释义\n\nmodel.py 查看 forward 函数：主要是传入v,a，然后顺序是\n\nst_gcns -> tpcnns[0] + prelus[0] -> tpcnns[1:n_txpcnn-1] + prelus[1:n_txpcnn-1] -> tpcnn_ouput\n\n\n1\n\n\n分模块解析\n\ncuda_visible_devices=0 python3 train.py --lr 0.01 --n_stgcnn 1 --n_txpcnn 5  --dataset eth --tag social-stgcnn-eth --use_lrschd --num_epochs 250\n\n\n1\n\n\n * st_gcns: st_gcn模型，原版的 n_stgcnn 为1，是 1 层 st_gcn\n\n * input_feat 为 2，output_feat 为 5\n\n * tp_cnns: tp_cnn模型，其实也就是几层卷积层，原版的 n_txpcnn 为5，就是5层卷积层\n\n * seq_len 为 8，pred_seq_len 为 12\n\n * prelus 就是 一些 prelu() 函数\n\n改模型的话\n\n1、重点解析 st_gcns 模型\n\n2、tp_cnns 没有加残差结构\n\n3、tp_cnns 没有加注意力机制\n\n4、没有加环境输入",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Self-Tuning for Data-Efficient Deep Learning",frontmatter:{title:"Self-Tuning for Data-Efficient Deep Learning",date:"2021-07-29T16:58:07.000Z",permalink:"/pages/a86584/",categories:["论文阅读","半监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/08.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/08.Self-Tuning%20for%20Data-Efficient%20Deep%20Learning.html",relativePath:"01.学术搬砖/08.论文阅读-半监督图像分割/08.Self-Tuning for Data-Efficient Deep Learning.md",key:"v-10762118",path:"/pages/a86584/",headers:[{level:2,title:"Self-Tuning for Data-Efficient Deep Learning",slug:"self-tuning-for-data-efficient-deep-learning",normalizedTitle:"self-tuning for data-efficient deep learning",charIndex:2}],headersStr:"Self-Tuning for Data-Efficient Deep Learning",content:"# Self-Tuning for Data-Efficient Deep Learning\n\n# 作者：Ximei Wang, Jinghan Gao, Mingsheng Long, Jianmin Wang\n\n# 发表：ICML 2021\n\n# 单位：THU\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * 论文链接\n * 代码链接\n * 作者解读\n * Poster\n * Slides",normalizedContent:"# self-tuning for data-efficient deep learning\n\n# 作者：ximei wang, jinghan gao, mingsheng long, jianmin wang\n\n# 发表：icml 2021\n\n# 单位：thu\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * 论文链接\n * 代码链接\n * 作者解读\n * poster\n * slides",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"FixMatch Simplifying Semi-Supervised Learning with Consistency and Confidence",frontmatter:{title:"FixMatch Simplifying Semi-Supervised Learning with Consistency and Confidence",date:"2021-07-29T16:58:41.000Z",permalink:"/pages/29ae03/",categories:["论文阅读","半监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/08.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/09.FixMatch%20Simplifying%20Semi-Supervised%20Learning%20with%20Consistency%20and%20Confidence.html",relativePath:"01.学术搬砖/08.论文阅读-半监督图像分割/09.FixMatch Simplifying Semi-Supervised Learning with Consistency and Confidence.md",key:"v-cf4b3f26",path:"/pages/29ae03/",headers:[{level:2,title:"FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",slug:"fixmatch-simplifying-semi-supervised-learning-with-consistency-and-confidence",normalizedTitle:"fixmatch: simplifying semi-supervised learning with consistency and confidence",charIndex:2}],headersStr:"FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",content:"# FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence\n\n# 作者：Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel\n\n# 单位：Google Research\n\n# 发表：NeurIPS 2020\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2001.07685\n * https://github.com/google-research/fixmatch",normalizedContent:"# fixmatch: simplifying semi-supervised learning with consistency and confidence\n\n# 作者：kihyuk sohn, david berthelot, chun-liang li, zizhao zhang, nicholas carlini, ekin d. cubuk, alex kurakin, han zhang, colin raffel\n\n# 单位：google research\n\n# 发表：neurips 2020\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2001.07685\n * https://github.com/google-research/fixmatch",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation A Baseline Investigation",frontmatter:{title:"Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation A Baseline Investigation",date:"2021-07-29T17:26:16.000Z",permalink:"/pages/fc0825/",categories:["论文阅读","半监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/08.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/10.Re-distributing%20Biased%20Pseudo%20Labels%20for%20Semi-supervised%20Semantic%20Segmentation%20A%20Baseline%20Investigation.html",relativePath:"01.学术搬砖/08.论文阅读-半监督图像分割/10.Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation A Baseline Investigation.md",key:"v-d10e5544",path:"/pages/fc0825/",headers:[{level:2,title:"Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation",slug:"re-distributing-biased-pseudo-labels-for-semi-supervised-semantic-segmentation-a-baseline-investigation",normalizedTitle:"re-distributing biased pseudo labels for semi-supervised semantic segmentation: a baseline investigation",charIndex:2}],headersStr:"Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation",content:"# Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation\n\n# 作者：Ruifei He, Jihan Yang, Xiaojuan Qi\n\n# 单位：HKU, ZJU\n\n# 发表：ICCV 2021 Oral\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2107.11279",normalizedContent:"# re-distributing biased pseudo labels for semi-supervised semantic segmentation: a baseline investigation\n\n# 作者：ruifei he, jihan yang, xiaojuan qi\n\n# 单位：hku, zju\n\n# 发表：iccv 2021 oral\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2107.11279",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Mean teachers are better role models Weight-averaged consistency targets improve semi-supervised deep learning results",frontmatter:{title:"Mean teachers are better role models Weight-averaged consistency targets improve semi-supervised deep learning results",date:"2021-07-29T17:37:28.000Z",permalink:"/pages/8246b6/",categories:["论文阅读","半监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/08.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/11.Mean%20teachers%20are%20better%20role%20models%20Weight-averaged%20consistency%20targets%20improve%20semi-supervised%20deep%20learning%20results.html",relativePath:"01.学术搬砖/08.论文阅读-半监督图像分割/11.Mean teachers are better role models Weight-averaged consistency targets improve semi-supervised deep learning results.md",key:"v-5d54258d",path:"/pages/8246b6/",headersStr:null,content:"# Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results\n\n# 作者：Ruifei He, Jihan Yang, Xiaojuan Qi\n\n# 单位：HKU, ZJU\n\n# 发表：NeurIPS 2017\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1703.01780\n\n * https://github.com/CuriousAI/mean-teacher\n\n * Poster\n\n * Slides\n\n * ",normalizedContent:"# mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results\n\n# 作者：ruifei he, jihan yang, xiaojuan qi\n\n# 单位：hku, zju\n\n# 发表：neurips 2017\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/1703.01780\n\n * https://github.com/curiousai/mean-teacher\n\n * poster\n\n * slides\n\n * ",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"SCAN Learning to Classify Images without Labels",frontmatter:{title:"SCAN Learning to Classify Images without Labels",date:"2021-09-06T13:53:41.000Z",permalink:"/pages/0f51c6/",categories:["学术搬砖","论文阅读-小样本学习"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/10.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/02.SCAN%20Learning%20to%20Classify%20Images%20without%20Labels.html",relativePath:"01.学术搬砖/10.论文阅读-小样本学习/02.SCAN Learning to Classify Images without Labels.md",key:"v-798c7e0f",path:"/pages/0f51c6/",headers:[{level:2,title:"SCAN: Learning to Classify Images without Labels",slug:"scan-learning-to-classify-images-without-labels",normalizedTitle:"scan: learning to classify images without labels",charIndex:2}],headersStr:"SCAN: Learning to Classify Images without Labels",content:"# SCAN: Learning to Classify Images without Labels\n\n# 单位：KU Leuven/ESAT-PSI, ETH Zurich/CVL, TRACE\n\n# 作者：Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, Luc Van Goo\n\n# 发表：ECCV 2020\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n * 使用 SimCLR 来训代理任务\n * 执行 聚类\n * 执行 self-labeling\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://paperswithcode.com/paper/learning-to-classify-images-without-labels",normalizedContent:"# scan: learning to classify images without labels\n\n# 单位：ku leuven/esat-psi, eth zurich/cvl, trace\n\n# 作者：wouter van gansbeke, simon vandenhende, stamatios georgoulis, marc proesmans, luc van goo\n\n# 发表：eccv 2020\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n * 使用 simclr 来训代理任务\n * 执行 聚类\n * 执行 self-labeling\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://paperswithcode.com/paper/learning-to-classify-images-without-labels",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Improving Unsupervised Image Clustering With Robust Learning",frontmatter:{title:"Improving Unsupervised Image Clustering With Robust Learning",date:"2021-09-06T13:53:08.000Z",permalink:"/pages/bdd933/",categories:["学术搬砖","论文阅读-小样本学习"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/10.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/01.Improving%20Unsupervised%20Image%20Clustering%20With%20Robust%20Learning.html",relativePath:"01.学术搬砖/10.论文阅读-小样本学习/01.Improving Unsupervised Image Clustering With Robust Learning.md",key:"v-4b21ce4b",path:"/pages/bdd933/",headers:[{level:2,title:"Improving Unsupervised Image Clustering With Robust Learning",slug:"improving-unsupervised-image-clustering-with-robust-learning",normalizedTitle:"improving unsupervised image clustering with robust learning",charIndex:2}],headersStr:"Improving Unsupervised Image Clustering With Robust Learning",content:"# Improving Unsupervised Image Clustering With Robust Learning\n\n# 单位：School of Computing, KAIST, Data Science Group, Institute for Basic Science\n\n# 作者：Sungwon Park, Sungwon Han, Sundong Kim, Danu Kim, Sungkyu Park, Seunghoon Hong, Meeyoung Cha\n\n# 发表：CVPR 2021\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://paperswithcode.com/paper/improving-unsupervised-image-clustering-with",normalizedContent:"# improving unsupervised image clustering with robust learning\n\n# 单位：school of computing, kaist, data science group, institute for basic science\n\n# 作者：sungwon park, sungwon han, sundong kim, danu kim, sungkyu park, seunghoon hong, meeyoung cha\n\n# 发表：cvpr 2021\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://paperswithcode.com/paper/improving-unsupervised-image-clustering-with",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Sill-Net Feature Augmentation with Separated Illumination Representation",frontmatter:{title:"Sill-Net Feature Augmentation with Separated Illumination Representation",date:"2021-09-06T13:59:26.000Z",permalink:"/pages/83a0c4/",categories:["学术搬砖","论文阅读-小样本学习"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/10.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/03.Sill-Net%20Feature%20Augmentation%20with%20Separated%20Illumination%20Representation.html",relativePath:"01.学术搬砖/10.论文阅读-小样本学习/03.Sill-Net Feature Augmentation with Separated Illumination Representation.md",key:"v-21b50551",path:"/pages/83a0c4/",headers:[{level:2,title:"Sill-Net: Feature Augmentation with Separated Illumination Representation",slug:"sill-net-feature-augmentation-with-separated-illumination-representation",normalizedTitle:"sill-net: feature augmentation with separated illumination representation",charIndex:2}],headersStr:"Sill-Net: Feature Augmentation with Separated Illumination Representation",content:"# Sill-Net: Feature Augmentation with Separated Illumination Representation\n\n# 单位：THUAI\n\n# 作者：Haipeng Zhang, Zhong Cao, Ziang Yan, Changshui Zhang\n\n# 发表：Arxiv\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://paperswithcode.com/paper/sill-net-feature-augmentation-with-separated",normalizedContent:"# sill-net: feature augmentation with separated illumination representation\n\n# 单位：thuai\n\n# 作者：haipeng zhang, zhong cao, ziang yan, changshui zhang\n\n# 发表：arxiv\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://paperswithcode.com/paper/sill-net-feature-augmentation-with-separated",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"(MoCov1) Momentum Contrast for Unsupervised Visual Representation Learning",frontmatter:{title:"(MoCov1) Momentum Contrast for Unsupervised Visual Representation Learning",date:"2021-04-14T23:37:22.000Z",permalink:"/pages/1e8d33/",categories:["论文阅读","自监督学习"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/11.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/00.(MoCov1)%20Momentum%20Contrast%20for%20Unsupervised%20Visual%20Representation%20Learning.html",relativePath:"01.学术搬砖/11.论文阅读-自监督学习/00.(MoCov1) Momentum Contrast for Unsupervised Visual Representation Learning.md",key:"v-3d25093c",path:"/pages/1e8d33/",headers:[{level:3,title:"Momentum Contrast for Unsupervised Visual Representation Learning",slug:"momentum-contrast-for-unsupervised-visual-representation-learning",normalizedTitle:"momentum contrast for unsupervised visual representation learning",charIndex:2}],headersStr:"Momentum Contrast for Unsupervised Visual Representation Learning",content:"# Momentum Contrast for Unsupervised Visual Representation Learning\n\n# 作者：FAIR Kaiming组\n\n# 摘要\n\nlinear protocol\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事",normalizedContent:"# momentum contrast for unsupervised visual representation learning\n\n# 作者：fair kaiming组\n\n# 摘要\n\nlinear protocol\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"(SimCLRv1) A Simple Framework for Contrastive Learning of Visual Representations",frontmatter:{title:"(SimCLRv1) A Simple Framework for Contrastive Learning of Visual Representations",date:"2021-04-14T23:37:22.000Z",permalink:"/pages/464aed/",categories:["论文阅读","自监督学习"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/11.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/01.(SimCLRv1)%20A%20Simple%20Framework%20for%20Contrastive%20Learning%20of%20Visual%20Representations.html",relativePath:"01.学术搬砖/11.论文阅读-自监督学习/01.(SimCLRv1) A Simple Framework for Contrastive Learning of Visual Representations.md",key:"v-74f65bd4",path:"/pages/464aed/",headers:[{level:3,title:"A Simple Framework for Contrastive Learning of Visual Representations",slug:"a-simple-framework-for-contrastive-learning-of-visual-representations",normalizedTitle:"a simple framework for contrastive learning of visual representations",charIndex:2}],headersStr:"A Simple Framework for Contrastive Learning of Visual Representations",content:"# A Simple Framework for Contrastive Learning of Visual Representations\n\n# 作者：Google Hinton组\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n\n\n# 总结\n\n# 论文的贡献\n\n作者说很多 idea 以前都出现过\n\n * idea1：Make two views of the same example agree\n   \n   * Becker & Hinton(1992)\n   * instance discrimination -Dosovitskiy et al. (2014)\n   * \n\n * idea2：Use random crop and color distortion for data augmentations\n\n * idea3：Encoder + Projection head\n\n * idea4：Use cross entropy loss as contrastive loss\n\nSimCLRv1 把很多的设计合到一起确实就可以达到很好的性能\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n1、20201014 VALSE Webinar 自监督学习（有SimCLRv1的一作分享）\n\n 2. ",normalizedContent:"# a simple framework for contrastive learning of visual representations\n\n# 作者：google hinton组\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n\n\n# 总结\n\n# 论文的贡献\n\n作者说很多 idea 以前都出现过\n\n * idea1：make two views of the same example agree\n   \n   * becker & hinton(1992)\n   * instance discrimination -dosovitskiy et al. (2014)\n   * \n\n * idea2：use random crop and color distortion for data augmentations\n\n * idea3：encoder + projection head\n\n * idea4：use cross entropy loss as contrastive loss\n\nsimclrv1 把很多的设计合到一起确实就可以达到很好的性能\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n1、20201014 valse webinar 自监督学习（有simclrv1的一作分享）\n\n 2. ",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"(SimCLRv2) Big Self-Supervised Models are Strong Semi-Supervised Learners",frontmatter:{title:"(SimCLRv2) Big Self-Supervised Models are Strong Semi-Supervised Learners",date:"2021-04-14T23:37:22.000Z",permalink:"/pages/1fa222/",categories:["论文阅读","自监督学习"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/11.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/02.(SimCLRv2)%20Big%20Self-Supervised%20Models%20are%20Strong%20Semi-Supervised%20Learners.html",relativePath:"01.学术搬砖/11.论文阅读-自监督学习/02.(SimCLRv2) Big Self-Supervised Models are Strong Semi-Supervised Learners.md",key:"v-2243d7fc",path:"/pages/1fa222/",headers:[{level:3,title:"（SimCLRv2）Big Self-Supervised Models are Strong Semi-Supervised Learners",slug:"simclrv2-big-self-supervised-models-are-strong-semi-supervised-learners",normalizedTitle:"（simclrv2）big self-supervised models are strong semi-supervised learners",charIndex:2}],headersStr:"（SimCLRv2）Big Self-Supervised Models are Strong Semi-Supervised Learners",content:"# （SimCLRv2）Big Self-Supervised Models are Strong Semi-Supervised Learners\n\n# 作者：Google Hinton组\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n\n\n\n\nBig CNN 用无监督学习来pretraining，然后去fine-tuning\n\n再做一个无监督的蒸馏，train一个student model\n\n无监督的蒸馏：用finetune的model来计算一个soft label，然后去train这个student model\n\n# 论文的不足\n\n# 论文如何讲故事\n\n两个利用无标记数据的范式\n\n * Task-agnostic use of unlabeled data\n   * Unsupervised pre-training + supervised fine-tuning\n * Task-specific use of unlabeled data\n   * Self-training，pseudo-labeling\n   * Label consistency regularization\n   * Other label propogation\n\n# 参考资料",normalizedContent:"# （simclrv2）big self-supervised models are strong semi-supervised learners\n\n# 作者：google hinton组\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n\n\n\n\nbig cnn 用无监督学习来pretraining，然后去fine-tuning\n\n再做一个无监督的蒸馏，train一个student model\n\n无监督的蒸馏：用finetune的model来计算一个soft label，然后去train这个student model\n\n# 论文的不足\n\n# 论文如何讲故事\n\n两个利用无标记数据的范式\n\n * task-agnostic use of unlabeled data\n   * unsupervised pre-training + supervised fine-tuning\n * task-specific use of unlabeled data\n   * self-training，pseudo-labeling\n   * label consistency regularization\n   * other label propogation\n\n# 参考资料",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"(InstDis) Unsupervised Feature Learning via Non-Parametric Instance Discrimination",frontmatter:{title:"(InstDis) Unsupervised Feature Learning via Non-Parametric Instance Discrimination",date:"2021-04-14T23:37:22.000Z",permalink:"/pages/bb05d4/",categories:["论文阅读","自监督学习"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/11.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/03.(InstDis)%20Unsupervised%20Feature%20Learning%20via%20Non-Parametric%20Instance%20Discrimination.html",relativePath:"01.学术搬砖/11.论文阅读-自监督学习/03.(InstDis) Unsupervised Feature Learning via Non-Parametric Instance Discrimination.md",key:"v-45276cd6",path:"/pages/bb05d4/",headers:[{level:3,title:"(InstDis) Unsupervised Feature Learning via Non-Parametric Instance Discrimination",slug:"instdis-unsupervised-feature-learning-via-non-parametric-instance-discrimination",normalizedTitle:"(instdis) unsupervised feature learning via non-parametric instance discrimination",charIndex:2}],headersStr:"(InstDis) Unsupervised Feature Learning via Non-Parametric Instance Discrimination",content:"# (InstDis) Unsupervised Feature Learning via Non-Parametric Instance Discrimination\n\n# 作者：\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料",normalizedContent:"# (instdis) unsupervised feature learning via non-parametric instance discrimination\n\n# 作者：\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"(CPC) Representation Learning with Contrastive Predictive Coding",frontmatter:{title:"(CPC) Representation Learning with Contrastive Predictive Coding",date:"2021-04-14T23:37:22.000Z",permalink:"/pages/2b22cb/",categories:["论文阅读","自监督学习"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/11.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/04.(CPC)%20Representation%20Learning%20with%20Contrastive%20Predictive%20Coding.html",relativePath:"01.学术搬砖/11.论文阅读-自监督学习/04.(CPC) Representation Learning with Contrastive Predictive Coding.md",key:"v-093768b9",path:"/pages/2b22cb/",headers:[{level:3,title:"Representation Learning with Contrastive Predictive Coding",slug:"representation-learning-with-contrastive-predictive-coding",normalizedTitle:"representation learning with contrastive predictive coding",charIndex:2}],headersStr:"Representation Learning with Contrastive Predictive Coding",content:"# Representation Learning with Contrastive Predictive Coding\n\n# 作者：\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料",normalizedContent:"# representation learning with contrastive predictive coding\n\n# 作者：\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"(CMC) Contrastive Multiview Coding, also contains implementations for MoCo and InstDis",frontmatter:{title:"(CMC) Contrastive Multiview Coding, also contains implementations for MoCo and InstDis",date:"2021-04-14T23:37:22.000Z",permalink:"/pages/f6097d/",categories:["论文阅读","自监督学习"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/11.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/05.(CMC)%20Contrastive%20Multiview%20Coding,%20also%20contains%20implementations%20for%20MoCo%20and%20InstDis.html",relativePath:"01.学术搬砖/11.论文阅读-自监督学习/05.(CMC) Contrastive Multiview Coding, also contains implementations for MoCo and InstDis.md",key:"v-2e7a230d",path:"/pages/f6097d/",headers:[{level:3,title:"(CMC) Contrastive Multiview Coding",slug:"cmc-contrastive-multiview-coding",normalizedTitle:"(cmc) contrastive multiview coding",charIndex:2}],headersStr:"(CMC) Contrastive Multiview Coding",content:'# (CMC) Contrastive Multiview Coding\n\n# 作者：\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * [ECCV 2020] "Contrastive Multiview Coding", also contains implementations for MoCo and InstDis',normalizedContent:'# (cmc) contrastive multiview coding\n\n# 作者：\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * [eccv 2020] "contrastive multiview coding", also contains implementations for moco and instdis',charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Semi-supervised Semantic Segmentation via Strong-weak Dual-branch Network",frontmatter:{title:"Semi-supervised Semantic Segmentation via Strong-weak Dual-branch Network",date:"2021-04-14T23:43:36.000Z",permalink:"/pages/b3d215/",categories:["论文阅读","半监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/08.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/01.Semi-supervised%20Semantic%20Segmentation%20via%20Strong-weak%20Dual-branch%20Network.html",relativePath:"01.学术搬砖/08.论文阅读-半监督图像分割/01.Semi-supervised Semantic Segmentation via Strong-weak Dual-branch Network.md",key:"v-0ab31c4e",path:"/pages/b3d215/",headers:[{level:2,title:"Semi-supervised Semantic Segmentation via Strong-weak Dual-branch Network",slug:"semi-supervised-semantic-segmentation-via-strong-weak-dual-branch-network",normalizedTitle:"semi-supervised semantic segmentation via strong-weak dual-branch network",charIndex:2}],headersStr:"Semi-supervised Semantic Segmentation via Strong-weak Dual-branch Network",content:"# Semi-supervised Semantic Segmentation via Strong-weak Dual-branch Network\n\n# 作者：中山大学 杨猛（ETH Zurich博士后）\n\n# 发表：ECCV 2020 Spotlight\n\n# 摘要\n\n现有的工作探索了大量的技巧来推动弱监督语义分割，但就性能而言与全监督方法还有不小的差距。在真实应用中，除了大量的弱监督的图像，还存在少量可用的像素级的标注，基于这种标签的组成，半监督方法在语义分割任务中有前景。现有的方法简单地将两类标注融到一起训练一个分割网络，然后，我们发现这种处理方法存在问题，并且其性能甚至比只使用强监督标签的样本要差，这说明弱标签的潜力并未被完全挖掘。\n\n为了能够完全挖掘出弱标签的潜能，我们提出了一个强-弱双分支网络来分别对待强标签和弱标签，从而将大量不准确的弱监督与强监督区分开来。我们设计了一个共享的网络组件利用对强弱标注的联合辨别。同时，所提出的双分支独立的处理全监督和弱监督学习，能够有效消除他们的彼此干扰。这种方法只需要较少的额外计算成本，却带来了显著的性能改进，在两个标准数据集上的实验展示了该方法的有效性。\n\n# 阅读\n\n# 论文的目的及结论\n\n想利用双分支分别处理强弱标注的样本，两个分支独立地处理全监督和弱监督学习，能够有效消除他们彼此间的干扰，该方法只需要较少的额外计算成本，却带来了显著的性能改进。\n\n# 论文的实验\n\n\n\n\n\n图一和图二是作者用来说明：如果弱标签使用不当是会损害模型性能的。\n\n\n\n图六是作者针对 λ\\lambdaλ 做的实验\n\n\n\n表一是VOC数据集上的消融实验，展现了双分支的有效性\n\n\n\n表二是和其他方法的对比，作者提出结果是从 FickleNet 论文中直接复制的。\n\n\n\n表三体现了在COCO val set 上每类的 IoU 结果，\n\n\n\n表七展示了图像样本，主要是和 DSRG 方法做对比。\n\n# 论文的方法\n\n弱监督方法估计出的代理监督信息相较于手工标记来说是质量较差的。对于精细标注和弱标注的语义分割任务来讲，一个方法是训两个不同的网络然后将输出取平均。这种做法不仅需要维护两个网络，而且单独的训练无法交换监督的信息。所以为了信息共享、消除样本不平衡以及监督信号不一致，提出了双分支网络来处理不同类型的监督信号。\n\n\n\n图三是整个方法的总览。将整个训练集分为 XsX_sXs 和 XwX_wXw 两部分，分别代表强标签和弱标签集。弱标签集的监督信号由代理监督生成器 GGG 生成：miw=G(xiw)m_i^w = G(x_i^w)miw =G(xiw ) ，代理监督生成器需要一些额外的信息，例如该图像的类别标签。\n\n3.1 过采样对单分支网络没有帮助\n\n\n\n以前的方法侧重于生成更精确的监督信号，但他们并未关注如何协调强弱标签。当图像场景复杂时会有相当多低质量的图像标注（如图四），如果对强弱标签做相近的处理会导致回传的梯度偏向不正确的弱标签，因此会导致新能下降。作者尝试了对强标签做过采样，这一操作是有帮助的但是仍然比不过只使用强标签的性能。总的来讲，过采样对于单分支网络并没有帮助。\n\n3.2 强-弱双分支网络\n\n网络架构由三部分组成：backbone、neck、两个并行的相同架构的分支。实验部分主要基于 VGG16 网络。Backbone 网络就是去掉全连接层的分类网络。Neck 模块就是一系列额外的卷积层能够提升特定任务上的性能，其可被后续的并行分支共享。强弱分支有相同的网络架构，其区别是训练监督信号不同，强分支的监督信号是强标签，弱分支的监督信号是弱标签。这种处理不同监督信号的方式比较新颖的，因为现存的半监督语义分割方法基本都是单分支的网络，多分支网络也并没有做不同监督信号的处理，两个分支有独立的参数 fs(Z)f_s(Z)fs (Z) 和 fw(Z)f_w(Z)fw (Z) ，损失函数是交叉熵损失，两个分支的损失权重相同：Ldata=Lce(ss,ms)+Lce(sw,mw)L_{data} = L_{ce}(s^s,m^s) + L_{ce}(s^w,m^w)Ldata =Lce (ss,ms)+Lce (sw,mw)\n\n3.3 为什么双分支网络有帮助\n\n在训练过程中，我们需要将使用同等多的强标签样本和弱标签样本，在半监督语义分割任务中，这里往往存在大量的弱监督样本，所以在弱监督数据迭代完之前强监督样本已经重复了很多次，实际上是对强监督样本过采样。这种方式能够减轻样本不平衡的影响\n\n双分支网络避免了不同监督信息的互相干扰，可以较好地消除监督不一致的现象。额外的弱监督样本提供了对象的大概位置，从而在一定程度上将正则化引入了 Backbone 中，提升了网络的泛化能力。\n\n3.4 实施细节\n\n当网络训练完成后，弱分支就不再被需要了，因为弱监督样本的信息已经被编码到 Backbone 以及 Neck 模块中了，所以在推理阶段只需要强监督分支的结果即可\n\n# 论文的背景\n\n想利用双分支网络，给予不同的监督信号，弱监督样本起到的其实是正则化的作用，而过采样强监督样本是有帮助的。\n\n# 总结\n\n# 论文的贡献\n\n论文主要的贡献是着眼于改进半监督语义分割的 Image-level 样本的利用方式，作者使用共享 Backbone 和 Neck的双分支网络，分为 Strong 分支以及 Weak 分支，将强监督样本送入 Strong 分支，将弱监督样本送入 Weak 分支，可以较好的消除监督不一致的现象，有助于缓解强弱样本不平衡问题。训练完成后，弱分支就不再需要了，其在训练过程中起到正则化的作用，加强了 Backbone 以及 Neck 网络的泛化能力。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * 半监督分割模型Semi-supervised Semantic Segmentation via Strong-weak Dual-branch Network论文阅读笔记\n\n * https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500766.pdf",normalizedContent:"# semi-supervised semantic segmentation via strong-weak dual-branch network\n\n# 作者：中山大学 杨猛（eth zurich博士后）\n\n# 发表：eccv 2020 spotlight\n\n# 摘要\n\n现有的工作探索了大量的技巧来推动弱监督语义分割，但就性能而言与全监督方法还有不小的差距。在真实应用中，除了大量的弱监督的图像，还存在少量可用的像素级的标注，基于这种标签的组成，半监督方法在语义分割任务中有前景。现有的方法简单地将两类标注融到一起训练一个分割网络，然后，我们发现这种处理方法存在问题，并且其性能甚至比只使用强监督标签的样本要差，这说明弱标签的潜力并未被完全挖掘。\n\n为了能够完全挖掘出弱标签的潜能，我们提出了一个强-弱双分支网络来分别对待强标签和弱标签，从而将大量不准确的弱监督与强监督区分开来。我们设计了一个共享的网络组件利用对强弱标注的联合辨别。同时，所提出的双分支独立的处理全监督和弱监督学习，能够有效消除他们的彼此干扰。这种方法只需要较少的额外计算成本，却带来了显著的性能改进，在两个标准数据集上的实验展示了该方法的有效性。\n\n# 阅读\n\n# 论文的目的及结论\n\n想利用双分支分别处理强弱标注的样本，两个分支独立地处理全监督和弱监督学习，能够有效消除他们彼此间的干扰，该方法只需要较少的额外计算成本，却带来了显著的性能改进。\n\n# 论文的实验\n\n\n\n\n\n图一和图二是作者用来说明：如果弱标签使用不当是会损害模型性能的。\n\n\n\n图六是作者针对 λ\\lambdaλ 做的实验\n\n\n\n表一是voc数据集上的消融实验，展现了双分支的有效性\n\n\n\n表二是和其他方法的对比，作者提出结果是从 ficklenet 论文中直接复制的。\n\n\n\n表三体现了在coco val set 上每类的 iou 结果，\n\n\n\n表七展示了图像样本，主要是和 dsrg 方法做对比。\n\n# 论文的方法\n\n弱监督方法估计出的代理监督信息相较于手工标记来说是质量较差的。对于精细标注和弱标注的语义分割任务来讲，一个方法是训两个不同的网络然后将输出取平均。这种做法不仅需要维护两个网络，而且单独的训练无法交换监督的信息。所以为了信息共享、消除样本不平衡以及监督信号不一致，提出了双分支网络来处理不同类型的监督信号。\n\n\n\n图三是整个方法的总览。将整个训练集分为 xsx_sxs 和 xwx_wxw 两部分，分别代表强标签和弱标签集。弱标签集的监督信号由代理监督生成器 ggg 生成：miw=g(xiw)m_i^w = g(x_i^w)miw =g(xiw ) ，代理监督生成器需要一些额外的信息，例如该图像的类别标签。\n\n3.1 过采样对单分支网络没有帮助\n\n\n\n以前的方法侧重于生成更精确的监督信号，但他们并未关注如何协调强弱标签。当图像场景复杂时会有相当多低质量的图像标注（如图四），如果对强弱标签做相近的处理会导致回传的梯度偏向不正确的弱标签，因此会导致新能下降。作者尝试了对强标签做过采样，这一操作是有帮助的但是仍然比不过只使用强标签的性能。总的来讲，过采样对于单分支网络并没有帮助。\n\n3.2 强-弱双分支网络\n\n网络架构由三部分组成：backbone、neck、两个并行的相同架构的分支。实验部分主要基于 vgg16 网络。backbone 网络就是去掉全连接层的分类网络。neck 模块就是一系列额外的卷积层能够提升特定任务上的性能，其可被后续的并行分支共享。强弱分支有相同的网络架构，其区别是训练监督信号不同，强分支的监督信号是强标签，弱分支的监督信号是弱标签。这种处理不同监督信号的方式比较新颖的，因为现存的半监督语义分割方法基本都是单分支的网络，多分支网络也并没有做不同监督信号的处理，两个分支有独立的参数 fs(z)f_s(z)fs (z) 和 fw(z)f_w(z)fw (z) ，损失函数是交叉熵损失，两个分支的损失权重相同：ldata=lce(ss,ms)+lce(sw,mw)l_{data} = l_{ce}(s^s,m^s) + l_{ce}(s^w,m^w)ldata =lce (ss,ms)+lce (sw,mw)\n\n3.3 为什么双分支网络有帮助\n\n在训练过程中，我们需要将使用同等多的强标签样本和弱标签样本，在半监督语义分割任务中，这里往往存在大量的弱监督样本，所以在弱监督数据迭代完之前强监督样本已经重复了很多次，实际上是对强监督样本过采样。这种方式能够减轻样本不平衡的影响\n\n双分支网络避免了不同监督信息的互相干扰，可以较好地消除监督不一致的现象。额外的弱监督样本提供了对象的大概位置，从而在一定程度上将正则化引入了 backbone 中，提升了网络的泛化能力。\n\n3.4 实施细节\n\n当网络训练完成后，弱分支就不再被需要了，因为弱监督样本的信息已经被编码到 backbone 以及 neck 模块中了，所以在推理阶段只需要强监督分支的结果即可\n\n# 论文的背景\n\n想利用双分支网络，给予不同的监督信号，弱监督样本起到的其实是正则化的作用，而过采样强监督样本是有帮助的。\n\n# 总结\n\n# 论文的贡献\n\n论文主要的贡献是着眼于改进半监督语义分割的 image-level 样本的利用方式，作者使用共享 backbone 和 neck的双分支网络，分为 strong 分支以及 weak 分支，将强监督样本送入 strong 分支，将弱监督样本送入 weak 分支，可以较好的消除监督不一致的现象，有助于缓解强弱样本不平衡问题。训练完成后，弱分支就不再需要了，其在训练过程中起到正则化的作用，加强了 backbone 以及 neck 网络的泛化能力。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * 半监督分割模型semi-supervised semantic segmentation via strong-weak dual-branch network论文阅读笔记\n\n * https://www.ecva.net/papers/eccv_2020/papers_eccv/papers/123500766.pdf",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"(HPT) Self-Supervised Pretraining Improves Self-Supervised Pretraining",frontmatter:{title:"(HPT) Self-Supervised Pretraining Improves Self-Supervised Pretraining",date:"2021-04-14T23:37:22.000Z",permalink:"/pages/682176/",categories:["论文阅读","自监督学习"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/11.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/06.(HPT)%20Self-Supervised%20Pretraining%20Improves%20Self-Supervised%20Pretraining.html",relativePath:"01.学术搬砖/11.论文阅读-自监督学习/06.(HPT) Self-Supervised Pretraining Improves Self-Supervised Pretraining.md",key:"v-1c7c9f96",path:"/pages/682176/",headers:[{level:3,title:"Self-Supervised Pretraining Improves Self-Supervised Pretraining",slug:"self-supervised-pretraining-improves-self-supervised-pretraining",normalizedTitle:"self-supervised pretraining improves self-supervised pretraining",charIndex:2}],headersStr:"Self-Supervised Pretraining Improves Self-Supervised Pretraining",content:"# Self-Supervised Pretraining Improves Self-Supervised Pretraining\n\n# 作者：UC Berkeley\n\n# 代码：Repository providing a wide range of self-supervised pretrained models for computer vision tasks.\n\n# 摘要\n\n自监督的预训练在很多视觉任务上被证明是有效的，但其要求也较高，较大的计算开销，较长的学习时间以及大规模的数据，并且其对数据增广的方法是十分敏感的。先前的工作表明：与其目标域数据大相径庭的源域上进行模型的预训练（例如用于医学影像领域的模型在 ImageNet 上预训练），会比随机初始化重头训的模型性能还要差。\n\n本篇论文探索了 HPT （分层预训练范式），通过已有的预训练模型来初始化预训练进程可以减少拟合时间，并提升精度。通过在 16 个视觉数据集上的实验，HPT 的拟合速度可以比以往方法快 80倍，提升了模型的精度，并且还提升了自监督过程中对于数据增广方法的以及预训练数据量的鲁棒性。总而言之，HPT 提供了一个简单的框架利用更低的计算资源获得更好的预训练表示。\n\n# 论文的目的及结论\n\n * 降低计算开销\n\n * 提升拟合速度以及模型精度\n\n * 提升自监督方法的鲁棒性\n\n# 论文的实验\n\n# 4.1 实验数据集介绍\n\n文章在很多数据及上做了评估，包括航空领域的的 xView 和 RESISC 的数据集，自动驾驶的 BDD 以及 VIPER 数据集，医学影像的 Chexpert 和 Chest-X-ray-kids 数据集，自然界的 COCO-2014 以及 Pascal VOC 2007+2012，以及 DomainNet 和 Oxford Flowers。\n\n# 4.2 实验设置介绍\n\n自监督的预训练通常都使用如下的三种评估方法：\n\n * 可分离性：测试线性模型能否根据学到的特征区分数据集上的不同类别，好的特征表示应当是线性可分的\n * 迁移性：测试模型在新的数据集和任务上 finetune 后的性能，好的特征表示会更容易泛化到下游的任务中。\n * 半监督：测试模型在有限数据标注下的性能，越好的特征表示会有越少的性能下降。\n\n在实验中，作者使用 MoCov2 作为自监督算法，MoCov2 使用了 InfoNCE 损失函数，是很多基于对比学习的预训练算法的核心。所有的训练都是使用 4 个 GPU 在标准的ResNet-50 backbone 下进行，使用 MoCo 的默认训练参数（超参数都放在附录中）。文章定义了四个自监督预训练的策略：\n\n * Base：将训了 800 个 epoch 的 MoCov2 模型迁移，并且使用目标数据集更新 BN 的非训练的mean （均值）和varience（方差），作者提出调整BN的均值和方差可以提高迁移学习的性能\n * Target：在目标数据集上随机初始化参数，使用 MoCov2 进行训练\n * HPT：使用 MoCov2 论文中的在ImageNet上训了 800 epoch 的模型作为初始模型，在对目标数据集进行预训练之前，选择性地对源数据集进行预训练。\n * HPT-BN：只训练 BN 的均值和方差两个参数\n\n现有的工作在进行性能评估时都十分依赖预训练的超参数，但是在实践中无法使用未标记数据的评估效果来调整超参数，因此，为了强调 HPT 的实践效果，我们使用了默认的训练超参数，batch size 设置为256。\n\n# 4.3 定量的实验结论与分析\n\n4.3.1 线性可分的分析\n\n\n\n首先通过一个线性可分的评估器评估了所提取特征的质量。将 batchsize 设置为 512 训练一个线性模型，最高学习率为{0.3, 3, 30}（？？？迷惑）。与文献[29] 类似的是，我们也使用steps而不是epochs来允许夸数据集的计算代价比较。对于Target 预训练而言，作者训练了{5k, 50k, 100k, 200k, 400k} steps，如果在100k 到200k steps 中性能提升的话我们才训练400k steps。为了做参考，单张P100的 GPU-Day 为25k steps。我们预训练 HPT 对于很多{50，500，5k，500k} steps，HPT-BN 预训练 5k steps后我们可以观察到性能上的微小变化。\n\n上图是他们论文的 Linear separability evaluation 实验结果。第一步都是在 ImageNet (Base)上训800 个 epoch，第二步有两种实验设置，\n\n * 以 HPT Base-Target 的方法训 50、500、5k、50k 个 iteration\n * 以 HPT Base-Target(BN) 的方法训5k、50k、100k、200k 、400个 iteration\n\n在16个数据集中的15个都观察到了 HPT 在 5k steps后就基本收敛了，并且这一现象与目标数据集的大小无关。 HPT 以及 HPT-BN 比Base transfer 以及 Target Pretain 组表现的更好，即使他们训了400k steps，这一速度快了80 倍。唯一一个Target pretraining 优于 HPT 的数据集是 quickdraw （一个众包收集的庞大的绘画二分类数据集）这表明如果存在较大的域间差异，直接的 transfer 的话是不太 work 的。\n\nHPT 在很多数据集上都提升了性能，在和 ImageNet 十分相似的数据集上和Base transfer 相近的表现，在两个医学数据集上 HPT 和 Base Transfer 也有相近的性能，但 HPT 只需要训 5k steps，而Base Transfer 需要训200k 和 100k 个 step。\n\n此外，HPT 在 5k steps 之后出现过拟合现象，尤其是在较小的数据集上，因此作者建议采用非常短的 HPT 预训练步骤，例如 5k iterations，并且这与数据集大小无关。\n\n4.3.2 半监督的迁移能力分析\n\n\n\n该实验测试的是额外预训练的收益是否会在 finetuning 模型参数的过程中被抵消。对于每个预训练策略，我们选择了在线性可分的实验中表现最优的模 型，使用了1000个随机挑选的标签（没有考虑类别均衡），但每个类别都会至少出现一次。作者使用了两种学习率的组合（0.01，0.001），以及两种finetuning 的 schedules （2500 steps，90 epochs），batch size 设置为 512，记录了每个数据集和模型最优的性能\n\n上图是他们论文的 Semi-supervised evaluation 实验结果，验证了方法的半监督 finetuning 的性能。带条纹的 bar 就是 HPT pretraining 的结果，作者观察到和线性可分性验证类似的结果，HPT 在 其中 15 个数据集上都有最好的性能，除了 quickdraw 数据集（domain gap 过大）。一个关键的结论是 HPT 在半监督的实验设置下也是有性能增益的，HPT 和 Base model 的特征表示有足够的区分度，以至于完整模型的 finetuning 也无法说明变化。\n\n作者还注意到 HPT-BN 在 线性可分性实验中有时会比 HPT 好，但是在finetuning 所有参数的时候HPT-BN 从未超过HPT 的性能。这个结果表明：预训练只调整 BN 参数获得的性能增益，在有监督的 finetuning 下是冗余的。还有一个发现是：Base 和 Target pretraining 的性能表现 对数据集是高度依赖的，但HPT 是一直都有较好的性能。\n\n# 4.3 预训练的质量分析\n\n\n\n这个实验探索的是，首先在源域上进行 pretraining，再在目标域上进行 pretraining，最后再transferring 任务上的 HPT 的性能。测试了三个目标域的数据集：Chest-X-ray-kids，sketch，UC-Merced。我们为每一个目标域都选择了源域，选择超过 Base model 最优的性能。生成了三种实验\n\n * ImageNet -> Chexpert -> Chest-X-ray-kids\n * ImageNet -> clipart -> sketch\n * ImageNet -> RESISC -> UC-Merced\n\n上图比较了对目标域的1000个数据的子集进行finetuning，测试了如下策略\n\n * B：直接使用Base model\n * T：直接使用Target model\n * B+S：Base 后使用 Source pretraining\n * B+T：Base 后使用 Target pretraining\n * B+S+T：Base 后使用 Source pretraining 再使用 Target pretraining\n * B+S+T-BN：Base 后使用 Source pretraining 再使用 Target pretraining BN 的参数\n\n结论表明，HPT 策略取得了最优性能。\n\n# 4.4 对下游任务（检测和分割）的迁移分析\n\n**使用的模型：**对于Pascal 和 BDD数据集，对 Faster R-CNN R50-C4 模型使用HPT 预训练策略，再在目标数据集上进行 finetune。对于COCO数据集，使用Mask-RCNN-C4。每个实验跑三次，记录 COCO AP 的中位数结果。\n\n**使用的train/valid/test划分：**对于Pascal，在train2007+2012数据集上做finetuning，在test2007上做测试。对于BDD数据集，使用官方的train/test 的划分，从训练集中挑10k的图像作为验证集。对于COCO数据集，使用2017版的划分，在1x的schedule下进行训练\n\n\n\n上面的表格比较了各个策略在三个数据集上的性能。HPT策略（BT 以及 BT-BN）的性能都要优于常用的Target 和 Base 方法。Pascal数据集上的 B-S-T 策略在pretraining 所有模型参数的时候有性能提升，并且如果仅仅对BN的参数进行pretraining的时候，结果仍然保持一致。这表明，虽然 BN 的参数可以找到更好的pretraining模型，但是普通的 从 source-target 的pretraining 却不总是能够带来性能的增益。\n\n跨数据集来看，整体的性能收益较小，但是我们认为这些结果表明，无论是 MoCo 在ImageNet 上的pretraining 还是 在目标数据集上的finetuning任务，HPT 没有直接学习冗余信息。还有一个发现是：在目标检测任务上，仅仅tuning BN 的参数也可以带来性能的增益。\n\n# 4.5 HPT 的鲁棒性\n\n这一部分研究了 HPT 对影响自监督预训练（例如增强策略和预训练数据集大小）有效性的常见因素的鲁棒性。对于这些鲁棒性实验，我们使用了BDD，RESISC和Chexpert数据集，因为它们提供了数据域和大小的多样性。我们使用与第4.2节相同的超参数来测量线性可分性。\n\n数据增强策略的鲁棒性\n\n\n\n预训练数据集规模的鲁棒性\n\n\n\n# 4.6 Domain Adaptation 的相关分析\n\n作者将HPT 应用于Domain Adaptation。目标是给定源域上有标注的数据进行训练，然后在未见过的目标域上进行图像分类任务。训练的步骤如下：\n\n在标准的 MSRA ImageNet 模型的基础上，使用 HPT 策略在源域和目标域数据集上再去训一个模型。用这个模型去初始化MME（一种半监督方法） 的特征提取器。在每个budget level 的末期，对目标域的整个测试集做性能的评估。我们在拥有 7个 budget level，345个类别的DomainNet 数据及上开展了实验，随着目标标签的增加\n\n * from real to clip\n * from real to sketch\n\n使用 EfficientNet_B2 作为backbone\n\n\n\n# 论文的方法\n\n\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\nHPT 的 novelty 是什么？\n\n# 论文的不足\n\n# 论文如何讲故事\n\n#",normalizedContent:"# self-supervised pretraining improves self-supervised pretraining\n\n# 作者：uc berkeley\n\n# 代码：repository providing a wide range of self-supervised pretrained models for computer vision tasks.\n\n# 摘要\n\n自监督的预训练在很多视觉任务上被证明是有效的，但其要求也较高，较大的计算开销，较长的学习时间以及大规模的数据，并且其对数据增广的方法是十分敏感的。先前的工作表明：与其目标域数据大相径庭的源域上进行模型的预训练（例如用于医学影像领域的模型在 imagenet 上预训练），会比随机初始化重头训的模型性能还要差。\n\n本篇论文探索了 hpt （分层预训练范式），通过已有的预训练模型来初始化预训练进程可以减少拟合时间，并提升精度。通过在 16 个视觉数据集上的实验，hpt 的拟合速度可以比以往方法快 80倍，提升了模型的精度，并且还提升了自监督过程中对于数据增广方法的以及预训练数据量的鲁棒性。总而言之，hpt 提供了一个简单的框架利用更低的计算资源获得更好的预训练表示。\n\n# 论文的目的及结论\n\n * 降低计算开销\n\n * 提升拟合速度以及模型精度\n\n * 提升自监督方法的鲁棒性\n\n# 论文的实验\n\n# 4.1 实验数据集介绍\n\n文章在很多数据及上做了评估，包括航空领域的的 xview 和 resisc 的数据集，自动驾驶的 bdd 以及 viper 数据集，医学影像的 chexpert 和 chest-x-ray-kids 数据集，自然界的 coco-2014 以及 pascal voc 2007+2012，以及 domainnet 和 oxford flowers。\n\n# 4.2 实验设置介绍\n\n自监督的预训练通常都使用如下的三种评估方法：\n\n * 可分离性：测试线性模型能否根据学到的特征区分数据集上的不同类别，好的特征表示应当是线性可分的\n * 迁移性：测试模型在新的数据集和任务上 finetune 后的性能，好的特征表示会更容易泛化到下游的任务中。\n * 半监督：测试模型在有限数据标注下的性能，越好的特征表示会有越少的性能下降。\n\n在实验中，作者使用 mocov2 作为自监督算法，mocov2 使用了 infonce 损失函数，是很多基于对比学习的预训练算法的核心。所有的训练都是使用 4 个 gpu 在标准的resnet-50 backbone 下进行，使用 moco 的默认训练参数（超参数都放在附录中）。文章定义了四个自监督预训练的策略：\n\n * base：将训了 800 个 epoch 的 mocov2 模型迁移，并且使用目标数据集更新 bn 的非训练的mean （均值）和varience（方差），作者提出调整bn的均值和方差可以提高迁移学习的性能\n * target：在目标数据集上随机初始化参数，使用 mocov2 进行训练\n * hpt：使用 mocov2 论文中的在imagenet上训了 800 epoch 的模型作为初始模型，在对目标数据集进行预训练之前，选择性地对源数据集进行预训练。\n * hpt-bn：只训练 bn 的均值和方差两个参数\n\n现有的工作在进行性能评估时都十分依赖预训练的超参数，但是在实践中无法使用未标记数据的评估效果来调整超参数，因此，为了强调 hpt 的实践效果，我们使用了默认的训练超参数，batch size 设置为256。\n\n# 4.3 定量的实验结论与分析\n\n4.3.1 线性可分的分析\n\n\n\n首先通过一个线性可分的评估器评估了所提取特征的质量。将 batchsize 设置为 512 训练一个线性模型，最高学习率为{0.3, 3, 30}（？？？迷惑）。与文献[29] 类似的是，我们也使用steps而不是epochs来允许夸数据集的计算代价比较。对于target 预训练而言，作者训练了{5k, 50k, 100k, 200k, 400k} steps，如果在100k 到200k steps 中性能提升的话我们才训练400k steps。为了做参考，单张p100的 gpu-day 为25k steps。我们预训练 hpt 对于很多{50，500，5k，500k} steps，hpt-bn 预训练 5k steps后我们可以观察到性能上的微小变化。\n\n上图是他们论文的 linear separability evaluation 实验结果。第一步都是在 imagenet (base)上训800 个 epoch，第二步有两种实验设置，\n\n * 以 hpt base-target 的方法训 50、500、5k、50k 个 iteration\n * 以 hpt base-target(bn) 的方法训5k、50k、100k、200k 、400个 iteration\n\n在16个数据集中的15个都观察到了 hpt 在 5k steps后就基本收敛了，并且这一现象与目标数据集的大小无关。 hpt 以及 hpt-bn 比base transfer 以及 target pretain 组表现的更好，即使他们训了400k steps，这一速度快了80 倍。唯一一个target pretraining 优于 hpt 的数据集是 quickdraw （一个众包收集的庞大的绘画二分类数据集）这表明如果存在较大的域间差异，直接的 transfer 的话是不太 work 的。\n\nhpt 在很多数据集上都提升了性能，在和 imagenet 十分相似的数据集上和base transfer 相近的表现，在两个医学数据集上 hpt 和 base transfer 也有相近的性能，但 hpt 只需要训 5k steps，而base transfer 需要训200k 和 100k 个 step。\n\n此外，hpt 在 5k steps 之后出现过拟合现象，尤其是在较小的数据集上，因此作者建议采用非常短的 hpt 预训练步骤，例如 5k iterations，并且这与数据集大小无关。\n\n4.3.2 半监督的迁移能力分析\n\n\n\n该实验测试的是额外预训练的收益是否会在 finetuning 模型参数的过程中被抵消。对于每个预训练策略，我们选择了在线性可分的实验中表现最优的模 型，使用了1000个随机挑选的标签（没有考虑类别均衡），但每个类别都会至少出现一次。作者使用了两种学习率的组合（0.01，0.001），以及两种finetuning 的 schedules （2500 steps，90 epochs），batch size 设置为 512，记录了每个数据集和模型最优的性能\n\n上图是他们论文的 semi-supervised evaluation 实验结果，验证了方法的半监督 finetuning 的性能。带条纹的 bar 就是 hpt pretraining 的结果，作者观察到和线性可分性验证类似的结果，hpt 在 其中 15 个数据集上都有最好的性能，除了 quickdraw 数据集（domain gap 过大）。一个关键的结论是 hpt 在半监督的实验设置下也是有性能增益的，hpt 和 base model 的特征表示有足够的区分度，以至于完整模型的 finetuning 也无法说明变化。\n\n作者还注意到 hpt-bn 在 线性可分性实验中有时会比 hpt 好，但是在finetuning 所有参数的时候hpt-bn 从未超过hpt 的性能。这个结果表明：预训练只调整 bn 参数获得的性能增益，在有监督的 finetuning 下是冗余的。还有一个发现是：base 和 target pretraining 的性能表现 对数据集是高度依赖的，但hpt 是一直都有较好的性能。\n\n# 4.3 预训练的质量分析\n\n\n\n这个实验探索的是，首先在源域上进行 pretraining，再在目标域上进行 pretraining，最后再transferring 任务上的 hpt 的性能。测试了三个目标域的数据集：chest-x-ray-kids，sketch，uc-merced。我们为每一个目标域都选择了源域，选择超过 base model 最优的性能。生成了三种实验\n\n * imagenet -> chexpert -> chest-x-ray-kids\n * imagenet -> clipart -> sketch\n * imagenet -> resisc -> uc-merced\n\n上图比较了对目标域的1000个数据的子集进行finetuning，测试了如下策略\n\n * b：直接使用base model\n * t：直接使用target model\n * b+s：base 后使用 source pretraining\n * b+t：base 后使用 target pretraining\n * b+s+t：base 后使用 source pretraining 再使用 target pretraining\n * b+s+t-bn：base 后使用 source pretraining 再使用 target pretraining bn 的参数\n\n结论表明，hpt 策略取得了最优性能。\n\n# 4.4 对下游任务（检测和分割）的迁移分析\n\n**使用的模型：**对于pascal 和 bdd数据集，对 faster r-cnn r50-c4 模型使用hpt 预训练策略，再在目标数据集上进行 finetune。对于coco数据集，使用mask-rcnn-c4。每个实验跑三次，记录 coco ap 的中位数结果。\n\n**使用的train/valid/test划分：**对于pascal，在train2007+2012数据集上做finetuning，在test2007上做测试。对于bdd数据集，使用官方的train/test 的划分，从训练集中挑10k的图像作为验证集。对于coco数据集，使用2017版的划分，在1x的schedule下进行训练\n\n\n\n上面的表格比较了各个策略在三个数据集上的性能。hpt策略（bt 以及 bt-bn）的性能都要优于常用的target 和 base 方法。pascal数据集上的 b-s-t 策略在pretraining 所有模型参数的时候有性能提升，并且如果仅仅对bn的参数进行pretraining的时候，结果仍然保持一致。这表明，虽然 bn 的参数可以找到更好的pretraining模型，但是普通的 从 source-target 的pretraining 却不总是能够带来性能的增益。\n\n跨数据集来看，整体的性能收益较小，但是我们认为这些结果表明，无论是 moco 在imagenet 上的pretraining 还是 在目标数据集上的finetuning任务，hpt 没有直接学习冗余信息。还有一个发现是：在目标检测任务上，仅仅tuning bn 的参数也可以带来性能的增益。\n\n# 4.5 hpt 的鲁棒性\n\n这一部分研究了 hpt 对影响自监督预训练（例如增强策略和预训练数据集大小）有效性的常见因素的鲁棒性。对于这些鲁棒性实验，我们使用了bdd，resisc和chexpert数据集，因为它们提供了数据域和大小的多样性。我们使用与第4.2节相同的超参数来测量线性可分性。\n\n数据增强策略的鲁棒性\n\n\n\n预训练数据集规模的鲁棒性\n\n\n\n# 4.6 domain adaptation 的相关分析\n\n作者将hpt 应用于domain adaptation。目标是给定源域上有标注的数据进行训练，然后在未见过的目标域上进行图像分类任务。训练的步骤如下：\n\n在标准的 msra imagenet 模型的基础上，使用 hpt 策略在源域和目标域数据集上再去训一个模型。用这个模型去初始化mme（一种半监督方法） 的特征提取器。在每个budget level 的末期，对目标域的整个测试集做性能的评估。我们在拥有 7个 budget level，345个类别的domainnet 数据及上开展了实验，随着目标标签的增加\n\n * from real to clip\n * from real to sketch\n\n使用 efficientnet_b2 作为backbone\n\n\n\n# 论文的方法\n\n\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\nhpt 的 novelty 是什么？\n\n# 论文的不足\n\n# 论文如何讲故事\n\n#",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"SPICE Semantic Pseudo-labeling for Image Clustering",frontmatter:{title:"SPICE Semantic Pseudo-labeling for Image Clustering",date:"2021-09-06T13:05:28.000Z",permalink:"/pages/a5a6bb/",categories:["学术搬砖","论文阅读-小样本学习"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/10.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/00.SPICE%20Semantic%20Pseudo-labeling%20for%20Image%20Clustering.html",relativePath:"01.学术搬砖/10.论文阅读-小样本学习/00.SPICE Semantic Pseudo-labeling for Image Clustering.md",key:"v-b995e478",path:"/pages/a5a6bb/",headers:[{level:2,title:"SPICE: Semantic Pseudo-labeling for Image Clustering",slug:"spice-semantic-pseudo-labeling-for-image-clustering",normalizedTitle:"spice: semantic pseudo-labeling for image clustering",charIndex:2}],headersStr:"SPICE: Semantic Pseudo-labeling for Image Clustering",content:"# SPICE: Semantic Pseudo-labeling for Image Clustering\n\n# 单位：Rensselaer Polytechnic Institute\n\n# 作者：Chuang Niu, Ge Wang\n\n# 发表：Arxiv\n\n# 摘要\n\nHowever, these methods have limited performance when directly using the label features to measure the similarity among samples. This is because the category-level features lose too much instance-level information to accurately measure the relations of instances\n\nSCAN [34] was proposed to leverage the embedding features of a representation learning model to search for similar samples across the whole dataset, and then courage the model to output the same labels for similar instances, achieving significantly better results.\n\nHowever, the local nearest samples in the embedding space do not alwayshave the same semantics especially when the samples liearound the borderlines of different clusters as shown in Fig.1-(a) and (c), which may compromise the performance\n\n\n\nwe call the similarity between the cluster prototype and instance samples as the semantic similarity,\n\nTo this end, we propose a semantic pseudo-labeling method that as-signs the same cluster label to semantically similar samples\n\nSpecifically, for each batch of samples, we use the most confident samples predicted by the classification model to compute the prototype of each cluster, and then spread thesemantic labels of prototypes to their neighbors based onthe semantic similarity.\n\nGiven these pseudo labels, the clas-sification model can be optimized using the classification loss directly, which is then used to compute prototypes in the next iteration.\n\nTo reduce the inconsistency of similar samples, we design a local consistency principle to select a set of reliably labeled images from the clustering results as in Fig. 1-(b), and thenreformulate the unsupervised task into a semi-supervisedpseudo-labeling process for performance boosting.\n\n类的 prototype 如何计算？\n\n * 使用分类模型最 confident 的样本\n\n分类模型可以使用伪标签优化，在下一个 iteration 再来计算 prototype。\n\nlocal consistency principle 来选择最可靠的标记图像。\n\nSPICE framework that explicitly leverages both the\n\n * discrepancy among semantic clusters\n * the similarity among instance samples\n * to adaptively label training samples in batch-wise.\n\nMain Contributions:\n\n * 使用 pseudo labels 以及 cls loss 来训练分类网络，利用语义类间的差异和样本实例间的相似性\n * 提出新的 pseudo-labelding 方法来利用语义一致性，减小在 borderline 上的样本的语义不一致性\n * 设计了 double softmax 交叉熵损失函数来让模型做出 confident 的预测，能够提升聚类的性能\n * 设计了 local consistency principle 有效地减少 semantic inconsistency，将原始的聚类问题转化为半监督学习范式\n * SPICE 在留个图像聚类的 benchmark 上达到了SOTA\n\nActually, these methods aim to train the classification model in the unsupervised setting while using multiple indirect loss functions, such as sample relations [5], invariant information [21, 27], mutual information [35], partition confidence maximisation [17], attention [28], and entropy[28, 17, 34, 27].\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n用 MoCo-v2 来做表示学习\n\nweak augmentation: Fixmatch 中的 flip-and-shift augmentation\n\nstrong augmentation：SCAN 中相同的策略\n\nM 设置为1000，10个类别\n\nm2 设置为 128\n\nSPICE-self 有 10个 CLSHead， 性能最好的 head 会被选做最后的head\n\nconfident ratio 设置为 0.5\n\n# 论文的方法\n\nSPICE 框架分为三个主要的阶段\n\n * 首先预训练一个无监督的表示模型，从 SCAN 中修改而来，接着该 CNN 冻结参数，在下面两个阶段提取embedding feature\n * SPICE-Self 目标是基于无监督设置下提取的特征训练一个分类模型，SPICE-Slef 有三个分支\n   * 第一个分支将原始的图像作为输入，输出 embedding feature\n   * 第二个分支将weakly transformed 图像作为输入，预测语义标签\n   * 第三个分支将前两个分支的输出作为输入，使用基于语义相似性的 pseudo-labeling 算法生成伪标签，来用于第三个分支的监督信号\n   * SPICE-Self 只需要训分类模型的 light-weight classification head 即可\n * SPICE-Semi 首先基于 SPICE-self 结果的局部语义一致性决定一个 reliably labeled set，就把聚类问题转化成半监督学习问题\n\nSPICE-Self 算法\n\n\n\n# 总结\n\n# 论文的贡献\n\n论文主要是针对弱监督语义分割提出了 Puzzle 模块，将原始图像分块后再算一个CAMs，并与原始的 CAMs 做一个重建损失，三项损失联合优化分类网络，提升 CAMs 的精度。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://paperswithcode.com/paper/spice-semantic-pseudo-labeling-for-image",normalizedContent:"# spice: semantic pseudo-labeling for image clustering\n\n# 单位：rensselaer polytechnic institute\n\n# 作者：chuang niu, ge wang\n\n# 发表：arxiv\n\n# 摘要\n\nhowever, these methods have limited performance when directly using the label features to measure the similarity among samples. this is because the category-level features lose too much instance-level information to accurately measure the relations of instances\n\nscan [34] was proposed to leverage the embedding features of a representation learning model to search for similar samples across the whole dataset, and then courage the model to output the same labels for similar instances, achieving significantly better results.\n\nhowever, the local nearest samples in the embedding space do not alwayshave the same semantics especially when the samples liearound the borderlines of different clusters as shown in fig.1-(a) and (c), which may compromise the performance\n\n\n\nwe call the similarity between the cluster prototype and instance samples as the semantic similarity,\n\nto this end, we propose a semantic pseudo-labeling method that as-signs the same cluster label to semantically similar samples\n\nspecifically, for each batch of samples, we use the most confident samples predicted by the classification model to compute the prototype of each cluster, and then spread thesemantic labels of prototypes to their neighbors based onthe semantic similarity.\n\ngiven these pseudo labels, the clas-sification model can be optimized using the classification loss directly, which is then used to compute prototypes in the next iteration.\n\nto reduce the inconsistency of similar samples, we design a local consistency principle to select a set of reliably labeled images from the clustering results as in fig. 1-(b), and thenreformulate the unsupervised task into a semi-supervisedpseudo-labeling process for performance boosting.\n\n类的 prototype 如何计算？\n\n * 使用分类模型最 confident 的样本\n\n分类模型可以使用伪标签优化，在下一个 iteration 再来计算 prototype。\n\nlocal consistency principle 来选择最可靠的标记图像。\n\nspice framework that explicitly leverages both the\n\n * discrepancy among semantic clusters\n * the similarity among instance samples\n * to adaptively label training samples in batch-wise.\n\nmain contributions:\n\n * 使用 pseudo labels 以及 cls loss 来训练分类网络，利用语义类间的差异和样本实例间的相似性\n * 提出新的 pseudo-labelding 方法来利用语义一致性，减小在 borderline 上的样本的语义不一致性\n * 设计了 double softmax 交叉熵损失函数来让模型做出 confident 的预测，能够提升聚类的性能\n * 设计了 local consistency principle 有效地减少 semantic inconsistency，将原始的聚类问题转化为半监督学习范式\n * spice 在留个图像聚类的 benchmark 上达到了sota\n\nactually, these methods aim to train the classification model in the unsupervised setting while using multiple indirect loss functions, such as sample relations [5], invariant information [21, 27], mutual information [35], partition confidence maximisation [17], attention [28], and entropy[28, 17, 34, 27].\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n用 moco-v2 来做表示学习\n\nweak augmentation: fixmatch 中的 flip-and-shift augmentation\n\nstrong augmentation：scan 中相同的策略\n\nm 设置为1000，10个类别\n\nm2 设置为 128\n\nspice-self 有 10个 clshead， 性能最好的 head 会被选做最后的head\n\nconfident ratio 设置为 0.5\n\n# 论文的方法\n\nspice 框架分为三个主要的阶段\n\n * 首先预训练一个无监督的表示模型，从 scan 中修改而来，接着该 cnn 冻结参数，在下面两个阶段提取embedding feature\n * spice-self 目标是基于无监督设置下提取的特征训练一个分类模型，spice-slef 有三个分支\n   * 第一个分支将原始的图像作为输入，输出 embedding feature\n   * 第二个分支将weakly transformed 图像作为输入，预测语义标签\n   * 第三个分支将前两个分支的输出作为输入，使用基于语义相似性的 pseudo-labeling 算法生成伪标签，来用于第三个分支的监督信号\n   * spice-self 只需要训分类模型的 light-weight classification head 即可\n * spice-semi 首先基于 spice-self 结果的局部语义一致性决定一个 reliably labeled set，就把聚类问题转化成半监督学习问题\n\nspice-self 算法\n\n\n\n# 总结\n\n# 论文的贡献\n\n论文主要是针对弱监督语义分割提出了 puzzle 模块，将原始图像分块后再算一个cams，并与原始的 cams 做一个重建损失，三项损失联合优化分类网络，提升 cams 的精度。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://paperswithcode.com/paper/spice-semantic-pseudo-labeling-for-image",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Learning from Pixel-Level Label Noise A NewPerspective for Semi-Supervised SemanticSegmentation",frontmatter:{title:"Learning from Pixel-Level Label Noise A NewPerspective for Semi-Supervised SemanticSegmentation",date:"2021-05-07T16:46:58.000Z",permalink:"/pages/c672e6/",categories:["论文阅读","半监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/08.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/00.Learning%20from%20Pixel-Level%20Label%20Noise%20A%20NewPerspective%20for%20Semi-Supervised%20SemanticSegmentation.html",relativePath:"01.学术搬砖/08.论文阅读-半监督图像分割/00.Learning from Pixel-Level Label Noise A NewPerspective for Semi-Supervised SemanticSegmentation.md",key:"v-a1f0ce22",path:"/pages/c672e6/",headers:[{level:2,title:"Learning from Pixel-Level Label Noise: A New Perspective for Semi-Supervised Semantic Segmentation",slug:"learning-from-pixel-level-label-noise-a-new-perspective-for-semi-supervised-semantic-segmentation",normalizedTitle:"learning from pixel-level label noise: a new perspective for semi-supervised semantic segmentation",charIndex:2}],headersStr:"Learning from Pixel-Level Label Noise: A New Perspective for Semi-Supervised Semantic Segmentation",content:"# Learning from Pixel-Level Label Noise: A New Perspective for Semi-Supervised Semantic Segmentation\n\n# 作者：北交团队\n\n# 发表：Arxiv\n\n# 摘要\n\n这篇文章旨在解决半监督的语义分割问题，该问题定义为：拥有少许的像素级别的标签（强监督信号）以及大量图像级别的标签（弱监督信号）的语义分割。许多现有的方法致力于以图像级别的标签生成精确的像素级别的标注，然而我们观察到这些生成的标签包含许多噪声标签。基于这个 Motivation，作者提出以像素级的带噪学习视角来建模此问题。现存的标签带噪的方法，主要集中在图像分类的任务上，不能捕捉到一张图像中相邻像素标签之间的关系。所以，作者提出了基于图的标签噪声检测和纠正的框架来处理像素级别的带噪标签。对于使用 CAM 技术生成的像素级带噪标签，我们使用强监督信号训一个干净的分割模型来从这些带噪的标签中检测出干净的标签，使用的 loss 是交叉熵损失。\n\n然后，作者使用了超像素图来表示像素间的空间相邻性以及语义相似性。最后我们使用检测出的干净标签来训一个图注意力网络，并用其来修正带噪标签。在PASCAL VOC 2012，PASCAL-Context 以及 MS-COCO 数据及上做了实验，并达到了sota的结果，并且超越了一些全监督的模型。\n\n# 阅读\n\n# 论文的目的及结论\n\n论文想从带噪学习的角度来建模半监督语义分割问题，使用的方法是先对 CAM 生成的伪标签进行 clean 以及 noisy 的分类，再使用 clean 的标签训一个图注意力网络来修正 noisy 的标签，最终使用所有的图像再来全监督的去训最后的模型。\n\n# 论文的实验\n\n\n\n图一和图二展示了在 PASCAL VOC 2012 上的 mIoU 随着 DcD^cDc 增大的结果变化。\n\n\n\n图三展示了和其他方法半监督语义分割方法的比较，图四展示了不同 DcD^cDc 下的分割模型结果，作者提出的方法甚至可以超过全监督的性能\n\n\n\n图五是消融实验，展示了在有无噪声检测以及有无标签修正情况下的模型性能。图六也是消融实验，展示了 CRF 在伪标签和分割中的作用。\n\n# 论文的方法\n\n\n\n这是论文的方法总览，论文将其分为四个阶段。\n\n * 首先，使用 CAM 生成 pixel-level 的标签\n * 为了确保能够较为精确的检测出相对干净的伪标签，利用少量的强监督样本训练一个干净的分割模型\n * 利用基于超像素的图建模图中像素的空间相邻性和语义相似性，利用 GAT 来修正带噪标签\n * 修正后的标签作为伪标签来训一个分割网络\n\n下面将分别介绍每个阶段的细节\n\n# 1、从弱标注中生成 pixel-level 的标签\n\n使用 CAM 来生成分割标签，架构就是典型的分类网络，在 GAP （Global Average Pooling）之后接全连接层。激活score 在0.05以下的视作背景，生成的伪标签作为初始标签\n\n# 2、检测干净以及带噪的标签\n\n先利用少量的强监督样本训练一个干净的分割模型，然后对每个弱监督样本都生成对应的预测。将该预测与 CAM生成的伪标签计算交叉熵损失来区分干净以及带噪的标签。并且设置一个阈值 θ\\thetaθ 来区分干净以及带噪的标签。利用这些干净的标签来训练后续的 GAT。\n\n由于该阶段对 θ\\thetaθ 会比较敏感，作者对强监督样本都做了预测以及 CAM 生成伪标签的步骤，根据强监督样本的 loss 分布可以较为容易的确定适合的 θ\\thetaθ 参数，更多详细的实验在之后的消融实验里。\n\n# 3、修正带噪标签\n\n借鉴了【34】的方法，建立了一个基于超像素的图来建模像素的空间相邻性和语义相似度，然后 clean 的标签就被编码到图中，最后使用 GAT 来修正带噪标签。作者提到 GCN 也可以用于修正，也做了关于 GCN 和 GAT 的对比。\n\n（1）超像素图的构建\n\n超像素可以提供一个更大的，局部均匀一致的区域，并且能够保留很多用于精确分割的结构信息。作者将一张图像转化为一个超像素的图 G=(V,E,A)G = (V,E,A)G=(V,E,A) ，其中 VVV 代表顶点集合，EEE 代表边的集合，AAA 代表邻接矩阵\n\n顶点构建：使用 SLIC [41] 的方法将一张图转换为一个超像素集合，作者的实验中一张图分割为了1000个超像素，并且使用了深层的特征图来捕捉高层的语义特征，将其与超像素进行整合，整合的过程中用了线性插值恢复到原图尺寸。最终对于每个超像素都能够得到 512 维的 CNN 特征向量\n\n边构建：利用图像中像素的空间相邻性和语义相似性来构造边。空间相邻性意味着相邻的像素更有可能有相似的标签，语义相似性意味着相同标签的像素可能共享了相似的语义信息。作者假设两个空间相邻的节点有相似的语义内容（同一类）。具体而言，先建立空间相邻矩阵 Wl=[wlij]n×nW_l = [w_l^{ij}]_{n\\times n}Wl =[wlij ]n×n ，如果相邻即为1，不相邻即为0。语义相似度矩阵Ws=[wsij]n×nW_s = [w_s^{ij}]_{n \\times n}Ws =[wsij ]n×n ，每个超像素有 512 维的 CNN 特征向量，彼此之间的语义相似度即为 wsij=wlij×exp(∣∣∣vi−vj∣2h)w_s^{ij} = w_l^{ij} \\times exp(\\frac{|||v_i-v_j|}{2h})wsij =wlij ×exp(2h∣∣∣vi −vj ∣ ) ，viv_ivi 和 vjv_jvj 是每个超像素的特征向量，hhh 是特征向量的维度，这里是 512。\n\n邻接矩阵生成： 即为下面的公式，如果语义相似度小于 γ\\gammaγ 且小于顶点 iii 所有邻接点语义相似度的最大值即为0，否则为1。此外，如果 aija_{ij}aij 为 0 的话，边 εij\\varepsilon_{ij}εij 也会被移除\n\n * γ=μ(Ws)−σ(Ws)\\gamma = \\mu (W_s)-\\sigma(W_s)γ=μ(Ws )−σ(Ws )\n * μ(⋅)\\mu(·)μ(⋅) 是均值\n * σ(⋅)\\sigma(·)σ(⋅) 是标准差\n\n\n\n（2）修正带噪标签\n\n编码干净伪标签：CAM生成的干净伪标签将被编码进图中作为监督信息来训练 GAT。为每个超像素分配类别的过程是多数原则\n\n利用 GAT 修正噪声标签：参考[15]，引入顶点上的自注意力机制以计算注意力系数，用于体现顶点 jjj 特征的重要性。为顶点 iii 的邻接顶点都计算一个注意力系数 eije_{ij}eij 。为了系数能够跨顶点进行比较，利用 softmax 函数做了标准化。然后使用多头注意力来实现以上的图注意力机制，每个节点最后的输出特征如下：\n\n\n\nαij\\alpha_{ij}αij 是由第 iii 头注意力机制标准化之后的注意力系数，WglW_g^lWgl 是相应的输入线性变化的权重矩阵，在实验中，作者实现了两层的 GAT 用于标签的修正。修正的模型为 Z=f(V,A)Z=f(V,A)Z=f(V,A)，VVV 是由式 (7) 计算得到的超像素特征矩阵，AAA 是由式 (4) 计算得到的邻接矩阵，所有干净伪标签的交叉熵损失被定义为：\n\n\n\npip_ipi 是超像素 spisp_ispi 所对应的标签，PPP 是超像素的数量，ziz_izi 则是 GAT 对于该超像素的输出\n\n# 5、训练分割网络\n\n带噪的伪标签被 GAT 修正之后，可以根据其对应的超像素来恢复每个像素的标签，然后通过 CRF 对修正的分割标签做细化，以更好的估计物体的形状。最终所有的样本用于全监督学习的训练\n\n# 论文的背景\n\n将半监督学习视作伪标签噪声问题，核心点有两个：如何区分带噪标签以及如何对带噪标签做修正。前者作者直接用CAM 生成的伪标签以及用干净数据训练的分割模型的预测做一个交叉熵损失得到分数，再通过分数阈值进行区分；后者作者使用基于图的方法对超像素的一些约束进行建模，建模过程言之有理，不过如果对像素进行修正的我没有看懂。准备发邮件给作者问下\n\n# 总结\n\n# 论文的贡献\n\n论文的主要贡献是从噪声的视角去看待半监督的语义分割任务，引入基于超像素的图建模图中像素的空间相邻性和语义相似性，利用 GAT 来修正带噪标签，也用到了CRF做细化，将修正后的标签一起再来训分割模型。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2103.14242",normalizedContent:"# learning from pixel-level label noise: a new perspective for semi-supervised semantic segmentation\n\n# 作者：北交团队\n\n# 发表：arxiv\n\n# 摘要\n\n这篇文章旨在解决半监督的语义分割问题，该问题定义为：拥有少许的像素级别的标签（强监督信号）以及大量图像级别的标签（弱监督信号）的语义分割。许多现有的方法致力于以图像级别的标签生成精确的像素级别的标注，然而我们观察到这些生成的标签包含许多噪声标签。基于这个 motivation，作者提出以像素级的带噪学习视角来建模此问题。现存的标签带噪的方法，主要集中在图像分类的任务上，不能捕捉到一张图像中相邻像素标签之间的关系。所以，作者提出了基于图的标签噪声检测和纠正的框架来处理像素级别的带噪标签。对于使用 cam 技术生成的像素级带噪标签，我们使用强监督信号训一个干净的分割模型来从这些带噪的标签中检测出干净的标签，使用的 loss 是交叉熵损失。\n\n然后，作者使用了超像素图来表示像素间的空间相邻性以及语义相似性。最后我们使用检测出的干净标签来训一个图注意力网络，并用其来修正带噪标签。在pascal voc 2012，pascal-context 以及 ms-coco 数据及上做了实验，并达到了sota的结果，并且超越了一些全监督的模型。\n\n# 阅读\n\n# 论文的目的及结论\n\n论文想从带噪学习的角度来建模半监督语义分割问题，使用的方法是先对 cam 生成的伪标签进行 clean 以及 noisy 的分类，再使用 clean 的标签训一个图注意力网络来修正 noisy 的标签，最终使用所有的图像再来全监督的去训最后的模型。\n\n# 论文的实验\n\n\n\n图一和图二展示了在 pascal voc 2012 上的 miou 随着 dcd^cdc 增大的结果变化。\n\n\n\n图三展示了和其他方法半监督语义分割方法的比较，图四展示了不同 dcd^cdc 下的分割模型结果，作者提出的方法甚至可以超过全监督的性能\n\n\n\n图五是消融实验，展示了在有无噪声检测以及有无标签修正情况下的模型性能。图六也是消融实验，展示了 crf 在伪标签和分割中的作用。\n\n# 论文的方法\n\n\n\n这是论文的方法总览，论文将其分为四个阶段。\n\n * 首先，使用 cam 生成 pixel-level 的标签\n * 为了确保能够较为精确的检测出相对干净的伪标签，利用少量的强监督样本训练一个干净的分割模型\n * 利用基于超像素的图建模图中像素的空间相邻性和语义相似性，利用 gat 来修正带噪标签\n * 修正后的标签作为伪标签来训一个分割网络\n\n下面将分别介绍每个阶段的细节\n\n# 1、从弱标注中生成 pixel-level 的标签\n\n使用 cam 来生成分割标签，架构就是典型的分类网络，在 gap （global average pooling）之后接全连接层。激活score 在0.05以下的视作背景，生成的伪标签作为初始标签\n\n# 2、检测干净以及带噪的标签\n\n先利用少量的强监督样本训练一个干净的分割模型，然后对每个弱监督样本都生成对应的预测。将该预测与 cam生成的伪标签计算交叉熵损失来区分干净以及带噪的标签。并且设置一个阈值 θ\\thetaθ 来区分干净以及带噪的标签。利用这些干净的标签来训练后续的 gat。\n\n由于该阶段对 θ\\thetaθ 会比较敏感，作者对强监督样本都做了预测以及 cam 生成伪标签的步骤，根据强监督样本的 loss 分布可以较为容易的确定适合的 θ\\thetaθ 参数，更多详细的实验在之后的消融实验里。\n\n# 3、修正带噪标签\n\n借鉴了【34】的方法，建立了一个基于超像素的图来建模像素的空间相邻性和语义相似度，然后 clean 的标签就被编码到图中，最后使用 gat 来修正带噪标签。作者提到 gcn 也可以用于修正，也做了关于 gcn 和 gat 的对比。\n\n（1）超像素图的构建\n\n超像素可以提供一个更大的，局部均匀一致的区域，并且能够保留很多用于精确分割的结构信息。作者将一张图像转化为一个超像素的图 g=(v,e,a)g = (v,e,a)g=(v,e,a) ，其中 vvv 代表顶点集合，eee 代表边的集合，aaa 代表邻接矩阵\n\n顶点构建：使用 slic [41] 的方法将一张图转换为一个超像素集合，作者的实验中一张图分割为了1000个超像素，并且使用了深层的特征图来捕捉高层的语义特征，将其与超像素进行整合，整合的过程中用了线性插值恢复到原图尺寸。最终对于每个超像素都能够得到 512 维的 cnn 特征向量\n\n边构建：利用图像中像素的空间相邻性和语义相似性来构造边。空间相邻性意味着相邻的像素更有可能有相似的标签，语义相似性意味着相同标签的像素可能共享了相似的语义信息。作者假设两个空间相邻的节点有相似的语义内容（同一类）。具体而言，先建立空间相邻矩阵 wl=[wlij]n×nw_l = [w_l^{ij}]_{n\\times n}wl =[wlij ]n×n ，如果相邻即为1，不相邻即为0。语义相似度矩阵ws=[wsij]n×nw_s = [w_s^{ij}]_{n \\times n}ws =[wsij ]n×n ，每个超像素有 512 维的 cnn 特征向量，彼此之间的语义相似度即为 wsij=wlij×exp(∣∣∣vi−vj∣2h)w_s^{ij} = w_l^{ij} \\times exp(\\frac{|||v_i-v_j|}{2h})wsij =wlij ×exp(2h∣∣∣vi −vj ∣ ) ，viv_ivi 和 vjv_jvj 是每个超像素的特征向量，hhh 是特征向量的维度，这里是 512。\n\n邻接矩阵生成： 即为下面的公式，如果语义相似度小于 γ\\gammaγ 且小于顶点 iii 所有邻接点语义相似度的最大值即为0，否则为1。此外，如果 aija_{ij}aij 为 0 的话，边 εij\\varepsilon_{ij}εij 也会被移除\n\n * γ=μ(ws)−σ(ws)\\gamma = \\mu (w_s)-\\sigma(w_s)γ=μ(ws )−σ(ws )\n * μ(⋅)\\mu(·)μ(⋅) 是均值\n * σ(⋅)\\sigma(·)σ(⋅) 是标准差\n\n\n\n（2）修正带噪标签\n\n编码干净伪标签：cam生成的干净伪标签将被编码进图中作为监督信息来训练 gat。为每个超像素分配类别的过程是多数原则\n\n利用 gat 修正噪声标签：参考[15]，引入顶点上的自注意力机制以计算注意力系数，用于体现顶点 jjj 特征的重要性。为顶点 iii 的邻接顶点都计算一个注意力系数 eije_{ij}eij 。为了系数能够跨顶点进行比较，利用 softmax 函数做了标准化。然后使用多头注意力来实现以上的图注意力机制，每个节点最后的输出特征如下：\n\n\n\nαij\\alpha_{ij}αij 是由第 iii 头注意力机制标准化之后的注意力系数，wglw_g^lwgl 是相应的输入线性变化的权重矩阵，在实验中，作者实现了两层的 gat 用于标签的修正。修正的模型为 z=f(v,a)z=f(v,a)z=f(v,a)，vvv 是由式 (7) 计算得到的超像素特征矩阵，aaa 是由式 (4) 计算得到的邻接矩阵，所有干净伪标签的交叉熵损失被定义为：\n\n\n\npip_ipi 是超像素 spisp_ispi 所对应的标签，ppp 是超像素的数量，ziz_izi 则是 gat 对于该超像素的输出\n\n# 5、训练分割网络\n\n带噪的伪标签被 gat 修正之后，可以根据其对应的超像素来恢复每个像素的标签，然后通过 crf 对修正的分割标签做细化，以更好的估计物体的形状。最终所有的样本用于全监督学习的训练\n\n# 论文的背景\n\n将半监督学习视作伪标签噪声问题，核心点有两个：如何区分带噪标签以及如何对带噪标签做修正。前者作者直接用cam 生成的伪标签以及用干净数据训练的分割模型的预测做一个交叉熵损失得到分数，再通过分数阈值进行区分；后者作者使用基于图的方法对超像素的一些约束进行建模，建模过程言之有理，不过如果对像素进行修正的我没有看懂。准备发邮件给作者问下\n\n# 总结\n\n# 论文的贡献\n\n论文的主要贡献是从噪声的视角去看待半监督的语义分割任务，引入基于超像素的图建模图中像素的空间相邻性和语义相似性，利用 gat 来修正带噪标签，也用到了crf做细化，将修正后的标签一起再来训分割模型。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2103.14242",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"(SimSiam) SimSiam Exploring Simple Siamese Representation Learning",frontmatter:{title:"(SimSiam) SimSiam Exploring Simple Siamese Representation Learning",date:"2021-08-02T21:07:45.000Z",permalink:"/pages/f3d018/",categories:["学术搬砖","论文阅读-自监督学习"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/11.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/07.(SimSiam)%20SimSiam%20Exploring%20Simple%20Siamese%20Representation%20Learning.html",relativePath:"01.学术搬砖/11.论文阅读-自监督学习/07.(SimSiam) SimSiam Exploring Simple Siamese Representation Learning.md",key:"v-c2fce59a",path:"/pages/f3d018/",headersStr:null,content:"# SimSiam: Exploring Simple Siamese Representation Learning\n\n# 单位：FAIR\n\n# 作者：Xinlei Chen, Kaiming He\n\n# 发表：CVPR 2021 (Best Paper Honorable Mention)\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 总结\n\n# 论文的贡献\n\n论文主要是针对弱监督语义分割提出了 Puzzle 模块，将原始图像分块后再算一个CAMs，并与原始的 CAMs 做一个重建损失，三项损失联合优化分类网络，提升 CAMs 的精度。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2011.10566\n * https://github.com/facebookresearch/simsiam\n * https://www.bilibili.com/video/BV1pg411M7b6",normalizedContent:"# simsiam: exploring simple siamese representation learning\n\n# 单位：fair\n\n# 作者：xinlei chen, kaiming he\n\n# 发表：cvpr 2021 (best paper honorable mention)\n\n# 摘要\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 总结\n\n# 论文的贡献\n\n论文主要是针对弱监督语义分割提出了 puzzle 模块，将原始图像分块后再算一个cams，并与原始的 cams 做一个重建损失，三项损失联合优化分类网络，提升 cams 的精度。\n\n# 论文的不足\n\n# 论文如何讲故事\n\n# 参考资料\n\n * https://arxiv.org/abs/2011.10566\n * https://github.com/facebookresearch/simsiam\n * https://www.bilibili.com/video/bv1pg411m7b6",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Structured Knowledge Distillation for Semantic Segmentation",frontmatter:{title:"Structured Knowledge Distillation for Semantic Segmentation",date:"2022-06-03T19:04:28.000Z",permalink:"/pages/eb7136/",categories:["学术搬砖","语义分割中的知识蒸馏"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/12.%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%B8%AD%E7%9A%84%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/00.Structured%20Knowledge%20Distillation%20for%20Semantic%20Segmentation.html",relativePath:"01.学术搬砖/12.语义分割中的知识蒸馏/00.Structured Knowledge Distillation for Semantic Segmentation.md",key:"v-0bbe12ea",path:"/pages/eb7136/",headersStr:null,content:"# 1、Structured Knowledge Distillation for Semantic Segmentation（CVPR 2019 Oral）\n\n * Citations：215\n * Paper: https://arxiv.org/pdf/1903.04197.pdf\n * Paper: https://www.zpascal.net/cvpr2019/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.pdf\n * Code: https://github.com/irfanICMLL/structure_knowledge_distillation\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事",normalizedContent:"# 1、structured knowledge distillation for semantic segmentation（cvpr 2019 oral）\n\n * citations：215\n * paper: https://arxiv.org/pdf/1903.04197.pdf\n * paper: https://www.zpascal.net/cvpr2019/liu_structured_knowledge_distillation_for_semantic_segmentation_cvpr_2019_paper.pdf\n * code: https://github.com/irfanicmll/structure_knowledge_distillation\n\n# 阅读\n\n# 论文的目的及结论\n\n# 论文的实验\n\n# 论文的方法\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"Decompose to Adapt Domain Disentanglement Faster-RCNN for Cross-domain Object Detection",frontmatter:{title:"Decompose to Adapt Domain Disentanglement Faster-RCNN for Cross-domain Object Detection",date:"2022-03-15T22:40:16.000Z",permalink:"/pages/5f322e/",categories:["学术搬砖","论文阅读-其他文章"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/14.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%85%B6%E4%BB%96%E6%96%87%E7%AB%A0/01.Decompose%20to%20Adapt%20Domain%20Disentanglement%20Faster-RCNN%20for%20Cross-domain%20Object%20Detection.html",relativePath:"01.学术搬砖/14.论文阅读-其他文章/01.Decompose to Adapt Domain Disentanglement Faster-RCNN for Cross-domain Object Detection.md",key:"v-801da7ca",path:"/pages/5f322e/",headersStr:null,content:"# Decompose to Adapt: Domain Disentanglement Faster-RCNN for Cross-domain Object Detection\n\n# 作者：Dongnan Liu, Chaoyi Zhang, Yang Song, Heng Huang, Chenyu Wang, Michael Barnett, Weidong Cai\n\n# 摘要\n\n无监督领域适应（UDA）技术的最新进展在跨域计算机视觉任务中取得了巨大成功，通过弥合领域分布的差距，增强了数据驱动的深度学习架构的通用性。对于基于 UDA 的跨域目标检测方法来说，大多数都通过对抗学习策略引入领域不变特征的生成，从而缓解了domain bias。由于对抗训练过程的不稳定性，这些方法的域判别器的分类能力是有限的。所以提取的特征仍然包含领域相关的因素，为了解决这个问题，文章设计了一个域解构的Faster R-CNN (DDF) 来消除特征中领域相关的信息。DDF方法通过Global Triplet Disentanglement（GTD）模块以及Instance Similarity DIsentanglement（ISD）模块分别促进了全局和局部阶段的特征解构。在四个数据及上，DDF表现出 sota的性能，并且具有广泛的适用性。\n\n# 阅读\n\n# 介绍\n\n无监督的领域自适应（UDA）方法被提出用于从源域迁移那些领域无关的知识至目标域，这其中的大部分方法都会和对抗结构结合。在隐空间上领域无关和领域相关的特征是比较难解构的，主要原因是因为域分类器的分类能力不强，二者是因为对抗的训练存在不稳定性，其决策边界是不精确的。由此，其特征和特征都会偏向源域，降低性能。\n\n很多方法致力于解决特征分解的问题，建立了基于自编码器的结构，结合潜在的编码独立机制，以及分类器的正则化来从特征中解耦出那些域相关的特征。然而这存在很多问题，解释了很多\n\n我们的 DDF 可以在全局和局部的层面上进行特征解构，文中的全局特征即指Backbone 网络的输出，局部特征是指为定位和分类的 RoI 特征。设计了 GTD 模块与域判别器进行联合优化，其基于三元组特征解构机制，ISD 模块则基于共享及私有特征的相似性正则化来解决局部特征的分解。DDF方法在四个无监督域适应目标检测的任务上验证，并且达到sota的性能。\n\n# 论文的目的及结论\n\n# 论文的方法\n\n# 3.1 框架总览\n\n\n\n源域数据集表示为 DsD_sDs ，目标域数据集表示为 DtD_tDt 。图 1 则是 DDF 方法的总框架，在每个iteration，源域的图像是 xsx_sxs ，目标域的图像是 xtx_txt ，\n\n * 首先，使用Backbone 提取全局特征，使用一个基础的固定权重的特征编码器 EbE_bEb ，以及一个权重动态更新的域共享特征提取器 EsE_sEs\n * 为了促成特征的解构，我们设计了一个域私有特征编码器 EpE_pEp ，来获取域私有的特征，其表示为 FprisF_{pri}^sFpris 以及 FshasF_{sha}^sFshas 。对于全局特征的解构，FshasF_{sha}^sFshas 、FshatF_{sha}^tFshat 、FprisF_{pri}^sFpris 以及 FpritF_{pri}^tFprit 会被 GTD 模块优化，希望对齐源域和目标域域共享特征间的数据集分布，并扩大每个域内部域共享特征和域私有特征间的差异。利用 GTD 模块，全局层面的域私有的因素从域共享的特征中解构出来，用于检测任务的训练\n * 在局部级别的特征解构，我们试验了一个 RPN 以及 RoIAlign 层来提取FshasF_{sha}^sFshas 、FshatF_{sha}^tFshat 、FprisF_{pri}^sFpris 以及 FpritF_{pri}^tFprit 局部的实例特征，如公式（2）所示\n   * \n * MLP 是三层的全连接层，在公式~（2）中，IshasI_{sha}^sIshas 以及 IshatI_{sha}^tIshat 表示实例级别的域共享特征，IprisI_{pri}^sIpris 以及 IpritI_{pri}^tIprit 表示实例级别的域私有特征，最后着四个特征送入 ISD 模块基于特征相似度优化完成局部级别的特征解构，最终，实例级别的域共享特征（IshasI_{sha}^sIshas ）被用于目标的定位以及分类\n\n# 3.2 GTD 模块\n\n\n\n在之前的方法中，域共享特征 FshasF_{sha}^sFshas 以及 FshatF_{sha}^tFshat 都是通过在全局层面上优化对抗性判别器来保证的。如公式（3）所示，LceL_{ce}Lce 代表交叉熵损失，θEs\\theta_{E_s}θEs 以及 θD\\theta_DθD 代表特征提取器 EsE_sEs 以及域判别器 DglbD_{glb}Dglb 的参数。但由于对抗训练的不稳定性，特征不一定是完全领域不变的。现有的一些工作也在改进这一问题，但其需要非常大的 Batch-size。\n\n\n\n\n\n图 2 即是 GTD 模块，域判别器 DglbD_{glb}Dglb 需要区分FprisF_{pri}^sFpris 和 FpritF_{pri}^tFprit 两个特征，基于以上的假设，作者引入了域相关的分类loss 来增强 DglbD_{glb}Dglb 的分类能力，θEp\\theta_{E_p}θEp 代表了域私有编码器 EpE_pEp 的参数。\n\n\n\n为了进一步拉开域共享特征和域私有特征之间的差距，同时引入了三元组损失，如图（5）所示，希望尽可能的拉近源域共享特征和目标域共享特征的距离，减小共享特征和私有特征之间的距离，用来将共享特征和私有特征进一步分离。其中 d(f1,f2)=∣∣Dglb(f1)−Dglb(f2)∣∣d(f_1,f_2)=||D_{glb}(f_1)-D_{glb}(f_2)||d(f1 ,f2 )=∣∣Dglb (f1 )−Dglb (f2 )∣∣ ，用来衡量其 Softmax 函数后的 L2L_2L2 距离\n\n * GTD 模块的最终损失即为 LGTD=Lds+LtriL_{GTD} = L_{ds}+L{tri}LGTD =Lds +Ltri。\n\n# 3.3 ISD 模块\n\nISD模块基于特征相似度优化来进行特征对齐，\n\n * 首先如公式（2）所示获得局部的 IshatI_{sha}^tIshat IshasI_{sha}^sIshas IpritI_{pri}^tIprit IprisI_{pri}^sIpris ，特征的数量等同于 RoI 的数量\n * 然后扩大域共享实例特征以及域私有实例特征的分布距离，其 Loss 定义如下，sim()sim()sim() 代表余弦相似度。\n\n\n\n * Motivated by [7]：在理想的解构条件下，来自不同域的域私有因子都不应相交，因此，我们需要最大化域私有特征间的距离\n   \n   \n\n * 最终的 LISD=LISD−intra+LISD−interL_{ISD} = L_{ISD-intra} + L_{ISD-inter}LISD =LISD−intra +LISD−inter\n\n# 3.4 训练\n\n最终的损失 Lddf=Ldet+Ldi+LGTD+LISDL_{ddf} = L_{det} + L_{di} + L_{GTD} + L_{ISD}Lddf =Ldet +Ldi +LGTD +LISD ，在 loss 中没有带权重，避免了超参数调节\n\n# 论文的实验\n\n四个公共数据集上做了实验：Cityscapes、Foggy Cityscapes、SIM10K、KITTI\n\n\n\nGTD 模块可以避免 Batch-size 的问题，ISD 模块不需要额外参数，\n\n\n\n\n\n\n\n\n\n * DDF 方法的域共享特征能够特别关注实例对象，这对目标检测任务来讲特别关键\n * DDF 方法的域私有特征更加关注能够体现当前域特征的信息上，例如反映当天天气的一些背景\n * 这体现了EpE_pEp 和 EsE_sEs 的有效性，能够成功的提取到域私有和域共享特征\n\n\n\n * 计算了Cityscapes-> Foggy Cityscapes 的 Global以及 local stage的特征分布的距离，用了两个距离：Proxy A-distance（PAD）以及Earth Movers Distance（EMD），DDF方法能够得到更小的特征距离，能够得到域不变的特征以及更低的跨域特征差异\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事",normalizedContent:"# decompose to adapt: domain disentanglement faster-rcnn for cross-domain object detection\n\n# 作者：dongnan liu, chaoyi zhang, yang song, heng huang, chenyu wang, michael barnett, weidong cai\n\n# 摘要\n\n无监督领域适应（uda）技术的最新进展在跨域计算机视觉任务中取得了巨大成功，通过弥合领域分布的差距，增强了数据驱动的深度学习架构的通用性。对于基于 uda 的跨域目标检测方法来说，大多数都通过对抗学习策略引入领域不变特征的生成，从而缓解了domain bias。由于对抗训练过程的不稳定性，这些方法的域判别器的分类能力是有限的。所以提取的特征仍然包含领域相关的因素，为了解决这个问题，文章设计了一个域解构的faster r-cnn (ddf) 来消除特征中领域相关的信息。ddf方法通过global triplet disentanglement（gtd）模块以及instance similarity disentanglement（isd）模块分别促进了全局和局部阶段的特征解构。在四个数据及上，ddf表现出 sota的性能，并且具有广泛的适用性。\n\n# 阅读\n\n# 介绍\n\n无监督的领域自适应（uda）方法被提出用于从源域迁移那些领域无关的知识至目标域，这其中的大部分方法都会和对抗结构结合。在隐空间上领域无关和领域相关的特征是比较难解构的，主要原因是因为域分类器的分类能力不强，二者是因为对抗的训练存在不稳定性，其决策边界是不精确的。由此，其特征和特征都会偏向源域，降低性能。\n\n很多方法致力于解决特征分解的问题，建立了基于自编码器的结构，结合潜在的编码独立机制，以及分类器的正则化来从特征中解耦出那些域相关的特征。然而这存在很多问题，解释了很多\n\n我们的 ddf 可以在全局和局部的层面上进行特征解构，文中的全局特征即指backbone 网络的输出，局部特征是指为定位和分类的 roi 特征。设计了 gtd 模块与域判别器进行联合优化，其基于三元组特征解构机制，isd 模块则基于共享及私有特征的相似性正则化来解决局部特征的分解。ddf方法在四个无监督域适应目标检测的任务上验证，并且达到sota的性能。\n\n# 论文的目的及结论\n\n# 论文的方法\n\n# 3.1 框架总览\n\n\n\n源域数据集表示为 dsd_sds ，目标域数据集表示为 dtd_tdt 。图 1 则是 ddf 方法的总框架，在每个iteration，源域的图像是 xsx_sxs ，目标域的图像是 xtx_txt ，\n\n * 首先，使用backbone 提取全局特征，使用一个基础的固定权重的特征编码器 ebe_beb ，以及一个权重动态更新的域共享特征提取器 ese_ses\n * 为了促成特征的解构，我们设计了一个域私有特征编码器 epe_pep ，来获取域私有的特征，其表示为 fprisf_{pri}^sfpris 以及 fshasf_{sha}^sfshas 。对于全局特征的解构，fshasf_{sha}^sfshas 、fshatf_{sha}^tfshat 、fprisf_{pri}^sfpris 以及 fpritf_{pri}^tfprit 会被 gtd 模块优化，希望对齐源域和目标域域共享特征间的数据集分布，并扩大每个域内部域共享特征和域私有特征间的差异。利用 gtd 模块，全局层面的域私有的因素从域共享的特征中解构出来，用于检测任务的训练\n * 在局部级别的特征解构，我们试验了一个 rpn 以及 roialign 层来提取fshasf_{sha}^sfshas 、fshatf_{sha}^tfshat 、fprisf_{pri}^sfpris 以及 fpritf_{pri}^tfprit 局部的实例特征，如公式（2）所示\n   * \n * mlp 是三层的全连接层，在公式~（2）中，ishasi_{sha}^sishas 以及 ishati_{sha}^tishat 表示实例级别的域共享特征，iprisi_{pri}^sipris 以及 ipriti_{pri}^tiprit 表示实例级别的域私有特征，最后着四个特征送入 isd 模块基于特征相似度优化完成局部级别的特征解构，最终，实例级别的域共享特征（ishasi_{sha}^sishas ）被用于目标的定位以及分类\n\n# 3.2 gtd 模块\n\n\n\n在之前的方法中，域共享特征 fshasf_{sha}^sfshas 以及 fshatf_{sha}^tfshat 都是通过在全局层面上优化对抗性判别器来保证的。如公式（3）所示，lcel_{ce}lce 代表交叉熵损失，θes\\theta_{e_s}θes 以及 θd\\theta_dθd 代表特征提取器 ese_ses 以及域判别器 dglbd_{glb}dglb 的参数。但由于对抗训练的不稳定性，特征不一定是完全领域不变的。现有的一些工作也在改进这一问题，但其需要非常大的 batch-size。\n\n\n\n\n\n图 2 即是 gtd 模块，域判别器 dglbd_{glb}dglb 需要区分fprisf_{pri}^sfpris 和 fpritf_{pri}^tfprit 两个特征，基于以上的假设，作者引入了域相关的分类loss 来增强 dglbd_{glb}dglb 的分类能力，θep\\theta_{e_p}θep 代表了域私有编码器 epe_pep 的参数。\n\n\n\n为了进一步拉开域共享特征和域私有特征之间的差距，同时引入了三元组损失，如图（5）所示，希望尽可能的拉近源域共享特征和目标域共享特征的距离，减小共享特征和私有特征之间的距离，用来将共享特征和私有特征进一步分离。其中 d(f1,f2)=∣∣dglb(f1)−dglb(f2)∣∣d(f_1,f_2)=||d_{glb}(f_1)-d_{glb}(f_2)||d(f1 ,f2 )=∣∣dglb (f1 )−dglb (f2 )∣∣ ，用来衡量其 softmax 函数后的 l2l_2l2 距离\n\n * gtd 模块的最终损失即为 lgtd=lds+ltril_{gtd} = l_{ds}+l{tri}lgtd =lds +ltri。\n\n# 3.3 isd 模块\n\nisd模块基于特征相似度优化来进行特征对齐，\n\n * 首先如公式（2）所示获得局部的 ishati_{sha}^tishat ishasi_{sha}^sishas ipriti_{pri}^tiprit iprisi_{pri}^sipris ，特征的数量等同于 roi 的数量\n * 然后扩大域共享实例特征以及域私有实例特征的分布距离，其 loss 定义如下，sim()sim()sim() 代表余弦相似度。\n\n\n\n * motivated by [7]：在理想的解构条件下，来自不同域的域私有因子都不应相交，因此，我们需要最大化域私有特征间的距离\n   \n   \n\n * 最终的 lisd=lisd−intra+lisd−interl_{isd} = l_{isd-intra} + l_{isd-inter}lisd =lisd−intra +lisd−inter\n\n# 3.4 训练\n\n最终的损失 lddf=ldet+ldi+lgtd+lisdl_{ddf} = l_{det} + l_{di} + l_{gtd} + l_{isd}lddf =ldet +ldi +lgtd +lisd ，在 loss 中没有带权重，避免了超参数调节\n\n# 论文的实验\n\n四个公共数据集上做了实验：cityscapes、foggy cityscapes、sim10k、kitti\n\n\n\ngtd 模块可以避免 batch-size 的问题，isd 模块不需要额外参数，\n\n\n\n\n\n\n\n\n\n * ddf 方法的域共享特征能够特别关注实例对象，这对目标检测任务来讲特别关键\n * ddf 方法的域私有特征更加关注能够体现当前域特征的信息上，例如反映当天天气的一些背景\n * 这体现了epe_pep 和 ese_ses 的有效性，能够成功的提取到域私有和域共享特征\n\n\n\n * 计算了cityscapes-> foggy cityscapes 的 global以及 local stage的特征分布的距离，用了两个距离：proxy a-distance（pad）以及earth movers distance（emd），ddf方法能够得到更小的特征距离，能够得到域不变的特征以及更低的跨域特征差异\n\n# 论文的背景\n\n# 总结\n\n# 论文的贡献\n\n# 论文的不足\n\n# 论文如何讲故事",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"自监督系列代码",frontmatter:{title:"自监督系列代码",date:"2021-10-21T15:35:47.000Z",permalink:"/pages/09e28c/",categories:["学习笔记","代码实践-图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/11.%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/13.%E8%87%AA%E7%9B%91%E7%9D%A3%E7%B3%BB%E5%88%97%E4%BB%A3%E7%A0%81.html",relativePath:"01.学术搬砖/11.论文阅读-自监督学习/13.自监督系列代码.md",key:"v-2e36dcd8",path:"/pages/09e28c/",headers:[{level:2,title:"01、MoCo: Momentum Contrast for Unsupervised Visual Representation Learning",slug:"_01、moco-momentum-contrast-for-unsupervised-visual-representation-learning",normalizedTitle:"01、moco: momentum contrast for unsupervised visual representation learning",charIndex:2},{level:2,title:"02、SimCLR - A Simple Framework for Contrastive Learning of Visual Representations",slug:"_02、simclr-a-simple-framework-for-contrastive-learning-of-visual-representations",normalizedTitle:"02、simclr - a simple framework for contrastive learning of visual representations",charIndex:1135},{level:3,title:"Pre-trained models for SimCLRv1",slug:"pre-trained-models-for-simclrv1",normalizedTitle:"pre-trained models for simclrv1",charIndex:1264},{level:3,title:"Pre-trained models for SimCLRv2",slug:"pre-trained-models-for-simclrv2",normalizedTitle:"pre-trained models for simclrv2",charIndex:1635},{level:2,title:"03、SimSiam: Exploring Simple Siamese Representation Learning",slug:"_03、simsiam-exploring-simple-siamese-representation-learning",normalizedTitle:"03、simsiam: exploring simple siamese representation learning",charIndex:2924},{level:3,title:"Models and Logs",slug:"models-and-logs",normalizedTitle:"models and logs",charIndex:3034},{level:2,title:"04、Understanding Dimensional Collapse in Contrastive Self-supervised Learning",slug:"_04、understanding-dimensional-collapse-in-contrastive-self-supervised-learning",normalizedTitle:"04、understanding dimensional collapse in contrastive self-supervised learning",charIndex:3426},{level:2,title:"05、Improving Contrastive Learning by Visualizing Feature Transformation",slug:"_05、improving-contrastive-learning-by-visualizing-feature-transformation",normalizedTitle:"05、improving contrastive learning by visualizing feature transformation",charIndex:3508},{level:2,title:"06、Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning",slug:"_06、propagate-yourself-exploring-pixel-level-consistency-for-unsupervised-visual-representation-learning",normalizedTitle:"06、propagate yourself: exploring pixel-level consistency for unsupervised visual representation learning",charIndex:4589},{level:3,title:"6.1 Pascal VOC object detection",slug:"_6-1-pascal-voc-object-detection",normalizedTitle:"6.1 pascal voc object detection",charIndex:4698},{level:3,title:"6.2 COCO object detection",slug:"_6-2-coco-object-detection",normalizedTitle:"6.2 coco object detection",charIndex:5391},{level:2,title:"07、CVPR2021 | Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning",slug:"_07、cvpr2021-online-bag-of-visual-words-generation-for-unsupervised-representation-learning",normalizedTitle:"07、cvpr2021 | online bag-of-visual-words generation for unsupervised representation learning",charIndex:7032},{level:2,title:"08、NeurIPS 2020 | Unsupervised Learning of Visual Features by Contrasting Cluster Assignments",slug:"_08、neurips-2020-unsupervised-learning-of-visual-features-by-contrasting-cluster-assignments",normalizedTitle:"08、neurips 2020 | unsupervised learning of visual features by contrasting cluster assignments",charIndex:7394},{level:2,title:"09、ECCV 2020 | Learning to Classify Images without Labels",slug:"_09、eccv-2020-learning-to-classify-images-without-labels",normalizedTitle:"09、eccv 2020 | learning to classify images without labels",charIndex:8706},{level:2,title:"10、ICML 2020 | Self-Supervised Prototypical Transfer Learning for Few-Shot Classification",slug:"_10、icml-2020-self-supervised-prototypical-transfer-learning-for-few-shot-classification",normalizedTitle:"10、icml 2020 | self-supervised prototypical transfer learning for few-shot classification",charIndex:9202},{level:2,title:"11、NeurIPS 2020 | Bootstrap Your Own Latent",slug:"_11、neurips-2020-bootstrap-your-own-latent",normalizedTitle:"11、neurips 2020 | bootstrap your own latent",charIndex:9339},{level:2,title:"12、Efficient Self-Supervised Vision Transformers",slug:"_12、efficient-self-supervised-vision-transformers",normalizedTitle:"12、efficient self-supervised vision transformers",charIndex:9597},{level:2,title:"13、Emerging Properties in Self-Supervised Vision Transformers.",slug:"_13、emerging-properties-in-self-supervised-vision-transformers",normalizedTitle:"13、emerging properties in self-supervised vision transformers.",charIndex:10775}],headersStr:"01、MoCo: Momentum Contrast for Unsupervised Visual Representation Learning 02、SimCLR - A Simple Framework for Contrastive Learning of Visual Representations Pre-trained models for SimCLRv1 Pre-trained models for SimCLRv2 03、SimSiam: Exploring Simple Siamese Representation Learning Models and Logs 04、Understanding Dimensional Collapse in Contrastive Self-supervised Learning 05、Improving Contrastive Learning by Visualizing Feature Transformation 06、Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning 6.1 Pascal VOC object detection 6.2 COCO object detection 07、CVPR2021 | Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning 08、NeurIPS 2020 | Unsupervised Learning of Visual Features by Contrasting Cluster Assignments 09、ECCV 2020 | Learning to Classify Images without Labels 10、ICML 2020 | Self-Supervised Prototypical Transfer Learning for Few-Shot Classification 11、NeurIPS 2020 | Bootstrap Your Own Latent 12、Efficient Self-Supervised Vision Transformers 13、Emerging Properties in Self-Supervised Vision Transformers.",content:"# 01、MoCo: Momentum Contrast for Unsupervised Visual Representation Learning\n\n# 1.1 MoCo v1 & MoCo v2\n\nhttps://github.com/facebookresearch/moco\n\nModels\n\nOur pre-trained ResNet-50 models can be downloaded as following:\n\nEPOCHS    MLP   AUG+   COS   TOP-1 ACC.   MODEL   MD5        \nMoCo v1   200                             60.6    download   b251726a\nMoCo v2   200   ✓      ✓     ✓            67.7    download   59fd9945\nMoCo v2   800   ✓      ✓     ✓            71.1    download   a04e12f8\n\n# 1.2 MoCov3\n\nhttps://github.com/facebookresearch/moco-v3\n\nResNet-50, linear classification\n\nPRETRAIN EPOCHS   PRETRAIN CROPS   LINEAR ACC\n100               2x224            68.9\n300               2x224            72.8\n1000              2x224            74.6\n\nViT, linear classification\n\nMODEL       PRETRAIN EPOCHS   PRETRAIN CROPS   LINEAR ACC\nViT-Small   300               2x224            73.2\nViT-Base    300               2x224            76.7\n\nViT, end-to-end fine-tuning\n\nMODEL       PRETRAIN EPOCHS   PRETRAIN CROPS   E2E ACC\nViT-Small   300               2x224            81.4\nViT-Base    300               2x224            83.2\n\n\n# 02、SimCLR - A Simple Framework for Contrastive Learning of Visual Representations\n\nhttps://github.com/google-research/simclr\n\n\n# Pre-trained models for SimCLRv1\n\nThe pre-trained models (base network with linear classifier layer) can be found below. Note that for these SimCLRv1 checkpoints, the projection head is not available.\n\nMODEL CHECKPOINT AND HUB-MODULE   IMAGENET TOP-1\nResNet50 (1x)                     69.1\nResNet50 (2x)                     74.2\nResNet50 (4x)                     76.6\n\n\n# Pre-trained models for SimCLRv2\n\nDEPTH   WIDTH   SK      PARAM (M)   F-T (1%)   F-T(10%)   F-T(100%)   LINEAR EVAL   SUPERVISED\n50      1X      False   24          57.9       68.4       76.3        71.7          76.6\n50      1X      True    35          64.5       72.1       78.7        74.6          78.5\n50      2X      False   94          66.3       73.9       79.1        75.6          77.8\n50      2X      True    140         70.6       77.0       81.3        77.7          79.3\n101     1X      False   43          62.1       71.4       78.2        73.6          78.0\n101     1X      True    65          68.3       75.1       80.6        76.3          79.6\n101     2X      False   170         69.1       75.8       80.7        77.0          78.9\n101     2X      True    257         73.2       78.8       82.4        79.0          80.1\n152     1X      False   58          64.0       73.0       79.3        74.5          78.3\n152     1X      True    89          70.0       76.5       81.3        77.2          79.9\n152     2X      False   233         70.2       76.6       81.1        77.4          79.1\n152     2X      True    354         74.2       79.4       82.9        79.4          80.4\n152     3X      True    795         74.9       80.1       83.1        79.8          80.5\n\n\n# 03、SimSiam: Exploring Simple Siamese Representation Learning\n\nhttps://github.com/facebookresearch/simsiam\n\n\n# Models and Logs\n\nOur pre-trained ResNet-50 models and logs:\n\nPRE-TRAIN EPOCHS   BATCH SIZE   PRE-TRAIN CKPT   PRE-TRAIN LOG   LINEAR CLS. CKPT   LINEAR CLS. LOG   TOP-1 ACC.\n100                512          link             link            link               link              68.1\n100                256          link             link            link               link              68.3\n\n\n# 04、Understanding Dimensional Collapse in Contrastive Self-supervised Learning\n\n\n# 05、Improving Contrastive Learning by Visualizing Feature Transformation\n\nhttps://github.com/DTennant/CL-Visualizing-Feature-Transformation\n\nModels\n\nFor your convenience, we provide the following pre-trained models on ImageNet-1K and ImageNet-100.\n\nPRE-TRAIN METHOD   PRE-TRAIN DATASET   BACKBONE    #EPOCH   IMAGENET-1K          VOC DET AP50   COCO DET AP   LINK\nSupervised         ImageNet-1K         ResNet-50   -        76.1                 81.3           38.2          download\nMoCo-v1            ImageNet-1K         ResNet-50   200      60.6                 81.5           38.5          download\nMoCo-v1+FT         ImageNet-1K         ResNet-50   200      61.9                 82.0           39.0          download\nMoCo-v2            ImageNet-1K         ResNet-50   200      67.5                 82.4           39.0          download\nMoCo-v2+FT         ImageNet-1K         ResNet-50   200      69.6                 83.3           39.5          download\nMoCo-v1+FT         ImageNet-100        ResNet-50   200      IN-100 result 77.2   -              -             download\n\n\n# 06、Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning\n\n\n# 6.1 Pascal VOC object detection\n\n# Faster-RCNN with C4\n\nMETHOD          EPOCHS   ARCH        AP     AP50   AP75   DOWNLOAD\nScratch         -        ResNet-50   33.8   60.2   33.1   -\nSupervised      100      ResNet-50   53.5   81.3   58.8   -\nMoCo            200      ResNet-50   55.9   81.5   62.6   -\nSimCLR          1000     ResNet-50   56.3   81.9   62.5   -\nMoCo v2         800      ResNet-50   57.6   82.7   64.4   -\nInfoMin         200      ResNet-50   57.6   82.7   64.6   -\nInfoMin         800      ResNet-50   57.5   82.5   64.0   -\nPixPro (ours)   100      ResNet-50   58.8   83.0   66.5   config | model\nPixPro (ours)   400      ResNet-50   60.2   83.8   67.7   config | model\n\n\n# 6.2 COCO object detection\n\n# Mask-RCNN with FPN\n\nMETHOD          EPOCHS   ARCH        SCHEDULE   BBOX AP   MASK AP   DOWNLOAD\nScratch         -        ResNet-50   1x         32.8      29.9      -\nSupervised      100      ResNet-50   1x         39.7      35.9      -\nMoCo            200      ResNet-50   1x         39.4      35.6      -\nSimCLR          1000     ResNet-50   1x         39.8      35.9      -\nMoCo v2         800      ResNet-50   1x         40.4      36.4      -\nInfoMin         200      ResNet-50   1x         40.6      36.7      -\nInfoMin         800      ResNet-50   1x         40.4      36.6      -\nPixPro (ours)   100      ResNet-50   1x         40.8      36.8      config | model\nPixPro (ours)   100*     ResNet-50   1x         41.3      37.1      -\nPixPro (ours)   400*     ResNet-50   1x         41.4      37.4      -\n\n* Indicates methods with instance branch.\n\n# Mask-RCNN with C4\n\nMETHOD          EPOCHS   ARCH        SCHEDULE   BBOX AP   MASK AP   DOWNLOAD\nScratch         -        ResNet-50   1x         26.4      29.3      -\nSupervised      100      ResNet-50   1x         38.2      33.3      -\nMoCo            200      ResNet-50   1x         38.5      33.6      -\nSimCLR          1000     ResNet-50   1x         38.4      33.6      -\nMoCo v2         800      ResNet-50   1x         39.5      34.5      -\nInfoMin         200      ResNet-50   1x         39.0      34.1      -\nInfoMin         800      ResNet-50   1x         38.8      33.8      -\nPixPro (ours)   100      ResNet-50   1x         40.0      34.8      config | model\nPixPro (ours)   400      ResNet-50   1x         40.5      35.3      config | model\n\n\n# 07、CVPR2021 | Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning\n\nhttps://github.com/valeoai/obow\n\n# 7.1 ResNet50 pre-trained model\n\nMETHOD   EPOCHS   BATCH-SIZE   DATASET    IMAGENET LINEAR ACC.   LINKS TO PRE-TRAINED WEIGHTS\nOBoW     200      256          ImageNet   73.8                   entire model / only feature extractor\n\n\n# 08、NeurIPS 2020 | Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\n\nhttps://github.com/facebookresearch/swav\n\nMETHOD           EPOCHS   BATCH-SIZE   MULTI-CROP     IMAGENET TOP-1 ACC.   URL     ARGS\nSwAV             800      4096         2x224 + 6x96   75.3                  model   script\nSwAV             400      4096         2x224 + 6x96   74.6                  model   script\nSwAV             200      4096         2x224 + 6x96   73.9                  model   script\nSwAV             100      4096         2x224 + 6x96   72.1                  model   script\nSwAV             200      256          2x224 + 6x96   72.7                  model   script\nSwAV             400      256          2x224 + 6x96   74.3                  model   script\nSwAV             400      4096         2x224          70.1                  model   script\nDeepCluster-v2   800      4096         2x224 + 6x96   75.2                  model   script\nDeepCluster-v2   400      4096         2x160 + 4x96   74.3                  model   script\nDeepCluster-v2   400      4096         2x224          70.2                  model   script\nSeLa-v2          400      4096         2x160 + 4x96   71.8                  model   -\nSeLa-v2          400      4096         2x224          67.2                  model   -\n\n\n# 09、ECCV 2020 | Learning to Classify Images without Labels\n\nhttps://github.com/wvangansbeke/Unsupervised-Classification\n\nWe also train SCAN on ImageNet for 1000 clusters. We use 10 clusterheads and finally take the head with the lowest loss. The accuracy (ACC), normalized mutual information (NMI), adjusted mutual information (AMI) and adjusted rand index (ARI) are computed:\n\nMETHOD            ACC    NMI    AMI    ARI    DOWNLOAD LINK\nSCAN (ResNet50)   39.9   72.0   51.2   27.5   Download\n\n\n# 10、ICML 2020 | Self-Supervised Prototypical Transfer Learning for Few-Shot Classification\n\nhttps://github.com/indy-lab/ProtoTransfer\n\n\n# 11、NeurIPS 2020 | Bootstrap Your Own Latent\n\nhttps://github.com/deepmind/deepmind-research/tree/master/byol\n\nUsing this implementation should achieve a top-1 accuracy on Imagenet between 74.0% and 74.5% after about 8h of training using 512 Cloud TPU v3.\n\n\n# 12、Efficient Self-Supervised Vision Transformers\n\nhttps://github.com/microsoft/esvit\n\n# 12.1 Pretrained models\n\nYou can download the full checkpoint (trained with both view-level and region-level tasks, batch size=512 and ImageNet-1K.), which contains backbone and projection head weights for both student and teacher networks.\n\n * EsViT (Swin) with network configurations of increased model capacities, pre-trained with both view-level and region-level tasks. ResNet-50 trained with both tasks is shown as a reference.\n\nARCH                   PARAMS   LINEAR   K-NN    DOWNLOAD    LOGS             \nResNet-50              23M      75.7%    71.3%   full ckpt   train   linear   knn\nEsViT (Swin-T, W=7)    28M      78.0%    75.7%   full ckpt   train   linear   knn\nEsViT (Swin-S, W=7)    49M      79.5%    77.7%   full ckpt   train   linear   knn\nEsViT (Swin-B, W=7)    87M      80.4%    78.9%   full ckpt   train   linear   knn\nEsViT (Swin-T, W=14)   28M      78.7%    77.0%   full ckpt   train   linear   knn\nEsViT (Swin-S, W=14)   49M      80.8%    79.1%   full ckpt   train   linear   knn\nEsViT (Swin-B, W=14)   87M      81.3%    79.3%   full ckpt   train   linear   knn\n\n\n# 13、Emerging Properties in Self-Supervised Vision Transformers.\n\nhttps://github.com/facebookresearch/dino\n\n# 13.1 Pretrained models\n\nYou can choose to download only the weights of the pretrained backbone used for downstream tasks, or the full checkpoint which contains backbone and projection head weights for both student and teacher networks. We also provide the backbone in onnx format, as well as detailed arguments and training/evaluation logs. Note that DeiT-S and ViT-S names refer exactly to the same architecture.\n\nARCH        PARAMS   K-NN    LINEAR   DOWNLOAD                                         \nViT-S/16    21M      74.5%   77.0%    backbone only   full ckpt   onnx   args   logs   eval logs\nViT-S/8     21M      78.3%   79.7%    backbone only   full ckpt   onnx   args   logs   eval logs\nViT-B/16    85M      76.1%   78.2%    backbone only   full ckpt   onnx   args   logs   eval logs\nViT-B/8     85M      77.4%   80.1%    backbone only   full ckpt   onnx   args   logs   eval logs\nResNet-50   23M      67.5%   75.3%    backbone only   full ckpt   onnx   args   logs   eval logs\n\nWe also release XCiT models ([arXiv] [code]) trained with DINO:\n\nARCH                 PARAMS   K-NN    LINEAR   DOWNLOAD                                  \nxcit_small_12_p16    26M      76.0%   77.8%    backbone only   full ckpt   args   logs   eval\nxcit_small_12_p8     26M      77.1%   79.2%    backbone only   full ckpt   args   logs   eval\nxcit_medium_24_p16   84M      76.4%   78.8%    backbone only   full ckpt   args   logs   eval\nxcit_medium_24_p8    84M      77.9%   80.3%    backbone only   full ckpt   args   logs   eval",normalizedContent:"# 01、moco: momentum contrast for unsupervised visual representation learning\n\n# 1.1 moco v1 & moco v2\n\nhttps://github.com/facebookresearch/moco\n\nmodels\n\nour pre-trained resnet-50 models can be downloaded as following:\n\nepochs    mlp   aug+   cos   top-1 acc.   model   md5        \nmoco v1   200                             60.6    download   b251726a\nmoco v2   200   ✓      ✓     ✓            67.7    download   59fd9945\nmoco v2   800   ✓      ✓     ✓            71.1    download   a04e12f8\n\n# 1.2 mocov3\n\nhttps://github.com/facebookresearch/moco-v3\n\nresnet-50, linear classification\n\npretrain epochs   pretrain crops   linear acc\n100               2x224            68.9\n300               2x224            72.8\n1000              2x224            74.6\n\nvit, linear classification\n\nmodel       pretrain epochs   pretrain crops   linear acc\nvit-small   300               2x224            73.2\nvit-base    300               2x224            76.7\n\nvit, end-to-end fine-tuning\n\nmodel       pretrain epochs   pretrain crops   e2e acc\nvit-small   300               2x224            81.4\nvit-base    300               2x224            83.2\n\n\n# 02、simclr - a simple framework for contrastive learning of visual representations\n\nhttps://github.com/google-research/simclr\n\n\n# pre-trained models for simclrv1\n\nthe pre-trained models (base network with linear classifier layer) can be found below. note that for these simclrv1 checkpoints, the projection head is not available.\n\nmodel checkpoint and hub-module   imagenet top-1\nresnet50 (1x)                     69.1\nresnet50 (2x)                     74.2\nresnet50 (4x)                     76.6\n\n\n# pre-trained models for simclrv2\n\ndepth   width   sk      param (m)   f-t (1%)   f-t(10%)   f-t(100%)   linear eval   supervised\n50      1x      false   24          57.9       68.4       76.3        71.7          76.6\n50      1x      true    35          64.5       72.1       78.7        74.6          78.5\n50      2x      false   94          66.3       73.9       79.1        75.6          77.8\n50      2x      true    140         70.6       77.0       81.3        77.7          79.3\n101     1x      false   43          62.1       71.4       78.2        73.6          78.0\n101     1x      true    65          68.3       75.1       80.6        76.3          79.6\n101     2x      false   170         69.1       75.8       80.7        77.0          78.9\n101     2x      true    257         73.2       78.8       82.4        79.0          80.1\n152     1x      false   58          64.0       73.0       79.3        74.5          78.3\n152     1x      true    89          70.0       76.5       81.3        77.2          79.9\n152     2x      false   233         70.2       76.6       81.1        77.4          79.1\n152     2x      true    354         74.2       79.4       82.9        79.4          80.4\n152     3x      true    795         74.9       80.1       83.1        79.8          80.5\n\n\n# 03、simsiam: exploring simple siamese representation learning\n\nhttps://github.com/facebookresearch/simsiam\n\n\n# models and logs\n\nour pre-trained resnet-50 models and logs:\n\npre-train epochs   batch size   pre-train ckpt   pre-train log   linear cls. ckpt   linear cls. log   top-1 acc.\n100                512          link             link            link               link              68.1\n100                256          link             link            link               link              68.3\n\n\n# 04、understanding dimensional collapse in contrastive self-supervised learning\n\n\n# 05、improving contrastive learning by visualizing feature transformation\n\nhttps://github.com/dtennant/cl-visualizing-feature-transformation\n\nmodels\n\nfor your convenience, we provide the following pre-trained models on imagenet-1k and imagenet-100.\n\npre-train method   pre-train dataset   backbone    #epoch   imagenet-1k          voc det ap50   coco det ap   link\nsupervised         imagenet-1k         resnet-50   -        76.1                 81.3           38.2          download\nmoco-v1            imagenet-1k         resnet-50   200      60.6                 81.5           38.5          download\nmoco-v1+ft         imagenet-1k         resnet-50   200      61.9                 82.0           39.0          download\nmoco-v2            imagenet-1k         resnet-50   200      67.5                 82.4           39.0          download\nmoco-v2+ft         imagenet-1k         resnet-50   200      69.6                 83.3           39.5          download\nmoco-v1+ft         imagenet-100        resnet-50   200      in-100 result 77.2   -              -             download\n\n\n# 06、propagate yourself: exploring pixel-level consistency for unsupervised visual representation learning\n\n\n# 6.1 pascal voc object detection\n\n# faster-rcnn with c4\n\nmethod          epochs   arch        ap     ap50   ap75   download\nscratch         -        resnet-50   33.8   60.2   33.1   -\nsupervised      100      resnet-50   53.5   81.3   58.8   -\nmoco            200      resnet-50   55.9   81.5   62.6   -\nsimclr          1000     resnet-50   56.3   81.9   62.5   -\nmoco v2         800      resnet-50   57.6   82.7   64.4   -\ninfomin         200      resnet-50   57.6   82.7   64.6   -\ninfomin         800      resnet-50   57.5   82.5   64.0   -\npixpro (ours)   100      resnet-50   58.8   83.0   66.5   config | model\npixpro (ours)   400      resnet-50   60.2   83.8   67.7   config | model\n\n\n# 6.2 coco object detection\n\n# mask-rcnn with fpn\n\nmethod          epochs   arch        schedule   bbox ap   mask ap   download\nscratch         -        resnet-50   1x         32.8      29.9      -\nsupervised      100      resnet-50   1x         39.7      35.9      -\nmoco            200      resnet-50   1x         39.4      35.6      -\nsimclr          1000     resnet-50   1x         39.8      35.9      -\nmoco v2         800      resnet-50   1x         40.4      36.4      -\ninfomin         200      resnet-50   1x         40.6      36.7      -\ninfomin         800      resnet-50   1x         40.4      36.6      -\npixpro (ours)   100      resnet-50   1x         40.8      36.8      config | model\npixpro (ours)   100*     resnet-50   1x         41.3      37.1      -\npixpro (ours)   400*     resnet-50   1x         41.4      37.4      -\n\n* indicates methods with instance branch.\n\n# mask-rcnn with c4\n\nmethod          epochs   arch        schedule   bbox ap   mask ap   download\nscratch         -        resnet-50   1x         26.4      29.3      -\nsupervised      100      resnet-50   1x         38.2      33.3      -\nmoco            200      resnet-50   1x         38.5      33.6      -\nsimclr          1000     resnet-50   1x         38.4      33.6      -\nmoco v2         800      resnet-50   1x         39.5      34.5      -\ninfomin         200      resnet-50   1x         39.0      34.1      -\ninfomin         800      resnet-50   1x         38.8      33.8      -\npixpro (ours)   100      resnet-50   1x         40.0      34.8      config | model\npixpro (ours)   400      resnet-50   1x         40.5      35.3      config | model\n\n\n# 07、cvpr2021 | online bag-of-visual-words generation for unsupervised representation learning\n\nhttps://github.com/valeoai/obow\n\n# 7.1 resnet50 pre-trained model\n\nmethod   epochs   batch-size   dataset    imagenet linear acc.   links to pre-trained weights\nobow     200      256          imagenet   73.8                   entire model / only feature extractor\n\n\n# 08、neurips 2020 | unsupervised learning of visual features by contrasting cluster assignments\n\nhttps://github.com/facebookresearch/swav\n\nmethod           epochs   batch-size   multi-crop     imagenet top-1 acc.   url     args\nswav             800      4096         2x224 + 6x96   75.3                  model   script\nswav             400      4096         2x224 + 6x96   74.6                  model   script\nswav             200      4096         2x224 + 6x96   73.9                  model   script\nswav             100      4096         2x224 + 6x96   72.1                  model   script\nswav             200      256          2x224 + 6x96   72.7                  model   script\nswav             400      256          2x224 + 6x96   74.3                  model   script\nswav             400      4096         2x224          70.1                  model   script\ndeepcluster-v2   800      4096         2x224 + 6x96   75.2                  model   script\ndeepcluster-v2   400      4096         2x160 + 4x96   74.3                  model   script\ndeepcluster-v2   400      4096         2x224          70.2                  model   script\nsela-v2          400      4096         2x160 + 4x96   71.8                  model   -\nsela-v2          400      4096         2x224          67.2                  model   -\n\n\n# 09、eccv 2020 | learning to classify images without labels\n\nhttps://github.com/wvangansbeke/unsupervised-classification\n\nwe also train scan on imagenet for 1000 clusters. we use 10 clusterheads and finally take the head with the lowest loss. the accuracy (acc), normalized mutual information (nmi), adjusted mutual information (ami) and adjusted rand index (ari) are computed:\n\nmethod            acc    nmi    ami    ari    download link\nscan (resnet50)   39.9   72.0   51.2   27.5   download\n\n\n# 10、icml 2020 | self-supervised prototypical transfer learning for few-shot classification\n\nhttps://github.com/indy-lab/prototransfer\n\n\n# 11、neurips 2020 | bootstrap your own latent\n\nhttps://github.com/deepmind/deepmind-research/tree/master/byol\n\nusing this implementation should achieve a top-1 accuracy on imagenet between 74.0% and 74.5% after about 8h of training using 512 cloud tpu v3.\n\n\n# 12、efficient self-supervised vision transformers\n\nhttps://github.com/microsoft/esvit\n\n# 12.1 pretrained models\n\nyou can download the full checkpoint (trained with both view-level and region-level tasks, batch size=512 and imagenet-1k.), which contains backbone and projection head weights for both student and teacher networks.\n\n * esvit (swin) with network configurations of increased model capacities, pre-trained with both view-level and region-level tasks. resnet-50 trained with both tasks is shown as a reference.\n\narch                   params   linear   k-nn    download    logs             \nresnet-50              23m      75.7%    71.3%   full ckpt   train   linear   knn\nesvit (swin-t, w=7)    28m      78.0%    75.7%   full ckpt   train   linear   knn\nesvit (swin-s, w=7)    49m      79.5%    77.7%   full ckpt   train   linear   knn\nesvit (swin-b, w=7)    87m      80.4%    78.9%   full ckpt   train   linear   knn\nesvit (swin-t, w=14)   28m      78.7%    77.0%   full ckpt   train   linear   knn\nesvit (swin-s, w=14)   49m      80.8%    79.1%   full ckpt   train   linear   knn\nesvit (swin-b, w=14)   87m      81.3%    79.3%   full ckpt   train   linear   knn\n\n\n# 13、emerging properties in self-supervised vision transformers.\n\nhttps://github.com/facebookresearch/dino\n\n# 13.1 pretrained models\n\nyou can choose to download only the weights of the pretrained backbone used for downstream tasks, or the full checkpoint which contains backbone and projection head weights for both student and teacher networks. we also provide the backbone in onnx format, as well as detailed arguments and training/evaluation logs. note that deit-s and vit-s names refer exactly to the same architecture.\n\narch        params   k-nn    linear   download                                         \nvit-s/16    21m      74.5%   77.0%    backbone only   full ckpt   onnx   args   logs   eval logs\nvit-s/8     21m      78.3%   79.7%    backbone only   full ckpt   onnx   args   logs   eval logs\nvit-b/16    85m      76.1%   78.2%    backbone only   full ckpt   onnx   args   logs   eval logs\nvit-b/8     85m      77.4%   80.1%    backbone only   full ckpt   onnx   args   logs   eval logs\nresnet-50   23m      67.5%   75.3%    backbone only   full ckpt   onnx   args   logs   eval logs\n\nwe also release xcit models ([arxiv] [code]) trained with dino:\n\narch                 params   k-nn    linear   download                                  \nxcit_small_12_p16    26m      76.0%   77.8%    backbone only   full ckpt   args   logs   eval\nxcit_small_12_p8     26m      77.1%   79.2%    backbone only   full ckpt   args   logs   eval\nxcit_medium_24_p16   84m      76.4%   78.8%    backbone only   full ckpt   args   logs   eval\nxcit_medium_24_p8    84m      77.9%   80.3%    backbone only   full ckpt   args   logs   eval",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Awesome-weakly-supervised-semantic-segmentation",frontmatter:{title:"Awesome-weakly-supervised-semantic-segmentation",date:"2021-10-14T13:24:30.000Z",permalink:"/pages/865735/",categories:["学术搬砖","论文阅读-弱监督图像分割"],tags:[null]},regularPath:"/01.%E5%AD%A6%E6%9C%AF%E6%90%AC%E7%A0%96/13.%E5%AD%A6%E6%9C%AF%E6%96%87%E7%AB%A0%E6%90%9C%E9%9B%86/00.Awesome-Academic-Articals.html",relativePath:"01.学术搬砖/13.学术文章搜集/00.Awesome-Academic-Articals.md",key:"v-75d523ea",path:"/pages/865735/",headersStr:null,content:"Tricks\n\n * CAN：借助先验分布提升分类性能的简单后处理技巧：https://kexue.fm/archives/8728\n   \n   * 用先验分布来校正低置信度的预测结果，使得新的预测结果的分布更接近先验分布。\n\n * 数据增强：图像分类训练技巧之数据增强篇:https://zhuanlan.zhihu.com/p/430563265\n\n知识蒸馏\n\n * KDD 2020 | 优势特征蒸馏在淘宝推荐中的应用\n\n * 深度学习中的知识蒸馏技术（上）\n\nGCN\n\n * 2020年，我终于决定入门GCN\n\n * graph convolutional network有什么比较好的应用task？\n\n * \n\n图像分割\n\n * 单阶段实例分割综述\n\n端侧网络\n\n * 反向 Dropout！韩松团队最新工作NetAug：https://mp.weixin.qq.com/s/tBEpRZ3mAQpEmc27g2HuEA\n   * 现有的正则技术(比如数据增强、dropout)在大网络方面(比如ResNet50)方面通过添加噪声使其避免过拟合取得了极大成功。然而，我们发现：这些正则技术会损害TinyNN的性能(见下图)。 我们认为：不同于大网络通过增广数据提升性能，TinyNN应当通过增广模型提升性能 。这是因为：受限于模型大小，TinyNN往往存在欠拟合现象而非过拟合 。\n\n工业界实践\n\n * 图像检索在高德地图POI数据生产中的应用：https://www.modb.pro/db/156377\n   * \n   * 多模态图像检索\n\n学术\n\n * 在做算法工程师的道路上，你掌握了什么概念或技术使你感觉自我提升突飞猛进？：https://www.zhihu.com/question/436874654",normalizedContent:"tricks\n\n * can：借助先验分布提升分类性能的简单后处理技巧：https://kexue.fm/archives/8728\n   \n   * 用先验分布来校正低置信度的预测结果，使得新的预测结果的分布更接近先验分布。\n\n * 数据增强：图像分类训练技巧之数据增强篇:https://zhuanlan.zhihu.com/p/430563265\n\n知识蒸馏\n\n * kdd 2020 | 优势特征蒸馏在淘宝推荐中的应用\n\n * 深度学习中的知识蒸馏技术（上）\n\ngcn\n\n * 2020年，我终于决定入门gcn\n\n * graph convolutional network有什么比较好的应用task？\n\n * \n\n图像分割\n\n * 单阶段实例分割综述\n\n端侧网络\n\n * 反向 dropout！韩松团队最新工作netaug：https://mp.weixin.qq.com/s/tbeprz3maqpemc27g2huea\n   * 现有的正则技术(比如数据增强、dropout)在大网络方面(比如resnet50)方面通过添加噪声使其避免过拟合取得了极大成功。然而，我们发现：这些正则技术会损害tinynn的性能(见下图)。 我们认为：不同于大网络通过增广数据提升性能，tinynn应当通过增广模型提升性能 。这是因为：受限于模型大小，tinynn往往存在欠拟合现象而非过拟合 。\n\n工业界实践\n\n * 图像检索在高德地图poi数据生产中的应用：https://www.modb.pro/db/156377\n   * \n   * 多模态图像检索\n\n学术\n\n * 在做算法工程师的道路上，你掌握了什么概念或技术使你感觉自我提升突飞猛进？：https://www.zhihu.com/question/436874654",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"基于深度学习的目标检测技术",frontmatter:{title:"基于深度学习的目标检测技术",date:"2021-03-31T10:01:09.000Z",permalink:"/pages/f542b4/",categories:["计算机视觉","目标检测"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/01.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/00.%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%8A%80%E6%9C%AF.html",relativePath:"02.学习笔记/01.代码实践-目标检测/00.基于深度学习的目标检测技术.md",key:"v-4a6374f4",path:"/pages/f542b4/",headers:[{level:2,title:"基于深度学习的目标检测技术",slug:"基于深度学习的目标检测技术",normalizedTitle:"基于深度学习的目标检测技术",charIndex:2}],headersStr:"基于深度学习的目标检测技术",content:"# 基于深度学习的目标检测技术\n\n# 参考代码\n\nYOLOv5 in DOTA with CSL_label.(Oriented Object Detection)（Rotation Detection）（Rotated BBox）基于YOLOv5的旋转目标检测",normalizedContent:"# 基于深度学习的目标检测技术\n\n# 参考代码\n\nyolov5 in dota with csl_label.(oriented object detection)（rotation detection）（rotated bbox）基于yolov5的旋转目标检测",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"mmdetection voc",frontmatter:{title:"mmdetection voc",date:"2021-05-30T13:59:00.000Z",permalink:"/pages/0d3c60/",categories:["计算机视觉","目标检测"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/01.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/01.mmdetection%20voc.html",relativePath:"02.学习笔记/01.代码实践-目标检测/01.mmdetection voc.md",key:"v-232d2f76",path:"/pages/0d3c60/",headersStr:null,content:"mmdetection voc：https://www.daimajiaoliu.com/daima/4798062de9003f0\n\nmmdetection coco：\n\n1、ValueError:\n\nneed at least one array to concatenate",normalizedContent:"mmdetection voc：https://www.daimajiaoliu.com/daima/4798062de9003f0\n\nmmdetection coco：\n\n1、valueerror:\n\nneed at least one array to concatenate",charsets:{},lastUpdated:"2021/08/02, 21:04:52"},{title:"cowfits竞赛记录",frontmatter:{title:"cowfits竞赛记录",date:"2021-08-03T23:37:44.000Z",permalink:"/pages/c06123/",categories:["学习笔记","代码实践-目标检测"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/01.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/04.cowfits%E7%AB%9E%E8%B5%9B%E8%AE%B0%E5%BD%95.html",relativePath:"02.学习笔记/01.代码实践-目标检测/04.cowfits竞赛记录.md",key:"v-608f1e1c",path:"/pages/c06123/",headersStr:null,content:"# kaggle竞赛记录-CowBoy Outfits Detection\n\n# update 进度\n\n8月2日\n\n * 将 Notebook 里面的 config 跑通了，训了 20 个epoch，cowfitsv1 在验证集上的 best_mAP 是 0.7070\n\n8月3日\n\n * 写好了训练集和验证结果的可视化脚本，发现了标注的区别，voc 标注是 xyxy，coco 标注是 xywh\n\n * 将 cowfitsv1 提交至 codalab 上，public score 是 24.5599559956，出现了严重的线上线下不一致的问题\n\n * 数据方面的排查：是否是训练集中较少的类在测试集上较多，导致拟合不完全\n   \n   * 训练集存在标注噪声\n     * case id：67、72、77、84\n     * 解决方案：使用伪标签+高阈值修正\n   * md 原来 train 也是用的 valid 集合\n\n * 模型方面的排查\n   \n   * cowfits_resnest50_cascade_rcnn_fp16_coslr.py\n     \n     * 在 cowfits.py 的基础上将 backbone 换成 resnest50，换为了训练 24 Epoch\n   \n   * cowfits_resnest50_cascade_rcnn_fp16_steplr.py\n     \n     * 在 cowfits_resnest50_cascade_rcnn_fp16_coslr.py 基础上换为 steplr\n   \n   * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1.py\n     \n     * 在 cowfits_resnest50_cascade_rcnn_fp16_steplr.py 基础上添加 augmentation\n   \n   * cowfits_resnext101_cascade_rcnn_fp16_steplr.py\n     \n     * 在 cowfits.py 的基础上换为了训练 24 Epoch，并且使用 steplr\n   \n   * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1。py\n   \n   * ``\n\n8月4日\n\n * 8月3日结果分析\n   * cowfits_resnest50_cascade_rcnn_fp16_coslr\n     * best_bbox_mAP: 22 epoch, 0.3980\n     * 线上成绩：12.596259626\n   * cowfits_resnest50_cascade_rcnn_fp16_steplr\n     * best_bbox_mAP: 09 epoch, 0.4630\n     * 线上成绩：16.1779035046\n   * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1\n     * best_bbox_mAP: 21 epoch, 0.3880\n     * 线上成绩：33.3626696003\n   * cowfits_resnext101_cascade_rcnn_fp16_steplr\n     * best_bbox_mAP: 06 epoch, 0.3520\n     * 线上成绩：21.397139714\n\n8月5日\n\n * 对每个类别的数量作分析，探究类别不平衡问题\n\n * 对目标的尺寸作分析，并做可视化分析（类似yolov5），可以去借鉴代码\n\n * 对验证集的预测情况做混淆矩阵分析，看是哪部分样本容易预测错\n\n * 对昨天的多个模型做 TTA 测试以及 NMS 集成\n\n * 对模型进行调整\n   \n   * 调整训练尺度到1024\n     \n     * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1_1024.py\n       \n       \n       1\n       \n       * best_bbox_mAP: 24 epoch, 0.4230\n       * 线上成绩：33.2590759076\n     \n     * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1_1024_epoch5.py\n       \n       * best_bbox_mAP: 5 epoch, 0.0880\n       * 线上成绩： 4.9394939494\n     \n     * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1_1024_epoch10.py\n       \n       * best_bbox_mAP: 10 epoch, 0.2250\n       * 线上成绩：26.7409240924\n     \n     * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1_1024_epoch15.py\n       \n       * best_bbox_mAP: 15 epoch, 0.3760\n       * 线上成绩：30.0110011001\n     \n     * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1_1024_epoch20.py\n       \n       * best_bbox_mAP: 20 epoch, 0.3320\n       * 线上成绩：32.5043218608",normalizedContent:"# kaggle竞赛记录-cowboy outfits detection\n\n# update 进度\n\n8月2日\n\n * 将 notebook 里面的 config 跑通了，训了 20 个epoch，cowfitsv1 在验证集上的 best_map 是 0.7070\n\n8月3日\n\n * 写好了训练集和验证结果的可视化脚本，发现了标注的区别，voc 标注是 xyxy，coco 标注是 xywh\n\n * 将 cowfitsv1 提交至 codalab 上，public score 是 24.5599559956，出现了严重的线上线下不一致的问题\n\n * 数据方面的排查：是否是训练集中较少的类在测试集上较多，导致拟合不完全\n   \n   * 训练集存在标注噪声\n     * case id：67、72、77、84\n     * 解决方案：使用伪标签+高阈值修正\n   * md 原来 train 也是用的 valid 集合\n\n * 模型方面的排查\n   \n   * cowfits_resnest50_cascade_rcnn_fp16_coslr.py\n     \n     * 在 cowfits.py 的基础上将 backbone 换成 resnest50，换为了训练 24 epoch\n   \n   * cowfits_resnest50_cascade_rcnn_fp16_steplr.py\n     \n     * 在 cowfits_resnest50_cascade_rcnn_fp16_coslr.py 基础上换为 steplr\n   \n   * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1.py\n     \n     * 在 cowfits_resnest50_cascade_rcnn_fp16_steplr.py 基础上添加 augmentation\n   \n   * cowfits_resnext101_cascade_rcnn_fp16_steplr.py\n     \n     * 在 cowfits.py 的基础上换为了训练 24 epoch，并且使用 steplr\n   \n   * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1。py\n   \n   * ``\n\n8月4日\n\n * 8月3日结果分析\n   * cowfits_resnest50_cascade_rcnn_fp16_coslr\n     * best_bbox_map: 22 epoch, 0.3980\n     * 线上成绩：12.596259626\n   * cowfits_resnest50_cascade_rcnn_fp16_steplr\n     * best_bbox_map: 09 epoch, 0.4630\n     * 线上成绩：16.1779035046\n   * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1\n     * best_bbox_map: 21 epoch, 0.3880\n     * 线上成绩：33.3626696003\n   * cowfits_resnext101_cascade_rcnn_fp16_steplr\n     * best_bbox_map: 06 epoch, 0.3520\n     * 线上成绩：21.397139714\n\n8月5日\n\n * 对每个类别的数量作分析，探究类别不平衡问题\n\n * 对目标的尺寸作分析，并做可视化分析（类似yolov5），可以去借鉴代码\n\n * 对验证集的预测情况做混淆矩阵分析，看是哪部分样本容易预测错\n\n * 对昨天的多个模型做 tta 测试以及 nms 集成\n\n * 对模型进行调整\n   \n   * 调整训练尺度到1024\n     \n     * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1_1024.py\n       \n       \n       1\n       \n       * best_bbox_map: 24 epoch, 0.4230\n       * 线上成绩：33.2590759076\n     \n     * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1_1024_epoch5.py\n       \n       * best_bbox_map: 5 epoch, 0.0880\n       * 线上成绩： 4.9394939494\n     \n     * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1_1024_epoch10.py\n       \n       * best_bbox_map: 10 epoch, 0.2250\n       * 线上成绩：26.7409240924\n     \n     * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1_1024_epoch15.py\n       \n       * best_bbox_map: 15 epoch, 0.3760\n       * 线上成绩：30.0110011001\n     \n     * cowfits_resnest50_cascade_rcnn_fp16_steplr_augmentationv1_1024_epoch20.py\n       \n       * best_bbox_map: 20 epoch, 0.3320\n       * 线上成绩：32.5043218608",charsets:{cjk:!0},lastUpdated:"2021/08/08, 21:56:44"},{title:"领域自适应",frontmatter:{title:"领域自适应",date:"2021-04-14T23:37:22.000Z",permalink:"/pages/7302ec/",categories:["计算机视觉","图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/01.%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94.html",relativePath:"02.学习笔记/02.代码实践-图像分割/01.领域自适应.md",key:"v-e9f21f7c",path:"/pages/7302ec/",headers:[{level:3,title:"1、什么是Domain Adaptation？",slug:"_1、什么是domain-adaptation",normalizedTitle:"1、什么是domain adaptation？",charIndex:2},{level:3,title:"2、Domain Adaptation 的发展现状",slug:"_2、domain-adaptation-的发展现状",normalizedTitle:"2、domain adaptation 的发展现状",charIndex:343},{level:3,title:"3、参考资料",slug:"_3、参考资料",normalizedTitle:"3、参考资料",charIndex:1336}],headersStr:"1、什么是Domain Adaptation？ 2、Domain Adaptation 的发展现状 3、参考资料",content:"# 1、什么是Domain Adaptation？\n\nDomain Adaptation 是源任务和目标任务一样，但是源域和目标域的数据分布不一样，并且源域有大量标记好的样本的迁移学习方法。这样就是如何把源域从大量的有标记样本中学习的知识迁移到目标域上解决相同的问题。\n\n领域自适应（Domain Adaptation）是迁移学习中的一种代表性方法，其定义为：源域（source domain）和目标域（target domain）共享相同的特征和类别，但是特征分布不同，如何利用信息丰富的源域样本来提升目标域模型的性能。源域表示与测试样本不同的领域，具有丰富的监督标注信息；目标域表示测试样本所在的领域，无标签或者只有少量标签。源域和目标域往往属于同一类任务，但是分布不同。\n\n\n# 2、Domain Adaptation 的发展现状\n\n# 2.1 Traditional DA（传统的非深度学习的DA）\n\nTCA将源域和目标域一起映射到一个高维的再生核希尔伯特空间。在此空间中最小化源和目标的数据距离，同时最大程度地保留它们各自的内部属性。GFK 将源域和目标域视作 Grassmann流形 中的两个点，希望找到合适的变换组成一条测地线的路径。\n\n# 2.2 Discrepancy-based DA（基于差异的）\n\nDDC 加入 MMD 距离来缩小源域和目标域的差异，有助于学习到对领域不敏感的特征表示。DAN加入多个适配层，并且用多核 MMD 替换单核 MMD，提升了域适应的性能。RTN 认为条件分布差异是由一个扰动函数，但是可以通过两层残差项（residual block）联系起来，利用Residual Function来区分source classifier和target classifier。JAN提出JMMD用于度量多个网络层的联合分布的差异。\n\n# 2.3 Adversarial-based DA（基于对抗的）\n\nRevGrad主要提出了利用一个domain classifier来增强网络迁移性的方法，基于对抗的方法来进行优化。iCAN 结合了collaborative learning 和 adversarial learning对lower blocks 和 higher blocks 分别制定了学习目标。MADA为了避免使用单个判别器时，不同种类的样本被错误地对齐，作者提出了用多个判别器来捕捉多模式结构，判别器的个数正好等于原域样本的种类数。 Weighted Adversarial Nets 选择出源域中与目标域那部分类别最接近的样本，给它们赋予高权重，然后进行迁移。\n\n# 2.4 Reconstruction-based DA（基于重建的）\n\nDRCN 利用源域和目标域共同提取特征，要求这些特征同时适合分类源域和重构目标域。DSN 提取不同域之间的公有特征以及利用公有特征进行迁移避免负迁移（negative transfer）\n\n# 2.5 Others\n\nAsymmetric Tri-training 在使用2个Classifier去构建目标域的Pseudo Label，另外一个分类器学习伪标签的特征表达\n\n\n# 3、参考资料\n\n * 《迁移学习简明手册》\n\n * Deep Visual Domain Adaptation: A Survey\n\n * [深度迁移学习综述 PPT](http://whdeng.cn/papers/deep%20domain%20adaptation%20tutorial-- small.pdf)\n\n- 深度迁移学习综述讲解视频",normalizedContent:"# 1、什么是domain adaptation？\n\ndomain adaptation 是源任务和目标任务一样，但是源域和目标域的数据分布不一样，并且源域有大量标记好的样本的迁移学习方法。这样就是如何把源域从大量的有标记样本中学习的知识迁移到目标域上解决相同的问题。\n\n领域自适应（domain adaptation）是迁移学习中的一种代表性方法，其定义为：源域（source domain）和目标域（target domain）共享相同的特征和类别，但是特征分布不同，如何利用信息丰富的源域样本来提升目标域模型的性能。源域表示与测试样本不同的领域，具有丰富的监督标注信息；目标域表示测试样本所在的领域，无标签或者只有少量标签。源域和目标域往往属于同一类任务，但是分布不同。\n\n\n# 2、domain adaptation 的发展现状\n\n# 2.1 traditional da（传统的非深度学习的da）\n\ntca将源域和目标域一起映射到一个高维的再生核希尔伯特空间。在此空间中最小化源和目标的数据距离，同时最大程度地保留它们各自的内部属性。gfk 将源域和目标域视作 grassmann流形 中的两个点，希望找到合适的变换组成一条测地线的路径。\n\n# 2.2 discrepancy-based da（基于差异的）\n\nddc 加入 mmd 距离来缩小源域和目标域的差异，有助于学习到对领域不敏感的特征表示。dan加入多个适配层，并且用多核 mmd 替换单核 mmd，提升了域适应的性能。rtn 认为条件分布差异是由一个扰动函数，但是可以通过两层残差项（residual block）联系起来，利用residual function来区分source classifier和target classifier。jan提出jmmd用于度量多个网络层的联合分布的差异。\n\n# 2.3 adversarial-based da（基于对抗的）\n\nrevgrad主要提出了利用一个domain classifier来增强网络迁移性的方法，基于对抗的方法来进行优化。ican 结合了collaborative learning 和 adversarial learning对lower blocks 和 higher blocks 分别制定了学习目标。mada为了避免使用单个判别器时，不同种类的样本被错误地对齐，作者提出了用多个判别器来捕捉多模式结构，判别器的个数正好等于原域样本的种类数。 weighted adversarial nets 选择出源域中与目标域那部分类别最接近的样本，给它们赋予高权重，然后进行迁移。\n\n# 2.4 reconstruction-based da（基于重建的）\n\ndrcn 利用源域和目标域共同提取特征，要求这些特征同时适合分类源域和重构目标域。dsn 提取不同域之间的公有特征以及利用公有特征进行迁移避免负迁移（negative transfer）\n\n# 2.5 others\n\nasymmetric tri-training 在使用2个classifier去构建目标域的pseudo label，另外一个分类器学习伪标签的特征表达\n\n\n# 3、参考资料\n\n * 《迁移学习简明手册》\n\n * deep visual domain adaptation: a survey\n\n * [深度迁移学习综述 ppt](http://whdeng.cn/papers/deep%20domain%20adaptation%20tutorial-- small.pdf)\n\n- 深度迁移学习综述讲解视频",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"如何计算一个模型的FPS,Params,GFLOPs",frontmatter:{title:"如何计算一个模型的FPS,Params,GFLOPs",date:"2021-04-14T23:37:22.000Z",permalink:"/pages/4e1e41/",categories:["计算机视觉","图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/02.%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B%E7%9A%84FPS,Params,GFLOPs.html",relativePath:"02.学习笔记/02.代码实践-图像分割/02.如何计算一个模型的FPS,Params,GFLOPs.md",key:"v-05797640",path:"/pages/4e1e41/",headers:[{level:2,title:"关于FPS，GFLOPs 以及 Params 的解释",slug:"关于fps-gflops-以及-params-的解释",normalizedTitle:"关于fps，gflops 以及 params 的解释",charIndex:2}],headersStr:"关于FPS，GFLOPs 以及 Params 的解释",content:'# 关于FPS，GFLOPs 以及 Params 的解释\n\n衡量一个模型是否轻量，常常使用这三个指标：FPS，GFLOPs，Params(M)\n\n * FPS: frames per second,即：每秒帧数，用于衡量模型的实时性能\n * GFLOPs: 是G floating point operations的缩写（s表复数)，即：10亿次浮点运算，用于衡量模型的计算量\n * Params(M): 是 Parameters，即：参数量，用于衡量模型的复杂度\n\n其中flops是容易产生歧义的，解释如下，参考 chen liu的回答\n\n * FLOPS：注意全大写，是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。\n * FLOPs：注意s小写，是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。\n * 1 GFLOPs = 10^9 FLOPs 即：10亿次浮点运算\n\n# 计算模型的参数量和GFLOPS\n\n参考代码：\n\n * https://github.com/Lyken17/pytorch-OpCounter\n * https://github.com/sovrasov/flops-counter.pytorch\n\n# 如何准确的计算一个模型的FPS?\n\nres = []\nfor id, (data, depth, img_name, img_size) in enumerate(test_loader):\n    torch.cuda.synchronize()\n    start = time.time()\n    predict= model_rgb(inputs, depth)  # 有待修改\n    torch.cuda.synchronize()\n    end = time.time()\n    res.append(end-start)\ntime_sum = 0\nfor i in res:\n    time_sum += i\nprint("FPS: %f"%(1.0/(time_sum/len(res))))\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n',normalizedContent:'# 关于fps，gflops 以及 params 的解释\n\n衡量一个模型是否轻量，常常使用这三个指标：fps，gflops，params(m)\n\n * fps: frames per second,即：每秒帧数，用于衡量模型的实时性能\n * gflops: 是g floating point operations的缩写（s表复数)，即：10亿次浮点运算，用于衡量模型的计算量\n * params(m): 是 parameters，即：参数量，用于衡量模型的复杂度\n\n其中flops是容易产生歧义的，解释如下，参考 chen liu的回答\n\n * flops：注意全大写，是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。\n * flops：注意s小写，是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。\n * 1 gflops = 10^9 flops 即：10亿次浮点运算\n\n# 计算模型的参数量和gflops\n\n参考代码：\n\n * https://github.com/lyken17/pytorch-opcounter\n * https://github.com/sovrasov/flops-counter.pytorch\n\n# 如何准确的计算一个模型的fps?\n\nres = []\nfor id, (data, depth, img_name, img_size) in enumerate(test_loader):\n    torch.cuda.synchronize()\n    start = time.time()\n    predict= model_rgb(inputs, depth)  # 有待修改\n    torch.cuda.synchronize()\n    end = time.time()\n    res.append(end-start)\ntime_sum = 0\nfor i in res:\n    time_sum += i\nprint("fps: %f"%(1.0/(time_sum/len(res))))\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n',charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"常见数据集的相关知识",frontmatter:{title:"常见数据集的相关知识",date:"2021-05-07T16:46:58.000Z",permalink:"/pages/679017/",categories:["计算机视觉","图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/03.%E5%B8%B8%E8%A7%81%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86.html",relativePath:"02.学习笔记/02.代码实践-图像分割/03.常见数据集的相关知识.md",key:"v-1218f70b",path:"/pages/679017/",headers:[{level:3,title:"1、Cityscapes 数据集",slug:"_1、cityscapes-数据集",normalizedTitle:"1、cityscapes 数据集",charIndex:2},{level:3,title:"2、VOC 数据集",slug:"_2、voc-数据集",normalizedTitle:"2、voc 数据集",charIndex:524},{level:3,title:"4、ADE20k 数据集",slug:"_4、ade20k-数据集",normalizedTitle:"4、ade20k 数据集",charIndex:3367},{level:3,title:"5、COCO数据集",slug:"_5、coco数据集",normalizedTitle:"5、coco数据集",charIndex:3384}],headersStr:"1、Cityscapes 数据集 2、VOC 数据集 4、ADE20k 数据集 5、COCO数据集",content:'# 1、Cityscapes 数据集\n\nCityscapes 是从五十个不同城市的街景视频序列中记录的数据集，拥有5000帧精细标注的图像，以及20000帧弱标注的图像。官方介绍中该数据集旨在用于：\n\n * 评估计算机视觉算法在城市场景理解主要任务上的性能，包含语义级、实例级和全景语义级别\n * 支持旨在利用大量弱标记数据的算法研究\n * 在CVPR 2020 的论文中，Cityscapes 数据集也拓展了 3D 边界框的标记\n * Cityscapes 数据集官网：https://www.cityscapes-dataset.com/\n * Cityscapes 数据集下载地址：https://www.cityscapes-dataset.com/downloads/\n\n以下是 Fine annotations 和 Coarse annotations 的示例，在Fine annotations 中有 2975 张用于训练和，500 张用于验证，以及 1525 张用于测试。在Coarse annotations 中，有额外的 19998 张带噪标记的图像。此外还有带GPS、温度等元数据的数据集，此处不再介绍。\n\n\n\n\n\n\n\n\n# 2、VOC 数据集\n\nPASCAL VOC 挑战赛全称是 “Pattern Analysis, Statical Modeling and Computational Learning Visual Object Classes"，PASCAL 是欧盟赞助的组织。这个竞赛从 2005 年办到了 2012 年，比较常用的数据集有 VOC 2007 以及VOC 2012\n\n * PASCAL VOC 官网：http://host.robots.ox.ac.uk:8080/pascal/VOC/\n * PASCAL VOC 2007：http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2007/index.html\n * PASCAL VOC 2012：http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2012/index.html\n\n下面简要介绍以下两个数据集的区别：\n\n年份     数据统计                                                       新内容                                               备注\n2007   共有二十个类别：                                                   类别从10增加到20；支持分割任务；在注释中加入截断标志；分类任务的评估方法改为Average   设立了20个类别，该数量将固定下来，并且这是为测试数据发布注释的最后一年\n       Person: person                                             Precision，以前是 ROC-AUC\n       Animal: bird, cat, cow, dog, horse, sheep\n       Vehicle: aeroplane, bicycle, boat, bus, car, motorbike,\n       train\n       Indoor: bottle, chair, dining table, potted plant, sofa,\n       tv/monitor\n       共有9963张图像，包含了 24640 个带注释的目标\n2012   共有二十个类别，训练集和验证集共包含11530张图像，包含27450个ROI 注释，以及6929个分割标签      分割数据集的规模显著增加；动作分类数据集中的人像带有身体关键点的标注                分类，检测以及person layout的数据集和 VOC2011 一样\n\n根据官网的分割 Examples 中所介绍的，类别索引是根据字典序来的：\n\n(1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle, 6=bus, 7=car , 8=cat, 9=chair, 10=cow, 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person, 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor)\n\n\n1\n\n\n在这篇 知乎文章 中有PASCAL VOC 数据集类别到调色板的映射\n\n\n\n4、PASCAL VOC 2012 Aug\n\n\n\n额外的数据来自论文《Semantic Contours from Inverse Detectors》\n\n * https://people.cs.umass.edu/~smaji/papers/contours-iccv11.pdf\n * http://home.bharathh.info/pubs/codes/SBD/download.html\n\n下面讲述以下如何将原始的PASCAL VOC 2012数据集和增强版的数据集合并到一起\n\nVOCdevkit/VOC2012为原始PASCAL VOC 2012数据集\n\n * images数据集的文件名为：JPEGImages，共17125张图片（其中2913张用于分割）\n * labels数据集文件名为：SegmentationClass，共2913张图片\n * 其中官方划分的train.txt 有1464张，val.txt有1449，测试集有1456张\n\nbenchmark_RELEASE为增强数据集\n\n * images数据集的文件名为：img，共11355张图片\n * labels数据集文件名为：inst，共11355张图片，为mat格式的(matlab格式)\n * 其中官方划分的train.txt 有8498张，val.txt有2857张\n\nvoc数据集标签：voc_trainval：2913 ，voc_train：1464，voc_val：1449\n\nsbd数据集标签：sbd_train：8498，sbd_val：2857\n\nsbd_train(8498)`=`和voc_train重复的图片(1133)`+`和voc_val重复的图片(545)`+`sbd_train真正补充的图片(6820)\n\n\n1\n\n\nsbd_val(2857)`=`和voc_train重复的图片(1)`+`和voc_val重复的图片(558)`+`sbd_val真正补充的图片(2298)\n\n\n1\n\n\n所以可以得到的最大的扩充数据集应为：\n\nvoc_train(1464)+voc_val(1449)+sbd_train真正补充的图片(6820)+sbd_val真正补充的图片(2298)=12031张标注图\n\n用原来的voc_val(1449)作为验证集，剩下的12031-voc_val(1449)=10582都可以用作训练，就是trainaug(10582)\n\n合并之后的trainaug.txt 一共有10582张训练数据\n\n * https://gist.githubusercontent.com/sun11/2dbda6b31acc7c6292d14a872d0c90b7/raw/5f5a5270089239ef2f6b65b1cc55208355b5acca/trainaug.txt\n\n参考资料：\n\n * https://blog.csdn.net/lscelory/article/details/98180917\n * https://www.sun11.me/blog/2018/how-to-use-10582-trainaug-images-on-DeeplabV3-code/\n\n\n# 4、ADE20k 数据集\n\n\n# 5、COCO数据集',normalizedContent:'# 1、cityscapes 数据集\n\ncityscapes 是从五十个不同城市的街景视频序列中记录的数据集，拥有5000帧精细标注的图像，以及20000帧弱标注的图像。官方介绍中该数据集旨在用于：\n\n * 评估计算机视觉算法在城市场景理解主要任务上的性能，包含语义级、实例级和全景语义级别\n * 支持旨在利用大量弱标记数据的算法研究\n * 在cvpr 2020 的论文中，cityscapes 数据集也拓展了 3d 边界框的标记\n * cityscapes 数据集官网：https://www.cityscapes-dataset.com/\n * cityscapes 数据集下载地址：https://www.cityscapes-dataset.com/downloads/\n\n以下是 fine annotations 和 coarse annotations 的示例，在fine annotations 中有 2975 张用于训练和，500 张用于验证，以及 1525 张用于测试。在coarse annotations 中，有额外的 19998 张带噪标记的图像。此外还有带gps、温度等元数据的数据集，此处不再介绍。\n\n\n\n\n\n\n\n\n# 2、voc 数据集\n\npascal voc 挑战赛全称是 “pattern analysis, statical modeling and computational learning visual object classes"，pascal 是欧盟赞助的组织。这个竞赛从 2005 年办到了 2012 年，比较常用的数据集有 voc 2007 以及voc 2012\n\n * pascal voc 官网：http://host.robots.ox.ac.uk:8080/pascal/voc/\n * pascal voc 2007：http://host.robots.ox.ac.uk:8080/pascal/voc/voc2007/index.html\n * pascal voc 2012：http://host.robots.ox.ac.uk:8080/pascal/voc/voc2012/index.html\n\n下面简要介绍以下两个数据集的区别：\n\n年份     数据统计                                                       新内容                                               备注\n2007   共有二十个类别：                                                   类别从10增加到20；支持分割任务；在注释中加入截断标志；分类任务的评估方法改为average   设立了20个类别，该数量将固定下来，并且这是为测试数据发布注释的最后一年\n       person: person                                             precision，以前是 roc-auc\n       animal: bird, cat, cow, dog, horse, sheep\n       vehicle: aeroplane, bicycle, boat, bus, car, motorbike,\n       train\n       indoor: bottle, chair, dining table, potted plant, sofa,\n       tv/monitor\n       共有9963张图像，包含了 24640 个带注释的目标\n2012   共有二十个类别，训练集和验证集共包含11530张图像，包含27450个roi 注释，以及6929个分割标签      分割数据集的规模显著增加；动作分类数据集中的人像带有身体关键点的标注                分类，检测以及person layout的数据集和 voc2011 一样\n\n根据官网的分割 examples 中所介绍的，类别索引是根据字典序来的：\n\n(1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle, 6=bus, 7=car , 8=cat, 9=chair, 10=cow, 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person, 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor)\n\n\n1\n\n\n在这篇 知乎文章 中有pascal voc 数据集类别到调色板的映射\n\n\n\n4、pascal voc 2012 aug\n\n\n\n额外的数据来自论文《semantic contours from inverse detectors》\n\n * https://people.cs.umass.edu/~smaji/papers/contours-iccv11.pdf\n * http://home.bharathh.info/pubs/codes/sbd/download.html\n\n下面讲述以下如何将原始的pascal voc 2012数据集和增强版的数据集合并到一起\n\nvocdevkit/voc2012为原始pascal voc 2012数据集\n\n * images数据集的文件名为：jpegimages，共17125张图片（其中2913张用于分割）\n * labels数据集文件名为：segmentationclass，共2913张图片\n * 其中官方划分的train.txt 有1464张，val.txt有1449，测试集有1456张\n\nbenchmark_release为增强数据集\n\n * images数据集的文件名为：img，共11355张图片\n * labels数据集文件名为：inst，共11355张图片，为mat格式的(matlab格式)\n * 其中官方划分的train.txt 有8498张，val.txt有2857张\n\nvoc数据集标签：voc_trainval：2913 ，voc_train：1464，voc_val：1449\n\nsbd数据集标签：sbd_train：8498，sbd_val：2857\n\nsbd_train(8498)`=`和voc_train重复的图片(1133)`+`和voc_val重复的图片(545)`+`sbd_train真正补充的图片(6820)\n\n\n1\n\n\nsbd_val(2857)`=`和voc_train重复的图片(1)`+`和voc_val重复的图片(558)`+`sbd_val真正补充的图片(2298)\n\n\n1\n\n\n所以可以得到的最大的扩充数据集应为：\n\nvoc_train(1464)+voc_val(1449)+sbd_train真正补充的图片(6820)+sbd_val真正补充的图片(2298)=12031张标注图\n\n用原来的voc_val(1449)作为验证集，剩下的12031-voc_val(1449)=10582都可以用作训练，就是trainaug(10582)\n\n合并之后的trainaug.txt 一共有10582张训练数据\n\n * https://gist.githubusercontent.com/sun11/2dbda6b31acc7c6292d14a872d0c90b7/raw/5f5a5270089239ef2f6b65b1cc55208355b5acca/trainaug.txt\n\n参考资料：\n\n * https://blog.csdn.net/lscelory/article/details/98180917\n * https://www.sun11.me/blog/2018/how-to-use-10582-trainaug-images-on-deeplabv3-code/\n\n\n# 4、ade20k 数据集\n\n\n# 5、coco数据集',charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"如何加载数据集",frontmatter:{title:"如何加载数据集",date:"2021-04-15T17:26:00.000Z",permalink:"/pages/80ffb0/",categories:["计算机视觉","图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/04.%E5%A6%82%E4%BD%95%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86.html",relativePath:"02.学习笔记/02.代码实践-图像分割/04.如何加载数据集.md",key:"v-76238680",path:"/pages/80ffb0/",headers:[{level:2,title:"如何加载数据集",slug:"如何加载数据集",normalizedTitle:"如何加载数据集",charIndex:2}],headersStr:"如何加载数据集",content:"# 如何加载数据集\n\n# Cityscapes 数据集\n\n# 类别数量及其所对应的 trainId 以及 color\n\n\n\n在 官方脚本 中可以看到标签的情况，截图如上。分为 8 个大类，其中 human 和 vehicle 两大类是拥有实例标签的。此外我们只需关注 ignoreInEval 字段为 False 的类别即可，共 19 类，所以我们在语义分割中对于每个像素都有 20 类输出，还有一个背景类别。并且实际参与训练的类别 id 是 trainId 字段的内容。\n\n# 标签可视化\n\n\n\n对于每张图像，官方数据集里都对应了四张标注，示例如上。\n\n * aachen_000000_000019_gtFine_color.png：彩色标注可视化\n\n * aachen_000000_000019_gtFine_instanceIds.png：标出 human 以及 vehicle 两个大类的实例 id\n\n * aachen_000000_000019_gtFine_labelIds.png：按照 labelId 来做可视化\n\n * aachen_000000_000019_gtFine_labelTrainIds.png：按照 TrainId 来做可视化\n\n * 官方脚本：https://github.com/mcordts/cityscapesScripts\n\n# 如何构建 Cityscapes 数据集的 Dataloader\n\n# 使用 PyTorch 的官方接口\nimport torchvision\nimport numpy as np\n\nroot = r'./Cityscapes/'\n\n# target_type = ['instance', 'semantic', 'color', 'polygon']\ntarget_type = 'instance'\n\ncityscapes_dataset = torchvision.datasets.Cityscapes(root, split='train', mode='fine', target_type=target_type, transform=None, target_transform=None)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n我们可以写如上的代码使用 PyTorch 官方提供的 Dataset 接口直接使用，但这样我觉得不够灵活，所以我决定还是自己实现 Dataloader。\n\n# 手动实现\nimport os\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport mmcv\nimport glob\n\nclass cityscapesDataset(Dataset):\n    def __init__(self, root, split='train', img_size=(1024, 512), img_norm=True, gt_type='gtCoarse'):\n        super(cityscapesDataset, self).__init__()\n        self.name_dataset = 'Cityscapes'\n        self.num_classes = 19\n        self.colormap = {\n            0: (0, 0, 0),  # unlabeled\n            1: (0, 0, 0),  # ego vehicle\n            2: (0, 0, 0),  # rectification border\n            3: (0, 0, 0),  # out of roi\n            4: (0, 0, 0),  # static\n            5: (0, 0, 0),  # dynamic\n            6: (0, 0, 0),  # ground\n            7: (128, 64, 128),  # road\n            8: (244, 35, 232),  # sidewalk\n            9: (0, 0, 0),  # parking\n            10: (0, 0, 0),  # rail track\n            11: (70, 70, 70),  # building\n            12: (102, 102, 156),  # wall\n            13: (190, 153, 153),  # fence\n            14: (0, 0, 0),  # guard rail\n            15: (0, 0, 0),  # bridge\n            16: (0, 0, 0),  # tunnel\n            17: (153, 153, 153),  # pole\n            18: (0, 0, 0),  # polegroup\n            19: (250, 170, 30),  # traffic light\n            20: (220, 220, 0),  # traffic sign\n            21: (107, 142, 35),  # vegetation\n            22: (152, 251, 152),  # terrain\n            23: (0, 130, 180),  # sky\n            24: (220, 20, 60),  # person\n            25: (255, 0, 0),  # rider\n            26: (0, 0, 142),  # car\n            27: (0, 0, 70),  # truck\n            28: (0, 60, 100),  # bus\n            29: (0, 0, 0),  # caravan\n            30: (0, 0, 0),  # trailer\n            31: (0, 80, 100),  # train\n            32: (0, 0, 230),  # motorcycle\n            33: (119, 11, 32),  # bicycle\n            -1: (0, 0, 0)  # license plate\n            # 5: (111, 74, 0),        # dynamic\n            # 6: (81,  0, 81),        # ground\n            # 9: (250, 170, 160),     # parking\n            # 10: (230, 150, 140),    # rail track\n            # 14: (180, 165, 180),    # guard rail\n            # 15: (150, 100, 100),    # bridge\n            # 16: (150, 120, 90),     # tunnel\n            # 18: (153, 153, 153),    # polegroup\n            # 29: (0,  0, 90),        # caravan\n            # 30: (0,  0, 110),       # trailer\n        }\n        self.labels = {\n            0: 'unlabeled',\n            1: 'ego vehicle',\n            2: 'rectification border',\n            3: 'out of roi',\n            4: 'static',\n            5: 'dynamic',\n            6: 'ground',\n            7: 'road',\n            8: 'sidewalk',\n            9: 'parking',\n            10: 'rail track',\n            11: 'building',\n            12: 'wall',\n            13: 'fence',\n            14: 'guard rail',\n            15: 'bridge',\n            16: 'tunnel',\n            17: 'pole',\n            18: 'polegroup',\n            19: 'traffic light',\n            20: 'traffic sign',\n            21: 'vegetation',\n            22: 'terrain',\n            23: 'sky',\n            24: 'person',\n            25: 'rider',\n            26: 'car',\n            27: 'truck',\n            28: 'bus',\n            29: 'caravan',\n            30: 'trailer',\n            31: 'train',\n            32: 'motorcycle',\n            33: 'bicycle',\n            -1: 'license plate'\n        }\n        self.trainId = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n        self.ignoreId = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n        self.classmap = dict(zip(self.trainId, range(self.num_classes)))\n\n        self.split = split\n        self.img_size = img_size\n        self.img_norm = img_norm\n        self.ignore_index = 250\n        self.gt_type = gt_type\n\n        self.root = root\n        self.images_base = os.path.join(self.root, \"leftImg8bit\", self.split)\n        self.annotations_base = os.path.join(self.root, self.gt_type, self.split)\n\n\n\n        self.files = {}\n        self.files[self.split] = glob.glob(os.path.join(self.images_base, '*/*.png'))\n        print('debug')\n\n\n    def __getitem__(self, index):\n        img_path = self.files[self.split][index].rstrip()\n        label_path = os.path.join(\n            self.annotations_base,\n            img_path.split(os.sep)[-2],\n            os.path.basename(img_path)[:-15] + f\"{self.gt_type}_labelIds.png\"\n        )\n        img = mmcv.imread(img_path, channel_order='rgb')\n        label = mmcv.imread(label_path, flag='grayscale')\n        label = self.encode_segmap(label)\n\n        # transform & augmentations\n        img, label = self.transform(img, label)\n        return img, label\n\n    def __len__(self):\n        return len(self.files[self.split])\n\n    def decode_segmap(self, mask):\n        r = mask.copy()\n        g = mask.copy()\n        b = mask.copy()\n\n        for cls in range(self.num_classes):\n            color_cls = self.trainId[cls]\n            r[mask == cls] = self.colormap[color_cls][0]\n            g[mask == cls] = self.colormap[color_cls][1]\n            b[mask == cls] = self.colormap[color_cls][2]\n        rgb = np.zeros((mask.shape[0], mask.shape[1], 3))\n        rgb[:, :, 0] = b / 255.0\n        rgb[:, :, 1] = g / 255.0\n        rgb[:, :, 2] = r / 255.0\n        return rgb\n\n\n    def encode_segmap(self, mask):\n        for ignorecls in self.ignoreId:\n            mask[mask == ignorecls] = self.ignore_index\n        for traincls in self.trainId:\n            mask[mask == traincls] = self.classmap[traincls]\n        return mask\n\n    def transform(self, img, label):\n        img = mmcv.imresize(img, (self.img_size[0], self.img_size[1]))\n        img = img.astype(np.float64)\n\n        if self.img_norm:\n            img = img.astype(float) / 255.0\n\n        classes = np.unique(label)\n        label = label.astype(float)\n        label = mmcv.imresize(label, (self.img_size[0], self.img_size[1]), interpolation='nearest')\n        label = label.astype(int)\n\n        if not np.all(classes == np.unique(label)):\n            print(\"WARN: resizing labels yielded fewer classes\")\n\n        if not np.all(np.unique(label[label != self.ignore_index]) < self.num_classes):\n            print(\"after det\", classes, np.unique(label))\n            raise ValueError(\"Segmentation map contained invalid class values\")\n\n        img = torch.from_numpy(img).float()\n        label = torch.from_numpy(label).long()\n        \n        return img, label\n\n# dataloader test\nif __name__ == '__main__':\n    train_dataset = cityscapesDataset(root='/home/muyun99/data/dataset/cityscapes', split='train', img_size=(1024, 512), img_norm=True, gt_type='gtFine')\n    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=8, pin_memory=True)\n    print(len(train_dataset))\n\n    for i, batch in enumerate(train_dataloader):\n        imgs, masks = batch\n        if i == 0:\n            print(\"单个img的size: \", imgs.shape)\n            print(\"单个mask的size: \", masks.shape)\n\n        img = imgs[0].numpy()\n        mask = masks[0].numpy()\n\n        mask = train_dataset.decode_segmap(mask)\n        mmcv.imshow(img, wait_time=1000)\n        mmcv.imshow(mask, wait_time=1000)\n        # break\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n\n\n# 参考资料\n\n * https://github.com/meetshah1995/pytorch-semseg/blob/master/ptsemseg/loader/cityscapes_loader.py\n\n * https://github.com/Tramac/awesome-semantic-segmentation-pytorch/blob/master/core/data/dataloader/cityscapes.py\n\n * https://github.com/fvisin/dataset_loaders/blob/master/dataset_loaders/images/cityscapes.py\n\n1、修改 ignore_id\n\n2、将 ignore_id 的类别修改为 255\n\n3、计算 loss 的时候除去255的类别",normalizedContent:"# 如何加载数据集\n\n# cityscapes 数据集\n\n# 类别数量及其所对应的 trainid 以及 color\n\n\n\n在 官方脚本 中可以看到标签的情况，截图如上。分为 8 个大类，其中 human 和 vehicle 两大类是拥有实例标签的。此外我们只需关注 ignoreineval 字段为 false 的类别即可，共 19 类，所以我们在语义分割中对于每个像素都有 20 类输出，还有一个背景类别。并且实际参与训练的类别 id 是 trainid 字段的内容。\n\n# 标签可视化\n\n\n\n对于每张图像，官方数据集里都对应了四张标注，示例如上。\n\n * aachen_000000_000019_gtfine_color.png：彩色标注可视化\n\n * aachen_000000_000019_gtfine_instanceids.png：标出 human 以及 vehicle 两个大类的实例 id\n\n * aachen_000000_000019_gtfine_labelids.png：按照 labelid 来做可视化\n\n * aachen_000000_000019_gtfine_labeltrainids.png：按照 trainid 来做可视化\n\n * 官方脚本：https://github.com/mcordts/cityscapesscripts\n\n# 如何构建 cityscapes 数据集的 dataloader\n\n# 使用 pytorch 的官方接口\nimport torchvision\nimport numpy as np\n\nroot = r'./cityscapes/'\n\n# target_type = ['instance', 'semantic', 'color', 'polygon']\ntarget_type = 'instance'\n\ncityscapes_dataset = torchvision.datasets.cityscapes(root, split='train', mode='fine', target_type=target_type, transform=none, target_transform=none)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n我们可以写如上的代码使用 pytorch 官方提供的 dataset 接口直接使用，但这样我觉得不够灵活，所以我决定还是自己实现 dataloader。\n\n# 手动实现\nimport os\nimport torch\nimport numpy as np\nfrom torch.utils.data import dataset\nfrom torch.utils.data import dataloader\nimport matplotlib.pyplot as plt\nimport mmcv\nimport glob\n\nclass cityscapesdataset(dataset):\n    def __init__(self, root, split='train', img_size=(1024, 512), img_norm=true, gt_type='gtcoarse'):\n        super(cityscapesdataset, self).__init__()\n        self.name_dataset = 'cityscapes'\n        self.num_classes = 19\n        self.colormap = {\n            0: (0, 0, 0),  # unlabeled\n            1: (0, 0, 0),  # ego vehicle\n            2: (0, 0, 0),  # rectification border\n            3: (0, 0, 0),  # out of roi\n            4: (0, 0, 0),  # static\n            5: (0, 0, 0),  # dynamic\n            6: (0, 0, 0),  # ground\n            7: (128, 64, 128),  # road\n            8: (244, 35, 232),  # sidewalk\n            9: (0, 0, 0),  # parking\n            10: (0, 0, 0),  # rail track\n            11: (70, 70, 70),  # building\n            12: (102, 102, 156),  # wall\n            13: (190, 153, 153),  # fence\n            14: (0, 0, 0),  # guard rail\n            15: (0, 0, 0),  # bridge\n            16: (0, 0, 0),  # tunnel\n            17: (153, 153, 153),  # pole\n            18: (0, 0, 0),  # polegroup\n            19: (250, 170, 30),  # traffic light\n            20: (220, 220, 0),  # traffic sign\n            21: (107, 142, 35),  # vegetation\n            22: (152, 251, 152),  # terrain\n            23: (0, 130, 180),  # sky\n            24: (220, 20, 60),  # person\n            25: (255, 0, 0),  # rider\n            26: (0, 0, 142),  # car\n            27: (0, 0, 70),  # truck\n            28: (0, 60, 100),  # bus\n            29: (0, 0, 0),  # caravan\n            30: (0, 0, 0),  # trailer\n            31: (0, 80, 100),  # train\n            32: (0, 0, 230),  # motorcycle\n            33: (119, 11, 32),  # bicycle\n            -1: (0, 0, 0)  # license plate\n            # 5: (111, 74, 0),        # dynamic\n            # 6: (81,  0, 81),        # ground\n            # 9: (250, 170, 160),     # parking\n            # 10: (230, 150, 140),    # rail track\n            # 14: (180, 165, 180),    # guard rail\n            # 15: (150, 100, 100),    # bridge\n            # 16: (150, 120, 90),     # tunnel\n            # 18: (153, 153, 153),    # polegroup\n            # 29: (0,  0, 90),        # caravan\n            # 30: (0,  0, 110),       # trailer\n        }\n        self.labels = {\n            0: 'unlabeled',\n            1: 'ego vehicle',\n            2: 'rectification border',\n            3: 'out of roi',\n            4: 'static',\n            5: 'dynamic',\n            6: 'ground',\n            7: 'road',\n            8: 'sidewalk',\n            9: 'parking',\n            10: 'rail track',\n            11: 'building',\n            12: 'wall',\n            13: 'fence',\n            14: 'guard rail',\n            15: 'bridge',\n            16: 'tunnel',\n            17: 'pole',\n            18: 'polegroup',\n            19: 'traffic light',\n            20: 'traffic sign',\n            21: 'vegetation',\n            22: 'terrain',\n            23: 'sky',\n            24: 'person',\n            25: 'rider',\n            26: 'car',\n            27: 'truck',\n            28: 'bus',\n            29: 'caravan',\n            30: 'trailer',\n            31: 'train',\n            32: 'motorcycle',\n            33: 'bicycle',\n            -1: 'license plate'\n        }\n        self.trainid = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n        self.ignoreid = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n        self.classmap = dict(zip(self.trainid, range(self.num_classes)))\n\n        self.split = split\n        self.img_size = img_size\n        self.img_norm = img_norm\n        self.ignore_index = 250\n        self.gt_type = gt_type\n\n        self.root = root\n        self.images_base = os.path.join(self.root, \"leftimg8bit\", self.split)\n        self.annotations_base = os.path.join(self.root, self.gt_type, self.split)\n\n\n\n        self.files = {}\n        self.files[self.split] = glob.glob(os.path.join(self.images_base, '*/*.png'))\n        print('debug')\n\n\n    def __getitem__(self, index):\n        img_path = self.files[self.split][index].rstrip()\n        label_path = os.path.join(\n            self.annotations_base,\n            img_path.split(os.sep)[-2],\n            os.path.basename(img_path)[:-15] + f\"{self.gt_type}_labelids.png\"\n        )\n        img = mmcv.imread(img_path, channel_order='rgb')\n        label = mmcv.imread(label_path, flag='grayscale')\n        label = self.encode_segmap(label)\n\n        # transform & augmentations\n        img, label = self.transform(img, label)\n        return img, label\n\n    def __len__(self):\n        return len(self.files[self.split])\n\n    def decode_segmap(self, mask):\n        r = mask.copy()\n        g = mask.copy()\n        b = mask.copy()\n\n        for cls in range(self.num_classes):\n            color_cls = self.trainid[cls]\n            r[mask == cls] = self.colormap[color_cls][0]\n            g[mask == cls] = self.colormap[color_cls][1]\n            b[mask == cls] = self.colormap[color_cls][2]\n        rgb = np.zeros((mask.shape[0], mask.shape[1], 3))\n        rgb[:, :, 0] = b / 255.0\n        rgb[:, :, 1] = g / 255.0\n        rgb[:, :, 2] = r / 255.0\n        return rgb\n\n\n    def encode_segmap(self, mask):\n        for ignorecls in self.ignoreid:\n            mask[mask == ignorecls] = self.ignore_index\n        for traincls in self.trainid:\n            mask[mask == traincls] = self.classmap[traincls]\n        return mask\n\n    def transform(self, img, label):\n        img = mmcv.imresize(img, (self.img_size[0], self.img_size[1]))\n        img = img.astype(np.float64)\n\n        if self.img_norm:\n            img = img.astype(float) / 255.0\n\n        classes = np.unique(label)\n        label = label.astype(float)\n        label = mmcv.imresize(label, (self.img_size[0], self.img_size[1]), interpolation='nearest')\n        label = label.astype(int)\n\n        if not np.all(classes == np.unique(label)):\n            print(\"warn: resizing labels yielded fewer classes\")\n\n        if not np.all(np.unique(label[label != self.ignore_index]) < self.num_classes):\n            print(\"after det\", classes, np.unique(label))\n            raise valueerror(\"segmentation map contained invalid class values\")\n\n        img = torch.from_numpy(img).float()\n        label = torch.from_numpy(label).long()\n        \n        return img, label\n\n# dataloader test\nif __name__ == '__main__':\n    train_dataset = cityscapesdataset(root='/home/muyun99/data/dataset/cityscapes', split='train', img_size=(1024, 512), img_norm=true, gt_type='gtfine')\n    train_dataloader = dataloader(train_dataset, batch_size=2, shuffle=true, num_workers=8, pin_memory=true)\n    print(len(train_dataset))\n\n    for i, batch in enumerate(train_dataloader):\n        imgs, masks = batch\n        if i == 0:\n            print(\"单个img的size: \", imgs.shape)\n            print(\"单个mask的size: \", masks.shape)\n\n        img = imgs[0].numpy()\n        mask = masks[0].numpy()\n\n        mask = train_dataset.decode_segmap(mask)\n        mmcv.imshow(img, wait_time=1000)\n        mmcv.imshow(mask, wait_time=1000)\n        # break\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n\n\n# 参考资料\n\n * https://github.com/meetshah1995/pytorch-semseg/blob/master/ptsemseg/loader/cityscapes_loader.py\n\n * https://github.com/tramac/awesome-semantic-segmentation-pytorch/blob/master/core/data/dataloader/cityscapes.py\n\n * https://github.com/fvisin/dataset_loaders/blob/master/dataset_loaders/images/cityscapes.py\n\n1、修改 ignore_id\n\n2、将 ignore_id 的类别修改为 255\n\n3、计算 loss 的时候除去255的类别",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"半监督与弱监督图像分割",frontmatter:{title:"半监督与弱监督图像分割",date:"2021-04-14T23:37:45.000Z",permalink:"/pages/b8e080/",categories:["计算机视觉","图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/05.%E5%8D%8A%E7%9B%91%E7%9D%A3%E4%B8%8E%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2.html",relativePath:"02.学习笔记/02.代码实践-图像分割/05.半监督与弱监督图像分割.md",key:"v-7e90e4e9",path:"/pages/b8e080/",headers:[{level:2,title:"半监督与弱监督语义分割",slug:"半监督与弱监督语义分割",normalizedTitle:"半监督与弱监督语义分割",charIndex:2},{level:3,title:"1、任务简介",slug:"_1、任务简介",normalizedTitle:"1、任务简介",charIndex:18},{level:3,title:"2、实验设置简介",slug:"_2、实验设置简介",normalizedTitle:"2、实验设置简介",charIndex:118},{level:3,title:"3、半监督语义分割的经典对比方法简介",slug:"_3、半监督语义分割的经典对比方法简介",normalizedTitle:"3、半监督语义分割的经典对比方法简介",charIndex:2300},{level:3,title:"4、半监督语义分割论文",slug:"_4、半监督语义分割论文",normalizedTitle:"4、半监督语义分割论文",charIndex:2359},{level:3,title:"5、弱监督语义分割论文",slug:"_5、弱监督语义分割论文",normalizedTitle:"5、弱监督语义分割论文",charIndex:3476}],headersStr:"半监督与弱监督语义分割 1、任务简介 2、实验设置简介 3、半监督语义分割的经典对比方法简介 4、半监督语义分割论文 5、弱监督语义分割论文",content:"# 半监督与弱监督语义分割\n\n\n# 1、任务简介\n\n半监督语义分割是指：数据集中有少量的 pixel-level 标注以及大量的 image-level 标注\n\n弱监督语义分割是指：数据集中全是 image-level 的标注\n\n\n# 2、实验设置简介\n\n# 半监督语义分割的实验设置\n\n半监督的主要数据集是PASCAL VOC 2012 数据集及其扩展数据集 SBD\n\n名称                                  训练集     验证集    测试     类别数\nPASCAL VOC 2012                     1464    1449   1456   21\nSemantic-Boundaries-Dataset (SBD)   8498    2857   0      21\nPASCAL VOC 2012 Aug                 10582   1449   1456   21\n\n感谢 博客 的介绍，下面是 PASCAL VOC 2012 Aug 中 10582张图像的由来：\n\n * sbd_train(8498)=和voc_train重复的图片(1133)+和voc_val重复的图片(545)+sbd_train真正补充的图片(6820)\n * sbd_val(2857)=和voc_train重复的图片(1)+和voc_val重复的图片(558)+sbd_val真正补充的图片(2298)\n * voc_train(1464) + voc_val(1449)+sbd_train真正补充的图片(6820) + sbd_val真正补充的图片(2298)=12031\n * 用原来的 voc_val(1449) 作为验证集，剩下的 12031 - voc_val(1449) =10582 都可以用作训练，就是trainaug(10582)\n\n常常会看到使用以下的实验设置代称，现在做一些解释\n\n * 1.4k strong ：即为原版PASCAL VOC 2012 的 1464 张训练集，带有 pixel-level 的强监督，一般作为Baseline\n * 10.6k weak：即为PASCAL VOC 2012 + SBD 的训练集之和，带有image-level 的弱监督。\n * 10.6k strong：即为PASCAL VOC 2012 + SBD 的训练集之和，带有pixel-level 的强监督，一般作为性能上限\n\n半监督语义分割常见的实验设置是\n\n * 1/8：PASCAL VOC 2012 12.5% labeled，即是使用1323张图像的标注\n * 1/20：PASCAL VOC 2012 5% labeled，即是使用529张图像的标注\n * 1/50：PASCAL VOC 2012 2% labeled，即是使用212张图像的标注\n * 1/106：PASCAL VOC 2012 1% labeled，即是使用100张图像的标注\n * 1.4k strong + 9.2k weak：使用1.4k 原始VOC数据集中的训练图像标注，剩下的9.2k 图像作为未标注图像\n\n半监督语义分割有两种常见的实验设置即为在PASCAL VOC 2012 Aug数据集上，使用1.4k strong + 9.2k weak，探索各类方法在这种数据分布下的实验性能。还有一种实验设置是，在PASCAL VOC 2012 Aug数据集上，使用1/50、1/20、1/8 的数据作为强监督，剩下的数据作为弱监督探索各类方法的实验性能。\n\n模型情况，一般来讲都是使用 Deeplab 系列的分割模型，主要有以下几种搭配\n\n * DeepLab v1-vgg16：VGG16 Backbone 的 DeepLabv1\n * DeepLab v2-resnet101：ResNet101 Backbone 的 DeepLabv2\n\n因为PASCAL VOC 2012 的验证集是给定了的，而测试集的精度需要去官网上 submit，所以一般论文都会报告 PASCAL VOC 2012 的 val set 上的性能，一部分论文会附上 test set 上的性能。\n\n * 参考资料：https://paperswithcode.com/task/semi-supervised-semantic-segmentation\n\n# 弱监督语义分割的实验设置\n\n常常会看到使用以下的实验设置代称，现在做一些解释\n\n * 1.4k strong ：即为原版PASCAL VOC 2012 的 1464 张训练集，带有 pixel-level 的强监督，一般作为Baseline\n * 10.6k weak：即为PASCAL VOC 2012 + SBD 的训练集之和，带有image-level 的弱监督。\n * 10.6k strong：即为PASCAL VOC 2012 + SBD 的训练集之和，带有pixel-level 的强监督，一般作为性能上限\n\n弱监督语义分割常见的实验设置是\n\n * 1.4k strong + 9.2k weak：使用1.4k 原始VOC数据集中的训练图像作为强监督，剩下的9.2k 图像作为弱监督\n\n * 参考资料：https://paperswithcode.com/task/weakly-supervised-semantic-segmentation\n\n\n# 3、半监督语义分割的经典对比方法简介\n\n# 半监督语义分割的经典对比方法\n\n# 弱监督语义分割的经典对比方法\n\n\n# 4、半监督语义分割论文\n\n * Learning from Pixel-Level Label Noise: A New Perspective for Semi-Supervised Semantic Segmentation.\n   \n   * 作者：Rumeng Yi, Yaping Huang, Qingji Guan, Mengyang Pu, and Runsheng Zhang. （北交）\n   \n   * 发表：arXiv\n   \n   * Code：无\n   \n   * 从噪声的视角去看待半监督的语义分割任务，引入基于超像素的图建模图中像素的空间相邻性和语义相似性，利用 GAT 来修正带噪标签，也用到了CRF做细化，将修正后的标签一起再来训分割模型。\n     \n     \n\n * Semi-supervised Semantic Segmentation via Strong-weak Dual-branch Network.\n   \n   * 作者：Wenfeng Luo, Meng Yang.（中山大学）\n   \n   * 发表：ECCV 2020 Spotlight\n   \n   * Code：无\n   \n   * 论文主要的贡献是着眼于改进半监督语义分割的 Image-level 样本的利用方式，作者使用共享 Backbone 和 Neck的双分支网络，分为 Strong 分支以及 Weak 分支，将强监督样本送入 Strong 分支，将弱监督样本送入 Weak 分支，可以较好的消除监督不一致的现象，有助于缓解强弱样本不平衡问题。训练完成后，弱分支就不再需要了，其在训练过程中起到正则化的作用，加强了 Backbone 以及 Neck 网络的泛化能力。\n     \n     \n\n * DMT: Dynamic Mutual Training for Semi-Supervised Learning.\n   \n   * 作者：Zhengyang Feng, Qianyu Zhou, Qiqi Gu, Xin Tan, Guangliang Cheng, Xuequan Lu, Jianping Shi, Lizhuang Ma.（商汤以及上交DMCV实验室）\n   \n   * 发表：arXiv， 准备投PR\n   \n   * 论文：GitHub\n   \n   * DMT 提出了一种方法，使用两个模型对伪标签生成动态权重。对样本划分了三种情况给予不同的权重，以迭代式的方法将伪标签 Refine 到越来越好，并且生成像素级的权重促进模型学习。\n     \n     \n\n\n# 5、弱监督语义分割论文\n\n * Puzzle-CAM: Improved localization via matching partial and full features.\n   \n   * 作者：Sanghyun Jo, In-Jae Yu. (KAIST)\n   \n   * 发表：ICIP 2021，arXiv\n   \n   * Code：GitHub\n   \n   * 性能：\n     \n     * DeepLabv3plus + ResNest-269\n       * val：66.9\n       * test：67.7\n     * DeepLabv3plus + ResNest-269\n       * val：71.9\n       * test：72.2\n   \n   * 论文主要的贡献是着眼于改进半监督语义分割的 Image-level 样本的利用方式，作者使用共享 Backbone 和 Neck的双分支网络，分为 Strong 分支以及 Weak 分支，将强监督样本送入 Strong 分支，将弱监督样本送入 Weak 分支，可以较好的消除监督不一致的现象，有助于缓解强弱样本不平衡问题。训练完成后，弱分支就不再需要了，其在训练过程中起到正则化的作用，加强了 Backbone 以及 Neck 网络的泛化能力。\n     \n     \n\n * Self-supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentaion.（SEAM）\n   \n   * 作者：\n   * 发表：arXiv\n   * Code：GitHub\n   * 性能：\n     * ResNet38d + AffinityNet + DeepLabv1\n     * val：64.5\n     * test：65.7\n\n * Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation.（DRS）（参考文献较为完善）\n   \n   * 发表：AAAI2021，arXiv\n   * Code：GitHub\n   * 性能：\n     * VGG-16 + AffinityNet + DeepLabv1\n       * val：63.6\n       * test：64.5\n     * ResNet-101 + AffinityNet + DeepLabv1\n       * val：66.8\n       * test：67.5\n     * ResNet-101 + AffinityNet + DeepLabv2\n       * val：71.2\n       * test：71.4\n\n * Causal Intervention for Weakly Supervised Semantic Segmentation.(CONTA)\n   \n   * 发表：NeuIPS 2020 Oral，arXiv\n   * Code：GitHub\n   * 性能：\n     * IRNet+CONTA\n       * val：65.3\n       * test：64.8\n     * SEAM+CONTA\n       * val：66.1\n       * test：66.7\n\n * Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations(IRNet)\n   \n   * 发表：CVPR 2019，CVPR 2019\n   * Code: GitHub\n   * 性能：\n     * ResNet50 + DeepLabv2\n     * val：63.5\n     * test：64.8\n\n * Group-Wise Semantic Mining for Weakly Supervised Semantic Segmentation.（参考文献较为完善）\n   \n   * 发表：AAAI2021，arXiv\n   * Code：GitHub\n   * 性能：\n     * VGG-16 + DeepLabv2\n       * val：63.3\n       * test：63.6\n     * ResNet-101 + DeepLabv2\n       * val：68.2\n       * test：68.5\n\n * Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks.\n   \n   * 发表：ICME 2021，arXiv\n   * Code：GitHub\n   * 性能：\n     * ResNet-101 + DeepLabv2\n       * val：66.7\n       * test：68.8\n     * ResNet-101 + DeepLabv2 + Backbone COCO Pretrain\n       * val：68.7\n       * test：69.3\n\n * Leveraging Instance-, Image- and Dataset-Level Information for Weakly Supervised Instance Segmentation.（LIID）（参考文献较为全面）\n   \n   * 发表：TPAMI 2020，IEEE\n   * Code：GitHub\n\n * 性能： - ResNet-101 + DeepLabv2 - val：66.5 - test：67.5 - ResNet-101 + DeepLabv2 + Backbone 24K ImageNet Pretrain - val：67.8 - test：68.3 - Res2Net-101 + DeepLabv2 - val：69.4 - test：70.4\n\n * Learning Integral Objects With Intra-Class Discriminator for Weakly-Supervised Semantic Segmentation.\n   \n   * 发表：CVPR 2020，CVPR\n   \n   * Code：GitHub\n   \n   * 性能：\n     \n     * VGG-16 + DeepLabv1\n       \n       * val：61.2\n       * test：60.9\n     \n     * ResNet101 + DeepLabv1\n     \n     * val：64.1\n     \n     * test：64.3\n\n * \n\n                                 实验设置                                              VAL SET   TEST SET   PAPER       CODE\nICD           CVPR 2020          ResNet101 + DeepLabv1                             64.1      64.3       CVPR        link\nLIID          TPAMI 2020         Res2Net-101 + DeepLabv2                           69.4      70.4       IEEE        GitHub\nWSGCN         ICME 2021          ResNet-101 + DeepLabv2 + Backbone COCO Pretrain   68.7      69.3       arXiv       GitHub\nGroup-WSSS    AAAI 2021          ResNet-101 + DeepLabv2                            68.2      68.5       arXiv       GitHub\nIRNet         CVPR 2019          ResNet50 + DeepLabv2                              63.5      64.8       CVPR 2019   GitHub\nCONTA         NeuIPS 2020 Oral   SEAM+CONTA                                        66.1      66.7       arXiv       GitHub\nDRS           AAAI 2021          ResNet-101 + DeepLabv2                            71.2      71.4       arXiv       GitHub\nSEAM          CVPR 2020 Oral     ResNet38d + AffinityNet + DeepLabv1               64.5      65.7       arXiv       GitHub\nPuzzle-CAM    ICIP 2021          ResNest-269 + DeepLabv3plus                       71.9      72.2       arXiv       GitHub\nAffinityNet   CVPR 2018          ResNet38d + ?                                     61.7      63.7       arXiv       GitHub\n\nDeepLabv1\n\n                                BACKBONE                                          VAL SET   TEST SET   PAPER       CODE\nIRNet        CVPR 2019          ResNet50 + DeepLabv2                              63.5      64.8       CVPR 2019   GitHub\n             CVPR 2020          ResNet101 + DeepLabv1                             64.1      64.3       CVPR        link\nLIID         TPAMI 2020         Res2Net-101 + DeepLabv2                           69.4      70.4       IEEE        GitHub\nWSGCN        ICME 2021          ResNet-101 + DeepLabv2 + Backbone COCO Pretrain   68.7      69.3       arXiv       GitHub\nGroup-WSSS   AAAI 2021          ResNet-101 + DeepLabv2                            68.2      68.5       arXiv       GitHub\n                                                                                                                   \nCONTA        NeuIPS 2020 Oral   SEAM+CONTA                                        66.1      66.7       arXiv       [GitHub](\n\nDeepLabv2\n\n                                BACKBONE                                          VAL SET   TEST SET   PAPER       CODE\nIRNet        CVPR 2019          ResNet50 + DeepLabv2                              63.5      64.8       CVPR 2019   GitHub\n             CVPR 2020          ResNet101 + DeepLabv1                             64.1      64.3       CVPR        link\nLIID         TPAMI 2020         Res2Net-101 + DeepLabv2                           69.4      70.4       IEEE        GitHub\nWSGCN        ICME 2021          ResNet-101 + DeepLabv2 + Backbone COCO Pretrain   68.7      69.3       arXiv       GitHub\nGroup-WSSS   AAAI 2021          ResNet-101 + DeepLabv2                            68.2      68.5       arXiv       GitHub\n                                                                                                                   \nCONTA        NeuIPS 2020 Oral   SEAM+CONTA                                        66.1      66.7       arXiv       \n\n\n\n上图是Group-WSSS，下一张图是DRS\n\n",normalizedContent:"# 半监督与弱监督语义分割\n\n\n# 1、任务简介\n\n半监督语义分割是指：数据集中有少量的 pixel-level 标注以及大量的 image-level 标注\n\n弱监督语义分割是指：数据集中全是 image-level 的标注\n\n\n# 2、实验设置简介\n\n# 半监督语义分割的实验设置\n\n半监督的主要数据集是pascal voc 2012 数据集及其扩展数据集 sbd\n\n名称                                  训练集     验证集    测试     类别数\npascal voc 2012                     1464    1449   1456   21\nsemantic-boundaries-dataset (sbd)   8498    2857   0      21\npascal voc 2012 aug                 10582   1449   1456   21\n\n感谢 博客 的介绍，下面是 pascal voc 2012 aug 中 10582张图像的由来：\n\n * sbd_train(8498)=和voc_train重复的图片(1133)+和voc_val重复的图片(545)+sbd_train真正补充的图片(6820)\n * sbd_val(2857)=和voc_train重复的图片(1)+和voc_val重复的图片(558)+sbd_val真正补充的图片(2298)\n * voc_train(1464) + voc_val(1449)+sbd_train真正补充的图片(6820) + sbd_val真正补充的图片(2298)=12031\n * 用原来的 voc_val(1449) 作为验证集，剩下的 12031 - voc_val(1449) =10582 都可以用作训练，就是trainaug(10582)\n\n常常会看到使用以下的实验设置代称，现在做一些解释\n\n * 1.4k strong ：即为原版pascal voc 2012 的 1464 张训练集，带有 pixel-level 的强监督，一般作为baseline\n * 10.6k weak：即为pascal voc 2012 + sbd 的训练集之和，带有image-level 的弱监督。\n * 10.6k strong：即为pascal voc 2012 + sbd 的训练集之和，带有pixel-level 的强监督，一般作为性能上限\n\n半监督语义分割常见的实验设置是\n\n * 1/8：pascal voc 2012 12.5% labeled，即是使用1323张图像的标注\n * 1/20：pascal voc 2012 5% labeled，即是使用529张图像的标注\n * 1/50：pascal voc 2012 2% labeled，即是使用212张图像的标注\n * 1/106：pascal voc 2012 1% labeled，即是使用100张图像的标注\n * 1.4k strong + 9.2k weak：使用1.4k 原始voc数据集中的训练图像标注，剩下的9.2k 图像作为未标注图像\n\n半监督语义分割有两种常见的实验设置即为在pascal voc 2012 aug数据集上，使用1.4k strong + 9.2k weak，探索各类方法在这种数据分布下的实验性能。还有一种实验设置是，在pascal voc 2012 aug数据集上，使用1/50、1/20、1/8 的数据作为强监督，剩下的数据作为弱监督探索各类方法的实验性能。\n\n模型情况，一般来讲都是使用 deeplab 系列的分割模型，主要有以下几种搭配\n\n * deeplab v1-vgg16：vgg16 backbone 的 deeplabv1\n * deeplab v2-resnet101：resnet101 backbone 的 deeplabv2\n\n因为pascal voc 2012 的验证集是给定了的，而测试集的精度需要去官网上 submit，所以一般论文都会报告 pascal voc 2012 的 val set 上的性能，一部分论文会附上 test set 上的性能。\n\n * 参考资料：https://paperswithcode.com/task/semi-supervised-semantic-segmentation\n\n# 弱监督语义分割的实验设置\n\n常常会看到使用以下的实验设置代称，现在做一些解释\n\n * 1.4k strong ：即为原版pascal voc 2012 的 1464 张训练集，带有 pixel-level 的强监督，一般作为baseline\n * 10.6k weak：即为pascal voc 2012 + sbd 的训练集之和，带有image-level 的弱监督。\n * 10.6k strong：即为pascal voc 2012 + sbd 的训练集之和，带有pixel-level 的强监督，一般作为性能上限\n\n弱监督语义分割常见的实验设置是\n\n * 1.4k strong + 9.2k weak：使用1.4k 原始voc数据集中的训练图像作为强监督，剩下的9.2k 图像作为弱监督\n\n * 参考资料：https://paperswithcode.com/task/weakly-supervised-semantic-segmentation\n\n\n# 3、半监督语义分割的经典对比方法简介\n\n# 半监督语义分割的经典对比方法\n\n# 弱监督语义分割的经典对比方法\n\n\n# 4、半监督语义分割论文\n\n * learning from pixel-level label noise: a new perspective for semi-supervised semantic segmentation.\n   \n   * 作者：rumeng yi, yaping huang, qingji guan, mengyang pu, and runsheng zhang. （北交）\n   \n   * 发表：arxiv\n   \n   * code：无\n   \n   * 从噪声的视角去看待半监督的语义分割任务，引入基于超像素的图建模图中像素的空间相邻性和语义相似性，利用 gat 来修正带噪标签，也用到了crf做细化，将修正后的标签一起再来训分割模型。\n     \n     \n\n * semi-supervised semantic segmentation via strong-weak dual-branch network.\n   \n   * 作者：wenfeng luo, meng yang.（中山大学）\n   \n   * 发表：eccv 2020 spotlight\n   \n   * code：无\n   \n   * 论文主要的贡献是着眼于改进半监督语义分割的 image-level 样本的利用方式，作者使用共享 backbone 和 neck的双分支网络，分为 strong 分支以及 weak 分支，将强监督样本送入 strong 分支，将弱监督样本送入 weak 分支，可以较好的消除监督不一致的现象，有助于缓解强弱样本不平衡问题。训练完成后，弱分支就不再需要了，其在训练过程中起到正则化的作用，加强了 backbone 以及 neck 网络的泛化能力。\n     \n     \n\n * dmt: dynamic mutual training for semi-supervised learning.\n   \n   * 作者：zhengyang feng, qianyu zhou, qiqi gu, xin tan, guangliang cheng, xuequan lu, jianping shi, lizhuang ma.（商汤以及上交dmcv实验室）\n   \n   * 发表：arxiv， 准备投pr\n   \n   * 论文：github\n   \n   * dmt 提出了一种方法，使用两个模型对伪标签生成动态权重。对样本划分了三种情况给予不同的权重，以迭代式的方法将伪标签 refine 到越来越好，并且生成像素级的权重促进模型学习。\n     \n     \n\n\n# 5、弱监督语义分割论文\n\n * puzzle-cam: improved localization via matching partial and full features.\n   \n   * 作者：sanghyun jo, in-jae yu. (kaist)\n   \n   * 发表：icip 2021，arxiv\n   \n   * code：github\n   \n   * 性能：\n     \n     * deeplabv3plus + resnest-269\n       * val：66.9\n       * test：67.7\n     * deeplabv3plus + resnest-269\n       * val：71.9\n       * test：72.2\n   \n   * 论文主要的贡献是着眼于改进半监督语义分割的 image-level 样本的利用方式，作者使用共享 backbone 和 neck的双分支网络，分为 strong 分支以及 weak 分支，将强监督样本送入 strong 分支，将弱监督样本送入 weak 分支，可以较好的消除监督不一致的现象，有助于缓解强弱样本不平衡问题。训练完成后，弱分支就不再需要了，其在训练过程中起到正则化的作用，加强了 backbone 以及 neck 网络的泛化能力。\n     \n     \n\n * self-supervised equivariant attention mechanism for weakly supervised semantic segmentaion.（seam）\n   \n   * 作者：\n   * 发表：arxiv\n   * code：github\n   * 性能：\n     * resnet38d + affinitynet + deeplabv1\n     * val：64.5\n     * test：65.7\n\n * discriminative region suppression for weakly-supervised semantic segmentation.（drs）（参考文献较为完善）\n   \n   * 发表：aaai2021，arxiv\n   * code：github\n   * 性能：\n     * vgg-16 + affinitynet + deeplabv1\n       * val：63.6\n       * test：64.5\n     * resnet-101 + affinitynet + deeplabv1\n       * val：66.8\n       * test：67.5\n     * resnet-101 + affinitynet + deeplabv2\n       * val：71.2\n       * test：71.4\n\n * causal intervention for weakly supervised semantic segmentation.(conta)\n   \n   * 发表：neuips 2020 oral，arxiv\n   * code：github\n   * 性能：\n     * irnet+conta\n       * val：65.3\n       * test：64.8\n     * seam+conta\n       * val：66.1\n       * test：66.7\n\n * weakly supervised learning of instance segmentation with inter-pixel relations(irnet)\n   \n   * 发表：cvpr 2019，cvpr 2019\n   * code: github\n   * 性能：\n     * resnet50 + deeplabv2\n     * val：63.5\n     * test：64.8\n\n * group-wise semantic mining for weakly supervised semantic segmentation.（参考文献较为完善）\n   \n   * 发表：aaai2021，arxiv\n   * code：github\n   * 性能：\n     * vgg-16 + deeplabv2\n       * val：63.3\n       * test：63.6\n     * resnet-101 + deeplabv2\n       * val：68.2\n       * test：68.5\n\n * weakly-supervised image semantic segmentation using graph convolutional networks.\n   \n   * 发表：icme 2021，arxiv\n   * code：github\n   * 性能：\n     * resnet-101 + deeplabv2\n       * val：66.7\n       * test：68.8\n     * resnet-101 + deeplabv2 + backbone coco pretrain\n       * val：68.7\n       * test：69.3\n\n * leveraging instance-, image- and dataset-level information for weakly supervised instance segmentation.（liid）（参考文献较为全面）\n   \n   * 发表：tpami 2020，ieee\n   * code：github\n\n * 性能： - resnet-101 + deeplabv2 - val：66.5 - test：67.5 - resnet-101 + deeplabv2 + backbone 24k imagenet pretrain - val：67.8 - test：68.3 - res2net-101 + deeplabv2 - val：69.4 - test：70.4\n\n * learning integral objects with intra-class discriminator for weakly-supervised semantic segmentation.\n   \n   * 发表：cvpr 2020，cvpr\n   \n   * code：github\n   \n   * 性能：\n     \n     * vgg-16 + deeplabv1\n       \n       * val：61.2\n       * test：60.9\n     \n     * resnet101 + deeplabv1\n     \n     * val：64.1\n     \n     * test：64.3\n\n * \n\n                                 实验设置                                              val set   test set   paper       code\nicd           cvpr 2020          resnet101 + deeplabv1                             64.1      64.3       cvpr        link\nliid          tpami 2020         res2net-101 + deeplabv2                           69.4      70.4       ieee        github\nwsgcn         icme 2021          resnet-101 + deeplabv2 + backbone coco pretrain   68.7      69.3       arxiv       github\ngroup-wsss    aaai 2021          resnet-101 + deeplabv2                            68.2      68.5       arxiv       github\nirnet         cvpr 2019          resnet50 + deeplabv2                              63.5      64.8       cvpr 2019   github\nconta         neuips 2020 oral   seam+conta                                        66.1      66.7       arxiv       github\ndrs           aaai 2021          resnet-101 + deeplabv2                            71.2      71.4       arxiv       github\nseam          cvpr 2020 oral     resnet38d + affinitynet + deeplabv1               64.5      65.7       arxiv       github\npuzzle-cam    icip 2021          resnest-269 + deeplabv3plus                       71.9      72.2       arxiv       github\naffinitynet   cvpr 2018          resnet38d + ?                                     61.7      63.7       arxiv       github\n\ndeeplabv1\n\n                                backbone                                          val set   test set   paper       code\nirnet        cvpr 2019          resnet50 + deeplabv2                              63.5      64.8       cvpr 2019   github\n             cvpr 2020          resnet101 + deeplabv1                             64.1      64.3       cvpr        link\nliid         tpami 2020         res2net-101 + deeplabv2                           69.4      70.4       ieee        github\nwsgcn        icme 2021          resnet-101 + deeplabv2 + backbone coco pretrain   68.7      69.3       arxiv       github\ngroup-wsss   aaai 2021          resnet-101 + deeplabv2                            68.2      68.5       arxiv       github\n                                                                                                                   \nconta        neuips 2020 oral   seam+conta                                        66.1      66.7       arxiv       [github](\n\ndeeplabv2\n\n                                backbone                                          val set   test set   paper       code\nirnet        cvpr 2019          resnet50 + deeplabv2                              63.5      64.8       cvpr 2019   github\n             cvpr 2020          resnet101 + deeplabv1                             64.1      64.3       cvpr        link\nliid         tpami 2020         res2net-101 + deeplabv2                           69.4      70.4       ieee        github\nwsgcn        icme 2021          resnet-101 + deeplabv2 + backbone coco pretrain   68.7      69.3       arxiv       github\ngroup-wsss   aaai 2021          resnet-101 + deeplabv2                            68.2      68.5       arxiv       github\n                                                                                                                   \nconta        neuips 2020 oral   seam+conta                                        66.1      66.7       arxiv       \n\n\n\n上图是group-wsss，下一张图是drs\n\n",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"常用数据集简介",frontmatter:{title:"常用数据集简介",date:"2021-08-03T20:12:08.000Z",permalink:"/pages/0bdb48/",categories:["学习笔记","代码实践-目标检测"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/01.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/03.%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E7%AE%80%E4%BB%8B.html",relativePath:"02.学习笔记/01.代码实践-目标检测/03.常用数据集简介.md",key:"v-60f648f9",path:"/pages/0bdb48/",headers:[{level:3,title:"COCO格式数据集",slug:"coco格式数据集",normalizedTitle:"coco格式数据集",charIndex:2}],headersStr:"COCO格式数据集",content:'# COCO格式数据集\n\n# 01、COCO 格式数据集标注解析\n\n\n\n分为四部分信息\n\n"info": {\n    "description": "CowboySuit",\n    "url": "http://github.com/dmlc/gluon-cv",\n    "version": "1.0",\n    "year": 2021,\n    "contributor": "GluonCV/AutoGluon",\n    "date_created": "2021/07/01"\n  \t},\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * info 主要描述数据的元信息，一般就是标注数据的版权和来源之类的\n\n"images": [\n      {\n      \t"id": 11684018977583795263,\n      \t"file_name": "a225f95b03ce0c3f.jpg",\n      \t"neg_category_ids": [\n        \t120\n      \t],\n      \t"pos_category_ids": [\n\t        69,\n    \t    216,\n        \t308,\n        \t433\n      \t],\n      \t"width": 1024,\n      \t"height": 683,\n      \t"source": "OpenImages"\n   \t  },\n     ...\n  \t]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n * images 是一个列表，列表里的每个实例都描述一张图像，图像比较重要的属性值如下\n   * id\n   * file_name\n   * height\n   * weight\n\n"annotations":[\n\t  {\n      \t"id": 9502719,\n      \t"image_id": 11684018977583795263,\n      \t"freebase_id": "/m/032b3c",\n      \t"category_id": 588,\n      \t"iscrowd": false,\n      \t"bbox": [\n        524.16,\n        255.0,\n        365.44,\n        411.98\n      \t],\n      \t"area": 150553.52\n   \t  }\n      ..\n  \t]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n * annotations 则是对应图像的标注，标注比较重要的属性值如下\n   * image_id\n   * category_id\n   * bbox: [x, y, width, height]\n   * bbox 的格式是目标框左上角的坐标以及该框的宽和高\n\n"categories": [\n    {\n        "id": 87,\n        "name": "belt",\n        "freebase_id": "/m/0176mf"\n    },\n    ...\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * categories 则是所有的类别，比较重要的属性值如下\n   * id\n   * name\n\n# 02、COCO 格式数据集标注可视化\n\n# 2.1 目标框的标注可视化\n\n# import cv2\nimport json\nimport os\n\nimport mmcv\nimport numpy as np\nfrom pycocotools.coco import COCO\n\n\ndef visualization_bbox(num_image, json_path, img_path):  # 需要画的第num副图片， 对应的json路径和图片路径\n    with open(json_path) as annos:\n        annotation_json = json.load(annos)\n\n    print(\'the annotation_json num_key is:\', len(annotation_json))  # 统计json文件的关键字长度\n    print(\'the annotation_json key is:\', annotation_json.keys())  # 读出json文件的关键字\n    print(\'the annotation_json num_images is:\', len(annotation_json[\'images\']))  # json文件中包含的图片数量\n\n    label_id_cls_dict = dict()\n    for i in range(len(annotation_json[\'categories\'][::])):\n        id = annotation_json[\'categories\'][i][\'id\']\n        name = annotation_json[\'categories\'][i][\'name\']\n        label_id_cls_dict[id] = name\n    print(\'the categories id to name dict is\', label_id_cls_dict)\n\n    id = annotation_json[\'images\'][num_image - 1][\'id\']  # 读取图片id\n    image_name = annotation_json[\'images\'][num_image - 1][\'file_name\']  # 读取图片名\n    image_path = os.path.join(img_path, str(image_name).zfill(5))  # 拼接图像路径\n    image = mmcv.imread(image_path)\n    bboxes = []\n    labels = []\n\n    for i in range(len(annotation_json[\'annotations\'][::])):\n        if annotation_json[\'annotations\'][i][\'image_id\'] == id:\n            x, y, w, h = annotation_json[\'annotations\'][i][\'bbox\']  # 读取边框\n            label_id = annotation_json[\'annotations\'][i][\'category_id\']\n            label = label_id_cls_dict[label_id]\n\n            # 将左上和右下的坐标加入列表\n            bboxes.append([int(x), int(y), int(x + w), int(y + h)])\n            labels.append(label)\n    print(\'The num_bbox of the display image is:\', len(bboxes))\n    mmcv.imshow_det_bboxes(image, np.array(bboxes), np.array(labels))\n\n\ndef visualization_bbox_coco_api(num_image, json_path, img_path):\n    coco = COCO(json_path)\n    img = list(coco.imgs.items())[num_image][1]\n\n    image_id = img[\'id\']  # 读取图像id\n    image_name = img[\'file_name\']  # 读取图像名字\n\n    image = mmcv.imread(os.path.join(img_path, image_name))  # 读取图像\n    annIds = coco.getAnnIds(imgIds=image_id)\n    anns = coco.loadAnns(annIds)\n\n    bboxes = []\n    labels = []\n    for i in range(len(annIds)):\n        x, y, w, h = anns[i][\'bbox\']  # 读取边框\n        label_id = anns[i][\'category_id\']\n        label = coco.cats[label_id][\'name\']\n        bboxes.append([int(x), int(y), int(x + w), int(y + h)])\n\n        labels.append(label)\n\n    print(\'The num_bbox of the display image is:\', len(bboxes))\n    mmcv.imshow_det_bboxes(image, np.array(bboxes), np.array(labels))\n\n\nif __name__ == "__main__":\n    train_json = \'/home/muyun99/competition/detection/data/kaggle_cowboyoutfits/annotations.json\'\n    train_path = \'/home/muyun99/competition/detection/data/kaggle_cowboyoutfits/images\'\n    for i in range(10):\n        visualization_bbox(i, train_json, train_path)\n        visualization_bbox_coco_api(i, train_json, train_path)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n',normalizedContent:'# coco格式数据集\n\n# 01、coco 格式数据集标注解析\n\n\n\n分为四部分信息\n\n"info": {\n    "description": "cowboysuit",\n    "url": "http://github.com/dmlc/gluon-cv",\n    "version": "1.0",\n    "year": 2021,\n    "contributor": "gluoncv/autogluon",\n    "date_created": "2021/07/01"\n  \t},\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * info 主要描述数据的元信息，一般就是标注数据的版权和来源之类的\n\n"images": [\n      {\n      \t"id": 11684018977583795263,\n      \t"file_name": "a225f95b03ce0c3f.jpg",\n      \t"neg_category_ids": [\n        \t120\n      \t],\n      \t"pos_category_ids": [\n\t        69,\n    \t    216,\n        \t308,\n        \t433\n      \t],\n      \t"width": 1024,\n      \t"height": 683,\n      \t"source": "openimages"\n   \t  },\n     ...\n  \t]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n * images 是一个列表，列表里的每个实例都描述一张图像，图像比较重要的属性值如下\n   * id\n   * file_name\n   * height\n   * weight\n\n"annotations":[\n\t  {\n      \t"id": 9502719,\n      \t"image_id": 11684018977583795263,\n      \t"freebase_id": "/m/032b3c",\n      \t"category_id": 588,\n      \t"iscrowd": false,\n      \t"bbox": [\n        524.16,\n        255.0,\n        365.44,\n        411.98\n      \t],\n      \t"area": 150553.52\n   \t  }\n      ..\n  \t]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n * annotations 则是对应图像的标注，标注比较重要的属性值如下\n   * image_id\n   * category_id\n   * bbox: [x, y, width, height]\n   * bbox 的格式是目标框左上角的坐标以及该框的宽和高\n\n"categories": [\n    {\n        "id": 87,\n        "name": "belt",\n        "freebase_id": "/m/0176mf"\n    },\n    ...\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n * categories 则是所有的类别，比较重要的属性值如下\n   * id\n   * name\n\n# 02、coco 格式数据集标注可视化\n\n# 2.1 目标框的标注可视化\n\n# import cv2\nimport json\nimport os\n\nimport mmcv\nimport numpy as np\nfrom pycocotools.coco import coco\n\n\ndef visualization_bbox(num_image, json_path, img_path):  # 需要画的第num副图片， 对应的json路径和图片路径\n    with open(json_path) as annos:\n        annotation_json = json.load(annos)\n\n    print(\'the annotation_json num_key is:\', len(annotation_json))  # 统计json文件的关键字长度\n    print(\'the annotation_json key is:\', annotation_json.keys())  # 读出json文件的关键字\n    print(\'the annotation_json num_images is:\', len(annotation_json[\'images\']))  # json文件中包含的图片数量\n\n    label_id_cls_dict = dict()\n    for i in range(len(annotation_json[\'categories\'][::])):\n        id = annotation_json[\'categories\'][i][\'id\']\n        name = annotation_json[\'categories\'][i][\'name\']\n        label_id_cls_dict[id] = name\n    print(\'the categories id to name dict is\', label_id_cls_dict)\n\n    id = annotation_json[\'images\'][num_image - 1][\'id\']  # 读取图片id\n    image_name = annotation_json[\'images\'][num_image - 1][\'file_name\']  # 读取图片名\n    image_path = os.path.join(img_path, str(image_name).zfill(5))  # 拼接图像路径\n    image = mmcv.imread(image_path)\n    bboxes = []\n    labels = []\n\n    for i in range(len(annotation_json[\'annotations\'][::])):\n        if annotation_json[\'annotations\'][i][\'image_id\'] == id:\n            x, y, w, h = annotation_json[\'annotations\'][i][\'bbox\']  # 读取边框\n            label_id = annotation_json[\'annotations\'][i][\'category_id\']\n            label = label_id_cls_dict[label_id]\n\n            # 将左上和右下的坐标加入列表\n            bboxes.append([int(x), int(y), int(x + w), int(y + h)])\n            labels.append(label)\n    print(\'the num_bbox of the display image is:\', len(bboxes))\n    mmcv.imshow_det_bboxes(image, np.array(bboxes), np.array(labels))\n\n\ndef visualization_bbox_coco_api(num_image, json_path, img_path):\n    coco = coco(json_path)\n    img = list(coco.imgs.items())[num_image][1]\n\n    image_id = img[\'id\']  # 读取图像id\n    image_name = img[\'file_name\']  # 读取图像名字\n\n    image = mmcv.imread(os.path.join(img_path, image_name))  # 读取图像\n    annids = coco.getannids(imgids=image_id)\n    anns = coco.loadanns(annids)\n\n    bboxes = []\n    labels = []\n    for i in range(len(annids)):\n        x, y, w, h = anns[i][\'bbox\']  # 读取边框\n        label_id = anns[i][\'category_id\']\n        label = coco.cats[label_id][\'name\']\n        bboxes.append([int(x), int(y), int(x + w), int(y + h)])\n\n        labels.append(label)\n\n    print(\'the num_bbox of the display image is:\', len(bboxes))\n    mmcv.imshow_det_bboxes(image, np.array(bboxes), np.array(labels))\n\n\nif __name__ == "__main__":\n    train_json = \'/home/muyun99/competition/detection/data/kaggle_cowboyoutfits/annotations.json\'\n    train_path = \'/home/muyun99/competition/detection/data/kaggle_cowboyoutfits/images\'\n    for i in range(10):\n        visualization_bbox(i, train_json, train_path)\n        visualization_bbox_coco_api(i, train_json, train_path)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n',charsets:{cjk:!0},lastUpdated:"2021/08/08, 21:56:44"},{title:"PASCAL VOC 2012调色板 color map生成源代码分析",frontmatter:{title:"PASCAL VOC 2012调色板 color map生成源代码分析",date:"2021-05-07T16:46:58.000Z",permalink:"/pages/15a0e4/",categories:["计算机视觉","图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/06.PASCAL%20VOC%202012%20%E8%B0%83%E8%89%B2%E6%9D%BF%20color%20map%20%E8%B5%8B%E5%80%BC.html",relativePath:"02.学习笔记/02.代码实践-图像分割/06.PASCAL VOC 2012 调色板 color map 赋值.md",key:"v-c1d3d84c",path:"/pages/15a0e4/",headersStr:null,content:"def putpalette(mask):\n    colormap = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128],\n               [128, 0, 128], [0, 128, 128], [128, 128, 128], [64, 0, 0],\n               [192, 0, 0], [64, 128, 0], [192, 128, 0], [64, 0, 128],\n               [192, 0, 128], [64, 128, 128], [192, 128, 128], [0, 64, 0],\n               [128, 64, 0], [0, 192, 0], [128, 192, 0], [0, 64, 128]]\n\n    r = mask.copy()\n    g = mask.copy()\n    b = mask.copy()\n\n    for cls in range(21):\n        r[mask == cls] = colormap[cls][0]\n        g[mask == cls] = colormap[cls][1]\n        b[mask == cls] = colormap[cls][2]\n\n    # b[mask == cls] = self.colormap[color_cls][2]\n\n    rgb = np.zeros((mask.shape[0], mask.shape[1], 3))\n    rgb[:, :, 0] = b\n    rgb[:, :, 1] = g\n    rgb[:, :, 2] = r\n\n    return rgb.astype('uint8')\n\n# 传入图像的 name 获取图像和标注的路径\ndir_img = os.path.join(args.voc12_root, args.img_dir, name+'.jpg')\ndir_gt = os.path.join(args.gt_dir, name+'.png')\n\n# 读入灰度图像\nimg = mmcv.imread(dir_img)\ngt = mmcv.imread(dir_gt, flag='grayscale')\n\n# 为灰色标注上色\ncolored_mask = putpalette(mask)\n\n# 将彩色标注与原图融合\nvis_mask = cv2.addWeighted(img, 0.5, colored_mask, 0.5, gamma=0.1)\nvis = concat_two_img(img, vis_mask)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\nnp 的 阈值化和 torch 的阈值化\n\nthreshold, upper, lower = 0.5, 1, 0\n\na = np.random.rand(4, 4)\na_np = np.where(a > threshold, upper, lower)\n\na_tensor = torch.from_numpy(a)\na_tensor_torch = torch.where(a_tensor > threshold, upper, lower)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n参考资料\n\n * https://yimiandai.me/post/voc-pillow/",normalizedContent:"def putpalette(mask):\n    colormap = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128],\n               [128, 0, 128], [0, 128, 128], [128, 128, 128], [64, 0, 0],\n               [192, 0, 0], [64, 128, 0], [192, 128, 0], [64, 0, 128],\n               [192, 0, 128], [64, 128, 128], [192, 128, 128], [0, 64, 0],\n               [128, 64, 0], [0, 192, 0], [128, 192, 0], [0, 64, 128]]\n\n    r = mask.copy()\n    g = mask.copy()\n    b = mask.copy()\n\n    for cls in range(21):\n        r[mask == cls] = colormap[cls][0]\n        g[mask == cls] = colormap[cls][1]\n        b[mask == cls] = colormap[cls][2]\n\n    # b[mask == cls] = self.colormap[color_cls][2]\n\n    rgb = np.zeros((mask.shape[0], mask.shape[1], 3))\n    rgb[:, :, 0] = b\n    rgb[:, :, 1] = g\n    rgb[:, :, 2] = r\n\n    return rgb.astype('uint8')\n\n# 传入图像的 name 获取图像和标注的路径\ndir_img = os.path.join(args.voc12_root, args.img_dir, name+'.jpg')\ndir_gt = os.path.join(args.gt_dir, name+'.png')\n\n# 读入灰度图像\nimg = mmcv.imread(dir_img)\ngt = mmcv.imread(dir_gt, flag='grayscale')\n\n# 为灰色标注上色\ncolored_mask = putpalette(mask)\n\n# 将彩色标注与原图融合\nvis_mask = cv2.addweighted(img, 0.5, colored_mask, 0.5, gamma=0.1)\nvis = concat_two_img(img, vis_mask)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\nnp 的 阈值化和 torch 的阈值化\n\nthreshold, upper, lower = 0.5, 1, 0\n\na = np.random.rand(4, 4)\na_np = np.where(a > threshold, upper, lower)\n\na_tensor = torch.from_numpy(a)\na_tensor_torch = torch.where(a_tensor > threshold, upper, lower)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n参考资料\n\n * https://yimiandai.me/post/voc-pillow/",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"语义分割数据集灰度分割图转彩色分割图代码",frontmatter:{title:"语义分割数据集灰度分割图转彩色分割图代码",date:"2021-04-14T23:37:45.000Z",permalink:"/pages/f64db3/",categories:["计算机视觉","图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/07.%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%95%B0%E6%8D%AE%E9%9B%86%E7%81%B0%E5%BA%A6%E5%88%86%E5%89%B2%E5%9B%BE%E8%BD%AC%E5%BD%A9%E8%89%B2%E5%88%86%E5%89%B2%E5%9B%BE%E4%BB%A3%E7%A0%81.html",relativePath:"02.学习笔记/02.代码实践-图像分割/07.语义分割数据集灰度分割图转彩色分割图代码.md",key:"v-41cb72a9",path:"/pages/f64db3/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/08/02, 21:04:52"},{title:"mmdetection COCO格式数据集",frontmatter:{title:"mmdetection COCO格式数据集",date:"2021-07-30T21:20:32.000Z",permalink:"/pages/1708c0/",categories:["计算机视觉","目标检测"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/01.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/02.mmdetection%20COCO%E6%A0%BC%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86.html",relativePath:"02.学习笔记/01.代码实践-目标检测/02.mmdetection COCO格式数据集.md",key:"v-ed2e8b86",path:"/pages/1708c0/",headersStr:null,content:"RuntimeError: DataLoader worker (pid 34888) is killed by signal: Terminated.\n\n1、定义数据种类，需要修改的地方在mmdetection/mmdet/datasets/coco.py。把CLASSES的那个tuple改为自己数据集对应的种类tuple即可。例如：\n\nCLASSES = ('belt', 'sunglasses', 'boot', 'cowboy_hat', 'jacket')\n\n\n1\n\n\n2、接着在mmdetection/mmdet/core/evaluation/class_names.py修改coco_classes数据集类别，这个关系到后面test的时候结果图中显示的类别名称。例如：\n\ndef coco_classes():\n    return [\n        'belt', 'sunglasses', 'boot', 'cowboy_hat', 'jacket'\n    ]\n\n\n1\n2\n3\n4\n",normalizedContent:"runtimeerror: dataloader worker (pid 34888) is killed by signal: terminated.\n\n1、定义数据种类，需要修改的地方在mmdetection/mmdet/datasets/coco.py。把classes的那个tuple改为自己数据集对应的种类tuple即可。例如：\n\nclasses = ('belt', 'sunglasses', 'boot', 'cowboy_hat', 'jacket')\n\n\n1\n\n\n2、接着在mmdetection/mmdet/core/evaluation/class_names.py修改coco_classes数据集类别，这个关系到后面test的时候结果图中显示的类别名称。例如：\n\ndef coco_classes():\n    return [\n        'belt', 'sunglasses', 'boot', 'cowboy_hat', 'jacket'\n    ]\n\n\n1\n2\n3\n4\n",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"复现PSA",frontmatter:{title:"复现PSA",date:"2021-07-01T17:12:39.000Z",permalink:"/pages/5e185e/",categories:["计算机视觉","图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/08.%E5%A4%8D%E7%8E%B0PSA.html",relativePath:"02.学习笔记/02.代码实践-图像分割/08.复现PSA.md",key:"v-0efa04a6",path:"/pages/5e185e/",headers:[{level:2,title:"复现 PSA 弱监督语义分割算法",slug:"复现-psa-弱监督语义分割算法",normalizedTitle:"复现 psa 弱监督语义分割算法",charIndex:2}],headersStr:"复现 PSA 弱监督语义分割算法",content:'# 复现 PSA 弱监督语义分割算法\n\n弱监督语义分割一般都有三个步骤：\n\n * 训练分类网络利用分类网络得到伪标签\n * 细化伪标签（较通用的即为本文的PSA）\n * 利用伪标签训练的分类模型\n\n一般的设置是：\n\n * 数据集：\n   * 使用VOC 2012 Aug为训练集，共 10582 张图像；\n   * 使用 VOC 2012 test 作为测试集，共 1449 张图像\n * 网络：\n   * 分类网络：VGG-16、ResNet-101、ResNet38d\n   * 分割网络：DeepLabv1、DeepLabv2\n   * DeepLab V1\n     * Paper: Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs.\n     * code: https://github.com/wangleihitcs/DeepLab-V1-PyTorch\n   * DeepLab V2\n     * Paper: Deeplab: Semantic image segmentationwith deep convolutional nets, atrous convolution, and fullyconnected crfs.（DeepLab V2）\n     * code: https://github.com/kazuto1011/deeplab-pytorch\n\n\n\n# 参考资料\n\n * https://sh-tsang.medium.com/resnet-38-wider-or-deeper-resnet-image-classification-semantic-segmentation-f297f2f73437\n * Wu, Zifeng, Chunhua Shen, and Anton Van Den Hengel. "Wider or deeper: Revisiting the resnet model for visual recognition." Pattern Recognition 90 (2019): 119-133.\n * ',normalizedContent:'# 复现 psa 弱监督语义分割算法\n\n弱监督语义分割一般都有三个步骤：\n\n * 训练分类网络利用分类网络得到伪标签\n * 细化伪标签（较通用的即为本文的psa）\n * 利用伪标签训练的分类模型\n\n一般的设置是：\n\n * 数据集：\n   * 使用voc 2012 aug为训练集，共 10582 张图像；\n   * 使用 voc 2012 test 作为测试集，共 1449 张图像\n * 网络：\n   * 分类网络：vgg-16、resnet-101、resnet38d\n   * 分割网络：deeplabv1、deeplabv2\n   * deeplab v1\n     * paper: semantic image segmentation with deep convolutional nets and fully connected crfs.\n     * code: https://github.com/wangleihitcs/deeplab-v1-pytorch\n   * deeplab v2\n     * paper: deeplab: semantic image segmentationwith deep convolutional nets, atrous convolution, and fullyconnected crfs.（deeplab v2）\n     * code: https://github.com/kazuto1011/deeplab-pytorch\n\n\n\n# 参考资料\n\n * https://sh-tsang.medium.com/resnet-38-wider-or-deeper-resnet-image-classification-semantic-segmentation-f297f2f73437\n * wu, zifeng, chunhua shen, and anton van den hengel. "wider or deeper: revisiting the resnet model for visual recognition." pattern recognition 90 (2019): 119-133.\n * ',charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"转换cityscapes 到对应的类别",frontmatter:{title:"转换cityscapes 到对应的类别",date:"2021-09-22T15:11:38.000Z",permalink:"/pages/6c2aa7/",categories:["学习笔记","代码实践-图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/09.%E8%BD%AC%E6%8D%A2cityscapes%20%E5%88%B0%E5%AF%B9%E5%BA%94%E7%9A%84%E7%B1%BB%E5%88%AB.html",relativePath:"02.学习笔记/02.代码实践-图像分割/09.转换cityscapes 到对应的类别.md",key:"v-2d34cbf4",path:"/pages/6c2aa7/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/09/26, 00:09:41"},{title:"上采样函数",frontmatter:{title:"上采样函数",date:"2021-09-23T20:37:36.000Z",permalink:"/pages/ce4f65/",categories:["学习笔记","代码实践-图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/10.%E4%B8%8A%E9%87%87%E6%A0%B7%E5%87%BD%E6%95%B0.html",relativePath:"02.学习笔记/02.代码实践-图像分割/10.上采样函数.md",key:"v-af5feadc",path:"/pages/ce4f65/",headersStr:null,content:"x = nn.functional.interpolate(x, scale_factor=8, mode='bilinear', align_corners=False) \n\n\n1\n\n\nlogits = [32,19,128,256]\n\nmask = [32, 512, 1024]\n\n将 [19,128,256] 上采样到 [19,512,1024]",normalizedContent:"x = nn.functional.interpolate(x, scale_factor=8, mode='bilinear', align_corners=false) \n\n\n1\n\n\nlogits = [32,19,128,256]\n\nmask = [32, 512, 1024]\n\n将 [19,128,256] 上采样到 [19,512,1024]",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"基于深度学习的图像分割技术",frontmatter:{title:"基于深度学习的图像分割技术",date:"2021-04-14T23:37:22.000Z",permalink:"/pages/14bcdb/",categories:["计算机视觉","图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/00.%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%8A%80%E6%9C%AF.html",relativePath:"02.学习笔记/02.代码实践-图像分割/00.基于深度学习的图像分割技术.md",key:"v-789b2126",path:"/pages/14bcdb/",headers:[{level:3,title:"语义分割入门",slug:"语义分割入门",normalizedTitle:"语义分割入门",charIndex:2},{level:3,title:"摘录了 Cityscapes 数据集上的语义分割数据集的精度",slug:"摘录了-cityscapes-数据集上的语义分割数据集的精度",normalizedTitle:"摘录了 cityscapes 数据集上的语义分割数据集的精度",charIndex:1826},{level:3,title:"实时语义分割",slug:"实时语义分割",normalizedTitle:"实时语义分割",charIndex:5113},{level:3,title:"实例分割",slug:"实例分割",normalizedTitle:"实例分割",charIndex:1735},{level:3,title:"全景分割",slug:"全景分割",normalizedTitle:"全景分割",charIndex:6999},{level:3,title:"语义分割的 Domain Adaptation",slug:"语义分割的-domain-adaptation",normalizedTitle:"语义分割的 domain adaptation",charIndex:7437},{level:3,title:"半监督分割",slug:"半监督分割",normalizedTitle:"半监督分割",charIndex:7679},{level:3,title:"分割的标注工具",slug:"分割的标注工具",normalizedTitle:"分割的标注工具",charIndex:7873},{level:3,title:"分割的性能评估",slug:"分割的性能评估",normalizedTitle:"分割的性能评估",charIndex:8031}],headersStr:"语义分割入门 摘录了 Cityscapes 数据集上的语义分割数据集的精度 实时语义分割 实例分割 全景分割 语义分割的 Domain Adaptation 半监督分割 分割的标注工具 分割的性能评估",content:'# 语义分割入门\n\n# 什么是语义分割\n\n图像分割是许多视觉理解系统中必不可少的组成部分，在医学影像分析、机器人感知、 视频监控、自动驾驶等领域都有着十分重要的地位。图像分割任务可以理解为基于语义信息和实例信息的像素级别的分类问题。\n\n参考资料：语义分割-从入门到放弃\n\n# 语义分割的发展现状\n\nFCN利用了全卷积网络产生特征，输入空间映射，实现了端到端的语义分割任务，成为深度学习技术应用于语义分割问题的基石。U-Net 通过上采样和 skip connection 融合高低层的特征信息，获得了更加精准的分割结果。SegNet 使用 Maxpooling indices 来增强位置信息，提高了 SegNet 的效率。\n\n也有学者提出了 DeepLab 算法[3,20–22]，经过不断演进后共有四个版本。DeepLabv1 模 型[3] 将深度卷积神经网络和概率图模型进行结合。作者指出：深度卷积神经网络下采样导 致细节信息丢失，并且其结构会限制空间定位精度，该算法则使用空洞卷积以及条件随机 场对模型进行了改进。空洞卷积，在保证较大感受野的同时不过分下采样丢失过多细节信 息。条件随机场用于接收卷积神经网络的最后一层的响应进行后处理，以较少的时间内完 成细粒度的定位。DeepLabv1 将卷积神经网络和条件随机场进行耦合，并且使用多尺度预 测方法提高了边界定位精度，能够较好的恢复对象边缘信息，在 GPU 上能够达到 8 FPS 的速度。DeepLabv2[20] 将下采样的层全部替换为空洞卷积，以更高的采样密度来计算特征 映射，其特征提取模块也从 VGG[9] 换到了 ResNet[11]，加强了网络的特征提取能力。作者 还提出了基于空洞卷积的空间池化金字塔（ASPP）模块，以不同采样率的空洞卷积进行 采样，以多个比例学习图像的上下文信息，丰富了特征的维度。DeepLabv3[21] 取消了前两 个版本中的条件随机场的后处理，重点关注了四种利用上下文信息的网络模块，包括图像 金字塔、编码器-解码器、上下文模块、空间金字塔池化。该算法加深了网络深度，同时调 整了网络的下采样率，减少信息的丢失，级联模块进行特征提取后，将特征输入到结合图 像级别特征的空间金字塔池化模块，完成了整个网络结构的搭建。作者在这些模块的搭建 了进行了大量的实验验证，最终演化出最终的结构，在 PASCAL VOC 2012 数据集上的测 试性能的达到了当时的最优水平。DeepLabv3+[22] 使用空洞卷积的 Xception[15] 进行特征采 样，其网络结构见图 2.4。作者在 DeepLabv3 的基础上添加了 Decoder 模块，将 Xception 提取出的特征与 ASPP 模块采样后的特征进行特征融合后共同上采样恢复图像分辨率，使 整个模型成为编码器-解码器结构。解码器模块可以获得更好的边界分割效果，有助于模型 性能的提升。\n\nRefineNet 精心设计了 Decoder 模块，并且增加了 residual connections，提升了网络的表达能力。讨论了空洞卷积的缺点。PSPNet 使用pyramid pooling整合context，使用auxiliary loss 提升网络的学习能力。DANet DANet是一种经典的应用self-Attention的网络，它引入了一种**自注意力机制来分别捕获空间维度和通道维度中的特征依赖关系。**提出了双重注意网络（DANet）来自适应地集成局部特征和全局依赖。在传统的扩张FCN之上附加两种类型的注意力模块，分别模拟空间和通道维度中的语义相互依赖性。\n\nHRNet通过并行多个分辨率的分支，加上不断进行不同分支之间的信息交互，同时达到强语义信息和精准位置信息的目的。我觉得最大的创新点还是能够从头到尾保持高分辨率，而不同分支的信息交互是为了补充通道数减少带来的信息损耗。OCR 方法提出的物体上下文信息的目的在于显式地增强物体信息，通过计算一组物体的区域特征表达，根据物体区域特征表示与像素特征表示之间的相似度将这些物体区域特征表示传播给每一个像素。\n\nPointRend 把语义分割以及实例分割问题（统称图像分割问题）当做一个渲染问题来解决。但本质上这篇论文其实是一个新型上采样方法，针对物体边缘的图像分割进行优化，使其在难以分割的物体边缘部分有更好的表现。\n\n\n# 摘录了 Cityscapes 数据集上的语义分割数据集的精度\n\nhttps://paperswithcode.com/sota/semantic-segmentation-on-cityscapes\n\n1、2014 DeepLab 63.1%\n\nSemantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs\n\n2、2014 FCN 65.3%\n\nFully Convolutional Networks for Semantic Segmentation\n\n3、2015 SegNet 57.0%\n\nSegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation\n\n5、2016 DeepLab-CRF (ResNet-101) 70.4%\n\nDeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs\n\n6、2016 RefineNet (ResNet-101) 73.6%\n\nRefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation\n\n7、2016 PSPNet (ResNet-101) 81.2%\n\nPyramid Scene Parsing Network\n\n9、2017 DeepLabv3 (ResNet-101 coarse)\n\nRethinking Atrous Convolution for Semantic Image Segmentation\n\n13、2018 DenseASPP (DenseNet-161) 80.6%\n\nDenseASPP for Semantic Segmentation in Street Scenes\n\n14、2018 PSANet (ResNet-101) 81.4%\n\nPSANet: Point-wise Spatial Attention Network for Scene Parsing\n\n15、2018 CCNet 81.4%\n\nCCNet: Criss-Cross Attention for Semantic Segmentation\n\n16、2018 DANet 81.5%\n\nDual Attention Network for Scene Segmentation\n\n17、2018 OCNet 81.7%\n\nOCNet: Object Context Network for Scene Parsing\n\n18、2018 DeepLabv3+ (Xception-JFT) 82.1%\n\nEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation\n\n19、2018 DeepLabv3Plus + SDCNetAug 83.5%\n\nImproving Semantic Segmentation via Video Propagation and Label Relaxation\n\n20、2019 Asymmetric ALNN 81.3%\n\nAsymmetric Non-local Neural Networks for Semantic Segmentation\n\n21、2019 BFP 81.4%\n\nBoundary-Aware Feature Propagation for Scene Segmentation\n\n22、2019 HRNet (HRNetV2-W48) 81.6%\n\nHigh-Resolution Representations for Labeling Pixels and Regions\n\n23、2019 OCR (ResNet-101) 81.8%\n\nObject-Contextual Representations for Semantic Segmentation\n\n24、2019 Auto-DeepLab-L 82.1%\n\nAuto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation\n\n25、2019 OCR (ResNet-101 coarse) 82.4%\n\n见第 23 条\n\n26、2019 OCR (HRNetV2-W48 coarse) 83.0%\n\n见第 23 条\n\n27、2019 HRNetV2+OCR (w/ASP) 83.7%\n\n见第 23 条\n\n28、2019 Panoptic-DeepLab 84.2%\n\nPanoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation\n\n29、2019 HRNetV2 + OCR + extra data 84.5%\n\n见第 23 条\n\n30、2020 ESANet-R34-NBt1D 80.09%\n\nEfficient RGB-D Semantic Segmentation for Indoor Scene Analysis\n\n31、2020 CPN (ResNet-101) 81.3%\n\nContext Prior for Scene Segmentation\n\n32、2020 HANet 83.2%\n\nCars Can\'t Fly up in the Sky: Improving Urban-Scene Segmentation via Height-driven Attention Networks\n\n33、2020 ResNest200 83.3%\n\nResNeSt: Split-Attention Networks\n\n34、2020 EfficientPS 84.21%\n\nEfficientPS: Efficient Panoptic Segmentation\n\n35、2020 HRNet-OCR(Hierarchical Multi-Scale Attention) 85.1%\n\nHierarchical Multi-Scale Attention for Semantic Segmentation\n\n36、2021 DDRNet-39 1.5x 82.4%\n\nDeep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes\n\n37、2021 CCA (ResNet-101) 82.6%\n\nCAA : Channelized Axial Attention for Semantic Segmentation\n\n38、2021 SETR\n\n * paper: [CVPR 2021] Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers\n * code: [Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers\n\n参考资料：\n\n语义分割刷怪进阶\n\n\n# 实时语义分割\n\n1、2016 ENet 58.3%\n\n * ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation\n\n2、2017 ICNet 70.6%\n\n * ICNet for Real-Time Semantic Segmentation on High-Resolution Images\n\n3、2018 ESPNet 60.3%\n\n * paper: ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation\n * code: ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation\n\n4、2018 ESPNetv2 66.2%\n\n * ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network\n\n5、2018 BiSeNet (ResNet-101) 78.9%\n\n * BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation\n\n6、2019 ESNet 70.7%\n\n * ESNet: An Efficient Symmetric Network for Real-time Semantic Segmentation\n\n7、2019 DFANet A 71.3%\n\n * DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation\n\n8、2019 FasterSeg 71.5%\n\n * FasterSeg: Searching for Faster Real-time Semantic Segmentation\n\n9、2020 SFSegNet (ECCV 2020 Oral)\n\n * paper: Semantic Flow for Fast and Accurate Scene Parsing\n * code: Implementation of Our ECCV-2020-oral paper: Semantic Flow for Fast and Accurate Scene Parsing\n\n10、DecoupleSegNets (ECCV 2020)\n\n * paper: Improving Semantic Segmentation via Decoupled Body and Edge Supervision\n\n * code: Implementation of Our ECCV2020-work: Improving Semantic Segmentation via Decoupled Body and Edge Supervision\n\n10、2021 DDRNet\n\n * paper: Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes\n\n * code: The official implementation of "Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes"\n\n参考资料：\n\n实时语义分割 看这一篇就够了！涵盖24篇文章——上篇\n\n实时语义分割 看这一篇就够了！涵盖24篇文章——下篇\n\n\n# 实例分割\n\n1、2021 BCNet（CVPR2021）\n\n * paper：Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers\n * code：Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers [CVPR 2021]\n\n\n# 全景分割\n\n1、2020 Axial-DeepLab (ECCV 2020 Spotlight)\n\n * paper: Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation\n\n * code: This is a PyTorch re-implementation of Axial-DeepLab (ECCV 2020 Spotlight)\n\n2、2020 PanopticFCN（CVPR 2021 Oral）\n\n * paper：Fully Convolutional Networks for Panoptic Segmentation\n * code：Fully Convolutional Networks for Panoptic Segmentation (CVPR2021 Oral)\n * intro：Panoptic FCN：真正End-to-End的全景分割\n\n\n# 语义分割的 Domain Adaptation\n\n1、2018 ADVENT（CVPR 2019）\n\n * paper: ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation\n * code: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation\n\n\n# 半监督分割\n\n1、2021 DTML（arxiv）\n\n * paper：Dual-Task Mutual Learning for Semi-Supervised Medical Image Segmentation\n * code：Dual-Task Mutual Learning for Semi-Supervised Medical Image Segmentation\n\n\n# 分割的标注工具\n\n1、Semantic Segmentation Editor\n\n * link：Web labeling tool for bitmap images and point clouds\n\n2、PixelAnnotationTool\n\n * link：PixelAnnotationTool\n\n\n# 分割的性能评估\n\n1、Boundary IoU API\n\n * link: Boundary IoU API (Beta version)',normalizedContent:'# 语义分割入门\n\n# 什么是语义分割\n\n图像分割是许多视觉理解系统中必不可少的组成部分，在医学影像分析、机器人感知、 视频监控、自动驾驶等领域都有着十分重要的地位。图像分割任务可以理解为基于语义信息和实例信息的像素级别的分类问题。\n\n参考资料：语义分割-从入门到放弃\n\n# 语义分割的发展现状\n\nfcn利用了全卷积网络产生特征，输入空间映射，实现了端到端的语义分割任务，成为深度学习技术应用于语义分割问题的基石。u-net 通过上采样和 skip connection 融合高低层的特征信息，获得了更加精准的分割结果。segnet 使用 maxpooling indices 来增强位置信息，提高了 segnet 的效率。\n\n也有学者提出了 deeplab 算法[3,20–22]，经过不断演进后共有四个版本。deeplabv1 模 型[3] 将深度卷积神经网络和概率图模型进行结合。作者指出：深度卷积神经网络下采样导 致细节信息丢失，并且其结构会限制空间定位精度，该算法则使用空洞卷积以及条件随机 场对模型进行了改进。空洞卷积，在保证较大感受野的同时不过分下采样丢失过多细节信 息。条件随机场用于接收卷积神经网络的最后一层的响应进行后处理，以较少的时间内完 成细粒度的定位。deeplabv1 将卷积神经网络和条件随机场进行耦合，并且使用多尺度预 测方法提高了边界定位精度，能够较好的恢复对象边缘信息，在 gpu 上能够达到 8 fps 的速度。deeplabv2[20] 将下采样的层全部替换为空洞卷积，以更高的采样密度来计算特征 映射，其特征提取模块也从 vgg[9] 换到了 resnet[11]，加强了网络的特征提取能力。作者 还提出了基于空洞卷积的空间池化金字塔（aspp）模块，以不同采样率的空洞卷积进行 采样，以多个比例学习图像的上下文信息，丰富了特征的维度。deeplabv3[21] 取消了前两 个版本中的条件随机场的后处理，重点关注了四种利用上下文信息的网络模块，包括图像 金字塔、编码器-解码器、上下文模块、空间金字塔池化。该算法加深了网络深度，同时调 整了网络的下采样率，减少信息的丢失，级联模块进行特征提取后，将特征输入到结合图 像级别特征的空间金字塔池化模块，完成了整个网络结构的搭建。作者在这些模块的搭建 了进行了大量的实验验证，最终演化出最终的结构，在 pascal voc 2012 数据集上的测 试性能的达到了当时的最优水平。deeplabv3+[22] 使用空洞卷积的 xception[15] 进行特征采 样，其网络结构见图 2.4。作者在 deeplabv3 的基础上添加了 decoder 模块，将 xception 提取出的特征与 aspp 模块采样后的特征进行特征融合后共同上采样恢复图像分辨率，使 整个模型成为编码器-解码器结构。解码器模块可以获得更好的边界分割效果，有助于模型 性能的提升。\n\nrefinenet 精心设计了 decoder 模块，并且增加了 residual connections，提升了网络的表达能力。讨论了空洞卷积的缺点。pspnet 使用pyramid pooling整合context，使用auxiliary loss 提升网络的学习能力。danet danet是一种经典的应用self-attention的网络，它引入了一种**自注意力机制来分别捕获空间维度和通道维度中的特征依赖关系。**提出了双重注意网络（danet）来自适应地集成局部特征和全局依赖。在传统的扩张fcn之上附加两种类型的注意力模块，分别模拟空间和通道维度中的语义相互依赖性。\n\nhrnet通过并行多个分辨率的分支，加上不断进行不同分支之间的信息交互，同时达到强语义信息和精准位置信息的目的。我觉得最大的创新点还是能够从头到尾保持高分辨率，而不同分支的信息交互是为了补充通道数减少带来的信息损耗。ocr 方法提出的物体上下文信息的目的在于显式地增强物体信息，通过计算一组物体的区域特征表达，根据物体区域特征表示与像素特征表示之间的相似度将这些物体区域特征表示传播给每一个像素。\n\npointrend 把语义分割以及实例分割问题（统称图像分割问题）当做一个渲染问题来解决。但本质上这篇论文其实是一个新型上采样方法，针对物体边缘的图像分割进行优化，使其在难以分割的物体边缘部分有更好的表现。\n\n\n# 摘录了 cityscapes 数据集上的语义分割数据集的精度\n\nhttps://paperswithcode.com/sota/semantic-segmentation-on-cityscapes\n\n1、2014 deeplab 63.1%\n\nsemantic image segmentation with deep convolutional nets and fully connected crfs\n\n2、2014 fcn 65.3%\n\nfully convolutional networks for semantic segmentation\n\n3、2015 segnet 57.0%\n\nsegnet: a deep convolutional encoder-decoder architecture for image segmentation\n\n5、2016 deeplab-crf (resnet-101) 70.4%\n\ndeeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs\n\n6、2016 refinenet (resnet-101) 73.6%\n\nrefinenet: multi-path refinement networks for high-resolution semantic segmentation\n\n7、2016 pspnet (resnet-101) 81.2%\n\npyramid scene parsing network\n\n9、2017 deeplabv3 (resnet-101 coarse)\n\nrethinking atrous convolution for semantic image segmentation\n\n13、2018 denseaspp (densenet-161) 80.6%\n\ndenseaspp for semantic segmentation in street scenes\n\n14、2018 psanet (resnet-101) 81.4%\n\npsanet: point-wise spatial attention network for scene parsing\n\n15、2018 ccnet 81.4%\n\nccnet: criss-cross attention for semantic segmentation\n\n16、2018 danet 81.5%\n\ndual attention network for scene segmentation\n\n17、2018 ocnet 81.7%\n\nocnet: object context network for scene parsing\n\n18、2018 deeplabv3+ (xception-jft) 82.1%\n\nencoder-decoder with atrous separable convolution for semantic image segmentation\n\n19、2018 deeplabv3plus + sdcnetaug 83.5%\n\nimproving semantic segmentation via video propagation and label relaxation\n\n20、2019 asymmetric alnn 81.3%\n\nasymmetric non-local neural networks for semantic segmentation\n\n21、2019 bfp 81.4%\n\nboundary-aware feature propagation for scene segmentation\n\n22、2019 hrnet (hrnetv2-w48) 81.6%\n\nhigh-resolution representations for labeling pixels and regions\n\n23、2019 ocr (resnet-101) 81.8%\n\nobject-contextual representations for semantic segmentation\n\n24、2019 auto-deeplab-l 82.1%\n\nauto-deeplab: hierarchical neural architecture search for semantic image segmentation\n\n25、2019 ocr (resnet-101 coarse) 82.4%\n\n见第 23 条\n\n26、2019 ocr (hrnetv2-w48 coarse) 83.0%\n\n见第 23 条\n\n27、2019 hrnetv2+ocr (w/asp) 83.7%\n\n见第 23 条\n\n28、2019 panoptic-deeplab 84.2%\n\npanoptic-deeplab: a simple, strong, and fast baseline for bottom-up panoptic segmentation\n\n29、2019 hrnetv2 + ocr + extra data 84.5%\n\n见第 23 条\n\n30、2020 esanet-r34-nbt1d 80.09%\n\nefficient rgb-d semantic segmentation for indoor scene analysis\n\n31、2020 cpn (resnet-101) 81.3%\n\ncontext prior for scene segmentation\n\n32、2020 hanet 83.2%\n\ncars can\'t fly up in the sky: improving urban-scene segmentation via height-driven attention networks\n\n33、2020 resnest200 83.3%\n\nresnest: split-attention networks\n\n34、2020 efficientps 84.21%\n\nefficientps: efficient panoptic segmentation\n\n35、2020 hrnet-ocr(hierarchical multi-scale attention) 85.1%\n\nhierarchical multi-scale attention for semantic segmentation\n\n36、2021 ddrnet-39 1.5x 82.4%\n\ndeep dual-resolution networks for real-time and accurate semantic segmentation of road scenes\n\n37、2021 cca (resnet-101) 82.6%\n\ncaa : channelized axial attention for semantic segmentation\n\n38、2021 setr\n\n * paper: [cvpr 2021] rethinking semantic segmentation from a sequence-to-sequence perspective with transformers\n * code: [rethinking semantic segmentation from a sequence-to-sequence perspective with transformers\n\n参考资料：\n\n语义分割刷怪进阶\n\n\n# 实时语义分割\n\n1、2016 enet 58.3%\n\n * enet: a deep neural network architecture for real-time semantic segmentation\n\n2、2017 icnet 70.6%\n\n * icnet for real-time semantic segmentation on high-resolution images\n\n3、2018 espnet 60.3%\n\n * paper: espnet: efficient spatial pyramid of dilated convolutions for semantic segmentation\n * code: espnet: efficient spatial pyramid of dilated convolutions for semantic segmentation\n\n4、2018 espnetv2 66.2%\n\n * espnetv2: a light-weight, power efficient, and general purpose convolutional neural network\n\n5、2018 bisenet (resnet-101) 78.9%\n\n * bisenet: bilateral segmentation network for real-time semantic segmentation\n\n6、2019 esnet 70.7%\n\n * esnet: an efficient symmetric network for real-time semantic segmentation\n\n7、2019 dfanet a 71.3%\n\n * dfanet: deep feature aggregation for real-time semantic segmentation\n\n8、2019 fasterseg 71.5%\n\n * fasterseg: searching for faster real-time semantic segmentation\n\n9、2020 sfsegnet (eccv 2020 oral)\n\n * paper: semantic flow for fast and accurate scene parsing\n * code: implementation of our eccv-2020-oral paper: semantic flow for fast and accurate scene parsing\n\n10、decouplesegnets (eccv 2020)\n\n * paper: improving semantic segmentation via decoupled body and edge supervision\n\n * code: implementation of our eccv2020-work: improving semantic segmentation via decoupled body and edge supervision\n\n10、2021 ddrnet\n\n * paper: deep dual-resolution networks for real-time and accurate semantic segmentation of road scenes\n\n * code: the official implementation of "deep dual-resolution networks for real-time and accurate semantic segmentation of road scenes"\n\n参考资料：\n\n实时语义分割 看这一篇就够了！涵盖24篇文章——上篇\n\n实时语义分割 看这一篇就够了！涵盖24篇文章——下篇\n\n\n# 实例分割\n\n1、2021 bcnet（cvpr2021）\n\n * paper：deep occlusion-aware instance segmentation with overlapping bilayers\n * code：deep occlusion-aware instance segmentation with overlapping bilayers [cvpr 2021]\n\n\n# 全景分割\n\n1、2020 axial-deeplab (eccv 2020 spotlight)\n\n * paper: axial-deeplab: stand-alone axial-attention for panoptic segmentation\n\n * code: this is a pytorch re-implementation of axial-deeplab (eccv 2020 spotlight)\n\n2、2020 panopticfcn（cvpr 2021 oral）\n\n * paper：fully convolutional networks for panoptic segmentation\n * code：fully convolutional networks for panoptic segmentation (cvpr2021 oral)\n * intro：panoptic fcn：真正end-to-end的全景分割\n\n\n# 语义分割的 domain adaptation\n\n1、2018 advent（cvpr 2019）\n\n * paper: advent: adversarial entropy minimization for domain adaptation in semantic segmentation\n * code: adversarial entropy minimization for domain adaptation in semantic segmentation\n\n\n# 半监督分割\n\n1、2021 dtml（arxiv）\n\n * paper：dual-task mutual learning for semi-supervised medical image segmentation\n * code：dual-task mutual learning for semi-supervised medical image segmentation\n\n\n# 分割的标注工具\n\n1、semantic segmentation editor\n\n * link：web labeling tool for bitmap images and point clouds\n\n2、pixelannotationtool\n\n * link：pixelannotationtool\n\n\n# 分割的性能评估\n\n1、boundary iou api\n\n * link: boundary iou api (beta version)',charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"mIoU的计算",frontmatter:{title:"mIoU的计算",date:"2022-01-13T09:35:29.000Z",permalink:"/pages/f55018/",categories:["学习笔记","代码实践-图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/12.mIoU%E7%9A%84%E8%AE%A1%E7%AE%97.html",relativePath:"02.学习笔记/02.代码实践-图像分割/12.mIoU的计算.md",key:"v-048810a1",path:"/pages/f55018/",headers:[{level:3,title:"mIoU 的计算",slug:"miou-的计算",normalizedTitle:"miou 的计算",charIndex:2}],headersStr:"mIoU 的计算",content:"# mIoU 的计算\n\n1、首先计算混淆矩阵\n\n\n\nself.confusion_matrix = np.zeros((self.num_class,)*2)\n\ndef _generate_matrix(self, gt_image, pre_image, ignore_labels):\n    mask = (gt_image >= 0) & (gt_image < self.num_class)\n    for IgLabel in ignore_labels:\n        mask &= (imgLabel != IgLabel)\n            \n    label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n    \n    count = np.bincount(label, minlength=self.num_class**2)\n    confusion_matrix = count.reshape(self.num_class, self.num_class)\n    return confusion_matrix\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n混淆矩阵是一个 num_class*num_class 的矩阵，表示每个类的\n\n * 首先，只计算 0~num_class 的mask\n * 对于 ignore_labels 的标签值，直接忽略掉\n * np.bincount 计算了从 0 到 n2−1n^2-1n2−1 这 n2n^2n2 个数中每个数出现的次数，返回值形状为 (n, n)\n\n计算某一个类的 IoU\n\n# hist 即为 混淆矩阵\ndef per_class_IoU(hist):\n    # 矩阵的对角线上的值组成的一维数组/矩阵的所有元素之和，返回值形状(n,)\n    return np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n\nmIoU = np.mean(per_class_iu(hist)\n\n\n1\n2\n3\n4\n5\n6\n\n\n计算某个类的 TP\n\n# hist 即为 混淆矩阵\ndef per_class_TP(hist):\n    # 矩阵的对角线上的值组成的一维数组/矩阵的所有元素之和，返回值形状(n,)\n    return np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n计算某个类的 TN\n\n# hist 即为 混淆矩阵\ndef per_class_TN(hist):\n\tpass\n\n\n1\n2\n3\n\n\n计算某个类的 FP\n\n# hist 即为 混淆矩阵\ndef per_class_FP(hist):\n\tpass\n\n\n1\n2\n3\n\n\n计算某个类的 FN\n\n# hist 即为 混淆矩阵\ndef per_class_FN(hist):\n    pass\n\n\n1\n2\n3\n\n\n\n\n# 在预测的所有正样本中，预测正确的比例\nprecision = TP / (TP + FP)\n\n# 在所有正样本中，预测为正样本的比例\nrecall = TP / (TP + FN)\n\n# 精确率和召回率的一种权衡\nF1 = 2 * precision * recall /(precision + recall)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n参考资料\n\n * https://yearing1017.gitee.io/2020/02/07/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%8C%87%E6%A0%87MIoU/",normalizedContent:"# miou 的计算\n\n1、首先计算混淆矩阵\n\n\n\nself.confusion_matrix = np.zeros((self.num_class,)*2)\n\ndef _generate_matrix(self, gt_image, pre_image, ignore_labels):\n    mask = (gt_image >= 0) & (gt_image < self.num_class)\n    for iglabel in ignore_labels:\n        mask &= (imglabel != iglabel)\n            \n    label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n    \n    count = np.bincount(label, minlength=self.num_class**2)\n    confusion_matrix = count.reshape(self.num_class, self.num_class)\n    return confusion_matrix\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n混淆矩阵是一个 num_class*num_class 的矩阵，表示每个类的\n\n * 首先，只计算 0~num_class 的mask\n * 对于 ignore_labels 的标签值，直接忽略掉\n * np.bincount 计算了从 0 到 n2−1n^2-1n2−1 这 n2n^2n2 个数中每个数出现的次数，返回值形状为 (n, n)\n\n计算某一个类的 iou\n\n# hist 即为 混淆矩阵\ndef per_class_iou(hist):\n    # 矩阵的对角线上的值组成的一维数组/矩阵的所有元素之和，返回值形状(n,)\n    return np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n\nmiou = np.mean(per_class_iu(hist)\n\n\n1\n2\n3\n4\n5\n6\n\n\n计算某个类的 tp\n\n# hist 即为 混淆矩阵\ndef per_class_tp(hist):\n    # 矩阵的对角线上的值组成的一维数组/矩阵的所有元素之和，返回值形状(n,)\n    return np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n计算某个类的 tn\n\n# hist 即为 混淆矩阵\ndef per_class_tn(hist):\n\tpass\n\n\n1\n2\n3\n\n\n计算某个类的 fp\n\n# hist 即为 混淆矩阵\ndef per_class_fp(hist):\n\tpass\n\n\n1\n2\n3\n\n\n计算某个类的 fn\n\n# hist 即为 混淆矩阵\ndef per_class_fn(hist):\n    pass\n\n\n1\n2\n3\n\n\n\n\n# 在预测的所有正样本中，预测正确的比例\nprecision = tp / (tp + fp)\n\n# 在所有正样本中，预测为正样本的比例\nrecall = tp / (tp + fn)\n\n# 精确率和召回率的一种权衡\nf1 = 2 * precision * recall /(precision + recall)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n参考资料\n\n * https://yearing1017.gitee.io/2020/02/07/%e8%af%ad%e4%b9%89%e5%88%86%e5%89%b2%e6%8c%87%e6%a0%87miou/",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"Multi-label 分类中如何计算 mAP",frontmatter:{title:"Multi-label 分类中如何计算 mAP",date:"2022-01-14T15:07:48.000Z",permalink:"/pages/a0a28d/",categories:["学习笔记","代码实践-图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/13.Multi-label%20%E5%88%86%E7%B1%BB%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%20mAP.html",relativePath:"02.学习笔记/02.代码实践-图像分割/13.Multi-label 分类中如何计算 mAP.md",key:"v-c8ffdb4e",path:"/pages/a0a28d/",headersStr:null,content:"prediction:\ntensor([5.3048e-04, 9.0192e-04, 6.0176e-04, 1.1318e-04, 6.6526e-03, 1.0745e-03,\n        1.3762e-03, 9.9780e-01, 8.6294e-03, 3.1465e-04, 4.0600e-03, 3.5634e-03,\n        3.5838e-04, 8.7827e-04, 3.8405e-02, 3.0164e-03, 3.6145e-04, 8.4642e-03,\n        3.0172e-03, 7.4142e-03])\n        \ngt:\ntensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.])\n\nmAP(pred=prediction_batch, target=labels_batch)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n将 tensor 按照某个阈值而到 1 和 0\n\nprediction_threshold = torch.where(prediction_batch > 0.5, 1, 0)\n\n\n1\n",normalizedContent:"prediction:\ntensor([5.3048e-04, 9.0192e-04, 6.0176e-04, 1.1318e-04, 6.6526e-03, 1.0745e-03,\n        1.3762e-03, 9.9780e-01, 8.6294e-03, 3.1465e-04, 4.0600e-03, 3.5634e-03,\n        3.5838e-04, 8.7827e-04, 3.8405e-02, 3.0164e-03, 3.6145e-04, 8.4642e-03,\n        3.0172e-03, 7.4142e-03])\n        \ngt:\ntensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.])\n\nmap(pred=prediction_batch, target=labels_batch)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n将 tensor 按照某个阈值而到 1 和 0\n\nprediction_threshold = torch.where(prediction_batch > 0.5, 1, 0)\n\n\n1\n",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"自监督学习的一些文章",frontmatter:{title:"自监督学习的一些文章",date:"2021-03-24T00:30:00.000Z",permalink:"/pages/aad696/",categories:["计算机视觉","自监督学习"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/03.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/00.%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%96%87%E7%AB%A0.html",relativePath:"02.学习笔记/03.代码实践-自监督学习/00.自监督学习的一些文章.md",key:"v-4867182a",path:"/pages/aad696/",headers:[{level:3,title:"1、Proxy task 的设计",slug:"_1、proxy-task-的设计",normalizedTitle:"1、proxy task 的设计",charIndex:11},{level:3,title:"2、基于对比学习的文章",slug:"_2、基于对比学习的文章",normalizedTitle:"2、基于对比学习的文章",charIndex:1957},{level:3,title:"3、参考文献",slug:"_3、参考文献",normalizedTitle:"3、参考文献",charIndex:4730}],headersStr:"1、Proxy task 的设计 2、基于对比学习的文章 3、参考文献",content:"# 一些文章\n\n\n# 1、Proxy task 的设计\n\n# 1.1 基于先验知识的设计\n\n1.1.1 利用 Motion propagation 来设计 proxy task\n\nSelf-Supervised Learning via Conditional Motion Propagation\n\n1.1.2 利用 Motion Prediction 来设计 proxy task\n\nDense Optical Flow Prediction From a Static Image\n\n# 1.2 基于连贯性的设计\n\n1.2.1 利用 Jigsaw Puzzles （物体的语义连贯性）来设计proxy task\n\nUnsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\n\n1.2.2 利用视频中物体运动的时间连贯性来设计 proxy task\n\nShuffle and Learn: Unsupervised Learning using Temporal Order Verification\n\n\n\n# 1.3 基于数据内部结构的设计\n\n目前很火的基于contrastive learning的方法，包括NPID, MoCo, SimCLR等，我们可以将它们统一为instance discrimination [6]任务。如下图，这类任务通常对图片做各种变换，然后优化目标是同一张图片的不同变换在特征空间中尽量接近，不同图片在特征空间中尽量远离。\n\n1.3.1 Momentum Contrast for Unsupervised Visual Representation Learning (MoCov1)\n\nMomentum Contrast for Unsupervised Visual Representation Learning\n\n1.3.2 Improved Baselines with Momentum Contrastive Learning (Mocov2)\n\nImproved Baselines with Momentum Contrastive Learning\n\n1.3.3 A Simple Framework for Contrastive Learning of Visual Representations (SimCLRv1)\n\nA Simple Framework for Contrastive Learning of Visual Representations\n\n1.3.4 Big Self-Supervised Models are Strong Semi-Supervised Learners (SimCLRv2)\n\nBig Self-Supervised Models are Strong Semi-Supervised Learners\n\n这一篇本质上应该是算半监督学习\n\n1.3.5 Bootstrap your own latent: A new approach to self-supervised Learning (BYOL)\n\nBootstrap your own latent: A new approach to self-supervised Learning\n\n# 1.4 其他的设计\n\n1.4.1利用 图像着色来设计 proxy task\n\nColorful Image Colorization\n\n1.4.2 利用 Image Inpainting 来设计 proxy task\n\nContext Encoders: Feature Learning by Inpainting\n\n\n\n1.4.3 利用 Rotation Prediction 来设计 proxy task\n\nUnsupervised Representation Learning by Predicting Image Rotations\n\n\n\n1.4.4 利用 Instance Discrimination 来设计 proxy task\n\n1.4.5利用 Counting 来设计 proxy task\n\n1.4.6利用 Moving foreground segmentation 来设计 proxy task\n\n1.4.7 利用Context Prediction 来设计 proxy task\n\nUnsupervised Visual Representation Learning by Context Prediction\n\n\n\n\n# 2、基于对比学习的文章\n\n\n\n1.3.1 Momentum Contrast for Unsupervised Visual Representation Learning (MoCov1)\n\nMomentum Contrast for Unsupervised Visual Representation Learning\n\n1.3.2 Improved Baselines with Momentum Contrastive Learning (Mocov2)\n\nImproved Baselines with Momentum Contrastive Learning\n\n1.3.3 A Simple Framework for Contrastive Learning of Visual Representations (SimCLRv1)\n\nA Simple Framework for Contrastive Learning of Visual Representations\n\n1.3.4 Big Self-Supervised Models are Strong Semi-Supervised Learners (SimCLRv2)\n\nBig Self-Supervised Models are Strong Semi-Supervised Learners\n\n这一篇本质上应该是算半监督学习\n\n1.3.5 Bootstrap your own latent: A new approach to self-supervised Learning (BYOL)\n\nBootstrap your own latent: A new approach to self-supervised Learning\n\n1.3.6 Data-Efficient Image Recognition with Contrastive Predictive Coding (CPCv2)\n\nData-Efficient Image Recognition with Contrastive Predictive Coding\n\n1.3.7 Unsupervised Learning of Visual Featuresby Contrasting Cluster Assignments (SwAV)\n\nUnsupervised Learning of Visual Featuresby Contrasting Cluster Assignments\n\n1.3.8 Exploring Simple Siamese Representation Learning（SimSiam）\n\nExploring Simple Siamese Representation Learning\n\n1.3.9 Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised VisualRepresentation Learning（PixPro, CVPR 2021）\n\npaper: Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning\n\ncode: Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning, CVPR 2021\n\n1.3.10 AdCo: Adversarial Contrast for Efficient Learning of Unsupervised Representations from Self-Trained Negative Adversaries（AdCo, CVPR 2021）\n\npaper: AdCo: Adversarial Contrast for Efficient Learning of Unsupervised Representations from Self-Trained Negative Adversaries\n\ncode: AdCo: Adversarial Contrast for Efficient Learning of Unsupervised Representations from Self-Trained Negative Adversaries\n\nDATE      TRAINING PARADIGM   METHOD          BACKBONE             LABEL FRACTION   TOP-1 ACCURACY\n2019-11   Semi Sup.           Noisy Student   EfficientNet(480M)   100+extra        88.4-\n-         Sup.                -               ResNet50(24M)        100              76.5\n2020-06   Self Sup            SwAV            ResNet50(24M)        0                75.3\n2020-06   Self Sup            BYOL            ResNet50(24M)        0                74.3\n2020-03   Self Sup            Moco v2         ResNet50(24M)        0                71.1\n2020-02   Self Sup            SimCLR v1       ResNet50(24M)        0                69.3\n2019-05   Self Sup            CPCv2           ResNet50(24M)        0                63.8\n2019-11   Self Sup            Moco v1         ResNet50(24M)        0                60.6\n\n\n# 3、参考文献\n\n * 自监督学习的一些思考\n\n * 自监督学习（Self-Supervised Learning） 2018-2020年发展综述\n\n * 自监督学习(Self-Supervised Learning)综述_1\n\n * 自监督学习(Self-Supervised Learning)综述_2\n\n * Contrastive Self-Supervised Learning\n\n * 「上帝视角」看对比自监督学习，SimCLR、CPC、AMDIM并无本质差异\n\n * Self-Supervised Image Classification on ImageNet\n\n * Self-Supervised Image Classification on ImageNet (finetuned)\n\n * Self-supervised Learning: Generative or Contrastive\n\n * Self-supervised Learning: Generative or Contrastive 自监督学习2020综述\n\n * A curated list of awesome self-supervised methods\n\n4、参考代码\n\n * A python library for self-supervised learning on images.\n\n * Self-Supervised Learning Toolbox and Benchmark",normalizedContent:"# 一些文章\n\n\n# 1、proxy task 的设计\n\n# 1.1 基于先验知识的设计\n\n1.1.1 利用 motion propagation 来设计 proxy task\n\nself-supervised learning via conditional motion propagation\n\n1.1.2 利用 motion prediction 来设计 proxy task\n\ndense optical flow prediction from a static image\n\n# 1.2 基于连贯性的设计\n\n1.2.1 利用 jigsaw puzzles （物体的语义连贯性）来设计proxy task\n\nunsupervised learning of visual representations by solving jigsaw puzzles\n\n1.2.2 利用视频中物体运动的时间连贯性来设计 proxy task\n\nshuffle and learn: unsupervised learning using temporal order verification\n\n\n\n# 1.3 基于数据内部结构的设计\n\n目前很火的基于contrastive learning的方法，包括npid, moco, simclr等，我们可以将它们统一为instance discrimination [6]任务。如下图，这类任务通常对图片做各种变换，然后优化目标是同一张图片的不同变换在特征空间中尽量接近，不同图片在特征空间中尽量远离。\n\n1.3.1 momentum contrast for unsupervised visual representation learning (mocov1)\n\nmomentum contrast for unsupervised visual representation learning\n\n1.3.2 improved baselines with momentum contrastive learning (mocov2)\n\nimproved baselines with momentum contrastive learning\n\n1.3.3 a simple framework for contrastive learning of visual representations (simclrv1)\n\na simple framework for contrastive learning of visual representations\n\n1.3.4 big self-supervised models are strong semi-supervised learners (simclrv2)\n\nbig self-supervised models are strong semi-supervised learners\n\n这一篇本质上应该是算半监督学习\n\n1.3.5 bootstrap your own latent: a new approach to self-supervised learning (byol)\n\nbootstrap your own latent: a new approach to self-supervised learning\n\n# 1.4 其他的设计\n\n1.4.1利用 图像着色来设计 proxy task\n\ncolorful image colorization\n\n1.4.2 利用 image inpainting 来设计 proxy task\n\ncontext encoders: feature learning by inpainting\n\n\n\n1.4.3 利用 rotation prediction 来设计 proxy task\n\nunsupervised representation learning by predicting image rotations\n\n\n\n1.4.4 利用 instance discrimination 来设计 proxy task\n\n1.4.5利用 counting 来设计 proxy task\n\n1.4.6利用 moving foreground segmentation 来设计 proxy task\n\n1.4.7 利用context prediction 来设计 proxy task\n\nunsupervised visual representation learning by context prediction\n\n\n\n\n# 2、基于对比学习的文章\n\n\n\n1.3.1 momentum contrast for unsupervised visual representation learning (mocov1)\n\nmomentum contrast for unsupervised visual representation learning\n\n1.3.2 improved baselines with momentum contrastive learning (mocov2)\n\nimproved baselines with momentum contrastive learning\n\n1.3.3 a simple framework for contrastive learning of visual representations (simclrv1)\n\na simple framework for contrastive learning of visual representations\n\n1.3.4 big self-supervised models are strong semi-supervised learners (simclrv2)\n\nbig self-supervised models are strong semi-supervised learners\n\n这一篇本质上应该是算半监督学习\n\n1.3.5 bootstrap your own latent: a new approach to self-supervised learning (byol)\n\nbootstrap your own latent: a new approach to self-supervised learning\n\n1.3.6 data-efficient image recognition with contrastive predictive coding (cpcv2)\n\ndata-efficient image recognition with contrastive predictive coding\n\n1.3.7 unsupervised learning of visual featuresby contrasting cluster assignments (swav)\n\nunsupervised learning of visual featuresby contrasting cluster assignments\n\n1.3.8 exploring simple siamese representation learning（simsiam）\n\nexploring simple siamese representation learning\n\n1.3.9 propagate yourself: exploring pixel-level consistency for unsupervised visualrepresentation learning（pixpro, cvpr 2021）\n\npaper: propagate yourself: exploring pixel-level consistency for unsupervised visual representation learning\n\ncode: propagate yourself: exploring pixel-level consistency for unsupervised visual representation learning, cvpr 2021\n\n1.3.10 adco: adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries（adco, cvpr 2021）\n\npaper: adco: adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries\n\ncode: adco: adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries\n\ndate      training paradigm   method          backbone             label fraction   top-1 accuracy\n2019-11   semi sup.           noisy student   efficientnet(480m)   100+extra        88.4-\n-         sup.                -               resnet50(24m)        100              76.5\n2020-06   self sup            swav            resnet50(24m)        0                75.3\n2020-06   self sup            byol            resnet50(24m)        0                74.3\n2020-03   self sup            moco v2         resnet50(24m)        0                71.1\n2020-02   self sup            simclr v1       resnet50(24m)        0                69.3\n2019-05   self sup            cpcv2           resnet50(24m)        0                63.8\n2019-11   self sup            moco v1         resnet50(24m)        0                60.6\n\n\n# 3、参考文献\n\n * 自监督学习的一些思考\n\n * 自监督学习（self-supervised learning） 2018-2020年发展综述\n\n * 自监督学习(self-supervised learning)综述_1\n\n * 自监督学习(self-supervised learning)综述_2\n\n * contrastive self-supervised learning\n\n * 「上帝视角」看对比自监督学习，simclr、cpc、amdim并无本质差异\n\n * self-supervised image classification on imagenet\n\n * self-supervised image classification on imagenet (finetuned)\n\n * self-supervised learning: generative or contrastive\n\n * self-supervised learning: generative or contrastive 自监督学习2020综述\n\n * a curated list of awesome self-supervised methods\n\n4、参考代码\n\n * a python library for self-supervised learning on images.\n\n * self-supervised learning toolbox and benchmark",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"当我们在谈论图像竞赛EDA时在谈论些什么",frontmatter:{title:"当我们在谈论图像竞赛EDA时在谈论些什么",date:"2021-06-27T12:48:38.000Z",permalink:"/pages/86aedf/",categories:["计算机视觉","视觉竞赛"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/04.%E7%AB%9E%E8%B5%9B%E7%AC%94%E8%AE%B0-%E8%A7%86%E8%A7%89%E7%AB%9E%E8%B5%9B/00.%E5%BD%93%E6%88%91%E4%BB%AC%E5%9C%A8%E8%B0%88%E8%AE%BA%E5%9B%BE%E5%83%8F%E7%AB%9E%E8%B5%9BEDA%E6%97%B6%E5%9C%A8%E8%B0%88%E8%AE%BA%E4%BA%9B%E4%BB%80%E4%B9%88.html",relativePath:"02.学习笔记/04.竞赛笔记-视觉竞赛/00.当我们在谈论图像竞赛EDA时在谈论些什么.md",key:"v-db08b16e",path:"/pages/86aedf/",headers:[{level:3,title:"常用的EDA（Exploratory Data Analysis）",slug:"常用的eda-exploratory-data-analysis",normalizedTitle:"常用的eda（exploratory data analysis）",charIndex:2}],headersStr:"常用的EDA（Exploratory Data Analysis）",content:"# 常用的EDA（Exploratory Data Analysis）\n\n1、是否存在类别不平衡现象\n\n\n\n2、在划分多折训练测试集时，要针对不同任务做细致考量\n\n * 目标检测：要考虑目标检测框的数量分布，种类分布，甚至面积大小分布\n * 图像分类：需要考虑类别不平衡（或长尾数据）的现象\n\n3、在做目标检测任务时，通常需要先统计目标检测框的数量分布，类别分布，框面积分布，框长宽比分布\n\n4、查找重复图像\n\n * https://www.kaggle.com/stehai/duplicate-images\n * https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563\n\n5、图片的文字信息、图片的 EXIF 时间信息\n\n * https://www.kaggle.com/toshik/older-images-tend-to-be-new-whale",normalizedContent:"# 常用的eda（exploratory data analysis）\n\n1、是否存在类别不平衡现象\n\n\n\n2、在划分多折训练测试集时，要针对不同任务做细致考量\n\n * 目标检测：要考虑目标检测框的数量分布，种类分布，甚至面积大小分布\n * 图像分类：需要考虑类别不平衡（或长尾数据）的现象\n\n3、在做目标检测任务时，通常需要先统计目标检测框的数量分布，类别分布，框面积分布，框长宽比分布\n\n4、查找重复图像\n\n * https://www.kaggle.com/stehai/duplicate-images\n * https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563\n\n5、图片的文字信息、图片的 exif 时间信息\n\n * https://www.kaggle.com/toshik/older-images-tend-to-be-new-whale",charsets:{cjk:!0},lastUpdated:"2021/09/12, 20:42:58"},{title:"名词解释",frontmatter:{title:"名词解释",date:"2021-03-24T22:36:39.000Z",permalink:"/pages/a3f895/",categories:["计算机视觉","自监督学习"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/03.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/01.%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A.html",relativePath:"02.学习笔记/03.代码实践-自监督学习/01.名词解释.md",key:"v-60eddab5",path:"/pages/a3f895/",headersStr:null,content:'介绍互信息之前，先引入信息论的一些概念\n\n# Information （信息量）\n\n刻画消除不确定性所需要的信息量，发生的事情概率越大，所带来的信息量越小，反之其信息量越大\n\nI(x)=−log(p(x))=log1p(x)I(x) = -log(p(x)) = log \\frac{1}{p(x)}I(x)=−log(p(x))=logp(x)1\n\n例如：\n\n * 事件A：扔骰子的点数大于3点，其概率为 12\\frac{1}{2}21\n\n * 事件B：扔骰子的点数为6，其概率为16\\frac{1}{6}61\n\n * 我们认为事件 B 体现的信息量更大\n\n# Entropy（熵）\n\n信息熵用于衡量整体所带来的信息量的大小，即利用期望进行评估“事件香农信息 量 x 事件概率的累加"，也是信息熵的概念\n\nH(U)=E[−logpi]=∑i=1npi∗log1piH(U) = E[-logp_i] = \\sum_{i=1}^n{p_i*log\\frac{1}{p_i}}H(U)=E[−logpi ]=∑i=1n pi ∗logpi 1\n\n例如：\n\n * abbbb 其信息熵 H(U)=−15log15−45log45≈0.54H(U)=-\\frac{1}{5}log\\frac{1}{5}-\\frac{4}{5}log\\frac{4}{5} \\approx 0.54H(U)=−51 log51 −54 log54 ≈0.54\n\n * abcde 其信息熵 H(U)=5(−15log15)≈1.61H(U)=5(-\\frac{1}{5}log\\frac{1}{5}) \\approx 1.61H(U)=5(−51 log51 )≈1.61\n\n * 计算得到的 "abcde" 信息熵要大于 "abbbb"，其整体的信息量也更大\n\n# Cross Entropy（交叉熵）\n\n两个随机变量的熵\n\nH(p,q)=∑1Np(x)log1q(x)H(p,q) = \\sum_{1}^{N}p(x)log\\frac{1}{q(x)}H(p,q)=∑1N p(x)logq(x)1\n\n熵的连锁规则：\n\nH(X,Y)=H(X)+H(Y∣X)=H(Y)+H(X∣Y)H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)H(X,Y)=H(X)+H(Y∣X)=H(Y)+H(X∣Y)\n\n# KL Divergence（KL散度或相对熵）\n\n用于衡量概率分布间的差异，也就是信息熵的差异\n\nDKL(p∣∣q)=H(p,q)−H(p)=∑1N[p(xi)log1p(xi)−p(xi)log1q(xi)]D_{KL}(p||q) = H(p,q)-H(p) = \\sum_1^N[p(x_i)log\\frac{1}{p(x_i)}-p(x_i)log\\frac{1}{q(x_i)}]DKL (p∣∣q)=H(p,q)−H(p)=∑1N [p(xi )logp(xi )1 −p(xi )logq(xi )1 ]\n\n整理一下可得：\n\nDKL(p∣∣q)=H(p,q)−H(p)=∑1Np(x)logp(x)q(x)D_{KL}(p||q) = H(p,q)-H(p) = \\sum_{1}^N p(x)log\\frac{p(x)}{q(x)}DKL (p∣∣q)=H(p,q)−H(p)=∑1N p(x)logq(x)p(x)\n\nKL 散度的最大特点是不对称，即DKL(p∣∣q)≠DKL(q∣∣p)D_{KL}(p||q) \\neq D_{KL}(q||p)DKL (p∣∣q)=DKL (q∣∣p)\n\n# JS Divergence（JS散度）\n\nJS 散度的特点是其对称，即DJS(p∣∣q)≠DJS(q∣∣p)D_{JS}(p||q) \\neq D_{JS}(q||p)DJS (p∣∣q)=DJS (q∣∣p)\n\n# Mutual Information（互信息）\n\n已知两个变量x,yx,yx,y，若 p(x,y)=p(x)p(y)p(x,y) = p(x)p(y)p(x,y)=p(x)p(y) ，则两个随机变量 x,yx,yx,y 独立。由贝叶斯公式即可得到：\n\np(y∣x)=p(x,y)/p(x)=p(x)p(y)/p(x)=p(y)p(y|x) = p(x,y)/p(x) = p(x)p(y)/p(x) = p(y)p(y∣x)=p(x,y)/p(x)=p(x)p(y)/p(x)=p(y)\n\n独立性的判别公式反映了已知 xxx 的情况下，yyy 的分布是否会发生改变（能否为 yyy 带来新的信息）.然而独立性只能表示两变量是否有关系，而不能描述他们的关系强弱。\n\n所以引入互信息来量化的评价随机变量之间依赖关系的强弱。定义互信息 I(x,y)I(x,y)I(x,y):\n\nI(X,Y)=H(X)−H(X∣Y)=∑x∈X∑y∈Yp(x,y)logp(x,y)p(x)p(y)I(X,Y) = H(X)-H(X|Y) = \\sum_{x\\in{X}}\\sum_{y\\in{Y}}p(x,y)log\\frac{p(x,y)}{p(x)p(y)}I(X,Y)=H(X)−H(X∣Y)=∑x∈X ∑y∈Y p(x,y)logp(x)p(y)p(x,y)\n\n互信息的性质\n\n * 对称性：I(xi,yj)=I(yj,xi)I(x_i,y_j)=I(y_j,x_i)I(xi ,yj )=I(yj ,xi )\n * 非负性：I(xi,yj)=0I(x_i,y_j) = 0I(xi ,yj )=0\n * X与Y独立时：I(X,Y)=0I(X,Y) = 0I(X,Y)=0',normalizedContent:'介绍互信息之前，先引入信息论的一些概念\n\n# information （信息量）\n\n刻画消除不确定性所需要的信息量，发生的事情概率越大，所带来的信息量越小，反之其信息量越大\n\ni(x)=−log(p(x))=log1p(x)i(x) = -log(p(x)) = log \\frac{1}{p(x)}i(x)=−log(p(x))=logp(x)1\n\n例如：\n\n * 事件a：扔骰子的点数大于3点，其概率为 12\\frac{1}{2}21\n\n * 事件b：扔骰子的点数为6，其概率为16\\frac{1}{6}61\n\n * 我们认为事件 b 体现的信息量更大\n\n# entropy（熵）\n\n信息熵用于衡量整体所带来的信息量的大小，即利用期望进行评估“事件香农信息 量 x 事件概率的累加"，也是信息熵的概念\n\nh(u)=e[−logpi]=∑i=1npi∗log1pih(u) = e[-logp_i] = \\sum_{i=1}^n{p_i*log\\frac{1}{p_i}}h(u)=e[−logpi ]=∑i=1n pi ∗logpi 1\n\n例如：\n\n * abbbb 其信息熵 h(u)=−15log15−45log45≈0.54h(u)=-\\frac{1}{5}log\\frac{1}{5}-\\frac{4}{5}log\\frac{4}{5} \\approx 0.54h(u)=−51 log51 −54 log54 ≈0.54\n\n * abcde 其信息熵 h(u)=5(−15log15)≈1.61h(u)=5(-\\frac{1}{5}log\\frac{1}{5}) \\approx 1.61h(u)=5(−51 log51 )≈1.61\n\n * 计算得到的 "abcde" 信息熵要大于 "abbbb"，其整体的信息量也更大\n\n# cross entropy（交叉熵）\n\n两个随机变量的熵\n\nh(p,q)=∑1np(x)log1q(x)h(p,q) = \\sum_{1}^{n}p(x)log\\frac{1}{q(x)}h(p,q)=∑1n p(x)logq(x)1\n\n熵的连锁规则：\n\nh(x,y)=h(x)+h(y∣x)=h(y)+h(x∣y)h(x,y)=h(x)+h(y|x)=h(y)+h(x|y)h(x,y)=h(x)+h(y∣x)=h(y)+h(x∣y)\n\n# kl divergence（kl散度或相对熵）\n\n用于衡量概率分布间的差异，也就是信息熵的差异\n\ndkl(p∣∣q)=h(p,q)−h(p)=∑1n[p(xi)log1p(xi)−p(xi)log1q(xi)]d_{kl}(p||q) = h(p,q)-h(p) = \\sum_1^n[p(x_i)log\\frac{1}{p(x_i)}-p(x_i)log\\frac{1}{q(x_i)}]dkl (p∣∣q)=h(p,q)−h(p)=∑1n [p(xi )logp(xi )1 −p(xi )logq(xi )1 ]\n\n整理一下可得：\n\ndkl(p∣∣q)=h(p,q)−h(p)=∑1np(x)logp(x)q(x)d_{kl}(p||q) = h(p,q)-h(p) = \\sum_{1}^n p(x)log\\frac{p(x)}{q(x)}dkl (p∣∣q)=h(p,q)−h(p)=∑1n p(x)logq(x)p(x)\n\nkl 散度的最大特点是不对称，即dkl(p∣∣q)=dkl(q∣∣p)d_{kl}(p||q) \\neq d_{kl}(q||p)dkl (p∣∣q)=dkl (q∣∣p)\n\n# js divergence（js散度）\n\njs 散度的特点是其对称，即djs(p∣∣q)=djs(q∣∣p)d_{js}(p||q) \\neq d_{js}(q||p)djs (p∣∣q)=djs (q∣∣p)\n\n# mutual information（互信息）\n\n已知两个变量x,yx,yx,y，若 p(x,y)=p(x)p(y)p(x,y) = p(x)p(y)p(x,y)=p(x)p(y) ，则两个随机变量 x,yx,yx,y 独立。由贝叶斯公式即可得到：\n\np(y∣x)=p(x,y)/p(x)=p(x)p(y)/p(x)=p(y)p(y|x) = p(x,y)/p(x) = p(x)p(y)/p(x) = p(y)p(y∣x)=p(x,y)/p(x)=p(x)p(y)/p(x)=p(y)\n\n独立性的判别公式反映了已知 xxx 的情况下，yyy 的分布是否会发生改变（能否为 yyy 带来新的信息）.然而独立性只能表示两变量是否有关系，而不能描述他们的关系强弱。\n\n所以引入互信息来量化的评价随机变量之间依赖关系的强弱。定义互信息 i(x,y)i(x,y)i(x,y):\n\ni(x,y)=h(x)−h(x∣y)=∑x∈x∑y∈yp(x,y)logp(x,y)p(x)p(y)i(x,y) = h(x)-h(x|y) = \\sum_{x\\in{x}}\\sum_{y\\in{y}}p(x,y)log\\frac{p(x,y)}{p(x)p(y)}i(x,y)=h(x)−h(x∣y)=∑x∈x ∑y∈y p(x,y)logp(x)p(y)p(x,y)\n\n互信息的性质\n\n * 对称性：i(xi,yj)=i(yj,xi)i(x_i,y_j)=i(y_j,x_i)i(xi ,yj )=i(yj ,xi )\n * 非负性：i(xi,yj)=0i(x_i,y_j) = 0i(xi ,yj )=0\n * x与y独立时：i(x,y)=0i(x,y) = 0i(x,y)=0',charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"组会思路",frontmatter:{title:"组会思路",date:"2021-03-25T00:18:59.000Z",permalink:"/pages/e182f4/",categories:["计算机视觉","自监督学习"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/03.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/02.%E7%BB%84%E4%BC%9A%E6%80%9D%E8%B7%AF.html",relativePath:"02.学习笔记/03.代码实践-自监督学习/02.组会思路.md",key:"v-68c570d3",path:"/pages/e182f4/",headersStr:null,content:"# 1、为什么要使用自监督学习呢？\n\n * 人工设计的标签可能不是最优的\n\n * 充分利用未标记样本\n\n * 探索人类是如何学习的\n\n\n\n# 2、为什么无监督的pretraining > Supervised pretraining？\n\n * 迁移学习主要迁移的是底层的特征，而不是高层的语义信息\n\n * 无监督模型可以保留更多的空间关系，有监督学习会丢失一些空间信息\n\n# 3、基于生成模型的方法（Generative Methods）\n\n * BigBiGAN：ImageNet top1: 56.6%\n * Image GPT: 69.0%\n\n# 4、视频上的自监督学习\n\n * Shuffle and Learn：时序正确和错误的二分类\n * SpeedNet：视频速度正常和加速的二分类\n * 视频自监督的提升空间还比较大\n\n# 5、多模态上的自监督学习\n\n * 声音可以监督视觉的学习：牛的叫声大致相同\n\n# 6、未来的展望\n\n * 为下游任务设计更加好的预训练模型\n * 多模态的自监督预训练以及应用\n * 设计更加巧妙的策略去自动地搜集更有用的data/label（主动学习?）\n\n什么是 linear protocol？\n\n为了克服基本的OOD和泛化问题，尽管许多工作专注于为神经网络设计新的体系结构，但另一个简单而有效的解决方案是扩大训练数据集以使多个样本“分布内”。但是，事实是，尽管在这个大数据时代中有大量未标记的Web数据可用，但是带有人工标记的高质量数据可能会非常昂贵。例如，数据标签公司Scale.ai3对图像分割标签收取每张图像$ 6.4的费用。包含10k +高质量样本的图像细分数据集可能需要花费一百万美元。\n\n生成式的：训练编码器将输入的信息编码为明确的向量z，并解码器从z重建x（例如，完形填空测试，图形生成）\n\n基于对比：训练编码器对输入信号编码到显式向量z，以测量相似度（例如，互信息最大化，实例区分）\n\n它们的主要区别在于模型架构和目标。图2显示了详细的概念比较。 4.它们的体系结构可以统一为两个通用组件：生成器和鉴别器，并且生成器可以进一步分解为编码器和解码器。不同的是：1）对于潜在的分布z：在生成和对比方法中，它们是显式的，并且经常被下游任务利用； 2）对于判别器：生成器方法没有判别器，而GAN和对比法则具有判别器。对比判别器（例如，具有2-3层的多层感知器）的参数要比GAN少（例如，标准ResNet [53]）。3）出于目标：生成方法使用重构损失，对比方法使用对比相似性度量（例如InfoNCE），生成对比方法将分布差异作为损失（例如，JS散度，Wasserstein距离）。与下游任务相关的经过适当设计的训练目标可以将我们随机初始化的模型转变为出色的预训练特征提取器。例如，对比学习被发现对于几乎所有视觉分类任务都是有用的。这可能是因为对比对象正在建模不同图像实例之间的类不变性。对比损失使包含相同对象类别的图像更加相似。它使包含不同类的对象不太相似，基本上符合下游图像分类，对象检测和其他基于分类的任务。自我监督学习的艺术主要在于为未标记的数据定义适当的目标。",normalizedContent:"# 1、为什么要使用自监督学习呢？\n\n * 人工设计的标签可能不是最优的\n\n * 充分利用未标记样本\n\n * 探索人类是如何学习的\n\n\n\n# 2、为什么无监督的pretraining > supervised pretraining？\n\n * 迁移学习主要迁移的是底层的特征，而不是高层的语义信息\n\n * 无监督模型可以保留更多的空间关系，有监督学习会丢失一些空间信息\n\n# 3、基于生成模型的方法（generative methods）\n\n * bigbigan：imagenet top1: 56.6%\n * image gpt: 69.0%\n\n# 4、视频上的自监督学习\n\n * shuffle and learn：时序正确和错误的二分类\n * speednet：视频速度正常和加速的二分类\n * 视频自监督的提升空间还比较大\n\n# 5、多模态上的自监督学习\n\n * 声音可以监督视觉的学习：牛的叫声大致相同\n\n# 6、未来的展望\n\n * 为下游任务设计更加好的预训练模型\n * 多模态的自监督预训练以及应用\n * 设计更加巧妙的策略去自动地搜集更有用的data/label（主动学习?）\n\n什么是 linear protocol？\n\n为了克服基本的ood和泛化问题，尽管许多工作专注于为神经网络设计新的体系结构，但另一个简单而有效的解决方案是扩大训练数据集以使多个样本“分布内”。但是，事实是，尽管在这个大数据时代中有大量未标记的web数据可用，但是带有人工标记的高质量数据可能会非常昂贵。例如，数据标签公司scale.ai3对图像分割标签收取每张图像$ 6.4的费用。包含10k +高质量样本的图像细分数据集可能需要花费一百万美元。\n\n生成式的：训练编码器将输入的信息编码为明确的向量z，并解码器从z重建x（例如，完形填空测试，图形生成）\n\n基于对比：训练编码器对输入信号编码到显式向量z，以测量相似度（例如，互信息最大化，实例区分）\n\n它们的主要区别在于模型架构和目标。图2显示了详细的概念比较。 4.它们的体系结构可以统一为两个通用组件：生成器和鉴别器，并且生成器可以进一步分解为编码器和解码器。不同的是：1）对于潜在的分布z：在生成和对比方法中，它们是显式的，并且经常被下游任务利用； 2）对于判别器：生成器方法没有判别器，而gan和对比法则具有判别器。对比判别器（例如，具有2-3层的多层感知器）的参数要比gan少（例如，标准resnet [53]）。3）出于目标：生成方法使用重构损失，对比方法使用对比相似性度量（例如infonce），生成对比方法将分布差异作为损失（例如，js散度，wasserstein距离）。与下游任务相关的经过适当设计的训练目标可以将我们随机初始化的模型转变为出色的预训练特征提取器。例如，对比学习被发现对于几乎所有视觉分类任务都是有用的。这可能是因为对比对象正在建模不同图像实例之间的类不变性。对比损失使包含相同对象类别的图像更加相似。它使包含不同类的对象不太相似，基本上符合下游图像分类，对象检测和其他基于分类的任务。自我监督学习的艺术主要在于为未标记的数据定义适当的目标。",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"图像检索orReID 竞赛的 Tricks",frontmatter:{title:"图像检索orReID 竞赛的 Tricks",date:"2021-08-15T16:52:32.000Z",permalink:"/pages/7f2968/",categories:["学习笔记","竞赛笔记-视觉竞赛"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/04.%E7%AB%9E%E8%B5%9B%E7%AC%94%E8%AE%B0-%E8%A7%86%E8%A7%89%E7%AB%9E%E8%B5%9B/02.%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2orReID%20%E7%AB%9E%E8%B5%9B%E7%9A%84%20Tricks.html",relativePath:"02.学习笔记/04.竞赛笔记-视觉竞赛/02.图像检索orReID 竞赛的 Tricks.md",key:"v-035fdc6d",path:"/pages/7f2968/",headers:[{level:2,title:"Bag of Tricks and A Strong Baseline for Deep Person Re-identification",slug:"bag-of-tricks-and-a-strong-baseline-for-deep-person-re-identification",normalizedTitle:"bag of tricks and a strong baseline for deep person re-identification",charIndex:2},{level:3,title:"01、标准的 Baseline（github）",slug:"_01、标准的-baseline-github",normalizedTitle:"01、标准的 baseline（github）",charIndex:76},{level:3,title:"02、训练技巧",slug:"_02、训练技巧",normalizedTitle:"02、训练技巧",charIndex:625},{level:3,title:"03、实验分析",slug:"_03、实验分析",normalizedTitle:"03、实验分析",charIndex:2396},{level:3,title:"04、参考资料",slug:"_04、参考资料",normalizedTitle:"04、参考资料",charIndex:2563},{level:3,title:"05、华为 DIGIX 算法比赛方案学习",slug:"_05、华为-digix-算法比赛方案学习",normalizedTitle:"05、华为 digix 算法比赛方案学习",charIndex:4076}],headersStr:"Bag of Tricks and A Strong Baseline for Deep Person Re-identification 01、标准的 Baseline（github） 02、训练技巧 03、实验分析 04、参考资料 05、华为 DIGIX 算法比赛方案学习",content:"# Bag of Tricks and A Strong Baseline for Deep Person Re-identification\n\n\n# 01、标准的 Baseline（github）\n\n * 使用 ResNet50 ，加载 ImageNet 上的预训练参数，改变全连接层的维度为 N，N代表训练集中实体的数量\n * 随机采样 P 个实体以及每个人的 K 张图像来构建训练 batch，最终的 batch size B = PxK，论文中，将 P 设置为16，K设置为4，也就是说每个 batch 采样 16 个人，每个人采样 4 张图像（√）\n * 每张图像 resize 成 256 x 128，用零值 padding 了10像素，然后随机 crop 到256 x 128（√）\n * 每张图像以 0.5 的概率水平翻转（√）\n * 每张图像以 32-bit 的 [0,1] 之间的浮点数表示，利用ImageNet的均值和标准差来做归一化（√）\n * 模型输出 ReID 的特征 f 以及 ID 预测概率 p（√）\n * ReID 特征 f 用于计算 triplet loss，ID 预测概率 p 用于计算交叉熵，triplet loss 的 margin 设置为 0.3\n * Adam 优化器，初始的学习率设置为 0.00035 ，40th 和 70th 分别变为原来的0.1，一共有 120 个训练 epoch（√）\n\n\n# 02、训练技巧\n\n大多数训练技巧不用改变模型结构\n\n\n\n# 2.1 Warmup Le Rate\n\n\n\n\n\n学习率非常关键，Warmup 能够提升网络性能，\n\n# 2.2 Random Erasing Augmentation\n\n在行人的 ReID 中常常出现遮挡问题，应用 Random Erasing 的概率是 pep_epe ，会选择一个方形区域 IeI_eIe （We×HeW_e \\times H_eWe ×He ），用随机值擦除其原值，\n\nIII 表示原图，IeI_eIe 表示擦除的区域，SSS 和 SeS_eSe 分别代表原图尺寸和擦除区域的尺寸，re=Se/Sr_e = S_e/Sre =Se /S 表示擦除区域比例。\n\nrer_ere 一般是随机在 r1r_1r1 和 r2r_2r2 之间变动，本文的设置如下：\n\n * pe=0.5p_e=0.5pe =0.5\n * 0.02<Se<0.40.02<S_e<0.40.02<Se <0.4\n * r1=0.3,r2=3.33r_1=0.3, r_2=3.33r1 =0.3,r2 =3.33\n\n# 2.3 Label Smoothing\n\nID Embedding Network 是行人 ReID 中一个基础的 baseline，因为常用的交叉熵损失是计算输出的ID label 与真实 ID label 之间的损失，称之为 ID loss，其使用 Label Smoothing 是有帮助的\n\nγ\\gammaγ 是一个小的常量来鼓励模型对训练集更加不自信，本文设置其为 0.1，当训练集不是很大时，LS 可以显著提升模型性能\n\n# 2.4 Last Stride\n\n将 last stride 从 2 改为 1，可以提升特征图的尺寸，以 ResNet 为例，可以从 8x4 得到 16x8 的特征图，更高的空间分辨率可以带来十分显著的改进\n\n# 2.5 BNN Neck\n\n\n\n常用的 ReID 模型都会结合 ID loss 以及 triplet loss 来训练，在标准的 baseline 中，ID loss 和 triplet loss 约束同一个特征 f，但是这两个损失的目标在 embedding 空间上是不一致的\n\n如上图所示，Triplet loss 用于约束 feature ftf_tft ，ID loss 用于约束 fif_ifi ，最终在推理的时候使用 fif_ifi\n\nBNN neck 只在 ftf_tft 后面加了一层 BN 层，余弦距离比欧氏距离更好\n\n# 2.6 Center loss\n\nLTri=[dp−dn+α]+L_{Tri}= [d_p−d_n+ α ]_+LTri =[dp −dn +α]+\n\nTriplet Loss 如上式，dpd_pdp 和 dnd_ndn 分别表示正样本对和负样本对的特征距离，α\\alphaα 表示 triplet loss 的 margin ，本文设置为 0.3。Triplet loss 只考虑了特征距离之间的不同，忽略了其绝对值的大小\n\n * dp=0.3,dn=0.5,LTri=0.1dp= 0.3, dn= 0.5, L_{Tri} =0.1dp=0.3,dn=0.5,LTri =0.1\n * dp=1.3,dn=1.5,LTri=0.1dp= 1.3, dn= 1.5, L_{Tri}=0.1dp=1.3,dn=1.5,LTri =0.1\n\nL_C=\\frac{1}{2}\\sum_\\limits{j=1}^{B}||f_{t_j}−c_{y_j}||_2^2\n\n上图是 Center loss，其中 yjy_jyj 是 mini-batch 中第 jjj 个图像的标签。cyjc_{y_j}cyj 表示深度特征的第 i 个类别中心。BBB 是批量大小的数量。该公式有效地表征了类内变化。最小化中心损失会增加类内的紧凑性。\n\nL=LID+LTriplet+βLCL=L_{ID}+L_{Triplet}+βL_CL=LID +LTriplet +βLC\n\n我们的模型总共包含了上式三个损失，其中β\\betaβ 设置为 0.0005\n\n# 2.7 Reranking\n\n\n# 03、实验分析\n\n# 3.1 每个 Trick 的有效性\n\n\n\n# 3.2 BNN Neck 的有效性\n\n\n\n# 3.3 不同 batch_size 的有效性\n\n\n\n# 3.4 不同 Image Size 的影响\n\n\n\n875 x 606\n\n384 267\n\n384 192\n\n# 3.5 不同 Backbone 的影响\n\n\n\n\n# 04、参考资料\n\n# 4.1 本篇论文参考资料\n\npaper:https://ieeexplore.ieee.org/document/8930088\n\ncode: https://github.com/michuanhaohao/reid-strong-baseline\n\nslides: https://drive.google.com/file/d/1h9SgdJenvfoNp9PTUxPiz5_K5HFCho-V/view\n\n# 4.2 其他代码链接\n\nIBN-Network: https://github.com/XingangPan/IBN-Net\n\nPytorch ReID: https://github.com/layumi/Person_reID_baseline_pytorch\n\nAICity-ReID-2020: https://github.com/layumi/AICIty-reID-2020/tree/master/pytorch\n\nperson-reid-triplet-loss-baseline: https://github.com/huanghoujing/person-reid-triplet-loss-baseline\n\nfast-reid: https://github.com/JDAI-CV/fast-reid\n\nreid-strong-baseline: https://github.com/michuanhaohao/reid-strong-baseline\n\npytorch-metric-learning: https://github.com/KevinMusgrave/pytorch-metric-learning\n\ndeep-efficient-person-reid: https://github.com/lannguyen0910/deep-efficient-person-reid\n\nLUPerson: https://github.com/DengpanFu/LUPerson\n\nperson-reid-tiny-baseline: https://github.com/lulujianjie/person-reid-tiny-baseline\n\nHuawei_DIGIX_ImageRetri_Top2: https://github.com/lin-honghui/Huawei_DIGIX_ImageRetri_Top2\n\nPyRetri: https://github.com/PyRetri/PyRetri\n\ndeep-person-reid: https://github.com/KaiyangZhou/deep-person-reid\n\npytorch-center-loss: https://github.com/KaiyangZhou/pytorch-center-loss\n\nCurricularFace: https://github.com/HuangYG123/CurricularFace\n\narcface.py: https://github.com/Tencent/TFace/blob/master/torchkit/head/distfc/arcface.py\n\ncurricularface.py: https://github.com/Tencent/TFace/blob/master/torchkit/head/distfc/curricularface.py\n\nxbm 算法\n\n\n# 05、华为 DIGIX 算法比赛方案学习\n\n# 第二名：https://zhuanlan.zhihu.com/p/303371522\n\n代码：https://github.com/lin-honghui/Huawei_DIGIX_ImageRetri_Top2\n\n\n\n2.1 数据预处理\n\n细粒度商品检索是一个较难的任务，数据预处理的目的是简化学习目标，修正样本分布、提高网络的泛化能力。\n\n2.1.1 RescalePad\n\n针对训练集中图像分辨率不统一问题，我们采取的策略是在保持物体形变情况下进行resize。具体操作是：padding 短边至长边相同大小，再resize到指定尺寸。\n\n对于本次比赛任务，越大的 patch，越有利于图像细节信息的保留，更有助于性能提升，但显存和模型大小制约着patch增长。选拔赛时使用的显卡为1080Ti小水管，网络输入patch为448，512，决赛华为提供了较大显存的V100，使用了更大的分辨率576。\n\n2.1.2 Balance Sampler\n\n针对数据中存在的类别不平衡问题，我们过滤了类别数少于2的样本，网络训练时，以类别为基本单位进行采样，每次采样 n 个类别，每个类别采样 m 个样本，所有类别采样，保证每个类别以相同的概率被抽样。\n\n实验中，不同的 n x m 采样的 batchsize 对实验结果有较大影响（2.2中讨论）。\n\n2.1.3 DeNoise\n\n本次比赛数据中，存在较多人为添加的高斯噪声和椒盐噪声，还有少量模式未知的噪声。噪声图像的存在会加大检索任务的难度，为降低噪声图像干扰，我们针对 gallery、query 中模式较为固定的高斯噪声、椒盐噪声图像进行离线修复。\n\n2.1.4 数据增强\n\n数据增强除了模拟商品颜色、旋转、形状、尺度等变化多样性外，针对2.1.3中噪声图像修复可能带来的边缘结构破坏问题（下图第2行，去噪后边缘信息被破坏），我们引入了DeGaussian和DePepper来提高网络的鲁棒性。本次比赛使用的数据增强有（示例图第1行从左到右）：IAAPerspective、ShiftScaleRotate、ChannelShuffle、RandomFlip、ColorJitter、DeGaussian、DePepper。\n\n\n\n2.2 模型和训练\n\n2.2.1 模型设计\n\n由于比赛模型大小的限制，Backbone选择时优先考虑了参数量较少的EfficientNet和DenseNet。我们也测试过ResNet、ResNext、RegNet等网络，但在这次的数据中表现并不佳；\n\nDropout 在这次的任务中提升显著，加上dropout和数据增强后，基本可以通过本地训练集acc预估线上分数。除dropout外也尝试过DropBlock、Disout等正则化方案，但因为和BNHead会发生冲突，而在之前的实验中去除BNHead是掉点的，后续没有进一步尝试。\n\n决赛时我们最佳单模为EfficientNet-B4，初赛时最佳单模是DenseNet169-RAG。\n\n2.2.2 模型训练\n\n2.2.2.1 mini-batch\n\n对于Triplet Loss 这类损失函数，增大 mini-batch，更有利于困难样本的挖掘，加速网络收敛。但显存制约着batchsize增加。我们使用XBM[4]来增大训练batch，但实验中观察到一个有趣现象：\n\n * 开启XBM后，网络收敛确实加快，但并没有带来性能提升（甚至掉了一点点），在网络收敛的后期Triplet Loss依旧很高；\n * 训练时，先开启XBM训练一段时间后，再关闭XBM（减小了batchsize）训练则网络性能有提升；\n\n这里我们做了两个假设，一个是小batchsize训练的网络泛化性能更加，但在后续的对比实验证明并非如此。另一个假设是训练集可能存在重复的ID，盲目增大batchsize也增大了冲突概率，所以网络性能下降，并且也可以解释Triplet Loss很高的原因。\n\n我们通过一个简单的实验进行验证：对于Arcface、Amsoftmax训练收敛的分类网络，全连接层的每一行都可以视为网络学到的一个类别中心，各个类别中心余弦距离表征着类别之间的相似度。设置阈值对余弦相似度较高的类别中心进行聚类可视化可以发现，训练数据集中确实存在大量的重复ID。比如 ID-1908、ID-1528、ID-363、ID-2979、ID-1475都为同款诺基亚手机，ID-770、ID-1205、ID-1082（见下图）都为同款华为手机，根据聚类结果粗略估计，大概有150～200重复商品ID，约占训练数据5～6%。\n\n由于本次比赛不允许选手进行额外的标注，咨询了官方人员是否可以进行数据清洗也没有得到明确的答复，在确定数据集含有重复ID的情况下，我们也无法对数据集进行清洗。我们采取的方案是降低mini-batch中每次抽样的类别数n，而增大每个类别抽样数量m的方式，来降低发生冲突的概率。此外，由于重复ID的存在，Triplet Loss、Arcface等Loss margin取值也不应过大。\n\n * BackBone： EfficientNet、DenseNet\n\n * Pool：Generalized Mean Pooling [5]\n\n * Head ： BNHead [1]\n\n * Loss ： Triplet Loss + Arcface or Triplet Loss + Amsoftmax\n\n * 正则化：dropout\n\n * 其它组件：\n\n * * RAG [6]\n   * Nonlocal [7]\n   * IBN [8]\n\n2.2.2.2 实验细节\n\n超参数：根据选拔赛A榜进行调优。\n\n * Triplet Loss：margin = 0.6，权重为1；\n * Amsoftmax ： margin = 0.35，scale = 30，权重为 0.25；\n * Arcface ： margin = 0.35，scale = 30，权重为 0.25；\n * Dropout ： p = 0.2；\n\n训练加速：\n\n * 使用 Pytorch 1.6 自动混合精度加速；\n * 将 jpg 转 npy 加速IO；\n * 模型参数冻结：DenseNet169只解冻最后2个Stage， Efficient-B3 冻结 15/25 block，B4冻结16/31 block， b5冻结20/38 block；\n\n2.3 模型推理\n\n2.3.1 尺度增强\n\n推理时，增大 patch 为训练阶段1.1倍，以较小的计算代价换稳定的性能提升；\n\n2.3.2 旋转特征对齐\n\n商品角度旋转多样，而CNN对旋转不鲁棒，推理时将图像进行多个角度旋转预测，对得到的特征进行相加。\n\n2.3.3 多主体场景优化\n\n比赛后期上分到瓶颈期，通过统计发现存在部分样本，所有模型检索结果Top10几乎都不一致，这部分数据大概100+张，通过可视化总结，这部分样本大多为同个图像中存在多个主体、或者主体较小背景干扰严重。由于本次比赛不能引入额外的标注，我们无法引入额外的检测器进行主体检测再检索。我们尝试在仅使用attention map进行谱聚类的情况下进行主体检测，具体操作为：对于给定图像，计算attention map后进行阈值化处理，过滤出高响应的区域，对高响应区域像素进行 k=3 对最近邻建图，然后通过图切割得到多个子图，每个子图则大概率为一个主体，对切割得到的主体所在位置，对feature map 重新Crop & Pooling 再进行检索，最后对所有检测结果进行重排序。这部分优化虽然直接带来的结果提升不大，但对模型融合时提分显著。\n\n2.4 后处理&模型融合\n\n后处理：我们只做了 K-reciprocal，在 PyRetri基础上实现GPU加速（约加速3倍）和半精度优化（32g内存以内）;\n\n模型融合：我们使用加权投票的融合方式，统计检索结果中top10图像出现的频率及顺序（e.g. top1权重为1，top2权重为1/2 ... ）;\n\n# 第三名：https://zhuanlan.zhihu.com/p/297669395\n\n解决思路\n\n * 长尾分布\n\n * * pk采样，每个batch里的类别数量是平衡的\n   * 二阶段训练，这个是参考一些解决长尾数据的论文思路，先用所有数据训练，让网络尽可能的学到更多的数据，然后这时网络会倾向于预测数量多的类别，第二阶段，对数量多的类别进行欠采样，经过二阶段训练，大约提升0.2%。\n\n * 目标背景多\n\n * * 由于比赛赛题的限制，只可以用imagenet的预训练权重，而且只提供了id标签，我们采用了弱监督切割的方法，根据热力图的响应值，设定一个阈值，然后去掉背景区域，大约提升0.6%\n\n * 测试集加入旋转，翻转等干扰操作\n\n * * 加入旋转操作，好多正常的图片，例如冰箱，成了横向放置，和正常情况不一样。我们也相应的训练阶段和测试阶段加入了翻转和旋转操作，验证集提升2%，由于提交次数有限，测试集没有经过验证。\n\n * 类内差异大，类间差异小\n\n * * 用的经典的softmax+triplet训练，为了扩大难样本挖掘，我们还使用了xbm算法，辅助triplet训练。其实有尝试过其他sota loss，可能是参数不太合适，最后是softmax+triplet效果最好\n\ntrick\n\n * gem\n * rerank\n * bnneck\n * channle shuffle\n * batchdrop\n * 随机擦除\n * 混合精度训练\n\n模型\n\n * resnest101\n * resnest50\n * res2net101\n\n集成\n\n * 我们用了六个模型，然后采用距离平均方法，我们也采用了投票法和concat形式，发现距离加权更好一些。\n\n加权算法:\n\n * 投票法\n\n * concat 形式\n\n * 距离加权（√）",normalizedContent:"# bag of tricks and a strong baseline for deep person re-identification\n\n\n# 01、标准的 baseline（github）\n\n * 使用 resnet50 ，加载 imagenet 上的预训练参数，改变全连接层的维度为 n，n代表训练集中实体的数量\n * 随机采样 p 个实体以及每个人的 k 张图像来构建训练 batch，最终的 batch size b = pxk，论文中，将 p 设置为16，k设置为4，也就是说每个 batch 采样 16 个人，每个人采样 4 张图像（√）\n * 每张图像 resize 成 256 x 128，用零值 padding 了10像素，然后随机 crop 到256 x 128（√）\n * 每张图像以 0.5 的概率水平翻转（√）\n * 每张图像以 32-bit 的 [0,1] 之间的浮点数表示，利用imagenet的均值和标准差来做归一化（√）\n * 模型输出 reid 的特征 f 以及 id 预测概率 p（√）\n * reid 特征 f 用于计算 triplet loss，id 预测概率 p 用于计算交叉熵，triplet loss 的 margin 设置为 0.3\n * adam 优化器，初始的学习率设置为 0.00035 ，40th 和 70th 分别变为原来的0.1，一共有 120 个训练 epoch（√）\n\n\n# 02、训练技巧\n\n大多数训练技巧不用改变模型结构\n\n\n\n# 2.1 warmup le rate\n\n\n\n\n\n学习率非常关键，warmup 能够提升网络性能，\n\n# 2.2 random erasing augmentation\n\n在行人的 reid 中常常出现遮挡问题，应用 random erasing 的概率是 pep_epe ，会选择一个方形区域 iei_eie （we×hew_e \\times h_ewe ×he ），用随机值擦除其原值，\n\niii 表示原图，iei_eie 表示擦除的区域，sss 和 ses_ese 分别代表原图尺寸和擦除区域的尺寸，re=se/sr_e = s_e/sre =se /s 表示擦除区域比例。\n\nrer_ere 一般是随机在 r1r_1r1 和 r2r_2r2 之间变动，本文的设置如下：\n\n * pe=0.5p_e=0.5pe =0.5\n * 0.02<se<0.40.02<s_e<0.40.02<se <0.4\n * r1=0.3,r2=3.33r_1=0.3, r_2=3.33r1 =0.3,r2 =3.33\n\n# 2.3 label smoothing\n\nid embedding network 是行人 reid 中一个基础的 baseline，因为常用的交叉熵损失是计算输出的id label 与真实 id label 之间的损失，称之为 id loss，其使用 label smoothing 是有帮助的\n\nγ\\gammaγ 是一个小的常量来鼓励模型对训练集更加不自信，本文设置其为 0.1，当训练集不是很大时，ls 可以显著提升模型性能\n\n# 2.4 last stride\n\n将 last stride 从 2 改为 1，可以提升特征图的尺寸，以 resnet 为例，可以从 8x4 得到 16x8 的特征图，更高的空间分辨率可以带来十分显著的改进\n\n# 2.5 bnn neck\n\n\n\n常用的 reid 模型都会结合 id loss 以及 triplet loss 来训练，在标准的 baseline 中，id loss 和 triplet loss 约束同一个特征 f，但是这两个损失的目标在 embedding 空间上是不一致的\n\n如上图所示，triplet loss 用于约束 feature ftf_tft ，id loss 用于约束 fif_ifi ，最终在推理的时候使用 fif_ifi\n\nbnn neck 只在 ftf_tft 后面加了一层 bn 层，余弦距离比欧氏距离更好\n\n# 2.6 center loss\n\nltri=[dp−dn+α]+l_{tri}= [d_p−d_n+ α ]_+ltri =[dp −dn +α]+\n\ntriplet loss 如上式，dpd_pdp 和 dnd_ndn 分别表示正样本对和负样本对的特征距离，α\\alphaα 表示 triplet loss 的 margin ，本文设置为 0.3。triplet loss 只考虑了特征距离之间的不同，忽略了其绝对值的大小\n\n * dp=0.3,dn=0.5,ltri=0.1dp= 0.3, dn= 0.5, l_{tri} =0.1dp=0.3,dn=0.5,ltri =0.1\n * dp=1.3,dn=1.5,ltri=0.1dp= 1.3, dn= 1.5, l_{tri}=0.1dp=1.3,dn=1.5,ltri =0.1\n\nl_c=\\frac{1}{2}\\sum_\\limits{j=1}^{b}||f_{t_j}−c_{y_j}||_2^2\n\n上图是 center loss，其中 yjy_jyj 是 mini-batch 中第 jjj 个图像的标签。cyjc_{y_j}cyj 表示深度特征的第 i 个类别中心。bbb 是批量大小的数量。该公式有效地表征了类内变化。最小化中心损失会增加类内的紧凑性。\n\nl=lid+ltriplet+βlcl=l_{id}+l_{triplet}+βl_cl=lid +ltriplet +βlc\n\n我们的模型总共包含了上式三个损失，其中β\\betaβ 设置为 0.0005\n\n# 2.7 reranking\n\n\n# 03、实验分析\n\n# 3.1 每个 trick 的有效性\n\n\n\n# 3.2 bnn neck 的有效性\n\n\n\n# 3.3 不同 batch_size 的有效性\n\n\n\n# 3.4 不同 image size 的影响\n\n\n\n875 x 606\n\n384 267\n\n384 192\n\n# 3.5 不同 backbone 的影响\n\n\n\n\n# 04、参考资料\n\n# 4.1 本篇论文参考资料\n\npaper:https://ieeexplore.ieee.org/document/8930088\n\ncode: https://github.com/michuanhaohao/reid-strong-baseline\n\nslides: https://drive.google.com/file/d/1h9sgdjenvfonp9ptuxpiz5_k5hfcho-v/view\n\n# 4.2 其他代码链接\n\nibn-network: https://github.com/xingangpan/ibn-net\n\npytorch reid: https://github.com/layumi/person_reid_baseline_pytorch\n\naicity-reid-2020: https://github.com/layumi/aicity-reid-2020/tree/master/pytorch\n\nperson-reid-triplet-loss-baseline: https://github.com/huanghoujing/person-reid-triplet-loss-baseline\n\nfast-reid: https://github.com/jdai-cv/fast-reid\n\nreid-strong-baseline: https://github.com/michuanhaohao/reid-strong-baseline\n\npytorch-metric-learning: https://github.com/kevinmusgrave/pytorch-metric-learning\n\ndeep-efficient-person-reid: https://github.com/lannguyen0910/deep-efficient-person-reid\n\nluperson: https://github.com/dengpanfu/luperson\n\nperson-reid-tiny-baseline: https://github.com/lulujianjie/person-reid-tiny-baseline\n\nhuawei_digix_imageretri_top2: https://github.com/lin-honghui/huawei_digix_imageretri_top2\n\npyretri: https://github.com/pyretri/pyretri\n\ndeep-person-reid: https://github.com/kaiyangzhou/deep-person-reid\n\npytorch-center-loss: https://github.com/kaiyangzhou/pytorch-center-loss\n\ncurricularface: https://github.com/huangyg123/curricularface\n\narcface.py: https://github.com/tencent/tface/blob/master/torchkit/head/distfc/arcface.py\n\ncurricularface.py: https://github.com/tencent/tface/blob/master/torchkit/head/distfc/curricularface.py\n\nxbm 算法\n\n\n# 05、华为 digix 算法比赛方案学习\n\n# 第二名：https://zhuanlan.zhihu.com/p/303371522\n\n代码：https://github.com/lin-honghui/huawei_digix_imageretri_top2\n\n\n\n2.1 数据预处理\n\n细粒度商品检索是一个较难的任务，数据预处理的目的是简化学习目标，修正样本分布、提高网络的泛化能力。\n\n2.1.1 rescalepad\n\n针对训练集中图像分辨率不统一问题，我们采取的策略是在保持物体形变情况下进行resize。具体操作是：padding 短边至长边相同大小，再resize到指定尺寸。\n\n对于本次比赛任务，越大的 patch，越有利于图像细节信息的保留，更有助于性能提升，但显存和模型大小制约着patch增长。选拔赛时使用的显卡为1080ti小水管，网络输入patch为448，512，决赛华为提供了较大显存的v100，使用了更大的分辨率576。\n\n2.1.2 balance sampler\n\n针对数据中存在的类别不平衡问题，我们过滤了类别数少于2的样本，网络训练时，以类别为基本单位进行采样，每次采样 n 个类别，每个类别采样 m 个样本，所有类别采样，保证每个类别以相同的概率被抽样。\n\n实验中，不同的 n x m 采样的 batchsize 对实验结果有较大影响（2.2中讨论）。\n\n2.1.3 denoise\n\n本次比赛数据中，存在较多人为添加的高斯噪声和椒盐噪声，还有少量模式未知的噪声。噪声图像的存在会加大检索任务的难度，为降低噪声图像干扰，我们针对 gallery、query 中模式较为固定的高斯噪声、椒盐噪声图像进行离线修复。\n\n2.1.4 数据增强\n\n数据增强除了模拟商品颜色、旋转、形状、尺度等变化多样性外，针对2.1.3中噪声图像修复可能带来的边缘结构破坏问题（下图第2行，去噪后边缘信息被破坏），我们引入了degaussian和depepper来提高网络的鲁棒性。本次比赛使用的数据增强有（示例图第1行从左到右）：iaaperspective、shiftscalerotate、channelshuffle、randomflip、colorjitter、degaussian、depepper。\n\n\n\n2.2 模型和训练\n\n2.2.1 模型设计\n\n由于比赛模型大小的限制，backbone选择时优先考虑了参数量较少的efficientnet和densenet。我们也测试过resnet、resnext、regnet等网络，但在这次的数据中表现并不佳；\n\ndropout 在这次的任务中提升显著，加上dropout和数据增强后，基本可以通过本地训练集acc预估线上分数。除dropout外也尝试过dropblock、disout等正则化方案，但因为和bnhead会发生冲突，而在之前的实验中去除bnhead是掉点的，后续没有进一步尝试。\n\n决赛时我们最佳单模为efficientnet-b4，初赛时最佳单模是densenet169-rag。\n\n2.2.2 模型训练\n\n2.2.2.1 mini-batch\n\n对于triplet loss 这类损失函数，增大 mini-batch，更有利于困难样本的挖掘，加速网络收敛。但显存制约着batchsize增加。我们使用xbm[4]来增大训练batch，但实验中观察到一个有趣现象：\n\n * 开启xbm后，网络收敛确实加快，但并没有带来性能提升（甚至掉了一点点），在网络收敛的后期triplet loss依旧很高；\n * 训练时，先开启xbm训练一段时间后，再关闭xbm（减小了batchsize）训练则网络性能有提升；\n\n这里我们做了两个假设，一个是小batchsize训练的网络泛化性能更加，但在后续的对比实验证明并非如此。另一个假设是训练集可能存在重复的id，盲目增大batchsize也增大了冲突概率，所以网络性能下降，并且也可以解释triplet loss很高的原因。\n\n我们通过一个简单的实验进行验证：对于arcface、amsoftmax训练收敛的分类网络，全连接层的每一行都可以视为网络学到的一个类别中心，各个类别中心余弦距离表征着类别之间的相似度。设置阈值对余弦相似度较高的类别中心进行聚类可视化可以发现，训练数据集中确实存在大量的重复id。比如 id-1908、id-1528、id-363、id-2979、id-1475都为同款诺基亚手机，id-770、id-1205、id-1082（见下图）都为同款华为手机，根据聚类结果粗略估计，大概有150～200重复商品id，约占训练数据5～6%。\n\n由于本次比赛不允许选手进行额外的标注，咨询了官方人员是否可以进行数据清洗也没有得到明确的答复，在确定数据集含有重复id的情况下，我们也无法对数据集进行清洗。我们采取的方案是降低mini-batch中每次抽样的类别数n，而增大每个类别抽样数量m的方式，来降低发生冲突的概率。此外，由于重复id的存在，triplet loss、arcface等loss margin取值也不应过大。\n\n * backbone： efficientnet、densenet\n\n * pool：generalized mean pooling [5]\n\n * head ： bnhead [1]\n\n * loss ： triplet loss + arcface or triplet loss + amsoftmax\n\n * 正则化：dropout\n\n * 其它组件：\n\n * * rag [6]\n   * nonlocal [7]\n   * ibn [8]\n\n2.2.2.2 实验细节\n\n超参数：根据选拔赛a榜进行调优。\n\n * triplet loss：margin = 0.6，权重为1；\n * amsoftmax ： margin = 0.35，scale = 30，权重为 0.25；\n * arcface ： margin = 0.35，scale = 30，权重为 0.25；\n * dropout ： p = 0.2；\n\n训练加速：\n\n * 使用 pytorch 1.6 自动混合精度加速；\n * 将 jpg 转 npy 加速io；\n * 模型参数冻结：densenet169只解冻最后2个stage， efficient-b3 冻结 15/25 block，b4冻结16/31 block， b5冻结20/38 block；\n\n2.3 模型推理\n\n2.3.1 尺度增强\n\n推理时，增大 patch 为训练阶段1.1倍，以较小的计算代价换稳定的性能提升；\n\n2.3.2 旋转特征对齐\n\n商品角度旋转多样，而cnn对旋转不鲁棒，推理时将图像进行多个角度旋转预测，对得到的特征进行相加。\n\n2.3.3 多主体场景优化\n\n比赛后期上分到瓶颈期，通过统计发现存在部分样本，所有模型检索结果top10几乎都不一致，这部分数据大概100+张，通过可视化总结，这部分样本大多为同个图像中存在多个主体、或者主体较小背景干扰严重。由于本次比赛不能引入额外的标注，我们无法引入额外的检测器进行主体检测再检索。我们尝试在仅使用attention map进行谱聚类的情况下进行主体检测，具体操作为：对于给定图像，计算attention map后进行阈值化处理，过滤出高响应的区域，对高响应区域像素进行 k=3 对最近邻建图，然后通过图切割得到多个子图，每个子图则大概率为一个主体，对切割得到的主体所在位置，对feature map 重新crop & pooling 再进行检索，最后对所有检测结果进行重排序。这部分优化虽然直接带来的结果提升不大，但对模型融合时提分显著。\n\n2.4 后处理&模型融合\n\n后处理：我们只做了 k-reciprocal，在 pyretri基础上实现gpu加速（约加速3倍）和半精度优化（32g内存以内）;\n\n模型融合：我们使用加权投票的融合方式，统计检索结果中top10图像出现的频率及顺序（e.g. top1权重为1，top2权重为1/2 ... ）;\n\n# 第三名：https://zhuanlan.zhihu.com/p/297669395\n\n解决思路\n\n * 长尾分布\n\n * * pk采样，每个batch里的类别数量是平衡的\n   * 二阶段训练，这个是参考一些解决长尾数据的论文思路，先用所有数据训练，让网络尽可能的学到更多的数据，然后这时网络会倾向于预测数量多的类别，第二阶段，对数量多的类别进行欠采样，经过二阶段训练，大约提升0.2%。\n\n * 目标背景多\n\n * * 由于比赛赛题的限制，只可以用imagenet的预训练权重，而且只提供了id标签，我们采用了弱监督切割的方法，根据热力图的响应值，设定一个阈值，然后去掉背景区域，大约提升0.6%\n\n * 测试集加入旋转，翻转等干扰操作\n\n * * 加入旋转操作，好多正常的图片，例如冰箱，成了横向放置，和正常情况不一样。我们也相应的训练阶段和测试阶段加入了翻转和旋转操作，验证集提升2%，由于提交次数有限，测试集没有经过验证。\n\n * 类内差异大，类间差异小\n\n * * 用的经典的softmax+triplet训练，为了扩大难样本挖掘，我们还使用了xbm算法，辅助triplet训练。其实有尝试过其他sota loss，可能是参数不太合适，最后是softmax+triplet效果最好\n\ntrick\n\n * gem\n * rerank\n * bnneck\n * channle shuffle\n * batchdrop\n * 随机擦除\n * 混合精度训练\n\n模型\n\n * resnest101\n * resnest50\n * res2net101\n\n集成\n\n * 我们用了六个模型，然后采用距离平均方法，我们也采用了投票法和concat形式，发现距离加权更好一些。\n\n加权算法:\n\n * 投票法\n\n * concat 形式\n\n * 距离加权（√）",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"kaggle-Classify Leaves 竞赛方案学习",frontmatter:{title:"kaggle-Classify Leaves 竞赛方案学习",date:"2021-06-28T15:59:05.000Z",permalink:"/pages/e518f8/",categories:["计算机视觉","视觉竞赛"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/04.%E7%AB%9E%E8%B5%9B%E7%AC%94%E8%AE%B0-%E8%A7%86%E8%A7%89%E7%AB%9E%E8%B5%9B/01.kaggle-Classify%20Leaves%20%E7%AB%9E%E8%B5%9B%E6%96%B9%E6%A1%88%E5%AD%A6%E4%B9%A0.html",relativePath:"02.学习笔记/04.竞赛笔记-视觉竞赛/01.kaggle-Classify Leaves 竞赛方案学习.md",key:"v-7f7ee08c",path:"/pages/e518f8/",headers:[{level:2,title:"kaggle-Classify Leaves 竞赛方案学习",slug:"kaggle-classify-leaves-竞赛方案学习",normalizedTitle:"kaggle-classify leaves 竞赛方案学习",charIndex:2},{level:3,title:"1、5th 解决方案",slug:"_1、5th-解决方案",normalizedTitle:"1、5th 解决方案",charIndex:36},{level:3,title:"2、7th 解决方案",slug:"_2、7th-解决方案",normalizedTitle:"2、7th 解决方案",charIndex:310},{level:3,title:"3、8th 解决方案",slug:"_3、8th-解决方案",normalizedTitle:"3、8th 解决方案",charIndex:1188},{level:3,title:"4、9th 解决方案",slug:"_4、9th-解决方案",normalizedTitle:"4、9th 解决方案",charIndex:1372},{level:3,title:"5、11th 解决方案",slug:"_5、11th-解决方案",normalizedTitle:"5、11th 解决方案",charIndex:2907},{level:3,title:"6、12th 解决方案",slug:"_6、12th-解决方案",normalizedTitle:"6、12th 解决方案",charIndex:4108},{level:3,title:"7、13th 解决方案",slug:"_7、13th-解决方案",normalizedTitle:"7、13th 解决方案",charIndex:4374},{level:3,title:"8、19th 解决方案",slug:"_8、19th-解决方案",normalizedTitle:"8、19th 解决方案",charIndex:4943},{level:3,title:"9、参考资料",slug:"_9、参考资料",normalizedTitle:"9、参考资料",charIndex:4999}],headersStr:"kaggle-Classify Leaves 竞赛方案学习 1、5th 解决方案 2、7th 解决方案 3、8th 解决方案 4、9th 解决方案 5、11th 解决方案 6、12th 解决方案 7、13th 解决方案 8、19th 解决方案 9、参考资料",content:"# kaggle-Classify Leaves 竞赛方案学习\n\n\n# 1、5th 解决方案\n\n模型：\n\tseresnext50和resnet50\n数据增强：\n\tresize 320, HorizontalFlip, VerticalFlip, Rotate, RandomBrightnesContrasr, ShiftScaleRotate, Normalize\n其他：\n\t优化器：AdamW \n\t学习率调整器：CosineAnnealingLR\n\t损失函数：CrossEntropy\n\t5折交叉验证， 最终结果为五折准确率最高平均， 两个网络各自平均后再做平均。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 2、7th 解决方案\n\n模型：\n\tResNeSt50 + ResNeXt50_32x4d + DenseNet161\n数据增强：\n\ttrain_transform = transforms.Compose([\n    # 随机裁剪图像，所得图像为原始面积的0.08到1之间，高宽比在3/4和4/3之间。\n    # 然后，缩放图像以创建224 x 224的新图像\n    transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)),\n    transforms.RandomHorizontalFlip(), \n    # 随机更改亮度，对比度和饱和度\n    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n    transforms.ToTensor(),   \n    # 标准化图像的每个通道\n    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n\t\n\t训练时的 CutMix 以及 预测时的 TTA\n\tCutMix: https://github.com/ildoonet/cutmix\n\tTTA: https://github.com/qubvel/ttach\n其他：\n\t总共训了 30 个 epoch\n\t优化器：使用 AdamW，torch.optim.AdamW(model.parameters(),lr=1e-4,weight_decay= 1e-3)\n\t学习率调整：CosineAnnealingLR: CosineAnnealingLR(optimizer,T_max=10)\n\t交叉验证：使用 5 折交叉验证\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 3、8th 解决方案\n\n\n# Using an LR ramp up because fine-tuning a pre-trained model.\n# Starting with a high LR would break the pre-trained weights.\n# 提到一开始使用大的 LR 会破坏原有的预训练权重\n\n\n\n1\n2\n3\n4\n5\n\n\n\n# 4、9th 解决方案\n\n模型：\n    0, resnet50d, input_size: 224 (selected)\n    1, efficientnet_b3, input_size: 224 (selected)\n    2, resnext50_32x4d, input_size: 224\n    3, inception_resnet_v2, input_size: 299 (selected)\n    4, vit_base_patch16_224, input_size: 224 (selected)\n    5, tf_efficientnet_b3_ns, input_size: 224\n    6, tf_efficientnet_b4_ns, input_size: 380 (selected)\n    7, resnest200e, input_size 320 (selected)\n    8, mixnet_s, input_size 224\n    9, mixnet_xl, input_size 224 (selected)\n    10, resnest50d, input_size 224\n数据增强：\n\ttransforms.Compose([\n        transforms.ToTensor(),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.5),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0),\n        transforms.RandomResizedCrop([CFG.size, CFG.size]),\n        transforms.Normalize(\n            mean=[0.5, 0.5, 0.5],\n            std=[0.5, 0.5, 0.5],\n        )\n其他：\n\t总共训了 50 个epoch\n\tbatch_size = 32\n\tcriterion = CrossEntropyLoss()\n\toptimizer = MADGRAD \n    learning_rate = 1e-4\n    weight_decay = 1e-9\n    scheduler = 'ReduceLROnPlateau' \n    factor = 0.2 # ReduceLROnPlateau\n    patience = 4 # ReduceLROnPlateau\n    eps = 1e-6   # ReduceLROnPlateau\n    交叉验证：5 折交叉验证\n    模型选择：Stacked mean combinations & Weighted average（值得学习）\n    \n    probs_3D = np.zeros([train.shape[0],num_class,num_models])\n    Weighted average 用来检验模型的重要性以及验证stacked mean method足够优秀\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# 5、11th 解决方案\n\n自建的比赛tricks库：https://github.com/seefun/TorchUtils\nBaseline:https://github.com/seefun/TorchUtils/blob/master/examples/kaggle_leaves_classification.ipynb\n\n\n\n模型：\n\n数据增强：\n\ttrain_transform = albumentations.Compose([\n    albumentations.RandomRotate90(p=0.5),\n    albumentations.Transpose(p=0.5),\n    albumentations.Flip(p=0.5),\n    albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.0625, rotate_limit=45, border_mode=1, p=0.5),\n    tu.randAugment(),\n    albumentations.Normalize(),\n    # 这里要使用 ToTensorV2()\n    AT.ToTensorV2(),\n    ])\n其他：\n\t使用混合精度训练，节省时间，甚至能因为大batch而提升性能\n\tscaler = torch.cuda.amp.GradScaler() # for AMP training\n\tscaler.scale(loss).backward()\n\tscaler.step(optimizer)\n    scaler.update()\n\t使用imagenet-21k上预训练的efficientnetv2模型：https://arxiv.org/abs/2104.00298\n\tmutli-dropout\n\tLR = 3e-4\n\tEPOCH = 36\n\tDECAY_SCALE = 20.0\n\tpooling时concat maxpooling与avgpooling\n\tmixup / Label Smoothing作为正则\n\t余弦学习率下降\n\t使用Ranger(RAdam+Lookahead+GC)/ RangerLars(Ranger+Lars)优化\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n总结：\n\n 1. 比赛数据集存在大量leak，训练测试集大量图片重复或接近，过拟合反而能获得更好结果；\n 2. 数据集存在很多噪声样本，过拟合这些错误标注反而能获得更高的LB；即LB本身并不可靠，更高的LB不代表真正更高的泛化性能；\n\n\n# 6、12th 解决方案\n\n在 11th 的解决方案上进行微调\n\n1、加入mixup，设为0.1（太大会使模型性能下降），使用\n2、epoch 加大到 72\n3、在 randAugment 中加入cutout\n4、加入LabelSmoothing: tu.LabelSmoothingCrossEntropy()\n- https://github.com/seefun/TorchUtils/blob/master/torch_utils/criterion/CrossEntropy.py\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 7、13th 解决方案\n\n模型：\n\t一个seresnext50，两个resnext50\n数据增强：\n\tResize 224            \n    transforms.RandomHorizontalFlip(p=0.5),   #随机水平翻转\n    transforms.RandomVerticalFlip(p=0.5),     #除了水平竖直反转之外其他的处理方法貌似都会降低acc       transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n其他：\n\tlr_scheduler:ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=0.0000001)\n\t在训练过程中使用 cutmix\n\t验证和测试的时候使用TTA: tta.ClassificationTTAWrapper(model_1, tta.aliases.flip_transform(),  merge_mode='mean')\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 8、19th 解决方案\n\n模型：\n\tresnest50d\n其他：\n\t三折交叉验证\n\n\n1\n2\n3\n4\n\n\n\n# 9、参考资料\n\n * Guide-to-ensembling-methods\n\n * Guide to Pytorch Learning Rate Scheduling\n\n * Mixup, Cutmix, FMix Visualisations",normalizedContent:"# kaggle-classify leaves 竞赛方案学习\n\n\n# 1、5th 解决方案\n\n模型：\n\tseresnext50和resnet50\n数据增强：\n\tresize 320, horizontalflip, verticalflip, rotate, randombrightnescontrasr, shiftscalerotate, normalize\n其他：\n\t优化器：adamw \n\t学习率调整器：cosineannealinglr\n\t损失函数：crossentropy\n\t5折交叉验证， 最终结果为五折准确率最高平均， 两个网络各自平均后再做平均。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 2、7th 解决方案\n\n模型：\n\tresnest50 + resnext50_32x4d + densenet161\n数据增强：\n\ttrain_transform = transforms.compose([\n    # 随机裁剪图像，所得图像为原始面积的0.08到1之间，高宽比在3/4和4/3之间。\n    # 然后，缩放图像以创建224 x 224的新图像\n    transforms.randomresizedcrop(224, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)),\n    transforms.randomhorizontalflip(), \n    # 随机更改亮度，对比度和饱和度\n    transforms.colorjitter(brightness=0.4, contrast=0.4, saturation=0.4),\n    transforms.totensor(),   \n    # 标准化图像的每个通道\n    transforms.normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n\t\n\t训练时的 cutmix 以及 预测时的 tta\n\tcutmix: https://github.com/ildoonet/cutmix\n\ttta: https://github.com/qubvel/ttach\n其他：\n\t总共训了 30 个 epoch\n\t优化器：使用 adamw，torch.optim.adamw(model.parameters(),lr=1e-4,weight_decay= 1e-3)\n\t学习率调整：cosineannealinglr: cosineannealinglr(optimizer,t_max=10)\n\t交叉验证：使用 5 折交叉验证\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n# 3、8th 解决方案\n\n\n# using an lr ramp up because fine-tuning a pre-trained model.\n# starting with a high lr would break the pre-trained weights.\n# 提到一开始使用大的 lr 会破坏原有的预训练权重\n\n\n\n1\n2\n3\n4\n5\n\n\n\n# 4、9th 解决方案\n\n模型：\n    0, resnet50d, input_size: 224 (selected)\n    1, efficientnet_b3, input_size: 224 (selected)\n    2, resnext50_32x4d, input_size: 224\n    3, inception_resnet_v2, input_size: 299 (selected)\n    4, vit_base_patch16_224, input_size: 224 (selected)\n    5, tf_efficientnet_b3_ns, input_size: 224\n    6, tf_efficientnet_b4_ns, input_size: 380 (selected)\n    7, resnest200e, input_size 320 (selected)\n    8, mixnet_s, input_size 224\n    9, mixnet_xl, input_size 224 (selected)\n    10, resnest50d, input_size 224\n数据增强：\n\ttransforms.compose([\n        transforms.totensor(),\n        transforms.randomhorizontalflip(p=0.5),\n        transforms.randomverticalflip(p=0.5),\n        transforms.colorjitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0),\n        transforms.randomresizedcrop([cfg.size, cfg.size]),\n        transforms.normalize(\n            mean=[0.5, 0.5, 0.5],\n            std=[0.5, 0.5, 0.5],\n        )\n其他：\n\t总共训了 50 个epoch\n\tbatch_size = 32\n\tcriterion = crossentropyloss()\n\toptimizer = madgrad \n    learning_rate = 1e-4\n    weight_decay = 1e-9\n    scheduler = 'reducelronplateau' \n    factor = 0.2 # reducelronplateau\n    patience = 4 # reducelronplateau\n    eps = 1e-6   # reducelronplateau\n    交叉验证：5 折交叉验证\n    模型选择：stacked mean combinations & weighted average（值得学习）\n    \n    probs_3d = np.zeros([train.shape[0],num_class,num_models])\n    weighted average 用来检验模型的重要性以及验证stacked mean method足够优秀\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n# 5、11th 解决方案\n\n自建的比赛tricks库：https://github.com/seefun/torchutils\nbaseline:https://github.com/seefun/torchutils/blob/master/examples/kaggle_leaves_classification.ipynb\n\n\n\n模型：\n\n数据增强：\n\ttrain_transform = albumentations.compose([\n    albumentations.randomrotate90(p=0.5),\n    albumentations.transpose(p=0.5),\n    albumentations.flip(p=0.5),\n    albumentations.shiftscalerotate(shift_limit=0.0625, scale_limit=0.0625, rotate_limit=45, border_mode=1, p=0.5),\n    tu.randaugment(),\n    albumentations.normalize(),\n    # 这里要使用 totensorv2()\n    at.totensorv2(),\n    ])\n其他：\n\t使用混合精度训练，节省时间，甚至能因为大batch而提升性能\n\tscaler = torch.cuda.amp.gradscaler() # for amp training\n\tscaler.scale(loss).backward()\n\tscaler.step(optimizer)\n    scaler.update()\n\t使用imagenet-21k上预训练的efficientnetv2模型：https://arxiv.org/abs/2104.00298\n\tmutli-dropout\n\tlr = 3e-4\n\tepoch = 36\n\tdecay_scale = 20.0\n\tpooling时concat maxpooling与avgpooling\n\tmixup / label smoothing作为正则\n\t余弦学习率下降\n\t使用ranger(radam+lookahead+gc)/ rangerlars(ranger+lars)优化\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n总结：\n\n 1. 比赛数据集存在大量leak，训练测试集大量图片重复或接近，过拟合反而能获得更好结果；\n 2. 数据集存在很多噪声样本，过拟合这些错误标注反而能获得更高的lb；即lb本身并不可靠，更高的lb不代表真正更高的泛化性能；\n\n\n# 6、12th 解决方案\n\n在 11th 的解决方案上进行微调\n\n1、加入mixup，设为0.1（太大会使模型性能下降），使用\n2、epoch 加大到 72\n3、在 randaugment 中加入cutout\n4、加入labelsmoothing: tu.labelsmoothingcrossentropy()\n- https://github.com/seefun/torchutils/blob/master/torch_utils/criterion/crossentropy.py\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 7、13th 解决方案\n\n模型：\n\t一个seresnext50，两个resnext50\n数据增强：\n\tresize 224            \n    transforms.randomhorizontalflip(p=0.5),   #随机水平翻转\n    transforms.randomverticalflip(p=0.5),     #除了水平竖直反转之外其他的处理方法貌似都会降低acc       transforms.totensor(),\n    transforms.normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n其他：\n\tlr_scheduler:reducelronplateau(optimizer, mode='min', factor=0.5, patience=3, verbose=true, min_lr=0.0000001)\n\t在训练过程中使用 cutmix\n\t验证和测试的时候使用tta: tta.classificationttawrapper(model_1, tta.aliases.flip_transform(),  merge_mode='mean')\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n# 8、19th 解决方案\n\n模型：\n\tresnest50d\n其他：\n\t三折交叉验证\n\n\n1\n2\n3\n4\n\n\n\n# 9、参考资料\n\n * guide-to-ensembling-methods\n\n * guide to pytorch learning rate scheduling\n\n * mixup, cutmix, fmix visualisations",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"kaggle 图像分割竞赛学习",frontmatter:{title:"kaggle 图像分割竞赛学习",date:"2021-08-30T16:55:18.000Z",permalink:"/pages/31d8c4/",categories:["学习笔记","竞赛笔记-视觉竞赛"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/04.%E7%AB%9E%E8%B5%9B%E7%AC%94%E8%AE%B0-%E8%A7%86%E8%A7%89%E7%AB%9E%E8%B5%9B/04.kaggle%20%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%AB%9E%E8%B5%9B%E5%AD%A6%E4%B9%A0.html",relativePath:"02.学习笔记/04.竞赛笔记-视觉竞赛/04.kaggle 图像分割竞赛学习.md",key:"v-03d1a531",path:"/pages/31d8c4/",headers:[{level:2,title:"kaggle 语义分割竞赛学习",slug:"kaggle-语义分割竞赛学习",normalizedTitle:"kaggle 语义分割竞赛学习",charIndex:2},{level:3,title:"2018 Data Science Bowl",slug:"_2018-data-science-bowl",normalizedTitle:"2018 data science bowl",charIndex:95},{level:3,title:"Airbus Ship Detection Challenge",slug:"airbus-ship-detection-challenge",normalizedTitle:"airbus ship detection challenge",charIndex:122},{level:3,title:"SIIM-ACR Pneumothorax Segmentation",slug:"siim-acr-pneumothorax-segmentation",normalizedTitle:"siim-acr pneumothorax segmentation",charIndex:158},{level:3,title:"TGS Salt Identification Challenge",slug:"tgs-salt-identification-challenge",normalizedTitle:"tgs salt identification challenge",charIndex:197},{level:3,title:"HuBMAP - Hacking the Kidney",slug:"hubmap-hacking-the-kidney",normalizedTitle:"hubmap - hacking the kidney",charIndex:330},{level:3,title:"Carvana Image Masking Challenge",slug:"carvana-image-masking-challenge",normalizedTitle:"carvana image masking challenge",charIndex:362}],headersStr:"kaggle 语义分割竞赛学习 2018 Data Science Bowl Airbus Ship Detection Challenge SIIM-ACR Pneumothorax Segmentation TGS Salt Identification Challenge HuBMAP - Hacking the Kidney Carvana Image Masking Challenge",content:"# kaggle 语义分割竞赛学习\n\n最近要开始做一个检测的单类别语义分割竞赛了，顺便完善一下自己的语义分割框架，本次准备先去 kaggle 上找些语义分割竞赛的技巧，学习总结一下\n\n\n# 2018 Data Science Bowl\n\n\n# Airbus Ship Detection Challenge\n\n\n# SIIM-ACR Pneumothorax Segmentation\n\n\n# TGS Salt Identification Challenge\n\n * https://zhuanlan.zhihu.com/p/107197049\n\n * +0.01 LB with snapshot ensembling and cyclic lr\n\n\n# HuBMAP - Hacking the Kidney\n\n\n# Carvana Image Masking Challenge",normalizedContent:"# kaggle 语义分割竞赛学习\n\n最近要开始做一个检测的单类别语义分割竞赛了，顺便完善一下自己的语义分割框架，本次准备先去 kaggle 上找些语义分割竞赛的技巧，学习总结一下\n\n\n# 2018 data science bowl\n\n\n# airbus ship detection challenge\n\n\n# siim-acr pneumothorax segmentation\n\n\n# tgs salt identification challenge\n\n * https://zhuanlan.zhihu.com/p/107197049\n\n * +0.01 lb with snapshot ensembling and cyclic lr\n\n\n# hubmap - hacking the kidney\n\n\n# carvana image masking challenge",charsets:{cjk:!0},lastUpdated:"2021/09/05, 19:05:19"},{title:"kaggle-CowBoy Outfits Detection 竞赛方案学习",frontmatter:{title:"kaggle-CowBoy Outfits Detection 竞赛方案学习",date:"2021-08-22T16:55:51.000Z",permalink:"/pages/9f35c1/",categories:["学习笔记","竞赛笔记-视觉竞赛"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/04.%E7%AB%9E%E8%B5%9B%E7%AC%94%E8%AE%B0-%E8%A7%86%E8%A7%89%E7%AB%9E%E8%B5%9B/03.kaggle-CowBoy%20Outfits%20Detection%20%E7%AB%9E%E8%B5%9B%E6%96%B9%E6%A1%88%E5%AD%A6%E4%B9%A0.html",relativePath:"02.学习笔记/04.竞赛笔记-视觉竞赛/03.kaggle-CowBoy Outfits Detection 竞赛方案学习.md",key:"v-6ed35a41",path:"/pages/9f35c1/",headers:[{level:2,title:"kaggle-CowBoy Outfits Detection 竞赛方案学习",slug:"kaggle-cowboy-outfits-detection-竞赛方案学习",normalizedTitle:"kaggle-cowboy outfits detection 竞赛方案学习",charIndex:2},{level:3,title:"01、沐神总结",slug:"_01、沐神总结",normalizedTitle:"01、沐神总结",charIndex:45},{level:3,title:"02、Kaggle 社区方案解析",slug:"_02、kaggle-社区方案解析",normalizedTitle:"02、kaggle 社区方案解析",charIndex:280},{level:3,title:"03、总结",slug:"_03、总结",normalizedTitle:"03、总结",charIndex:2503}],headersStr:"kaggle-CowBoy Outfits Detection 竞赛方案学习 01、沐神总结 02、Kaggle 社区方案解析 03、总结",content:"# kaggle-CowBoy Outfits Detection 竞赛方案学习\n\n\n# 01、沐神总结\n\n# 1.1 数据重采样\n\n * 将不足的类别样本复制多次\n * 在随机采样小批量时对每个类别使用不同的采样频率\n * 在计算损失时增大不足类别样本的权重\n * SMOTE： 在不足类样本的中选择相近对做插值\n * 数据增强：mixup 等\n\n# 1.2 所使用模型\n\n * YOLOX：YOLOv3 + anchor free\n * YOLOv5\n * Detectron2：Faster RCNN\n * 大都采用了多模型、k fold 融合\n\n\n# 02、Kaggle 社区方案解析\n\n# 1、2nd 解决方案：yolox-for-cowboyoutfits\n\n * 使用 Yolox-m 模型，训练了40个epoch，前5个epoch为warm up，中间 20 epoch 为加入aug的训练，后15个 epoch 为不加入 augmentation 的训练\n * 最后的测试结果为 58.46\n * 代码链接：https://github.com/Megvii-BaseDetection/YOLOX\n\n# 2、4th 解决方案：yolov5-transformer-module-ensemble\n\n * 使用三个模型\n   * Yolov5x6\n   * Yolov5x6-transformer(1 transformer module)\n   * Yolov5l6-transformer(2 transformer modules)\n * 构建验证集时：按照训练集比例抽样\n * 构建 model config 时候添加了 C3TR ，代表 Transformer module\n\n# 3、5th 解决方案：Challenge with imbalanced training data\n\n * # 统计count的轮子\n   from collections import Counter\n   for ann in data[\"annotations\"]:\n       list_cat.append(dict_cat[ann[\"category_id\"]])\n   Counter(list_cat)\n   \n   \n   1\n   2\n   3\n   4\n   5\n   \n\n * 使用 SMOTE 算法处理样本不平衡问题，使得所有的类别训练样本都是 2097 了\n   \n   * 使用的库是 https://github.com/scikit-learn-contrib/imbalanced-learn\n   \n   * from imblearn.over_sampling import SMOTE\n     x_train_only_id = pd.DataFrame(x_train['id'])\n     X_SMOTE, Y_SMOTE = SMOTE().fit_resample(x_train_only_id.values,y_train.values)\n     Counter(Y_SMOTE)\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * 使用模型是 yolov5s\n\n# 4、6th 解决方案：cowboy detectron2 faster-rcnn\n\n * # 定义的数据增强\n   class myMapper:\n       def __init__(self, cfg, is_train: bool = True):\n           if is_train:\n               aug_list = [T.ResizeShortestEdge([640, 800], sample_style='range'),\n                           T.RandomBrightness(0.8, 1.2),\n                           T.RandomContrast(0.8, 1.2),\n                           T.RandomSaturation(0.8, 1.2),\n                           T.RandomFlip(prob=0.5, horizontal=True, vertical=False)]\n           else:\n               aug_list = [T.ResizeShortestEdge(800, sample_style='choice')]\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   \n\n * 五折训练，每个模型 25 个 epoch，大概 10 个 epoch 就足以达到比较好的 mAP 值，最终使用 weighted boxes fusion 做结果的集成\n\n# 5、8th 解决方案：MMDetection+CascadeRCNN\n\n * CascadeRCNN 12 个 epoch\n * 为啥我的结果这么差？？？\n\n# 6、Megvii_YOLOX\n\n * 使用模型 yolox_x，分数为 64.36084，在初榜排第9\n\n# 7、YOLOV5+k-fold+ensemble\n\n * 如题所示，分组思路是基于StractifiedKfold, 但是因为对于目标检测，除了label balance 还有很多因素需要考虑，比如说目标检测框的数量分布，种类分布，甚至面积大小分布，所以这边参考了一下stackoverflow 上一个讨论。 使用了floor division //5\n\n# 8、windows_YOLOv5L\n\n * 没什么trick，算是一个工程上的实践\n\n\n# 03、总结\n\n此次目标检测的竞赛并不像图像分类的竞赛那样，大家在各类 trick 上都有所尝试。因为图像分类任务简单，训练迭代快，并且大家基本都能从零搭建起自己的分类代码框架，所以在修改上是十分自由的。而对于目标检测竞赛，大家基本都是在使用别人的框架：\n\n * YOLOv5 以及 最新出的 YOLOX 都受到了大家的青睐\n * 此外还是老朋友 mmDetection 和 Detectron2\n\n社区所分享方案中，大家所使用的个性化方案其实是比较有限的\n\n * 针对不平衡数据的过采样\n * 多折数据的划分\n * 多折训练及 WBF 集成\n\n这也充分说明了搞 CV 的同学对于大型框架的修改动手能力还是较弱的（和我一样），改改 config 是不能成为核心竞争力的，等把我的分割框架搭建完成就来从别人代码的基础上搞一个自己的检测框架",normalizedContent:"# kaggle-cowboy outfits detection 竞赛方案学习\n\n\n# 01、沐神总结\n\n# 1.1 数据重采样\n\n * 将不足的类别样本复制多次\n * 在随机采样小批量时对每个类别使用不同的采样频率\n * 在计算损失时增大不足类别样本的权重\n * smote： 在不足类样本的中选择相近对做插值\n * 数据增强：mixup 等\n\n# 1.2 所使用模型\n\n * yolox：yolov3 + anchor free\n * yolov5\n * detectron2：faster rcnn\n * 大都采用了多模型、k fold 融合\n\n\n# 02、kaggle 社区方案解析\n\n# 1、2nd 解决方案：yolox-for-cowboyoutfits\n\n * 使用 yolox-m 模型，训练了40个epoch，前5个epoch为warm up，中间 20 epoch 为加入aug的训练，后15个 epoch 为不加入 augmentation 的训练\n * 最后的测试结果为 58.46\n * 代码链接：https://github.com/megvii-basedetection/yolox\n\n# 2、4th 解决方案：yolov5-transformer-module-ensemble\n\n * 使用三个模型\n   * yolov5x6\n   * yolov5x6-transformer(1 transformer module)\n   * yolov5l6-transformer(2 transformer modules)\n * 构建验证集时：按照训练集比例抽样\n * 构建 model config 时候添加了 c3tr ，代表 transformer module\n\n# 3、5th 解决方案：challenge with imbalanced training data\n\n * # 统计count的轮子\n   from collections import counter\n   for ann in data[\"annotations\"]:\n       list_cat.append(dict_cat[ann[\"category_id\"]])\n   counter(list_cat)\n   \n   \n   1\n   2\n   3\n   4\n   5\n   \n\n * 使用 smote 算法处理样本不平衡问题，使得所有的类别训练样本都是 2097 了\n   \n   * 使用的库是 https://github.com/scikit-learn-contrib/imbalanced-learn\n   \n   * from imblearn.over_sampling import smote\n     x_train_only_id = pd.dataframe(x_train['id'])\n     x_smote, y_smote = smote().fit_resample(x_train_only_id.values,y_train.values)\n     counter(y_smote)\n     \n     \n     1\n     2\n     3\n     4\n     \n\n * 使用模型是 yolov5s\n\n# 4、6th 解决方案：cowboy detectron2 faster-rcnn\n\n * # 定义的数据增强\n   class mymapper:\n       def __init__(self, cfg, is_train: bool = true):\n           if is_train:\n               aug_list = [t.resizeshortestedge([640, 800], sample_style='range'),\n                           t.randombrightness(0.8, 1.2),\n                           t.randomcontrast(0.8, 1.2),\n                           t.randomsaturation(0.8, 1.2),\n                           t.randomflip(prob=0.5, horizontal=true, vertical=false)]\n           else:\n               aug_list = [t.resizeshortestedge(800, sample_style='choice')]\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   10\n   11\n   \n\n * 五折训练，每个模型 25 个 epoch，大概 10 个 epoch 就足以达到比较好的 map 值，最终使用 weighted boxes fusion 做结果的集成\n\n# 5、8th 解决方案：mmdetection+cascadercnn\n\n * cascadercnn 12 个 epoch\n * 为啥我的结果这么差？？？\n\n# 6、megvii_yolox\n\n * 使用模型 yolox_x，分数为 64.36084，在初榜排第9\n\n# 7、yolov5+k-fold+ensemble\n\n * 如题所示，分组思路是基于stractifiedkfold, 但是因为对于目标检测，除了label balance 还有很多因素需要考虑，比如说目标检测框的数量分布，种类分布，甚至面积大小分布，所以这边参考了一下stackoverflow 上一个讨论。 使用了floor division //5\n\n# 8、windows_yolov5l\n\n * 没什么trick，算是一个工程上的实践\n\n\n# 03、总结\n\n此次目标检测的竞赛并不像图像分类的竞赛那样，大家在各类 trick 上都有所尝试。因为图像分类任务简单，训练迭代快，并且大家基本都能从零搭建起自己的分类代码框架，所以在修改上是十分自由的。而对于目标检测竞赛，大家基本都是在使用别人的框架：\n\n * yolov5 以及 最新出的 yolox 都受到了大家的青睐\n * 此外还是老朋友 mmdetection 和 detectron2\n\n社区所分享方案中，大家所使用的个性化方案其实是比较有限的\n\n * 针对不平衡数据的过采样\n * 多折数据的划分\n * 多折训练及 wbf 集成\n\n这也充分说明了搞 cv 的同学对于大型框架的修改动手能力还是较弱的（和我一样），改改 config 是不能成为核心竞争力的，等把我的分割框架搭建完成就来从别人代码的基础上搞一个自己的检测框架",charsets:{cjk:!0},lastUpdated:"2021/09/05, 19:05:19"},{title:"few-shot learning 竞赛学习",frontmatter:{title:"few-shot learning 竞赛学习",date:"2021-09-05T13:32:41.000Z",permalink:"/pages/e4a923/",categories:["学习笔记","竞赛笔记-视觉竞赛"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/04.%E7%AB%9E%E8%B5%9B%E7%AC%94%E8%AE%B0-%E8%A7%86%E8%A7%89%E7%AB%9E%E8%B5%9B/05.few-shot%20learning%20%E7%AB%9E%E8%B5%9B%E5%AD%A6%E4%B9%A0-1.html",relativePath:"02.学习笔记/04.竞赛笔记-视觉竞赛/05.few-shot learning 竞赛学习-1.md",key:"v-026eefc1",path:"/pages/e4a923/",headers:[{level:2,title:"Few-shot Learning 竞赛学习-1",slug:"few-shot-learning-竞赛学习-1",normalizedTitle:"few-shot learning 竞赛学习-1",charIndex:2}],headersStr:"Few-shot Learning 竞赛学习-1",content:"# Few-shot Learning 竞赛学习-1\n\n最近离一个比赛结束还有不到一周，准备来快速学习下 few-shot learning 看能不能摸到奖（大雾）\n\n# 01、问题描述\n\n1.1 数据描述\n\n * 训练集有 49990 张图像，分为10类，每张图像的尺寸为 32 x 32\n   * 其中有 20 张有标注，每类2张，剩下 49970 张图像无标注\n * 验证集有10类，每类1张\n * 测试集有 10000 张\n\n1.2 竞赛问题定义\n\n按照 Few-shot learning 的定义来讲，训练集中随机抽取 C 个类别，每个类别 K 个样本（总共 CK 个数据），成为C-way K-shot 问题。\n\n该竞赛的任务被定义为 10-way 2-shot 问题，也就是 10 类，每类 2 张有标注\n\n# 02、竞赛学习\n\n# 2.1 参考资料[1] & 参考资料 [2]\n\n这里首先学习下参考资料[1]与参考资料[2]\n\n支撑集（Support Set）：即 C 类，每类 K 个样本所组成的 CK 个数据\n\n查询集（Query Set）：类似测试集，包含 Q 张未分类图像即这里的 10000 张\n\n这里重点关注下 Metric Based 方法，看起来比较容易实现。Metric Based 方法通过度量 batch 集中的样本和 support 集中样本的距离，借助最近邻的思想完成分类。\n\n度量学习的基本思想是学习单个数据（如图像）之间的距离函数。它已被证明对于解决小样本分类任务非常有效：度量学习算法通过将查询集图像与已标记的支持集图像进行比较来进行分类。\n\n * 将支持集和查询集的所有图像提取 Embedding，\n * 查询集中的每张图像都根据其与支持集图像的距离来进行分类，例如欧氏距离/余弦距离 以及 KNN 算法\n\n可以使用孪生网络（Siamese Network）\n\n * 训练时，通过组合的方式构造不同的成对样本，输入网络进行训练，在最上层通过样本对的距离来判断他们是否属于同一类，并产生对应的概率分布。\n\n * 在预测阶段，将测试样本集和支撑集之间每一个样本对都进行推理，预测结果为支撑集上概率最高的类别\n\n可以使用原型网络（Prototype Network）\n\n * 每个类都存在一个原型表达，该类的原型是 support set 在 embedding 空间中的均值\n * 分类问题即可变成在 embedding 空间中的最近邻问题\n\nRelation Network\n\n * 认为度量方式也是网络中重要的一环，需要对其进行建模\n\n该文章提出了统一的 Encode-Induction-Relation 描述框架\n\n# 2.2 参考资料[3]\n\n参考资料[3] 解释了 Meta-learning 方法的步骤\n\n训练时\n\n * 将训练集采样成 Support Set 以及 Query Set\n\n * 基于Support Set 生成分类模型 F\n\n * 利用模型 F 对 Query Set 进行分类预测生成 predict labels（pseudo-label）\n\n * 通过 query labels 和 predicted labels 进行 loss 的计算，从而更新网络参数\n\n测试时：\n\n * 利用分类模型 F 对 Query Set 进行预测\n\n# 2.3 参考资料[4]\n\n参考资料[4] 介绍了Metric Learning 中的损失函数\n\n * Contrastive loss\n   * 输入：两个样本组成的样本对，label 为该样本对是否属于同一类\n   * 超参数 maigin，表示不同类样本之间的距离应该超过该 margin 值\n * Triplet loss：\n   * 输入：一个三元组，query + positive sample + negative sample\n   * Triplet loss 要求 query 到 负样本的距离与 query 到正样本的距离之差要大于 margin 值\n   * Contrastive loss和triplet loss都很常用，一般来说，Triplet-Loss 的效果比 Contrastive Loss 的效果要好，因为他考虑了正负样本与锚点的距离关系。然而，这两种loss函数如果单独使用则会遭遇收敛速度慢的问题。在学习过程的后期，大多数样本都能满足损失函数的约束条件，这些样本对应进一步学习的贡献很小。因此，这两种损失函数都需要配合hard sample mining的学习策略一起使用，例如 FaceNet 提出的 simi-hard negative sample mining方法。\n * N-pair-ms loss\n   * 考虑 query 与多个类别的负样本之间的关系，促使 query 与其他所有类之间都保持距离，能够加快模型的收敛速度\n * Lifted Struct loss\n   * 基于 mini-batch 中的所有正负样本对来计算 loss\n   * 对于每一个正样本对{i，j}，挖掘出最困难的负样本，与 i 和 j 距离最近的负样本用于计算 triplet loss\n   * 优点：动态构建最困难的三元组\n * Ranked list loss\n   * 上述方法的缺点\n     * 上述损失函数提出加入负样本来获得结构化的信息，但是使用的负样本只是一小部分\n     * 这些损失函数没有考虑类内的数据分布，都追求将同一类压缩到同一个点上\n   * \n * Multi-Similarity loss\n   * 自相似性：根据样本对自身计算出的相似性，这是一种最常用也是最重要的相似性。例如，当一个负样本对的余弦相似性较大时，意味着很难把该样本对所对应的两种类别区分开来，这样的样本对对模型来说是困难的，也是有信息量的，对于模型学习更有区分度的特征很有帮助。另一方面，自相似性很难完整地描述embedding空间的样本分布情况。\n   * 正相对相似性：不仅考虑当前样本对自身的相似性，还考虑局部邻域内正样本对之间的相对关系。\n   * 负相对相似性：不仅考虑当前样本对自身的相似性，还考虑局部邻域内负样本对之间的相对关系。\n\n# 2.4 参考资料 [5]\n\n[5] 中比较清晰地说出了几种方法\n\n * 度量学习：提特征，算距离，KNN 等方式分类\n * FSL 定制的数据增强：增加数据的量用于 fine-tuning\n * 元学习：在任务级别上学习而不是在样本上学习\n\n# 2.5 Paperwithcode SOTA 论文的方法\n\n * Image Clustering on CIFAR-100\n   * Accuracy 0.584：https://paperswithcode.com/paper/spice-semantic-pseudo-labeling-for-image\n   * Accuracy 0.543：https://paperswithcode.com/paper/improving-unsupervised-image-clustering-with\n   * Accuracy 0.507：https://paperswithcode.com/paper/learning-to-classify-images-without-labels\n * Few-Shot Image Classification on CIFAR-FS 5-way (1-shot)\n   * Accuracy 87.79：https://paperswithcode.com/paper/transfer-learning-based-few-shot\n   * Accuracy 87.73：https://paperswithcode.com/paper/sill-net-feature-augmentation-with-separated\n   * Accuracy 87.69：https://paperswithcode.com/paper/leveraging-the-feature-distribution-in\n\n# 2.6 kaggle 竞赛的方法\n\n参考资料[6] 和[7] 两篇是知乎 “砍手豪” 大神所写的 kaggle 上 few-shot 竞赛的文章，值得学习，可以先看看是否能找到 baseline.\n\n * tricks\n   \n   * 实现 LR finder\n\n * EDA\n   \n   * 查找重复图像\n   * 模型可视化：https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563\n   * t-SNE 特征可视化\n\n * Siamese Network 神经网络\n   \n   * Siamese神经网络比较两个图像，并确定这两个图像是从同一条鲸鱼还是从不同的鲸鱼中获取\n   * https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563/data\n   * https://www.kaggle.com/seesee/siamese-pretrained-0-822\n   * https://www.kaggle.com/voglinio/siamese-two-pretrained-weights-0-855\n\n * Metric Learning 神经网络\n   \n   * Metric Learning 使用网络提取 Embedding，再比较 Embedding 之间的距离来进行分类\n   * https://www.kaggle.com/c/humpback-whale-identification/discussion/74647\n   * https://www.kaggle.com/iafoss/similarity-densenet121-0-805lb-kernel-time-limit\n\n * Winner model\n   \n   * 1st solution: https://www.kaggle.com/c/humpback-whale-identification/discussion/82366\n     * 四通道：RGB + Masks 作为输入（我们可以将RGB + Edges + Masks 作为输入）\n   * 2nd solution: https://www.kaggle.com/c/humpback-whale-identification/discussion/83885\n     * 三种 loss：arcface loss + triplet loss + focal loss\n     * 三个backbone：resnet101，seresnet101，seresnext101\n   * 4th solution: https://www.kaggle.com/c/humpback-whale-identification/discussion/82356\n     * 4th Place Solution: SIFT + Siamese\n     * 介绍了一些降低复杂度的方法\n\n# 2.7 其他方法\n\n * Deep Clustering\n   \n   * https://vissl.readthedocs.io/en/latest/ssl_approaches/deepclusterv2.html\n   * https://github.com/facebookresearch/vissl/blob/master/docs/source/ssl_approaches/deepclusterv2.rst\n\n * Simsiam\n   \n   * https://github.com/facebookresearch/simsiam\n\n孪生网络和对比学习有什么不同？\n\n * 孪生网络一般针对半监督问题设置\n * 对比学习一般针对无监督问题设置\n * \n\n2.2 Rotation Loss 是什么\n\n# 参考资料\n\n * [1] https://zhuanlan.zhihu.com/p/61215293\n\n * [2] https://zhuanlan.zhihu.com/p/110075024\n\n * [3] https://zhuanlan.zhihu.com/p/149983811\n\n * [4] https://zhuanlan.zhihu.com/p/82199561\n\n * [5] https://zhuanlan.zhihu.com/p/258562899\n\n * [6] https://zhuanlan.zhihu.com/p/87969454\n\n * [7] https://zhuanlan.zhihu.com/p/111644699",normalizedContent:"# few-shot learning 竞赛学习-1\n\n最近离一个比赛结束还有不到一周，准备来快速学习下 few-shot learning 看能不能摸到奖（大雾）\n\n# 01、问题描述\n\n1.1 数据描述\n\n * 训练集有 49990 张图像，分为10类，每张图像的尺寸为 32 x 32\n   * 其中有 20 张有标注，每类2张，剩下 49970 张图像无标注\n * 验证集有10类，每类1张\n * 测试集有 10000 张\n\n1.2 竞赛问题定义\n\n按照 few-shot learning 的定义来讲，训练集中随机抽取 c 个类别，每个类别 k 个样本（总共 ck 个数据），成为c-way k-shot 问题。\n\n该竞赛的任务被定义为 10-way 2-shot 问题，也就是 10 类，每类 2 张有标注\n\n# 02、竞赛学习\n\n# 2.1 参考资料[1] & 参考资料 [2]\n\n这里首先学习下参考资料[1]与参考资料[2]\n\n支撑集（support set）：即 c 类，每类 k 个样本所组成的 ck 个数据\n\n查询集（query set）：类似测试集，包含 q 张未分类图像即这里的 10000 张\n\n这里重点关注下 metric based 方法，看起来比较容易实现。metric based 方法通过度量 batch 集中的样本和 support 集中样本的距离，借助最近邻的思想完成分类。\n\n度量学习的基本思想是学习单个数据（如图像）之间的距离函数。它已被证明对于解决小样本分类任务非常有效：度量学习算法通过将查询集图像与已标记的支持集图像进行比较来进行分类。\n\n * 将支持集和查询集的所有图像提取 embedding，\n * 查询集中的每张图像都根据其与支持集图像的距离来进行分类，例如欧氏距离/余弦距离 以及 knn 算法\n\n可以使用孪生网络（siamese network）\n\n * 训练时，通过组合的方式构造不同的成对样本，输入网络进行训练，在最上层通过样本对的距离来判断他们是否属于同一类，并产生对应的概率分布。\n\n * 在预测阶段，将测试样本集和支撑集之间每一个样本对都进行推理，预测结果为支撑集上概率最高的类别\n\n可以使用原型网络（prototype network）\n\n * 每个类都存在一个原型表达，该类的原型是 support set 在 embedding 空间中的均值\n * 分类问题即可变成在 embedding 空间中的最近邻问题\n\nrelation network\n\n * 认为度量方式也是网络中重要的一环，需要对其进行建模\n\n该文章提出了统一的 encode-induction-relation 描述框架\n\n# 2.2 参考资料[3]\n\n参考资料[3] 解释了 meta-learning 方法的步骤\n\n训练时\n\n * 将训练集采样成 support set 以及 query set\n\n * 基于support set 生成分类模型 f\n\n * 利用模型 f 对 query set 进行分类预测生成 predict labels（pseudo-label）\n\n * 通过 query labels 和 predicted labels 进行 loss 的计算，从而更新网络参数\n\n测试时：\n\n * 利用分类模型 f 对 query set 进行预测\n\n# 2.3 参考资料[4]\n\n参考资料[4] 介绍了metric learning 中的损失函数\n\n * contrastive loss\n   * 输入：两个样本组成的样本对，label 为该样本对是否属于同一类\n   * 超参数 maigin，表示不同类样本之间的距离应该超过该 margin 值\n * triplet loss：\n   * 输入：一个三元组，query + positive sample + negative sample\n   * triplet loss 要求 query 到 负样本的距离与 query 到正样本的距离之差要大于 margin 值\n   * contrastive loss和triplet loss都很常用，一般来说，triplet-loss 的效果比 contrastive loss 的效果要好，因为他考虑了正负样本与锚点的距离关系。然而，这两种loss函数如果单独使用则会遭遇收敛速度慢的问题。在学习过程的后期，大多数样本都能满足损失函数的约束条件，这些样本对应进一步学习的贡献很小。因此，这两种损失函数都需要配合hard sample mining的学习策略一起使用，例如 facenet 提出的 simi-hard negative sample mining方法。\n * n-pair-ms loss\n   * 考虑 query 与多个类别的负样本之间的关系，促使 query 与其他所有类之间都保持距离，能够加快模型的收敛速度\n * lifted struct loss\n   * 基于 mini-batch 中的所有正负样本对来计算 loss\n   * 对于每一个正样本对{i，j}，挖掘出最困难的负样本，与 i 和 j 距离最近的负样本用于计算 triplet loss\n   * 优点：动态构建最困难的三元组\n * ranked list loss\n   * 上述方法的缺点\n     * 上述损失函数提出加入负样本来获得结构化的信息，但是使用的负样本只是一小部分\n     * 这些损失函数没有考虑类内的数据分布，都追求将同一类压缩到同一个点上\n   * \n * multi-similarity loss\n   * 自相似性：根据样本对自身计算出的相似性，这是一种最常用也是最重要的相似性。例如，当一个负样本对的余弦相似性较大时，意味着很难把该样本对所对应的两种类别区分开来，这样的样本对对模型来说是困难的，也是有信息量的，对于模型学习更有区分度的特征很有帮助。另一方面，自相似性很难完整地描述embedding空间的样本分布情况。\n   * 正相对相似性：不仅考虑当前样本对自身的相似性，还考虑局部邻域内正样本对之间的相对关系。\n   * 负相对相似性：不仅考虑当前样本对自身的相似性，还考虑局部邻域内负样本对之间的相对关系。\n\n# 2.4 参考资料 [5]\n\n[5] 中比较清晰地说出了几种方法\n\n * 度量学习：提特征，算距离，knn 等方式分类\n * fsl 定制的数据增强：增加数据的量用于 fine-tuning\n * 元学习：在任务级别上学习而不是在样本上学习\n\n# 2.5 paperwithcode sota 论文的方法\n\n * image clustering on cifar-100\n   * accuracy 0.584：https://paperswithcode.com/paper/spice-semantic-pseudo-labeling-for-image\n   * accuracy 0.543：https://paperswithcode.com/paper/improving-unsupervised-image-clustering-with\n   * accuracy 0.507：https://paperswithcode.com/paper/learning-to-classify-images-without-labels\n * few-shot image classification on cifar-fs 5-way (1-shot)\n   * accuracy 87.79：https://paperswithcode.com/paper/transfer-learning-based-few-shot\n   * accuracy 87.73：https://paperswithcode.com/paper/sill-net-feature-augmentation-with-separated\n   * accuracy 87.69：https://paperswithcode.com/paper/leveraging-the-feature-distribution-in\n\n# 2.6 kaggle 竞赛的方法\n\n参考资料[6] 和[7] 两篇是知乎 “砍手豪” 大神所写的 kaggle 上 few-shot 竞赛的文章，值得学习，可以先看看是否能找到 baseline.\n\n * tricks\n   \n   * 实现 lr finder\n\n * eda\n   \n   * 查找重复图像\n   * 模型可视化：https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563\n   * t-sne 特征可视化\n\n * siamese network 神经网络\n   \n   * siamese神经网络比较两个图像，并确定这两个图像是从同一条鲸鱼还是从不同的鲸鱼中获取\n   * https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563/data\n   * https://www.kaggle.com/seesee/siamese-pretrained-0-822\n   * https://www.kaggle.com/voglinio/siamese-two-pretrained-weights-0-855\n\n * metric learning 神经网络\n   \n   * metric learning 使用网络提取 embedding，再比较 embedding 之间的距离来进行分类\n   * https://www.kaggle.com/c/humpback-whale-identification/discussion/74647\n   * https://www.kaggle.com/iafoss/similarity-densenet121-0-805lb-kernel-time-limit\n\n * winner model\n   \n   * 1st solution: https://www.kaggle.com/c/humpback-whale-identification/discussion/82366\n     * 四通道：rgb + masks 作为输入（我们可以将rgb + edges + masks 作为输入）\n   * 2nd solution: https://www.kaggle.com/c/humpback-whale-identification/discussion/83885\n     * 三种 loss：arcface loss + triplet loss + focal loss\n     * 三个backbone：resnet101，seresnet101，seresnext101\n   * 4th solution: https://www.kaggle.com/c/humpback-whale-identification/discussion/82356\n     * 4th place solution: sift + siamese\n     * 介绍了一些降低复杂度的方法\n\n# 2.7 其他方法\n\n * deep clustering\n   \n   * https://vissl.readthedocs.io/en/latest/ssl_approaches/deepclusterv2.html\n   * https://github.com/facebookresearch/vissl/blob/master/docs/source/ssl_approaches/deepclusterv2.rst\n\n * simsiam\n   \n   * https://github.com/facebookresearch/simsiam\n\n孪生网络和对比学习有什么不同？\n\n * 孪生网络一般针对半监督问题设置\n * 对比学习一般针对无监督问题设置\n * \n\n2.2 rotation loss 是什么\n\n# 参考资料\n\n * [1] https://zhuanlan.zhihu.com/p/61215293\n\n * [2] https://zhuanlan.zhihu.com/p/110075024\n\n * [3] https://zhuanlan.zhihu.com/p/149983811\n\n * [4] https://zhuanlan.zhihu.com/p/82199561\n\n * [5] https://zhuanlan.zhihu.com/p/258562899\n\n * [6] https://zhuanlan.zhihu.com/p/87969454\n\n * [7] https://zhuanlan.zhihu.com/p/111644699",charsets:{cjk:!0},lastUpdated:"2021/09/12, 20:42:58"},{title:"DeepLab系列代码",frontmatter:{title:"DeepLab系列代码",date:"2021-10-14T12:58:11.000Z",permalink:"/pages/181ce5/",categories:["学习笔记","代码实践-图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/11.DeepLab%E7%B3%BB%E5%88%97%E4%BB%A3%E7%A0%81.html",relativePath:"02.学习笔记/02.代码实践-图像分割/11.DeepLab系列代码.md",key:"v-aae41b5e",path:"/pages/181ce5/",headersStr:null,content:"# 01 DeepLabv1\n\n * https://github.com/kazuto1011/deeplab-pytorch/blob/master/libs/models/deeplabv1.py\n\n# 02 DeepLabv2\n\nunofficial PyTorch implementation of DeepLab v2 [1] with a ResNet-101 backbone.\n\n * https://github.com/kazuto1011/deeplab-pytorch\n\n * https://github.com/kazuto1011/deeplab-pytorch/blob/master/libs/models/deeplabv2.py\n\n# 03 DeepLabv3\n\n * https://github.com/kazuto1011/deeplab-pytorch/blob/master/libs/models/deeplabv3.py\n\n# 04 DeepLabv3p\n\n * https://github.com/kazuto1011/deeplab-pytorch/blob/master/libs/models/deeplabv3plus.py\n\n * https://github.com/gyguo/awesome-weakly-supervised-semantic-segmentation-image",normalizedContent:"# 01 deeplabv1\n\n * https://github.com/kazuto1011/deeplab-pytorch/blob/master/libs/models/deeplabv1.py\n\n# 02 deeplabv2\n\nunofficial pytorch implementation of deeplab v2 [1] with a resnet-101 backbone.\n\n * https://github.com/kazuto1011/deeplab-pytorch\n\n * https://github.com/kazuto1011/deeplab-pytorch/blob/master/libs/models/deeplabv2.py\n\n# 03 deeplabv3\n\n * https://github.com/kazuto1011/deeplab-pytorch/blob/master/libs/models/deeplabv3.py\n\n# 04 deeplabv3p\n\n * https://github.com/kazuto1011/deeplab-pytorch/blob/master/libs/models/deeplabv3plus.py\n\n * https://github.com/gyguo/awesome-weakly-supervised-semantic-segmentation-image",charsets:{},lastUpdated:"2021/10/17, 16:23:18"},{title:"few-shot learning 竞赛学习-2",frontmatter:{title:"few-shot learning 竞赛学习-2",date:"2021-09-07T16:19:01.000Z",permalink:"/pages/8432b6/",categories:["学习笔记","竞赛笔记-视觉竞赛"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/04.%E7%AB%9E%E8%B5%9B%E7%AC%94%E8%AE%B0-%E8%A7%86%E8%A7%89%E7%AB%9E%E8%B5%9B/06.few-shot%20learning%20%E7%AB%9E%E8%B5%9B%E5%AD%A6%E4%B9%A0-2.html",relativePath:"02.学习笔记/04.竞赛笔记-视觉竞赛/06.few-shot learning 竞赛学习-2.md",key:"v-51bcc501",path:"/pages/8432b6/",headers:[{level:2,title:"Few-shot Learning 竞赛学习-2",slug:"few-shot-learning-竞赛学习-2",normalizedTitle:"few-shot learning 竞赛学习-2",charIndex:2}],headersStr:"Few-shot Learning 竞赛学习-2",content:'# Few-shot Learning 竞赛学习-2\n\n经过初步的调研和思考，觉得 Metric Learning 这种范式比较适合我快速上手\n\n最终决定使用目前 image clustering 任务 SOTA 结果的 SPICE 来做本次任务\n\n * SPICE 算法链接：https://paperswithcode.com/paper/spice-semantic-pseudo-labeling-for-image\n\n * SPICE 算法流程\n   \n   * 首先使用 MoCov2 算法无监督训练训 1000 个 epoch\n   * 使用预训练模型来提取 Embedding feature\n   * 训练 SPICE-Self\n   * 来区分 reliable images\n   * 训练 SPICE-Semi\n\n下面分步骤来解释下代码\n\n# 1、使用 MoCov2 算法无监督训练训 1000 个 epoch\n\n这里作者使用了 kaiming 开源的 moco代码 进行训练，稍微修改下即可，训练脚本如下：\n\npython train_moco.py --data_type prcv2021 --data ./datasets/prcv2021 --img_size 96 --save_folder ./results/prcv2021/moco --arch clusterresnet --start_epoch 0 --epochs 1000 --gpu 0 --resume ./results/prcv2021/moco/checkpoint_last.pth.tar --mlp --moco-t 0.2 --aug_plus --cos\n\n\n1\n\n\n# 2、使用预训练模型来提取 Embedding feature\n\n对所有训练样本（49900张图像）提取Embedding feature，核心代码如下：\n\n# create model\nmodel_sim = build_model_sim(cfg.model_sim)\n\n# Load similarity model\nif cfg.model_sim.pretrained is not None:\n    load_model_weights(model_sim, cfg.model_sim.pretrained, cfg.model_sim.model_type)\n    \n# Evalidation status\nmodel_sim.eval()\n\n# Define dataloader\ndataset_val = build_dataset(cfg.data_test)\nval_loader = torch.utils.data.DataLoader(dataset_val, batch_size=cfg.batch_size, shuffle=False, num_workers=1)\n\n# Define a AvgPool2d to pooling features\npool = nn.AdaptiveAvgPool2d(1)\nfeas_sim = []\n\nfor _, (images, _, labels, idx) in enumerate(val_loader):\n    images = images.to(cfg.gpu, non_blocking=True)\n    print(images.shape)\n    with torch.no_grad():\n        feas_sim_i = model_sim(images)\n        if len(feas_sim_i.shape) == 4:\n            feas_sim_i = pool(feas_sim_i)\n            feas_sim_i = torch.flatten(feas_sim_i, start_dim=1)\n        feas_sim_i = nn.functional.normalize(feas_sim_i, dim=1)\n        feas_sim.append(feas_sim_i.cpu())\n\nfeas_sim = torch.cat(feas_sim, dim=0)\nfeas_sim = feas_sim.numpy()\n\nnp.save("{}/feas_moco_512_l2.npy".format(cfg.results.output_dir), feas_sim)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n# 3、训练 SPICE-Self\n\n\n\nSPICE-Self 的流程如上图所示：\n\n * Unlabeled 经过 Weak 和 Strong 的数据增强，分别输入到上述预训练好的 CNN 中\n * Original 分支提取 Embedding，这里可以离线提取，如第二步\n * Weak 分支送入 CNN + CLSHead（10个Head），输出的 predicted probabilities 同Original 分支的Embedding 一起生成 Pseudo-Label\n * Strong 分支送入 CNN + CLSHead（10个head，与Weak分支共享参数），输出的 predicted probabilities 与Pseudo-Label 一起计算 DS-CE Loss来优化 Head\n\n注：\n\n * 上图的 CNN 和 CLSHead 都是共享参数的\n * CNN 是固定参数的，DS-CE Loss 只用来优化 CLSHead\n * 训练时训十个 CLSHead，预测时只用性能最优的 Head 推理\n\n\n\n上图是Pseudo labeling 的流程，\n\n * 计算每个样本和 Embedding Space 中 Prototype 的距离\n * 卡一个阈值来生成伪标签\n\n# 4、生成 Reliable Samples\n\n * 对于样本 xix_ixi ，根据 Embedding feature 间的余弦距离挑选 nsn_sns 个最近邻样本，所有样本的标签集合记为 LnsL_{n_s}Lns\n\n * 计算样本 xix_ixi 的局部一致性 βi\\beta_iβi\n   \n   * βi=1ns∑lj∈Lns1(lj=li)\\beta_i = \\frac{1}{n_s}\\sum\\limits_{l_j \\in L_{n_s}}1(l_j=l_i)βi =ns 1 lj ∈Lns ∑ 1(lj =li )\n   * 其实就是计算最近邻样本和自己标签一致的比例\n\n * 如果 βi>sc\\beta_i>s_cβi >sc , 则样本 xix_ixi 即可被视作 Reliable Samples\n\n * 将 Reliable Samples 作为已标注样本，剩余的作为未标注样本，由此将该问题转化为半监督问题，送入SPICE-Semi 算法框架中\n\n# 5、训练 SPICE-Semi\n\n\n\n * 将 Labeled 和 Unlabeled 样本经过 Weak 和 Strong 的数据增强，分别输入到上述预训练好的 CNN 中\n\n * Unlabeled Weak 分支输入到 CLS Model 中，生成 Unlabeled Prediction 作为伪标签\n\n * Unlabeled Strong 分支输入到 CLS Model 中，生成 Unlabeled Prediction，与伪标签做 CE Loss\n\n * labeled Weak 分支输入到 CLS Model 中，生成 Labeled Prediction，和 Reliable label 做 CE Loss\n\n# 6、与正确标签的索引相对应\n\n上述流程训出来的模型聚类效果可能会比较好，但是类别的索引可能会错乱，有多个解决方案\n\n# 7、实验结果\n\n不太行\n\n# 参考资料\n\n * SPICE paper: https://arxiv.org/abs/2103.09382v1',normalizedContent:'# few-shot learning 竞赛学习-2\n\n经过初步的调研和思考，觉得 metric learning 这种范式比较适合我快速上手\n\n最终决定使用目前 image clustering 任务 sota 结果的 spice 来做本次任务\n\n * spice 算法链接：https://paperswithcode.com/paper/spice-semantic-pseudo-labeling-for-image\n\n * spice 算法流程\n   \n   * 首先使用 mocov2 算法无监督训练训 1000 个 epoch\n   * 使用预训练模型来提取 embedding feature\n   * 训练 spice-self\n   * 来区分 reliable images\n   * 训练 spice-semi\n\n下面分步骤来解释下代码\n\n# 1、使用 mocov2 算法无监督训练训 1000 个 epoch\n\n这里作者使用了 kaiming 开源的 moco代码 进行训练，稍微修改下即可，训练脚本如下：\n\npython train_moco.py --data_type prcv2021 --data ./datasets/prcv2021 --img_size 96 --save_folder ./results/prcv2021/moco --arch clusterresnet --start_epoch 0 --epochs 1000 --gpu 0 --resume ./results/prcv2021/moco/checkpoint_last.pth.tar --mlp --moco-t 0.2 --aug_plus --cos\n\n\n1\n\n\n# 2、使用预训练模型来提取 embedding feature\n\n对所有训练样本（49900张图像）提取embedding feature，核心代码如下：\n\n# create model\nmodel_sim = build_model_sim(cfg.model_sim)\n\n# load similarity model\nif cfg.model_sim.pretrained is not none:\n    load_model_weights(model_sim, cfg.model_sim.pretrained, cfg.model_sim.model_type)\n    \n# evalidation status\nmodel_sim.eval()\n\n# define dataloader\ndataset_val = build_dataset(cfg.data_test)\nval_loader = torch.utils.data.dataloader(dataset_val, batch_size=cfg.batch_size, shuffle=false, num_workers=1)\n\n# define a avgpool2d to pooling features\npool = nn.adaptiveavgpool2d(1)\nfeas_sim = []\n\nfor _, (images, _, labels, idx) in enumerate(val_loader):\n    images = images.to(cfg.gpu, non_blocking=true)\n    print(images.shape)\n    with torch.no_grad():\n        feas_sim_i = model_sim(images)\n        if len(feas_sim_i.shape) == 4:\n            feas_sim_i = pool(feas_sim_i)\n            feas_sim_i = torch.flatten(feas_sim_i, start_dim=1)\n        feas_sim_i = nn.functional.normalize(feas_sim_i, dim=1)\n        feas_sim.append(feas_sim_i.cpu())\n\nfeas_sim = torch.cat(feas_sim, dim=0)\nfeas_sim = feas_sim.numpy()\n\nnp.save("{}/feas_moco_512_l2.npy".format(cfg.results.output_dir), feas_sim)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n# 3、训练 spice-self\n\n\n\nspice-self 的流程如上图所示：\n\n * unlabeled 经过 weak 和 strong 的数据增强，分别输入到上述预训练好的 cnn 中\n * original 分支提取 embedding，这里可以离线提取，如第二步\n * weak 分支送入 cnn + clshead（10个head），输出的 predicted probabilities 同original 分支的embedding 一起生成 pseudo-label\n * strong 分支送入 cnn + clshead（10个head，与weak分支共享参数），输出的 predicted probabilities 与pseudo-label 一起计算 ds-ce loss来优化 head\n\n注：\n\n * 上图的 cnn 和 clshead 都是共享参数的\n * cnn 是固定参数的，ds-ce loss 只用来优化 clshead\n * 训练时训十个 clshead，预测时只用性能最优的 head 推理\n\n\n\n上图是pseudo labeling 的流程，\n\n * 计算每个样本和 embedding space 中 prototype 的距离\n * 卡一个阈值来生成伪标签\n\n# 4、生成 reliable samples\n\n * 对于样本 xix_ixi ，根据 embedding feature 间的余弦距离挑选 nsn_sns 个最近邻样本，所有样本的标签集合记为 lnsl_{n_s}lns\n\n * 计算样本 xix_ixi 的局部一致性 βi\\beta_iβi\n   \n   * βi=1ns∑lj∈lns1(lj=li)\\beta_i = \\frac{1}{n_s}\\sum\\limits_{l_j \\in l_{n_s}}1(l_j=l_i)βi =ns 1 lj ∈lns ∑ 1(lj =li )\n   * 其实就是计算最近邻样本和自己标签一致的比例\n\n * 如果 βi>sc\\beta_i>s_cβi >sc , 则样本 xix_ixi 即可被视作 reliable samples\n\n * 将 reliable samples 作为已标注样本，剩余的作为未标注样本，由此将该问题转化为半监督问题，送入spice-semi 算法框架中\n\n# 5、训练 spice-semi\n\n\n\n * 将 labeled 和 unlabeled 样本经过 weak 和 strong 的数据增强，分别输入到上述预训练好的 cnn 中\n\n * unlabeled weak 分支输入到 cls model 中，生成 unlabeled prediction 作为伪标签\n\n * unlabeled strong 分支输入到 cls model 中，生成 unlabeled prediction，与伪标签做 ce loss\n\n * labeled weak 分支输入到 cls model 中，生成 labeled prediction，和 reliable label 做 ce loss\n\n# 6、与正确标签的索引相对应\n\n上述流程训出来的模型聚类效果可能会比较好，但是类别的索引可能会错乱，有多个解决方案\n\n# 7、实验结果\n\n不太行\n\n# 参考资料\n\n * spice paper: https://arxiv.org/abs/2103.09382v1',charsets:{cjk:!0},lastUpdated:"2021/09/12, 20:42:58"},{title:"遥感图像建筑物变化检测竞赛学习-1",frontmatter:{title:"遥感图像建筑物变化检测竞赛学习-1",date:"2021-09-12T23:08:24.000Z",permalink:"/pages/59507a/",categories:["学习笔记","竞赛笔记-视觉竞赛"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/04.%E7%AB%9E%E8%B5%9B%E7%AC%94%E8%AE%B0-%E8%A7%86%E8%A7%89%E7%AB%9E%E8%B5%9B/07.%E9%81%A5%E6%84%9F%E5%9B%BE%E5%83%8F%E5%BB%BA%E7%AD%91%E7%89%A9%E5%8F%98%E5%8C%96%E6%A3%80%E6%B5%8B%E7%AB%9E%E8%B5%9B%E5%AD%A6%E4%B9%A0-1.html",relativePath:"02.学习笔记/04.竞赛笔记-视觉竞赛/07.遥感图像建筑物变化检测竞赛学习-1.md",key:"v-58142f35",path:"/pages/59507a/",headersStr:null,content:"数据集介绍\n\n * 训练集：6000对图像\n\n * 验证集：2000对图像\n\n * 测试样本：2000对图像\n\n * 标注样例如下，包含涉及建筑物的地物变化\n\n\n\nhttps://github.com/LiheYoung/SenseEarth2020-ChangeDetection",normalizedContent:"数据集介绍\n\n * 训练集：6000对图像\n\n * 验证集：2000对图像\n\n * 测试样本：2000对图像\n\n * 标注样例如下，包含涉及建筑物的地物变化\n\n\n\nhttps://github.com/liheyoung/senseearth2020-changedetection",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"MMClassifiction 框架学习导言",frontmatter:{title:"MMClassifiction 框架学习导言",date:"2021-05-17T21:51:53.000Z",permalink:"/pages/fbe79c/",categories:["技术文章","MMClassification"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/00.MMClassifiction%20%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%A8%80.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/00.MMClassifiction 框架学习导言.md",key:"v-7d0d935a",path:"/pages/fbe79c/",headers:[{level:3,title:"MMClassifiction 框架学习导言",slug:"mmclassifiction-框架学习导言",normalizedTitle:"mmclassifiction 框架学习导言",charIndex:2}],headersStr:"MMClassifiction 框架学习导言",content:"# MMClassifiction 框架学习导言\n\n# 1 前言\n\n因为想要搭一个自己分类以及分割的code base，想要先学习一下优秀的框架设计理念，所以先从较简单的图像分类看起，看一下 MMClassification 的设计架构以及代码风格，为之后的学术和竞赛搬砖搭一个顺手的框架\n\n# 2 学习路径\n\n第一阶段\n\n * 首先了解是如何写config以及通过名称即能调用的（应该是注册）\n * 了解 mmcls.dataset 是如何加载数据的\n * 了解 mmcls.models 是如何搭建模型的\n * 了解 logger 以及 tensorboard 等feature\n * 目标：能够用类似的 config 形式调用模型，能够做一个自定义数据集的图像分类任务，并且有日志\n\n第二阶段\n\n * 支持多标签分类任务\n\n * 添加 feature，例如 fp16、NNI、CAM等\n\n * 了解 tests，完善测试流程\n\n * 添加 tools\n\n * 目标：能够有一些实用的小工具，能够 fp16 训练等\n\n第三阶段\n\n * 和落地部署结合起来\n * 目标：在树莓派上部署ONNX、TensorRT等",normalizedContent:"# mmclassifiction 框架学习导言\n\n# 1 前言\n\n因为想要搭一个自己分类以及分割的code base，想要先学习一下优秀的框架设计理念，所以先从较简单的图像分类看起，看一下 mmclassification 的设计架构以及代码风格，为之后的学术和竞赛搬砖搭一个顺手的框架\n\n# 2 学习路径\n\n第一阶段\n\n * 首先了解是如何写config以及通过名称即能调用的（应该是注册）\n * 了解 mmcls.dataset 是如何加载数据的\n * 了解 mmcls.models 是如何搭建模型的\n * 了解 logger 以及 tensorboard 等feature\n * 目标：能够用类似的 config 形式调用模型，能够做一个自定义数据集的图像分类任务，并且有日志\n\n第二阶段\n\n * 支持多标签分类任务\n\n * 添加 feature，例如 fp16、nni、cam等\n\n * 了解 tests，完善测试流程\n\n * 添加 tools\n\n * 目标：能够有一些实用的小工具，能够 fp16 训练等\n\n第三阶段\n\n * 和落地部署结合起来\n * 目标：在树莓派上部署onnx、tensorrt等",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"mmcls 是如何能够通过config 就搭建好一个模型的？",frontmatter:{title:"mmcls 是如何能够通过config 就搭建好一个模型的？",date:"2021-05-17T22:06:15.000Z",permalink:"/pages/a070a4/",categories:["技术文章","MMClassification"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/01.mmcls%20%E6%98%AF%E5%A6%82%E4%BD%95%E8%83%BD%E5%A4%9F%E9%80%9A%E8%BF%87config%20%E5%B0%B1%E6%90%AD%E5%BB%BA%E5%A5%BD%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B%E7%9A%84%EF%BC%9F.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/01.mmcls 是如何能够通过config 就搭建好一个模型的？.md",key:"v-087fae29",path:"/pages/a070a4/",headers:[{level:2,title:"MMClassification 是如何能够通过config 就搭建好一个模型的？",slug:"mmclassification-是如何能够通过config-就搭建好一个模型的",normalizedTitle:"mmclassification 是如何能够通过config 就搭建好一个模型的？",charIndex:2}],headersStr:"MMClassification 是如何能够通过config 就搭建好一个模型的？",content:"# MMClassification 是如何能够通过config 就搭建好一个模型的？",normalizedContent:"# mmclassification 是如何能够通过config 就搭建好一个模型的？",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"为自己的 inicls 框架加上 fp16 训练",frontmatter:{title:"为自己的 inicls 框架加上 fp16 训练",date:"2021-07-13T21:14:11.000Z",permalink:"/pages/e677b8/",categories:["技术文章","MMClassification"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/02.%E4%B8%BA%E8%87%AA%E5%B7%B1%E7%9A%84%20inicls%20%E6%A1%86%E6%9E%B6%E5%8A%A0%E4%B8%8A%20fp16%20%E8%AE%AD%E7%BB%83.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/02.为自己的 inicls 框架加上 fp16 训练.md",key:"v-25353850",path:"/pages/e677b8/",headers:[{level:3,title:"01、如何实现",slug:"_01、如何实现",normalizedTitle:"01、如何实现",charIndex:31},{level:3,title:"02、效果比较",slug:"_02、效果比较",normalizedTitle:"02、效果比较",charIndex:696},{level:3,title:"03、loss 变为nan",slug:"_03、loss-变为nan",normalizedTitle:"03、loss 变为nan",charIndex:2083},{level:3,title:"04、mmcls 是如何实现 fp16 训练的呢",slug:"_04、mmcls-是如何实现-fp16-训练的呢",normalizedTitle:"04、mmcls 是如何实现 fp16 训练的呢",charIndex:2630}],headersStr:"01、如何实现 02、效果比较 03、loss 变为nan 04、mmcls 是如何实现 fp16 训练的呢",content:'# 为自己的 inicls 框架加上 fp16 训练\n\n\n# 01、如何实现\n\n不得不说现在的库封装的都特别好，核心代码就加了几行\n\nfrom torch.cuda.amp import GradScaler, autocast\n\n\nfor iteration in range(max_iteration):\n    optimizer.zero_grad()\n\n    if cfg.fp16 is True:\n        with autocast():\n            logits = model(images)\n            loss = criterion(logits, labels).mean()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        logits = model(images)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\nif cfg.fp16:\n\twith autocast():\n\t\tlogits = model(images)\nelse:\n\tlogits = model(images)\n\n\n1\n2\n3\n4\n5\n\n\n\n# 02、效果比较\n\n# 2.1 脚本介绍\n\n实验的脚本非常简单，不得不说搭了这个框架之后做一些普通实验是真的很方便\n\nexport CUDA_VISIBLE_DEVICES=0\npython train.py config/resnet/resnet18_b16x8_kaggle_leaves.py --tag resnet_fp16 --options "fp16=True" "data.train.ann_file=train_fold0.csv" "data.val.ann_file=valid_fold0.csv"\n\npython train.py config/resnet/resnet18_b16x8_kaggle_leaves.py --tag resnet_not_fp16 --options "data.train.ann_file=train_fold0.csv" "data.val.ann_file=valid_fold0.csv"\n\n\n1\n2\n3\n4\n\n\n# 2.2 速度、显存、性能对比\n\n实验名称                         学习率     验证集上最优精度   训练时间     显存占用      备注\nresnet_fp16_lr_0.1           0.1     0.9553     13m27s   2300MiB   在 6k iteration 之后 loss 变为 nan\nresnet_fp16_lr_0.01          0.01    0.9455     12m59s   2300MiB   在 5k iteration 之后 loss 变为 nan\nresnet_fp16_lr_0.001         0.001   0.7126     13m31s   2300MiB   在 4k iteration 之后 loss 变为 nan\nresnet_nofp16_lr_0.1         0.1     0.9651     19m19s             未使用 fp16\nresnet_fp16_lr_0.1_v3        0.1     0.9635     12m10s   4360MiB   batch_size 改为 256，并且 loss 并未变为 nan\nresnet_fp16_lr_0.1_v4        0.1     0.9624     50m29s   4360MiB   batch_size 改为 256，并且将epoch 改为 200，在 8k iteration 之后 loss 变为\n                                                                   nan\nresnet_fp16_lr_0.1_v7_apex   0.1     0.933      16m2s    2300MiB   使用apex.amp，模式为01\nresnet_fp16_lr_0.1_v8        0.1     0.9654     13m28s   2300MiB   将 optimizer.step() 写到 else 语句里面\n\n\n# 03、loss 变为nan\n\n在我用默认配置跑的时候，发现在 7k 左右的 iteration ，loss 会突变为 nan，目前尚不知是什么原因，先尝试了把学习率给调低，并没有什么影响\n\n然后把 batch_size调高了发现不会出现nan了，\n\n下面尝试讲 batch_size 调高的同时加大epoch数\n\n破案了，发现是 optimizer 重复step了\n\nif cfg.fp16 is True:\n    with autocast():\n        logits = model(images)\n        loss = criterion(logits, labels)\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\nelse:\n    logits = model(images)\n    loss = criterion(logits, labels)\n    loss.backward()\noptimizer.step()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n将 optimizer.step() 写到里面即可\n\n\n# 04、mmcls 是如何实现 fp16 训练的呢\n\n参考资料：\n\n * PyTorch 源码解读之 torch.cuda.amp: 自动混合精度详解\n * 浅谈混合精度训练 imagenet\n * torch.cuda.amp 官方文档\n * torch.cuda.amp 官方示例',normalizedContent:'# 为自己的 inicls 框架加上 fp16 训练\n\n\n# 01、如何实现\n\n不得不说现在的库封装的都特别好，核心代码就加了几行\n\nfrom torch.cuda.amp import gradscaler, autocast\n\n\nfor iteration in range(max_iteration):\n    optimizer.zero_grad()\n\n    if cfg.fp16 is true:\n        with autocast():\n            logits = model(images)\n            loss = criterion(logits, labels).mean()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        logits = model(images)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\nif cfg.fp16:\n\twith autocast():\n\t\tlogits = model(images)\nelse:\n\tlogits = model(images)\n\n\n1\n2\n3\n4\n5\n\n\n\n# 02、效果比较\n\n# 2.1 脚本介绍\n\n实验的脚本非常简单，不得不说搭了这个框架之后做一些普通实验是真的很方便\n\nexport cuda_visible_devices=0\npython train.py config/resnet/resnet18_b16x8_kaggle_leaves.py --tag resnet_fp16 --options "fp16=true" "data.train.ann_file=train_fold0.csv" "data.val.ann_file=valid_fold0.csv"\n\npython train.py config/resnet/resnet18_b16x8_kaggle_leaves.py --tag resnet_not_fp16 --options "data.train.ann_file=train_fold0.csv" "data.val.ann_file=valid_fold0.csv"\n\n\n1\n2\n3\n4\n\n\n# 2.2 速度、显存、性能对比\n\n实验名称                         学习率     验证集上最优精度   训练时间     显存占用      备注\nresnet_fp16_lr_0.1           0.1     0.9553     13m27s   2300mib   在 6k iteration 之后 loss 变为 nan\nresnet_fp16_lr_0.01          0.01    0.9455     12m59s   2300mib   在 5k iteration 之后 loss 变为 nan\nresnet_fp16_lr_0.001         0.001   0.7126     13m31s   2300mib   在 4k iteration 之后 loss 变为 nan\nresnet_nofp16_lr_0.1         0.1     0.9651     19m19s             未使用 fp16\nresnet_fp16_lr_0.1_v3        0.1     0.9635     12m10s   4360mib   batch_size 改为 256，并且 loss 并未变为 nan\nresnet_fp16_lr_0.1_v4        0.1     0.9624     50m29s   4360mib   batch_size 改为 256，并且将epoch 改为 200，在 8k iteration 之后 loss 变为\n                                                                   nan\nresnet_fp16_lr_0.1_v7_apex   0.1     0.933      16m2s    2300mib   使用apex.amp，模式为01\nresnet_fp16_lr_0.1_v8        0.1     0.9654     13m28s   2300mib   将 optimizer.step() 写到 else 语句里面\n\n\n# 03、loss 变为nan\n\n在我用默认配置跑的时候，发现在 7k 左右的 iteration ，loss 会突变为 nan，目前尚不知是什么原因，先尝试了把学习率给调低，并没有什么影响\n\n然后把 batch_size调高了发现不会出现nan了，\n\n下面尝试讲 batch_size 调高的同时加大epoch数\n\n破案了，发现是 optimizer 重复step了\n\nif cfg.fp16 is true:\n    with autocast():\n        logits = model(images)\n        loss = criterion(logits, labels)\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\nelse:\n    logits = model(images)\n    loss = criterion(logits, labels)\n    loss.backward()\noptimizer.step()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n将 optimizer.step() 写到里面即可\n\n\n# 04、mmcls 是如何实现 fp16 训练的呢\n\n参考资料：\n\n * pytorch 源码解读之 torch.cuda.amp: 自动混合精度详解\n * 浅谈混合精度训练 imagenet\n * torch.cuda.amp 官方文档\n * torch.cuda.amp 官方示例',charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"为自己的 inicls 框架集成 Horovod",frontmatter:{title:"为自己的 inicls 框架集成 Horovod",date:"2021-07-15T23:42:44.000Z",permalink:"/pages/5a9505/",categories:["技术文章","MMClassification"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/03.%E4%B8%BA%E8%87%AA%E5%B7%B1%E7%9A%84%20inicls%20%E6%A1%86%E6%9E%B6%E9%9B%86%E6%88%90%20Horovod.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/03.为自己的 inicls 框架集成 Horovod.md",key:"v-d022b7f6",path:"/pages/5a9505/",headers:[{level:2,title:"为自己的 inicls 框架集成 Horovod",slug:"为自己的-inicls-框架集成-horovod",normalizedTitle:"为自己的 inicls 框架集成 horovod",charIndex:2}],headersStr:"为自己的 inicls 框架集成 Horovod",content:'# 为自己的 inicls 框架集成 Horovod\n\n1、改动代码\n\nimport torch\nimport horovod.torch as hvd\n\n# Initialize Horovod\nhvd.init()\n\n# Pin GPU to be used to process local rank (one GPU per process)\ntorch.cuda.set_device(hvd.local_rank())\n\n# Define dataset...\ntrain_dataset = ...\n\n# Partition dataset among workers using DistributedSampler\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(\n    train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=..., sampler=train_sampler)\n\n# Build model...\nmodel = ...\nmodel.cuda()\n\noptimizer = optim.SGD(model.parameters())\n\n# Add Horovod Distributed Optimizer\noptimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n\n# Broadcast parameters from rank 0 to all other processes.\nhvd.broadcast_parameters(model.state_dict(), root_rank=0)\n\nfor epoch in range(100):\n   for batch_idx, (data, target) in enumerate(train_loader):\n       optimizer.zero_grad()\n       output = model(data)\n       loss = F.nll_loss(output, target)\n       loss.backward()\n       optimizer.step()\n       if batch_idx % args.log_interval == 0:\n           print(\'Train Epoch: {} [{}/{}]\\tLoss: {}\'.format(\n               epoch, batch_idx * len(data), len(train_sampler), loss.item()))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n2、训练脚本\n\nexport CUDA_VISIBLE_DEVICES=0,1\npython train.py config/resnet/resnet18_b16x8_kaggle_leaves.py --tag resnet_fp16_lr_0.1_horovod --options "fp16=True" "data.train.ann_file=train_fold0.csv" "data.val.ann_file=valid_fold0.csv"\n\n\n\n1\n2\n3\n\n\n3、实验结果\n\n实验名称                              BATCH_SIZE   学习率   验证集上最优精度   训练时间     显存占用\nresnet_fp16_lr_0.1_horovod        64           0.1   0.9327     19m30s   每张卡 1600MiB\nresnet_fp16_lr_0.1_horovod_gpu0   64           0.1   0.9349     12m53s   GPU0 占用2100MiB\nresnet_fp16_lr_0.1_horovod_gpu1   64           0.1   0.9324     16m6s    GPU1 占用2100MiB\n\n# 参考资料\n\n * https://github.com/horovod/horovod/blob/master/docs/pytorch.rst',normalizedContent:'# 为自己的 inicls 框架集成 horovod\n\n1、改动代码\n\nimport torch\nimport horovod.torch as hvd\n\n# initialize horovod\nhvd.init()\n\n# pin gpu to be used to process local rank (one gpu per process)\ntorch.cuda.set_device(hvd.local_rank())\n\n# define dataset...\ntrain_dataset = ...\n\n# partition dataset among workers using distributedsampler\ntrain_sampler = torch.utils.data.distributed.distributedsampler(\n    train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n\ntrain_loader = torch.utils.data.dataloader(train_dataset, batch_size=..., sampler=train_sampler)\n\n# build model...\nmodel = ...\nmodel.cuda()\n\noptimizer = optim.sgd(model.parameters())\n\n# add horovod distributed optimizer\noptimizer = hvd.distributedoptimizer(optimizer, named_parameters=model.named_parameters())\n\n# broadcast parameters from rank 0 to all other processes.\nhvd.broadcast_parameters(model.state_dict(), root_rank=0)\n\nfor epoch in range(100):\n   for batch_idx, (data, target) in enumerate(train_loader):\n       optimizer.zero_grad()\n       output = model(data)\n       loss = f.nll_loss(output, target)\n       loss.backward()\n       optimizer.step()\n       if batch_idx % args.log_interval == 0:\n           print(\'train epoch: {} [{}/{}]\\tloss: {}\'.format(\n               epoch, batch_idx * len(data), len(train_sampler), loss.item()))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n2、训练脚本\n\nexport cuda_visible_devices=0,1\npython train.py config/resnet/resnet18_b16x8_kaggle_leaves.py --tag resnet_fp16_lr_0.1_horovod --options "fp16=true" "data.train.ann_file=train_fold0.csv" "data.val.ann_file=valid_fold0.csv"\n\n\n\n1\n2\n3\n\n\n3、实验结果\n\n实验名称                              batch_size   学习率   验证集上最优精度   训练时间     显存占用\nresnet_fp16_lr_0.1_horovod        64           0.1   0.9327     19m30s   每张卡 1600mib\nresnet_fp16_lr_0.1_horovod_gpu0   64           0.1   0.9349     12m53s   gpu0 占用2100mib\nresnet_fp16_lr_0.1_horovod_gpu1   64           0.1   0.9324     16m6s    gpu1 占用2100mib\n\n# 参考资料\n\n * https://github.com/horovod/horovod/blob/master/docs/pytorch.rst',charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"为自己的 inicls 框架集成 DALI",frontmatter:{title:"为自己的 inicls 框架集成 DALI",date:"2021-07-16T00:05:37.000Z",permalink:"/pages/37ee3e/",categories:["技术文章","MMClassification"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/04.%E4%B8%BA%E8%87%AA%E5%B7%B1%E7%9A%84%20inicls%20%E6%A1%86%E6%9E%B6%E9%9B%86%E6%88%90%20DALI.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/04.为自己的 inicls 框架集成 DALI.md",key:"v-1aa0d565",path:"/pages/37ee3e/",headers:[{level:2,title:"为自己的 inicls 框架集成 DALI",slug:"为自己的-inicls-框架集成-dali",normalizedTitle:"为自己的 inicls 框架集成 dali",charIndex:2}],headersStr:"为自己的 inicls 框架集成 DALI",content:"# 为自己的 inicls 框架集成 DALI\n\n01、安装 DALI\n\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110\n\n\n1\n\n\n02、在 PyTorch 框架里集成 DALI 框架\n\n03、参考资料\n\n * 官方安装文档\n\n * 官方示例",normalizedContent:"# 为自己的 inicls 框架集成 dali\n\n01、安装 dali\n\npip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110\n\n\n1\n\n\n02、在 pytorch 框架里集成 dali 框架\n\n03、参考资料\n\n * 官方安装文档\n\n * 官方示例",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"mmcv 使用",frontmatter:{title:"mmcv 使用",date:"2021-04-17T13:26:20.000Z",permalink:"/pages/944bac/",categories:["技术文章","框架解析"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/08.mmcv%20%E4%BD%BF%E7%94%A8%EF%BC%88%E4%B8%8A%EF%BC%89--Fileio&Image.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/08.mmcv 使用（上）--Fileio&Image.md",key:"v-7d2a1f50",path:"/pages/944bac/",headers:[{level:3,title:"mmcv 使用",slug:"mmcv-使用",normalizedTitle:"mmcv 使用",charIndex:2}],headersStr:"mmcv 使用",content:"# mmcv 使用\n\n最近发现 open-mmlab 开发的 mmcv 比较好用，故在此记录下使用说明，用的比较多的应该是File IO 和 Image 的相关内容\n\n# 文件读写（file_handlers）\n\n对外提供统一的文件读写API，例如下面的示例\n\nimport mmcv\n\n# load data from a file\ndata = mmcv.load('test.json')\ndata = mmcv.load('test.yaml')\ndata = mmcv.load('test.pkl')\n# load data from a file-like object\nwith open('test.json', 'r') as f:\n    data = mmcv.load(f, file_format='json')\n\n# dump data to a string\njson_str = mmcv.dump(data, file_format='json')\n\n# dump data to a file with a filename (infer format from file extension)\nmmcv.dump(data, 'out.pkl')\n\n# dump data to a file with a file-like object\nwith open('test.yaml', 'w') as f:\n    data = mmcv.dump(data, f, file_format='yaml')\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n# 图像操作\n\n# 图像的读写和展示\n\nimport mmcv\n\nimg = mmcv.imread('test.jpg')\nimg = mmcv.imread('test.jpg', flag='grayscale')\nimg_ = mmcv.imread(img) # nothing will happen, img_ = img\nmmcv.imwrite(img, 'out.jpg')\n\n\n1\n2\n3\n4\n5\n6\n\n\nmmcv.imshow('tests/data/color.jpg')\n# this is equivalent to\n\nfor i in range(10):\n    img = np.random.randint(256, size=(100, 100, 3), dtype=np.uint8)\n    mmcv.imshow(img, win_name='test image', wait_time=200)\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 图像的 Resize\n\n# resize to a given size\nmmcv.imresize(img, (1000, 600), return_scale=True)\n\n# resize to the same size of another image\nmmcv.imresize_like(img, dst_img, return_scale=False)\n\n# resize by a ratio\nmmcv.imrescale(img, 0.5)\n\n# resize so that the max edge no longer than 1000, short edge no longer than 800\n# without changing the aspect ratio\nmmcv.imrescale(img, (1000, 800))\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n参考资料\n\n * https://mmcv.readthedocs.io/en/latest/io.html\n * https://mmcv.readthedocs.io/en/latest/image.html",normalizedContent:"# mmcv 使用\n\n最近发现 open-mmlab 开发的 mmcv 比较好用，故在此记录下使用说明，用的比较多的应该是file io 和 image 的相关内容\n\n# 文件读写（file_handlers）\n\n对外提供统一的文件读写api，例如下面的示例\n\nimport mmcv\n\n# load data from a file\ndata = mmcv.load('test.json')\ndata = mmcv.load('test.yaml')\ndata = mmcv.load('test.pkl')\n# load data from a file-like object\nwith open('test.json', 'r') as f:\n    data = mmcv.load(f, file_format='json')\n\n# dump data to a string\njson_str = mmcv.dump(data, file_format='json')\n\n# dump data to a file with a filename (infer format from file extension)\nmmcv.dump(data, 'out.pkl')\n\n# dump data to a file with a file-like object\nwith open('test.yaml', 'w') as f:\n    data = mmcv.dump(data, f, file_format='yaml')\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n# 图像操作\n\n# 图像的读写和展示\n\nimport mmcv\n\nimg = mmcv.imread('test.jpg')\nimg = mmcv.imread('test.jpg', flag='grayscale')\nimg_ = mmcv.imread(img) # nothing will happen, img_ = img\nmmcv.imwrite(img, 'out.jpg')\n\n\n1\n2\n3\n4\n5\n6\n\n\nmmcv.imshow('tests/data/color.jpg')\n# this is equivalent to\n\nfor i in range(10):\n    img = np.random.randint(256, size=(100, 100, 3), dtype=np.uint8)\n    mmcv.imshow(img, win_name='test image', wait_time=200)\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 图像的 resize\n\n# resize to a given size\nmmcv.imresize(img, (1000, 600), return_scale=true)\n\n# resize to the same size of another image\nmmcv.imresize_like(img, dst_img, return_scale=false)\n\n# resize by a ratio\nmmcv.imrescale(img, 0.5)\n\n# resize so that the max edge no longer than 1000, short edge no longer than 800\n# without changing the aspect ratio\nmmcv.imrescale(img, (1000, 800))\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n参考资料\n\n * https://mmcv.readthedocs.io/en/latest/io.html\n * https://mmcv.readthedocs.io/en/latest/image.html",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"mmcv使用（中）--Config",frontmatter:{title:"mmcv使用（中）--Config",date:"2021-04-17T15:06:19.000Z",permalink:"/pages/125b38/",categories:["技术文章","框架解析"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/09.mmcv%E4%BD%BF%E7%94%A8%EF%BC%88%E4%B8%AD%EF%BC%89--Config.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/09.mmcv使用（中）--Config.md",key:"v-735b9b2c",path:"/pages/125b38/",headersStr:null,content:"# 参考资料\n\n * MMCV 核心组件分析(四): Config",normalizedContent:"# 参考资料\n\n * mmcv 核心组件分析(四): config",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"什么是 Register",frontmatter:{title:"什么是 Register",date:"2021-07-23T21:37:06.000Z",permalink:"/pages/16f307/",categories:["技术文章","MMClassification"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/10.%E4%BB%80%E4%B9%88%E6%98%AF%20Register.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/10.什么是 Register.md",key:"v-0ce6a857",path:"/pages/16f307/",headers:[{level:2,title:"什么是 Register",slug:"什么是-register",normalizedTitle:"什么是 register",charIndex:2},{level:3,title:"01、Register 的核心是装饰器",slug:"_01、register-的核心是装饰器",normalizedTitle:"01、register 的核心是装饰器",charIndex:19},{level:3,title:"02、mmcv 中 Registry 类的实现",slug:"_02、mmcv-中-registry-类的实现",normalizedTitle:"02、mmcv 中 registry 类的实现",charIndex:126}],headersStr:"什么是 Register 01、Register 的核心是装饰器 02、mmcv 中 Registry 类的实现",content:"# 什么是 Register\n\n\n# 01、Register 的核心是装饰器\n\n@ 是 python 的装饰语法糖\n\n@decorate\ndef func():\n\n\n1\n2\n\n\n等价于：\n\nfunc = decorate(func)\n\n\n1\n\n\n\n# 02、mmcv 中 Registry 类的实现\n\n# 2.1 核心逻辑\n\nclass Registry:\n    def __init__(self, name):\n        # 可实现注册类细分功能\n        self._name = name \n        # 内部核心内容，维护所有的已经注册好的 class\n        self._module_dict = dict()\n\n    def _register_module(self, module_class, module_name=None, force=False):\n        if not inspect.isclass(module_class):\n            raise TypeError('module must be a class, '\n                            f'but got {type(module_class)}')\n\n        if module_name is None:\n            module_name = module_class.__name__\n        if not force and module_name in self._module_dict:\n            raise KeyError(f'{module_name} is already registered '\n                           f'in {self.name}')\n        # 最核心代码\n        self._module_dict[module_name] = module_class\n\n    # 装饰器函数\n    def register_module(self, name=None, force=False, module=None):\n        if module is not None:\n            # 如果已经是 module，那就知道 增加到字典中即可\n            self._register_module(\n                module_class=module, module_name=name, force=force)\n            return module\n\n        # 最标准用法\n        # use it as a decorator: @x.register_module()\n        def _register(cls):\n            self._register_module(\n                module_class=cls, module_name=name, force=force)\n            return cls\n        return _register\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n# 2.2 类实例化\n\n在 MMCV 中所有的类实例化都是通过 build_from_cfg 函数实现，做的事情非常简单，就是给定 module_name，然后从 self._module_dict 提取即可。\n\ndef build_from_cfg(cfg, registry, default_args=None):\n    args = cfg.copy()\n\n    if default_args is not None:\n        for name, value in default_args.items():\n            args.setdefault(name, value)\n\n    obj_type = args.pop('type') # 注册 str 类名\n    if is_str(obj_type):\n        # 相当于 self._module_dict[obj_type]\n        obj_cls = registry.get(obj_type)\n        if obj_cls is None:\n            raise KeyError(\n                f'{obj_type} is not in the {registry.name} registry')\n\n    # 如果已经实例化了，那就直接返回\n    elif inspect.isclass(obj_type):\n        obj_cls = obj_type\n    else:\n        raise TypeError(\n            f'type must be a str or valid type, but got {type(obj_type)}')\n\n    # 最终初始化对于类，并且返回，就完成了一个类的实例化过程\n    return obj_cls(**args)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n# 2.3 完整的使用例子\n\n * 先创建一个 Register\n * 然后注册一个 module\n * 然后根据 cfg 以及 Register 调用\n\nCONVERTERS = Registry('converter')\n\n@CONVERTERS.register_module()\nclass Converter1(object):\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\nconverter_cfg = dict(type='Converter1', a=a_value, b=b_value)\nconverter = build_from_cfg(converter_cfg,CONVERTERS)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n",normalizedContent:"# 什么是 register\n\n\n# 01、register 的核心是装饰器\n\n@ 是 python 的装饰语法糖\n\n@decorate\ndef func():\n\n\n1\n2\n\n\n等价于：\n\nfunc = decorate(func)\n\n\n1\n\n\n\n# 02、mmcv 中 registry 类的实现\n\n# 2.1 核心逻辑\n\nclass registry:\n    def __init__(self, name):\n        # 可实现注册类细分功能\n        self._name = name \n        # 内部核心内容，维护所有的已经注册好的 class\n        self._module_dict = dict()\n\n    def _register_module(self, module_class, module_name=none, force=false):\n        if not inspect.isclass(module_class):\n            raise typeerror('module must be a class, '\n                            f'but got {type(module_class)}')\n\n        if module_name is none:\n            module_name = module_class.__name__\n        if not force and module_name in self._module_dict:\n            raise keyerror(f'{module_name} is already registered '\n                           f'in {self.name}')\n        # 最核心代码\n        self._module_dict[module_name] = module_class\n\n    # 装饰器函数\n    def register_module(self, name=none, force=false, module=none):\n        if module is not none:\n            # 如果已经是 module，那就知道 增加到字典中即可\n            self._register_module(\n                module_class=module, module_name=name, force=force)\n            return module\n\n        # 最标准用法\n        # use it as a decorator: @x.register_module()\n        def _register(cls):\n            self._register_module(\n                module_class=cls, module_name=name, force=force)\n            return cls\n        return _register\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n\n# 2.2 类实例化\n\n在 mmcv 中所有的类实例化都是通过 build_from_cfg 函数实现，做的事情非常简单，就是给定 module_name，然后从 self._module_dict 提取即可。\n\ndef build_from_cfg(cfg, registry, default_args=none):\n    args = cfg.copy()\n\n    if default_args is not none:\n        for name, value in default_args.items():\n            args.setdefault(name, value)\n\n    obj_type = args.pop('type') # 注册 str 类名\n    if is_str(obj_type):\n        # 相当于 self._module_dict[obj_type]\n        obj_cls = registry.get(obj_type)\n        if obj_cls is none:\n            raise keyerror(\n                f'{obj_type} is not in the {registry.name} registry')\n\n    # 如果已经实例化了，那就直接返回\n    elif inspect.isclass(obj_type):\n        obj_cls = obj_type\n    else:\n        raise typeerror(\n            f'type must be a str or valid type, but got {type(obj_type)}')\n\n    # 最终初始化对于类，并且返回，就完成了一个类的实例化过程\n    return obj_cls(**args)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n# 2.3 完整的使用例子\n\n * 先创建一个 register\n * 然后注册一个 module\n * 然后根据 cfg 以及 register 调用\n\nconverters = registry('converter')\n\n@converters.register_module()\nclass converter1(object):\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\nconverter_cfg = dict(type='converter1', a=a_value, b=b_value)\nconverter = build_from_cfg(converter_cfg,converters)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"什么是 ABCMeta",frontmatter:{title:"什么是 ABCMeta",date:"2021-07-22T22:30:35.000Z",permalink:"/pages/1a8345/",categories:["技术文章","MMClassification"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/11.%E4%BB%80%E4%B9%88%E6%98%AF%20ABCMeta.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/11.什么是 ABCMeta.md",key:"v-28b40785",path:"/pages/1a8345/",headers:[{level:2,title:"什么是 ABCMeta",slug:"什么是-abcmeta",normalizedTitle:"什么是 abcmeta",charIndex:2}],headersStr:"什么是 ABCMeta",content:'# 什么是 ABCMeta\n\nmmcls 在实现 backbone 时首先实现了一个抽象基类，实现如下\n\nfrom abc import ABCMeta, abstractmethod\n\nfrom mmcv.runner import BaseModule\n\n\nclass BaseBackbone(BaseModule, metaclass=ABCMeta):\n    """Base backbone.\n    This class defines the basic functions of a backbone. Any backbone that\n    inherits this class should at least define its own `forward` function.\n    """\n\n    def __init__(self, init_cfg=None):\n        super(BaseBackbone, self).__init__(init_cfg)\n\n    @abstractmethod\n    def forward(self, x):\n        """Forward computation.\n        Args:\n            x (tensor | tuple[tensor]): x could be a Torch.tensor or a tuple of\n                Torch.tensor, containing input data for forward computation.\n        """\n        pass\n\n    def train(self, mode=True):\n        """Set module status before forward computation.\n        Args:\n            mode (bool): Whether it is train_mode or test_mode\n        """\n        super(BaseBackbone, self).train(mode)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n可以看到 BaseBackbone 类继承了 BaseModule',normalizedContent:'# 什么是 abcmeta\n\nmmcls 在实现 backbone 时首先实现了一个抽象基类，实现如下\n\nfrom abc import abcmeta, abstractmethod\n\nfrom mmcv.runner import basemodule\n\n\nclass basebackbone(basemodule, metaclass=abcmeta):\n    """base backbone.\n    this class defines the basic functions of a backbone. any backbone that\n    inherits this class should at least define its own `forward` function.\n    """\n\n    def __init__(self, init_cfg=none):\n        super(basebackbone, self).__init__(init_cfg)\n\n    @abstractmethod\n    def forward(self, x):\n        """forward computation.\n        args:\n            x (tensor | tuple[tensor]): x could be a torch.tensor or a tuple of\n                torch.tensor, containing input data for forward computation.\n        """\n        pass\n\n    def train(self, mode=true):\n        """set module status before forward computation.\n        args:\n            mode (bool): whether it is train_mode or test_mode\n        """\n        super(basebackbone, self).train(mode)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n可以看到 basebackbone 类继承了 basemodule',charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"mmseg数据集",frontmatter:{title:"mmseg数据集",date:"2021-09-15T14:03:16.000Z",permalink:"/pages/6b655a/",categories:["学习笔记","框架解析-mmlab系列"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/12.mmseg%E6%95%B0%E6%8D%AE%E9%9B%86.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/12.mmseg数据集.md",key:"v-55caee54",path:"/pages/6b655a/",headers:[{level:2,title:"mmsegmentation 数据集",slug:"mmsegmentation-数据集",normalizedTitle:"mmsegmentation 数据集",charIndex:2}],headersStr:"mmsegmentation 数据集",content:'# mmsegmentation 数据集\n\n# CustomDataset 类\n\ndef __getitem__(self, idx):\n    """Get training/test data after pipeline.\n    \n    Args:\n    idx (int): Index of data.\n    \n    Returns:\n    dict: Training/test data (with annotation if `test_mode` is set\n    False).\n    """\n    \n    if self.test_mode:\n        return self.prepare_test_img(idx)\n    else:\n        return self.prepare_train_img(idx)\n    \ndef prepare_train_img(self, idx):\n        """Get training data and annotations after pipeline.\n    Args:\n        idx (int): Index of data.\n\n    Returns:\n        dict: Training data and annotation after pipeline with new keys\n            introduced by pipeline.\n    """\n\n    img_info = self.img_infos[idx]\n    ann_info = self.get_ann_info(idx)\n    results = dict(img_info=img_info, ann_info=ann_info)\n    self.pre_pipeline(results)\n    return self.pipeline(results)\n\ndef prepare_test_img(self, idx):\n    """Get testing data after pipeline.\n\n    Args:\n        idx (int): Index of data.\n\n    Returns:\n        dict: Testing data after pipeline with new keys introduced by\n            pipeline.\n    """\n\n    img_info = self.img_infos[idx]\n    results = dict(img_info=img_info)\n    self.pre_pipeline(results)\n    return self.pipeline(results)\n    \n   \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n',normalizedContent:'# mmsegmentation 数据集\n\n# customdataset 类\n\ndef __getitem__(self, idx):\n    """get training/test data after pipeline.\n    \n    args:\n    idx (int): index of data.\n    \n    returns:\n    dict: training/test data (with annotation if `test_mode` is set\n    false).\n    """\n    \n    if self.test_mode:\n        return self.prepare_test_img(idx)\n    else:\n        return self.prepare_train_img(idx)\n    \ndef prepare_train_img(self, idx):\n        """get training data and annotations after pipeline.\n    args:\n        idx (int): index of data.\n\n    returns:\n        dict: training data and annotation after pipeline with new keys\n            introduced by pipeline.\n    """\n\n    img_info = self.img_infos[idx]\n    ann_info = self.get_ann_info(idx)\n    results = dict(img_info=img_info, ann_info=ann_info)\n    self.pre_pipeline(results)\n    return self.pipeline(results)\n\ndef prepare_test_img(self, idx):\n    """get testing data after pipeline.\n\n    args:\n        idx (int): index of data.\n\n    returns:\n        dict: testing data after pipeline with new keys introduced by\n            pipeline.\n    """\n\n    img_info = self.img_infos[idx]\n    results = dict(img_info=img_info)\n    self.pre_pipeline(results)\n    return self.pipeline(results)\n    \n   \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n',charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"计算loss和计算metric",frontmatter:{title:"计算loss和计算metric",date:"2021-09-30T03:58:52.000Z",permalink:"/pages/9b8ee5/",categories:["学习笔记","框架解析-mmlab系列"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/14.%E8%AE%A1%E7%AE%97loss%E5%92%8C%E8%AE%A1%E7%AE%97metric.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/14.计算loss和计算metric.md",key:"v-307f090f",path:"/pages/9b8ee5/",headersStr:null,content:"计算loss时\n\n# true: torch.Size([4, 512, 1024])\n# pred: torch.Size([4, 19, 512, 1024])\n\n\n1\n2\n\n\n计算 metric 时\n\n# true: torch.Size([4, 512, 1024])\n# pred: torch.Size([4, 512, 1024])\n\n\n1\n2\n\n\n解决方法：\n\nargmax_pred = torch.argmax(pred, dim=1).detach()\n# true: torch.Size([4, 512, 1024])\n# pred: torch.Size([4, 19, 512, 1024])\n# argmax_pred: torch.Size([4, 512, 1024])\n\n\n1\n2\n3\n4\n",normalizedContent:"计算loss时\n\n# true: torch.size([4, 512, 1024])\n# pred: torch.size([4, 19, 512, 1024])\n\n\n1\n2\n\n\n计算 metric 时\n\n# true: torch.size([4, 512, 1024])\n# pred: torch.size([4, 512, 1024])\n\n\n1\n2\n\n\n解决方法：\n\nargmax_pred = torch.argmax(pred, dim=1).detach()\n# true: torch.size([4, 512, 1024])\n# pred: torch.size([4, 19, 512, 1024])\n# argmax_pred: torch.size([4, 512, 1024])\n\n\n1\n2\n3\n4\n",charsets:{cjk:!0},lastUpdated:"2021/10/03, 15:21:19"},{title:"尚未阅读的各类文章",frontmatter:{title:"尚未阅读的各类文章",date:"2021-08-02T21:16:38.000Z",permalink:"/pages/234602/",categories:["学习笔记","讲座记录-有意思的文章集合"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/06.%E8%AE%B2%E5%BA%A7%E8%AE%B0%E5%BD%95-%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E6%96%87%E7%AB%A0%E9%9B%86%E5%90%88/00.%E5%B0%9A%E6%9C%AA%E9%98%85%E8%AF%BB%E7%9A%84%E5%90%84%E7%B1%BB%E6%96%87%E7%AB%A0.html",relativePath:"02.学习笔记/06.讲座记录-有意思的文章集合/00.尚未阅读的各类文章.md",key:"v-66324249",path:"/pages/234602/",headersStr:null,content:" * 直播时各种背景是怎么实现的？聊一聊虚拟背景背后的技术\n\n * 使用Docker为无网络环境搭建深度学习环境\n\n * 常用卷积网络基础架构\n\n * DETR目标检测新范式带来的思考\n\n# 基于 CAM 擦除的\n\n1、Adversarial Complementary Learning for Weakly Supervised ObjectLocalization\n\n2、Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach\n\n3、Erasing Integrated Learning : A Simple yet Effective Approach for Weakly Supervised\n\n# 基于图像增强的\n\n1、Puzzle-CAM: Improved localization via matching partial and full features.\n\n2、Self-supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentaion.\n\n# 生成 CAM 的示意图\n\n[ClassActivationMaps_PyTorch](https://github.com/maubreville/ClassActivationMaps_PyTorch/blob/master/ClassActivationMaps_Demo_Resnet18.ipynb)\n\n * 跨越时空的难样本挖掘：https://www.zhihu.com/collection/148739489\n\n * 优Tech分享 | 腾讯优图在弱监督目标定位的研究及应用：https://zhuanlan.zhihu.com/p/393262933\n\n * 语义分割模型架构演进与相关论文阅读：https://blog.csdn.net/kevin_zhao_zl/article/details/106910045\n\n * 图像语义分割(12)-重新思考空洞卷积: 为弱监督和半监督语义分割设计的简捷方法：https://zhuanlan.zhihu.com/p/52935388\n\n * (NeurIPS 2019) Gated CRF Loss-一种用于弱监督图像语义分割的新型损失函数：https://zhuanlan.zhihu.com/p/83964531\n\n * (CVPR2019)图像语义分割(22)FickleNet-随机推理用于弱监督和半监督图像语义分割：https://zhuanlan.zhihu.com/p/81707287",normalizedContent:" * 直播时各种背景是怎么实现的？聊一聊虚拟背景背后的技术\n\n * 使用docker为无网络环境搭建深度学习环境\n\n * 常用卷积网络基础架构\n\n * detr目标检测新范式带来的思考\n\n# 基于 cam 擦除的\n\n1、adversarial complementary learning for weakly supervised objectlocalization\n\n2、object region mining with adversarial erasing: a simple classification to semantic segmentation approach\n\n3、erasing integrated learning : a simple yet effective approach for weakly supervised\n\n# 基于图像增强的\n\n1、puzzle-cam: improved localization via matching partial and full features.\n\n2、self-supervised equivariant attention mechanism for weakly supervised semantic segmentaion.\n\n# 生成 cam 的示意图\n\n[classactivationmaps_pytorch](https://github.com/maubreville/classactivationmaps_pytorch/blob/master/classactivationmaps_demo_resnet18.ipynb)\n\n * 跨越时空的难样本挖掘：https://www.zhihu.com/collection/148739489\n\n * 优tech分享 | 腾讯优图在弱监督目标定位的研究及应用：https://zhuanlan.zhihu.com/p/393262933\n\n * 语义分割模型架构演进与相关论文阅读：https://blog.csdn.net/kevin_zhao_zl/article/details/106910045\n\n * 图像语义分割(12)-重新思考空洞卷积: 为弱监督和半监督语义分割设计的简捷方法：https://zhuanlan.zhihu.com/p/52935388\n\n * (neurips 2019) gated crf loss-一种用于弱监督图像语义分割的新型损失函数：https://zhuanlan.zhihu.com/p/83964531\n\n * (cvpr2019)图像语义分割(22)ficklenet-随机推理用于弱监督和半监督图像语义分割：https://zhuanlan.zhihu.com/p/81707287",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"mmseg 推理单张图像并保存",frontmatter:{title:"mmseg 推理单张图像并保存",date:"2021-09-28T19:42:58.000Z",permalink:"/pages/1789a5/",categories:["学习笔记","框架解析-mmlab系列"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/13.mmseg%20%E6%8E%A8%E7%90%86%E5%8D%95%E5%BC%A0%E5%9B%BE%E5%83%8F%E5%B9%B6%E4%BF%9D%E5%AD%98.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/13.mmseg 推理单张图像并保存.md",key:"v-1690b4cd",path:"/pages/1789a5/",headersStr:null,content:"from mmseg.apis import inference_segmentor, init_segmentor\nimport mmcv\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\n\ndef generate_pseudo_masks(config_file, checkpoint_file, dir_save_pseudo_masks):\n    model = init_segmentor(config_file, checkpoint_file, device='cuda:1')\n    PALETTE = []\n    for i in range(150):\n        PALETTE.append([i, i, i])\n    model.PALETTE = PALETTE\n\n    if not os.path.exists(dir_save_pseudo_masks):\n        os.mkdir(dir_save_pseudo_masks)\n\n    for image_name in tqdm(list_images):\n        img = mmcv.imread(image_name)\n        result = inference_segmentor(model, img)\n        model.show_result(img, result, out_file=os.path.join(dir_save_pseudo_masks, image_name.split('/')[-1]), opacity=1)\n\n\nif __name__ == '__main__':\n    df = pd.read_csv(train_coarse_0.csv')\n    list_images = df['filename'].tolist()\n\n    config_file = 'configs/deeplabv3plus/deeplabv3plus_r101-d8_512x512_160k_ade20k.py'\n    checkpoint_file = 'checkpoints/ade20k/deeplabv3plus_r101-d8_512x512_160k_ade20k_20200615_123232-38ed86bb.pth'\n    dir_save_pseudo_masks = '/home/muyun99/data/dataset/Public-Dataset/Cityscapes/cityscapes_pseudo_mask/deeplabv3plus_r101-d8_512x512_160k_ade20k'\n    generate_pseudo_masks(config_file, checkpoint_file, dir_save_pseudo_masks)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n",normalizedContent:"from mmseg.apis import inference_segmentor, init_segmentor\nimport mmcv\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\n\ndef generate_pseudo_masks(config_file, checkpoint_file, dir_save_pseudo_masks):\n    model = init_segmentor(config_file, checkpoint_file, device='cuda:1')\n    palette = []\n    for i in range(150):\n        palette.append([i, i, i])\n    model.palette = palette\n\n    if not os.path.exists(dir_save_pseudo_masks):\n        os.mkdir(dir_save_pseudo_masks)\n\n    for image_name in tqdm(list_images):\n        img = mmcv.imread(image_name)\n        result = inference_segmentor(model, img)\n        model.show_result(img, result, out_file=os.path.join(dir_save_pseudo_masks, image_name.split('/')[-1]), opacity=1)\n\n\nif __name__ == '__main__':\n    df = pd.read_csv(train_coarse_0.csv')\n    list_images = df['filename'].tolist()\n\n    config_file = 'configs/deeplabv3plus/deeplabv3plus_r101-d8_512x512_160k_ade20k.py'\n    checkpoint_file = 'checkpoints/ade20k/deeplabv3plus_r101-d8_512x512_160k_ade20k_20200615_123232-38ed86bb.pth'\n    dir_save_pseudo_masks = '/home/muyun99/data/dataset/public-dataset/cityscapes/cityscapes_pseudo_mask/deeplabv3plus_r101-d8_512x512_160k_ade20k'\n    generate_pseudo_masks(config_file, checkpoint_file, dir_save_pseudo_masks)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n",charsets:{},lastUpdated:"2021/10/03, 15:21:19"},{title:"mmsegmentation框架解析（下）",frontmatter:{title:"mmsegmentation框架解析（下）",date:"2021-04-13T16:06:24.000Z",permalink:"/pages/759a29/",categories:["计算机视觉","图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/07.mmsegmentation%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90%EF%BC%88%E4%B8%8B%EF%BC%89.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/07.mmsegmentation框架解析（下）.md",key:"v-37c4561a",path:"/pages/759a29/",headers:[{level:2,title:"mmsegmentation 框架解析",slug:"mmsegmentation-框架解析",normalizedTitle:"mmsegmentation 框架解析",charIndex:2},{level:3,title:"Tutorial 5：训练技巧",slug:"tutorial-5-训练技巧",normalizedTitle:"tutorial 5：训练技巧",charIndex:26},{level:3,title:"Tutorial 6：自定义 Runtime 设置",slug:"tutorial-6-自定义-runtime-设置",normalizedTitle:"tutorial 6：自定义 runtime 设置",charIndex:1205}],headersStr:"mmsegmentation 框架解析 Tutorial 5：训练技巧 Tutorial 6：自定义 Runtime 设置",content:"# mmsegmentation 框架解析\n\n\n# Tutorial 5：训练技巧\n\n# 1、使用不同的学习率\n\n在语义分割中，有些方法提出若 head 的学习率比 backbone 的学习率大，能够得到更好的性能以及更快的拟合\n\n可以使用以下的配置来让 head 的 LR 是 backbone 的 10 倍\n\noptimizer=dict(\n    paramwise_cfg = dict(\n        custom_keys={\n            'head': dict(lr_mult=10.)}))\n\n\n1\n2\n3\n4\n\n\n# 2、在线难例挖掘\n\n可以使用以下的配置来使用像素级的在线难例挖掘\n\n使用这种方式，只有置信度在 0.7 以下的像素会被用作训练，并且我们将使用至少 100000 像素用做训练，如果没有定义 thresh ，那么会使用 min_kept 参数来选择训练的像素。\n\n_base_ = './pspnet_r50-d8_512x1024_40k_cityscapes.py'\nmodel=dict(\n    decode_head=dict(\n        sampler=dict(type='OHEMPixelSampler', thresh=0.7, min_kept=100000)) )\n\n\n1\n2\n3\n4\n\n\n# 3、类别不均衡损失\n\n可以使用以下的配置来为每个类别分类不同的权重，使用以下的配置可以调整 cityscapes 数据集的权重\n\nclass_weight 将作为 CrossEntropyLoss 的 weight 权重来计算损失\n\n_base_ = './pspnet_r50-d8_512x1024_40k_cityscapes.py'\nmodel=dict(\n    decode_head=dict(\n        loss_decode=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0,\n            # DeepLab used this class weight for cityscapes\n            class_weight=[0.8373, 0.9180, 0.8660, 1.0345, 1.0166, 0.9969, 0.9754,\n                        1.0489, 0.8786, 1.0023, 0.9539, 0.9843, 1.1116, 0.9037,\n                        1.0865, 1.0955, 1.0865, 1.1529, 1.0507])))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# Tutorial 6：自定义 Runtime 设置\n\n# 1、自定义优化设置\n\n# 1.1 使用 PyTorch 支持的 optimizer\n\n可以使用 PyTorch 支持的 optimizer，但是文档中提到使用 Adam 优化器会使性能下降很多\n\noptimizer = dict(type='Adam', lr=0.0003, weight_decay=0.0001)\n\n\n1\n\n\n# 1.2 使用定制 optimizer\n\n * 创建新目录 mmseg/core/optimizer\n * 在定义mmseg/core/optimizer/my_optimizer.py 文件中定义MyOptimizer 类\n\nfrom .registry import OPTIMIZERS\nfrom torch.optim import Optimizer\n\n@OPTIMIZERS.register_module()\nclass MyOptimizer(Optimizer):\n\n    def __init__(self, a, b, c)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * 在 mmseg/core/optimizer/__init__.py 中 import 该类\n\nfrom .my_optimizer import MyOptimizer\n\n\n1\n\n * 可以使用 custom_imports 来手动 import 该类\n\n\ncustom_imports = dict(imports=['mmseg.core.optimizer.my_optimizer'], allow_failed_imports=False)\n\n\n1\n2\n\n * 可以在配置中这样使用自定义的 Optimizer\n\noptimizer = dict(type='MyOptimizer', a=a_value, b=b_value, c=c_value)\n\n\n1\n\n\n# 1.3 定制 optimizer 构造函数\n\nfrom mmcv.utils import build_from_cfg\n\nfrom mmcv.runner.optimizer import OPTIMIZER_BUILDERS, OPTIMIZERS\nfrom mmseg.utils import get_root_logger\nfrom .my_optimizer import MyOptimizer\n\n\n@OPTIMIZER_BUILDERS.register_module()\nclass MyOptimizerConstructor(object):\n\n    def __init__(self, optimizer_cfg, paramwise_cfg=None):\n\n    def __call__(self, model):\n\n        return my_optimizer\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n# 1.4 额外设置\n\n * 使用梯度裁剪（gradient clip）\n\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n\n\n1\n2\n\n * 使用动量（momentum schedule）加速拟合\n\nlr_config = dict(\n    policy='cyclic',\n    target_ratio=(10, 1e-4),\n    cyclic_times=1,\n    step_ratio_up=0.4,\n)\nmomentum_config = dict(\n    policy='cyclic',\n    target_ratio=(0.85 / 0.95, 1),\n    cyclic_times=1,\n    step_ratio_up=0.4,\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 2、自定义训练 schedule\n\n默认是 40k/80k schedule，调用的是 MMCV 的 PolyLrUpdateeHook\n\n也支持其他的 LR Schedule，例如 CosineAnnealing 以及 Poly Schedule\n\n * Step schedule\n\nlr_config = dict(policy='step', step=[9, 10])\n\n\n1\n\n * ConsineAnnealing schedule\n\nlr_config = dict(\n    policy='CosineAnnealing',\n    warmup='linear',\n    warmup_iters=1000,\n    warmup_ratio=1.0 / 10,\n    min_lr_ratio=1e-5)\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 3、自定义 workflow\n\n * 以下语句意味着运行一个 epoch 训练，\n\nworkflow = [('train', 1)]\n\n\n1\n\n * 以下语句意味着运行一个epoch 训练，运行一个epoch 测试\n\n[('train', 1), ('val', 1)]\n\n\n1\n\n\n# 4、自定义 hook\n\n * 可以使用 MMCV 中实现的 hooks\n\ncustom_hooks = [\n    dict(type='MyHook', a=a_value, b=b_value, priority='NORMAL')\n]\n\n\n1\n2\n3\n\n\n * 修改默认的 runtime hooks，以下的 hooks 是没有在 custom_hooks 中注册的\n   \n   * log_config\n   * checkpoint_config\n   * evaluation\n   * lr_config\n   * optimizer_config\n   * momentum_config\n\n * Checkopint config\n   \n   用户可以设置max_keep_ckpts只保存少量的检查点，或者通过save_optimizer决定是否保存优化器的状态。更多的参数细节如下\n\ncheckpoint_config = dict(interval=1)\n\n\n1\n\n\n * Log config\n   \n   log_config 支持多个log hook，并可以设置间隔。现在，MMCV 支持 WandbLoggerHook，MlflowLoggerHook 以及 TensorboardLoggerHook。\n\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        dict(type='TensorboardLoggerHook')\n    ])\n\n\n1\n2\n3\n4\n5\n6\n\n * Evaluation config\n\nevaluation = dict(interval=1, metric='mIoU')\n\n\n1\n",normalizedContent:"# mmsegmentation 框架解析\n\n\n# tutorial 5：训练技巧\n\n# 1、使用不同的学习率\n\n在语义分割中，有些方法提出若 head 的学习率比 backbone 的学习率大，能够得到更好的性能以及更快的拟合\n\n可以使用以下的配置来让 head 的 lr 是 backbone 的 10 倍\n\noptimizer=dict(\n    paramwise_cfg = dict(\n        custom_keys={\n            'head': dict(lr_mult=10.)}))\n\n\n1\n2\n3\n4\n\n\n# 2、在线难例挖掘\n\n可以使用以下的配置来使用像素级的在线难例挖掘\n\n使用这种方式，只有置信度在 0.7 以下的像素会被用作训练，并且我们将使用至少 100000 像素用做训练，如果没有定义 thresh ，那么会使用 min_kept 参数来选择训练的像素。\n\n_base_ = './pspnet_r50-d8_512x1024_40k_cityscapes.py'\nmodel=dict(\n    decode_head=dict(\n        sampler=dict(type='ohempixelsampler', thresh=0.7, min_kept=100000)) )\n\n\n1\n2\n3\n4\n\n\n# 3、类别不均衡损失\n\n可以使用以下的配置来为每个类别分类不同的权重，使用以下的配置可以调整 cityscapes 数据集的权重\n\nclass_weight 将作为 crossentropyloss 的 weight 权重来计算损失\n\n_base_ = './pspnet_r50-d8_512x1024_40k_cityscapes.py'\nmodel=dict(\n    decode_head=dict(\n        loss_decode=dict(\n            type='crossentropyloss', use_sigmoid=false, loss_weight=1.0,\n            # deeplab used this class weight for cityscapes\n            class_weight=[0.8373, 0.9180, 0.8660, 1.0345, 1.0166, 0.9969, 0.9754,\n                        1.0489, 0.8786, 1.0023, 0.9539, 0.9843, 1.1116, 0.9037,\n                        1.0865, 1.0955, 1.0865, 1.1529, 1.0507])))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# tutorial 6：自定义 runtime 设置\n\n# 1、自定义优化设置\n\n# 1.1 使用 pytorch 支持的 optimizer\n\n可以使用 pytorch 支持的 optimizer，但是文档中提到使用 adam 优化器会使性能下降很多\n\noptimizer = dict(type='adam', lr=0.0003, weight_decay=0.0001)\n\n\n1\n\n\n# 1.2 使用定制 optimizer\n\n * 创建新目录 mmseg/core/optimizer\n * 在定义mmseg/core/optimizer/my_optimizer.py 文件中定义myoptimizer 类\n\nfrom .registry import optimizers\nfrom torch.optim import optimizer\n\n@optimizers.register_module()\nclass myoptimizer(optimizer):\n\n    def __init__(self, a, b, c)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n * 在 mmseg/core/optimizer/__init__.py 中 import 该类\n\nfrom .my_optimizer import myoptimizer\n\n\n1\n\n * 可以使用 custom_imports 来手动 import 该类\n\n\ncustom_imports = dict(imports=['mmseg.core.optimizer.my_optimizer'], allow_failed_imports=false)\n\n\n1\n2\n\n * 可以在配置中这样使用自定义的 optimizer\n\noptimizer = dict(type='myoptimizer', a=a_value, b=b_value, c=c_value)\n\n\n1\n\n\n# 1.3 定制 optimizer 构造函数\n\nfrom mmcv.utils import build_from_cfg\n\nfrom mmcv.runner.optimizer import optimizer_builders, optimizers\nfrom mmseg.utils import get_root_logger\nfrom .my_optimizer import myoptimizer\n\n\n@optimizer_builders.register_module()\nclass myoptimizerconstructor(object):\n\n    def __init__(self, optimizer_cfg, paramwise_cfg=none):\n\n    def __call__(self, model):\n\n        return my_optimizer\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n# 1.4 额外设置\n\n * 使用梯度裁剪（gradient clip）\n\noptimizer_config = dict(\n    _delete_=true, grad_clip=dict(max_norm=35, norm_type=2))\n\n\n1\n2\n\n * 使用动量（momentum schedule）加速拟合\n\nlr_config = dict(\n    policy='cyclic',\n    target_ratio=(10, 1e-4),\n    cyclic_times=1,\n    step_ratio_up=0.4,\n)\nmomentum_config = dict(\n    policy='cyclic',\n    target_ratio=(0.85 / 0.95, 1),\n    cyclic_times=1,\n    step_ratio_up=0.4,\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 2、自定义训练 schedule\n\n默认是 40k/80k schedule，调用的是 mmcv 的 polylrupdateehook\n\n也支持其他的 lr schedule，例如 cosineannealing 以及 poly schedule\n\n * step schedule\n\nlr_config = dict(policy='step', step=[9, 10])\n\n\n1\n\n * consineannealing schedule\n\nlr_config = dict(\n    policy='cosineannealing',\n    warmup='linear',\n    warmup_iters=1000,\n    warmup_ratio=1.0 / 10,\n    min_lr_ratio=1e-5)\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 3、自定义 workflow\n\n * 以下语句意味着运行一个 epoch 训练，\n\nworkflow = [('train', 1)]\n\n\n1\n\n * 以下语句意味着运行一个epoch 训练，运行一个epoch 测试\n\n[('train', 1), ('val', 1)]\n\n\n1\n\n\n# 4、自定义 hook\n\n * 可以使用 mmcv 中实现的 hooks\n\ncustom_hooks = [\n    dict(type='myhook', a=a_value, b=b_value, priority='normal')\n]\n\n\n1\n2\n3\n\n\n * 修改默认的 runtime hooks，以下的 hooks 是没有在 custom_hooks 中注册的\n   \n   * log_config\n   * checkpoint_config\n   * evaluation\n   * lr_config\n   * optimizer_config\n   * momentum_config\n\n * checkopint config\n   \n   用户可以设置max_keep_ckpts只保存少量的检查点，或者通过save_optimizer决定是否保存优化器的状态。更多的参数细节如下\n\ncheckpoint_config = dict(interval=1)\n\n\n1\n\n\n * log config\n   \n   log_config 支持多个log hook，并可以设置间隔。现在，mmcv 支持 wandbloggerhook，mlflowloggerhook 以及 tensorboardloggerhook。\n\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='textloggerhook'),\n        dict(type='tensorboardloggerhook')\n    ])\n\n\n1\n2\n3\n4\n5\n6\n\n * evaluation config\n\nevaluation = dict(interval=1, metric='miou')\n\n\n1\n",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"一些需要注意的点",frontmatter:{title:"一些需要注意的点",date:"2021-07-11T22:15:17.000Z",permalink:"/pages/1b9e2a/",categories:["技术文章","有意思的文章集合"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/06.%E8%AE%B2%E5%BA%A7%E8%AE%B0%E5%BD%95-%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E6%96%87%E7%AB%A0%E9%9B%86%E5%90%88/01.VALSE%20Paper%20%E6%8E%A2%E7%B4%A2%E7%AE%80%E5%8D%95%E5%AD%AA%E7%94%9F%E7%BD%91%E7%BB%9C%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0.html",relativePath:"02.学习笔记/06.讲座记录-有意思的文章集合/01.VALSE Paper 探索简单孪生网络表征学习.md",key:"v-7788edac",path:"/pages/1b9e2a/",headersStr:null,content:"【VALSE论文速览-02期】探索简单孪生网络表征学习：https://www.bilibili.com/video/BV1pg411M7b6\n\nSiamese/twin/dual Networks\n\n但是这样的 Encoder 很快就会发现平凡解或者退化解，这样的解就是 Encoder 对于所有的图片都 output 同样的输出，为了解决这种平凡解，有不同的解决方案\n\n * Contrastive Learning：要求不同图像的 view 会\n   \n   * 常用的损失是 InfoNCE\n   * 但是其弊端是需要较多的负样本才行\n     * 在 SimCLE 方法里，其用了 4096 的Batch Size\n     * 在 MoCo 方法里，用了 Momentum Queue 来储存负样本\n\n * SwAV 方法中，将样本 assign 到不同的 cluster 中，并且保证每个 cluster 的数目是大致均衡的\n\n * BYOL 方法中，引入了额外的 MLP 作为 predictor，并且使用了 momentum encoder\n   \n   * Momentum encoder\n     * 对于 encoder 的权重使用 EMA 的方式进行更新\n     * 所以权重不是根据梯度来更新的\n     * 但是需要保存两份权重的副本\n\n一个简单的孪生网络是否能够work？\n\nSimSiam 不需要负样本\n\nSiamese Network 是一个刻画不变性（invariance）的方法\n\n * Invariance：同一物体的两个 view 应当产生相同的输出\n * 卷积是平移不变性的归纳偏置，但是更多的不变性（例如颜色尺度旋转等）却较难设计对应的算子\n * 在这种角度看，孪生网络提供了一种数据驱动的 baseline\n * 所以在没有归纳偏置，例如 ViT 上也能够work，并且 work 得很好（MoCov3）",normalizedContent:"【valse论文速览-02期】探索简单孪生网络表征学习：https://www.bilibili.com/video/bv1pg411m7b6\n\nsiamese/twin/dual networks\n\n但是这样的 encoder 很快就会发现平凡解或者退化解，这样的解就是 encoder 对于所有的图片都 output 同样的输出，为了解决这种平凡解，有不同的解决方案\n\n * contrastive learning：要求不同图像的 view 会\n   \n   * 常用的损失是 infonce\n   * 但是其弊端是需要较多的负样本才行\n     * 在 simcle 方法里，其用了 4096 的batch size\n     * 在 moco 方法里，用了 momentum queue 来储存负样本\n\n * swav 方法中，将样本 assign 到不同的 cluster 中，并且保证每个 cluster 的数目是大致均衡的\n\n * byol 方法中，引入了额外的 mlp 作为 predictor，并且使用了 momentum encoder\n   \n   * momentum encoder\n     * 对于 encoder 的权重使用 ema 的方式进行更新\n     * 所以权重不是根据梯度来更新的\n     * 但是需要保存两份权重的副本\n\n一个简单的孪生网络是否能够work？\n\nsimsiam 不需要负样本\n\nsiamese network 是一个刻画不变性（invariance）的方法\n\n * invariance：同一物体的两个 view 应当产生相同的输出\n * 卷积是平移不变性的归纳偏置，但是更多的不变性（例如颜色尺度旋转等）却较难设计对应的算子\n * 在这种角度看，孪生网络提供了一种数据驱动的 baseline\n * 所以在没有归纳偏置，例如 vit 上也能够work，并且 work 得很好（mocov3）",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"VALSE Webinar 21-19 弱监督视觉学习：定位、分割和其他",frontmatter:{title:"VALSE Webinar 21-19 弱监督视觉学习：定位、分割和其他",date:"2021-07-16T23:39:49.000Z",permalink:"/pages/ada345/",categories:["技术文章","有意思的文章集合"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/06.%E8%AE%B2%E5%BA%A7%E8%AE%B0%E5%BD%95-%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E6%96%87%E7%AB%A0%E9%9B%86%E5%90%88/03.VALSE%20Webinar%2021-19%20%E5%BC%B1%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%AE%9A%E4%BD%8D%E3%80%81%E5%88%86%E5%89%B2%E5%92%8C%E5%85%B6%E4%BB%96.html",relativePath:"02.学习笔记/06.讲座记录-有意思的文章集合/03.VALSE Webinar 21-19 弱监督视觉学习：定位、分割和其他.md",key:"v-a5dc9b2a",path:"/pages/ada345/",headers:[{level:2,title:"VALSE Webinar 21-19 弱监督视觉学习：定位、分割和其他",slug:"valse-webinar-21-19-弱监督视觉学习-定位、分割和其他",normalizedTitle:"valse webinar 21-19 弱监督视觉学习：定位、分割和其他",charIndex:2},{level:3,title:"万方 中国科学院大学",slug:"万方-中国科学院大学",normalizedTitle:"万方 中国科学院大学",charIndex:123},{level:3,title:"肖继民 西交利物浦大学",slug:"肖继民-西交利物浦大学",normalizedTitle:"肖继民 西交利物浦大学",charIndex:865},{level:3,title:"Pannel 环节",slug:"pannel-环节",normalizedTitle:"pannel 环节",charIndex:1986}],headersStr:"VALSE Webinar 21-19 弱监督视觉学习：定位、分割和其他 万方 中国科学院大学 肖继民 西交利物浦大学 Pannel 环节",content:"# VALSE Webinar 21-19 弱监督视觉学习：定位、分割和其他\n\n * 20210714【弱监督视觉学习：定位、分割及其他】Panel：https://www.bilibili.com/video/BV1ao4y1D7Sr\n\n\n# 万方 中国科学院大学\n\n# 1、扩散激活：DANet: Divergent Activation for Weakly Supervised Object Localization. ICCV 2019\n\n（1）分层激活：Hierarchical Divergent Activation（HDA），试图对类别做做一个分层，先做父类的分类，再做子类别的分类。多尺度 CNN 得到特征金字塔，特征金字塔每一层有不同的激活区域\n\n（2）分歧激活：Discrepant Dovergent Activation（DDA），通过分歧约束，实现输出的激活图是有分歧的，将多个有分歧的 CAM 合并可以得到更完整的激活区域\n\n# 2、增强学习容忍度：Strengthen Learning Tolerance for Weakly Supervised Object Localization. CVPR 2021\n\n（1）对 top-k 的类别都进行一个判别，对语义相似的区域也会进行激活，再将结果合并\n\n（2）对图像做变换后，输出的 CAM 是有一致性的\n\n# 3、Transformer 有全局表示的特性：Transformer-Based Method：TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization. arXiv 2021\n\n（1）Transformer 对于目标的激活图是比较完整的，但是 Attention Map 是语义无关的，会将所有的类别的 Attention 全部输出\n\n（2）设计了一个语义相关的结构用于获取 Attention Map\n\n\n# 肖继民 西交利物浦大学\n\n# 1、扩散激活：Reliability Dose Matter: An End-to-End Weakly Supervised Semantic Segmentation Approach. AAAI 2020\n\n（1）端到端的去做弱监督语义分割\n\n（2）multi-scale CAM：做多尺度的CAM的测试然后进行融合\n\n（3）loss 分为两部分：对高置信度的区域做CE loss，Energy Loss是融合了 CRF Loss 和 一个loss weight，reliable region 有更小的 loss weight，相邻的点和颜色相近的点预测成同一类，如果不是同一类则进行惩罚\n\n（4）这篇工作最大的亮点是端到端，据说是第二篇端到端的工作\n\n# 2、显著性目标检测：Structure-Consistent Weakly Supervised Salient Object Detection with Local Saliency Coherence. AAAI 2021\n\n（1）改造版本的CRF Loss\n\n（2）Saliency Structure Consistency Loss：先CAM再增强和先增强后CAM的结果做一个一致性 loss，与SEAM一致\n\n# 3、图模型做弱监督分割：Affinity Attention Graph Neural Network for Weakly Supervised Semantic Segmentation. IEEE TPAMI 2021\n\n（1）将 image 转换为 graph，提出以下几点\n\n * 每个 feature pixel 作为 node，而不是super pixel\n * 对于任意两个 node 之间都有 edge\n * Edge 的值不止是 0 和 1\n\n（2）bbox 的 label + image label\n\n * image label：CAM\n * bbox label：Grab-Cut\n * 两个区域的作为 seed label\n\n（3）如何将 Image 转换为 Graph？\n\n * 借鉴并改进 Affinity CNN\n * 下面是 Ac Loss 的计算过程，只针对可靠区域计算\n * 在一定距离范围内，如果两个node类别信息一致，其edge值为1，加入 A+ 集合，否则为0，加入 A- 集合\n * 如何获得两个点的预测的边呢：两个点的feature做一个L1 loss（未听清），得到 D(i,j)\n * 下面是 Ar Loss 的计算过程，不限制区域，对所有区域都计算\n\n\n# Pannel 环节\n\n# 1、图像中的背景区域较为杂乱，导致背景类样本具有较大的类内散度，进而导致背景类自身难以准确建模，同时影响其他语义类别的建模，这个问题如何解决？\n\n1、类别无关的物体定位\n\n2、Positive Unlabel Learning，PU学习\n\n3、引入主动学习引入较为精确的标注\n\n4、类内散度不影响，前景背景类较为相似导致的问题，更加强大的预训练模型\n\n# 2、样本 label noise 的问题怎么解决？\n\n1、图像和标注进行质量评估，对于学习的影响也会比较大，课程学习，先用简单样本，再用难样本\n\n2、如何改造我们的全监督网络，让其的容错性更加高\n\n3、DNN 在比较大的噪声标注下还是比较鲁棒的（我觉得应当是对于分割来说的），co-training，\n\n4、co-training，Transformer 和 CNN 不同性质的网络\n\n5、label noise主要两个来源：伪标签和众包标注，通过软标记来解决噪声问题\n\n耿新老师：做伪标记的时候，用一个标记分布，用图神经网络做软标记？\n\nco-training + 标记分布，多模型都给一个标记分布\n\n# 3、现在的弱监督学习基本还是沿用MIL的建模方式，目前是否有其他的机器学习模型适用于弱监督学习？\n\n1、自学习，带噪声的半监督学习，Transformer和自监督Transformer可能会有更好的效果（DINO）\n\n2、特征空间的规整，在特征学习和后续的分类器做一个 match，新的联合优化的方式\n\n3、标记不完整（半监督，PU学习），不正确（label noise），不精确（image-lavel annotation，MIL，偏标记学习）\n\n# 4、弱监督下模型复杂度从理论上是否与效果成正相关？不准确、不全面的标注是否更加难以训练较大规模的网络模型？\n\n1、噪声非常大，大规模网络表现不好，例如 DeepLabv2 性能是比较好的，动态调整网络的复杂度\n\n2、Scaled-ViT , 标签噪声有多大，对表示学习的模型会产生什么特征空间，特征空间会产生什么，理论上给一个定义\n\n3、问题复杂度增加了，机器学习模型的复杂度应当增加。用最匹配的模型来解决\n\n# 5、弱监督学习在什么条件下可以逼近强监督学习的效果？什么场景下可以充分发挥弱监督学习的优势？\n\n1、弱监督学习的问题在于：前背景的界限过于模糊\n\n2、标注强一些，线标或者框标，只有3个点左右的gap，如果只有image-lavel的标注会带来10个点左右的差距，弱监督可以结合半监督，\n\n3、显著性信息的引入，合理地引入先验\n\n4、学习问题的一致性，理论上来讲，数据量是无穷大的时候，可以逼近强监督学习的效果；减小搜索空间：先验知识等等\n\n# 6、当前弱监督学习的核心挑战是什么？当数据不再能驱得动学习过程，是否应该回归模型驱动的学习方法？\n\n1、王：复杂场景的弱监督：引入视觉规律，显著性，边缘（有无纹理），超像素\n\n2、叶：表示学习的挑战是一样的，不完全标注、部分标注、现有的信息来估计不完全的信息，能够处理数据的Varience，传统方法引入也不失为一种策略：几何模型\n\n3、耿：小样本弱监督学习（医疗影像），边际递减效应，不断增加数据获得的收益会逐步递减，回归模型驱动，比如决策树，SVM，压缩感知等等\n\n# 7、弱监督学习在学术界和工业界的未来研究趋势是什么？是否需要新的benchmark？如何定义新的benchmark?\n\n1、万：主动学习、无监督学习结合\n\n2、肖：工业界的在固定的标注代价下，怎么提升最大的收益，标注的组合，领域迁移的工作\n\n3、王：互联网和医院，抗疫的工作\n\n4、叶：工业界可以利用弱监督学习去获取预训练模型，学术界应该探索新的范式，与无监督和表示学习之间的关系，视频的标注量\n\n5、耿：工业界关心的成本问题，先验知识，模型和数据的匹配，元学习",normalizedContent:"# valse webinar 21-19 弱监督视觉学习：定位、分割和其他\n\n * 20210714【弱监督视觉学习：定位、分割及其他】panel：https://www.bilibili.com/video/bv1ao4y1d7sr\n\n\n# 万方 中国科学院大学\n\n# 1、扩散激活：danet: divergent activation for weakly supervised object localization. iccv 2019\n\n（1）分层激活：hierarchical divergent activation（hda），试图对类别做做一个分层，先做父类的分类，再做子类别的分类。多尺度 cnn 得到特征金字塔，特征金字塔每一层有不同的激活区域\n\n（2）分歧激活：discrepant dovergent activation（dda），通过分歧约束，实现输出的激活图是有分歧的，将多个有分歧的 cam 合并可以得到更完整的激活区域\n\n# 2、增强学习容忍度：strengthen learning tolerance for weakly supervised object localization. cvpr 2021\n\n（1）对 top-k 的类别都进行一个判别，对语义相似的区域也会进行激活，再将结果合并\n\n（2）对图像做变换后，输出的 cam 是有一致性的\n\n# 3、transformer 有全局表示的特性：transformer-based method：ts-cam: token semantic coupled attention map for weakly supervised object localization. arxiv 2021\n\n（1）transformer 对于目标的激活图是比较完整的，但是 attention map 是语义无关的，会将所有的类别的 attention 全部输出\n\n（2）设计了一个语义相关的结构用于获取 attention map\n\n\n# 肖继民 西交利物浦大学\n\n# 1、扩散激活：reliability dose matter: an end-to-end weakly supervised semantic segmentation approach. aaai 2020\n\n（1）端到端的去做弱监督语义分割\n\n（2）multi-scale cam：做多尺度的cam的测试然后进行融合\n\n（3）loss 分为两部分：对高置信度的区域做ce loss，energy loss是融合了 crf loss 和 一个loss weight，reliable region 有更小的 loss weight，相邻的点和颜色相近的点预测成同一类，如果不是同一类则进行惩罚\n\n（4）这篇工作最大的亮点是端到端，据说是第二篇端到端的工作\n\n# 2、显著性目标检测：structure-consistent weakly supervised salient object detection with local saliency coherence. aaai 2021\n\n（1）改造版本的crf loss\n\n（2）saliency structure consistency loss：先cam再增强和先增强后cam的结果做一个一致性 loss，与seam一致\n\n# 3、图模型做弱监督分割：affinity attention graph neural network for weakly supervised semantic segmentation. ieee tpami 2021\n\n（1）将 image 转换为 graph，提出以下几点\n\n * 每个 feature pixel 作为 node，而不是super pixel\n * 对于任意两个 node 之间都有 edge\n * edge 的值不止是 0 和 1\n\n（2）bbox 的 label + image label\n\n * image label：cam\n * bbox label：grab-cut\n * 两个区域的作为 seed label\n\n（3）如何将 image 转换为 graph？\n\n * 借鉴并改进 affinity cnn\n * 下面是 ac loss 的计算过程，只针对可靠区域计算\n * 在一定距离范围内，如果两个node类别信息一致，其edge值为1，加入 a+ 集合，否则为0，加入 a- 集合\n * 如何获得两个点的预测的边呢：两个点的feature做一个l1 loss（未听清），得到 d(i,j)\n * 下面是 ar loss 的计算过程，不限制区域，对所有区域都计算\n\n\n# pannel 环节\n\n# 1、图像中的背景区域较为杂乱，导致背景类样本具有较大的类内散度，进而导致背景类自身难以准确建模，同时影响其他语义类别的建模，这个问题如何解决？\n\n1、类别无关的物体定位\n\n2、positive unlabel learning，pu学习\n\n3、引入主动学习引入较为精确的标注\n\n4、类内散度不影响，前景背景类较为相似导致的问题，更加强大的预训练模型\n\n# 2、样本 label noise 的问题怎么解决？\n\n1、图像和标注进行质量评估，对于学习的影响也会比较大，课程学习，先用简单样本，再用难样本\n\n2、如何改造我们的全监督网络，让其的容错性更加高\n\n3、dnn 在比较大的噪声标注下还是比较鲁棒的（我觉得应当是对于分割来说的），co-training，\n\n4、co-training，transformer 和 cnn 不同性质的网络\n\n5、label noise主要两个来源：伪标签和众包标注，通过软标记来解决噪声问题\n\n耿新老师：做伪标记的时候，用一个标记分布，用图神经网络做软标记？\n\nco-training + 标记分布，多模型都给一个标记分布\n\n# 3、现在的弱监督学习基本还是沿用mil的建模方式，目前是否有其他的机器学习模型适用于弱监督学习？\n\n1、自学习，带噪声的半监督学习，transformer和自监督transformer可能会有更好的效果（dino）\n\n2、特征空间的规整，在特征学习和后续的分类器做一个 match，新的联合优化的方式\n\n3、标记不完整（半监督，pu学习），不正确（label noise），不精确（image-lavel annotation，mil，偏标记学习）\n\n# 4、弱监督下模型复杂度从理论上是否与效果成正相关？不准确、不全面的标注是否更加难以训练较大规模的网络模型？\n\n1、噪声非常大，大规模网络表现不好，例如 deeplabv2 性能是比较好的，动态调整网络的复杂度\n\n2、scaled-vit , 标签噪声有多大，对表示学习的模型会产生什么特征空间，特征空间会产生什么，理论上给一个定义\n\n3、问题复杂度增加了，机器学习模型的复杂度应当增加。用最匹配的模型来解决\n\n# 5、弱监督学习在什么条件下可以逼近强监督学习的效果？什么场景下可以充分发挥弱监督学习的优势？\n\n1、弱监督学习的问题在于：前背景的界限过于模糊\n\n2、标注强一些，线标或者框标，只有3个点左右的gap，如果只有image-lavel的标注会带来10个点左右的差距，弱监督可以结合半监督，\n\n3、显著性信息的引入，合理地引入先验\n\n4、学习问题的一致性，理论上来讲，数据量是无穷大的时候，可以逼近强监督学习的效果；减小搜索空间：先验知识等等\n\n# 6、当前弱监督学习的核心挑战是什么？当数据不再能驱得动学习过程，是否应该回归模型驱动的学习方法？\n\n1、王：复杂场景的弱监督：引入视觉规律，显著性，边缘（有无纹理），超像素\n\n2、叶：表示学习的挑战是一样的，不完全标注、部分标注、现有的信息来估计不完全的信息，能够处理数据的varience，传统方法引入也不失为一种策略：几何模型\n\n3、耿：小样本弱监督学习（医疗影像），边际递减效应，不断增加数据获得的收益会逐步递减，回归模型驱动，比如决策树，svm，压缩感知等等\n\n# 7、弱监督学习在学术界和工业界的未来研究趋势是什么？是否需要新的benchmark？如何定义新的benchmark?\n\n1、万：主动学习、无监督学习结合\n\n2、肖：工业界的在固定的标注代价下，怎么提升最大的收益，标注的组合，领域迁移的工作\n\n3、王：互联网和医院，抗疫的工作\n\n4、叶：工业界可以利用弱监督学习去获取预训练模型，学术界应该探索新的范式，与无监督和表示学习之间的关系，视频的标注量\n\n5、耿：工业界关心的成本问题，先验知识，模型和数据的匹配，元学习",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"不确定性学习在视觉识别中的应用",frontmatter:{title:"不确定性学习在视觉识别中的应用",date:"2021-07-11T22:19:54.000Z",permalink:"/pages/49a01a/",categories:["技术文章","有意思的文章集合"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/06.%E8%AE%B2%E5%BA%A7%E8%AE%B0%E5%BD%95-%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E6%96%87%E7%AB%A0%E9%9B%86%E5%90%88/02.TeachBeat%20%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%AD%A6%E4%B9%A0%E5%9C%A8%E8%A7%86%E8%A7%89%E8%AF%86%E5%88%AB%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.html",relativePath:"02.学习笔记/06.讲座记录-有意思的文章集合/02.TeachBeat 不确定性学习在视觉识别中的应用.md",key:"v-348c00c9",path:"/pages/49a01a/",headers:[{level:2,title:"不确定性学习在视觉识别中的应用",slug:"不确定性学习在视觉识别中的应用",normalizedTitle:"不确定性学习在视觉识别中的应用",charIndex:2}],headersStr:"不确定性学习在视觉识别中的应用",content:"# 不确定性学习在视觉识别中的应用\n\nSimilarity score 和 Confidence score\n\nWe need to know that we don`t know\n\n为什么不确定性是比较重要的\n\n * 对于高风险的应用非常有价值\n   \n   * 对于医学印象分析\n   * 对于自动驾驶\n\n * 机器学习场景\n   \n   * 主动学习：挑那些不确定度高的样本来标注\n   * 强化学习：explore/exploit dilemma之间的tradeoff\n\n * Out-of-districution detection / Open set\n   \n   * CIFAR-10 作为训练集\n   * 人脸作为测试集，依然会输出一个类别，\n\n数据不确定性：被称作偶然不确定性\n\n * 通常的方法是增加测量的精度或许会有帮助\n\n模型不确定性：被称作认知不确定性\n\n * 通常的方法是增加训练数据大小\n\n深度神经网络是 deterministic（确定的）\n\n如何给 DNN 引入不确定性\n\n * Bayesian Neural Networks\n * Variational Inference for posterior approxomation\n * Dropout as Bayesian Networks + VI, also called Monte Carlo Dropout\n * Model ensemble for uncertainty estimation\n   * Model ensemble 的方差作为不确定性，均值作为输出\n   * MC Dropout is an especial model ensemble\n * Estimate Data Uncertainty in Regression\n\n模型不确定性\n\n数据不确定性：和 Label Noise 是有关的\n\nNIPS 2017 What Uncertainties Do We Need？(NIPS 2017)\n\n数据不确定性和模型不确定性的可视化\n\nData Uncertainty in Object Detection\n\n标注不确定的情况，有标注不一致的情况\n\n * Bounding Box Regressing with Uncertatinty for Accurate Object Detection, CVPR 2019\n * Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving, ICCV 2019\n\nData Uncertainty in Object Detection\n\n从确定的 enbedding 变为 概率分布的 enbedding，每个样本都有个均值和方差作为 enbedding，质量较低以及噪声严重 的图像的方差更大，但这里是针对 x->y 样本的 x 带噪的情况，例如Gaussian Blur 数据增强的分数\n\n总结\n\n# 参考资料\n\nhttps://www.techbeat.net/talk-info?id=359",normalizedContent:"# 不确定性学习在视觉识别中的应用\n\nsimilarity score 和 confidence score\n\nwe need to know that we don`t know\n\n为什么不确定性是比较重要的\n\n * 对于高风险的应用非常有价值\n   \n   * 对于医学印象分析\n   * 对于自动驾驶\n\n * 机器学习场景\n   \n   * 主动学习：挑那些不确定度高的样本来标注\n   * 强化学习：explore/exploit dilemma之间的tradeoff\n\n * out-of-districution detection / open set\n   \n   * cifar-10 作为训练集\n   * 人脸作为测试集，依然会输出一个类别，\n\n数据不确定性：被称作偶然不确定性\n\n * 通常的方法是增加测量的精度或许会有帮助\n\n模型不确定性：被称作认知不确定性\n\n * 通常的方法是增加训练数据大小\n\n深度神经网络是 deterministic（确定的）\n\n如何给 dnn 引入不确定性\n\n * bayesian neural networks\n * variational inference for posterior approxomation\n * dropout as bayesian networks + vi, also called monte carlo dropout\n * model ensemble for uncertainty estimation\n   * model ensemble 的方差作为不确定性，均值作为输出\n   * mc dropout is an especial model ensemble\n * estimate data uncertainty in regression\n\n模型不确定性\n\n数据不确定性：和 label noise 是有关的\n\nnips 2017 what uncertainties do we need？(nips 2017)\n\n数据不确定性和模型不确定性的可视化\n\ndata uncertainty in object detection\n\n标注不确定的情况，有标注不一致的情况\n\n * bounding box regressing with uncertatinty for accurate object detection, cvpr 2019\n * gaussian yolov3: an accurate and fast object detector using localization uncertainty for autonomous driving, iccv 2019\n\ndata uncertainty in object detection\n\n从确定的 enbedding 变为 概率分布的 enbedding，每个样本都有个均值和方差作为 enbedding，质量较低以及噪声严重 的图像的方差更大，但这里是针对 x->y 样本的 x 带噪的情况，例如gaussian blur 数据增强的分数\n\n总结\n\n# 参考资料\n\nhttps://www.techbeat.net/talk-info?id=359",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"样本生而不等",frontmatter:{title:"样本生而不等",date:"2021-07-11T21:46:36.000Z",permalink:"/pages/669e4a/",categories:["技术文章","有意思的文章集合"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/06.%E8%AE%B2%E5%BA%A7%E8%AE%B0%E5%BD%95-%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E6%96%87%E7%AB%A0%E9%9B%86%E5%90%88/05.TeachBeat%20%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E9%87%87%E6%A0%B7.html",relativePath:"02.学习笔记/06.讲座记录-有意思的文章集合/05.TeachBeat 物体检测中的训练样本采样.md",key:"v-0f686957",path:"/pages/669e4a/",headersStr:null,content:"# 参考资料\n\n * https://www.techbeat.net/article-info?id=743\n\n * 针对目标检测的正负样本采样问题，也就是挑哪些 bbox 送进 BBox Head 中进行训练\n\n * 主要有两个方面\n   \n   * 样本的不平衡性\n     \n     * OHME 只考虑了样本的难易程度，可能会对噪声样本更加敏感\n     * Focal Loss 在单阶段检测器中非常work，而在两阶段检测器中的提升有限\n     * 文章探寻了 IoU 和 困难度之间的关系，结论是：高 IoU 的负样本是比较困难的负样本。所以对于负样本采样，提出了 IoU-balanced Negative Sampling\n       * 第一步，把IoU的区间均匀的分成K个区间\n       * 第二步，在每个区间我们会sample N/K个bboxes\n     * 对于正样本采样，提出了 Instance-balanced Positive Sampling\n       * 如果整张图中有N个groundtruths的话，samples N个正样本，那么在K个ground truths附近，每个ground truth旁边sample N/K个正样本，同样，如果最后数量达不到要求，会用随机采样的框来补上。\n     * \n   \n   * 样本的重要性\n     \n     * 困难的样本是不是对于检测器来说是最重要的样本？\n     * 检测和分类分支对于样本的重要度评估是否一致？假定我有一个框，如果能够train出更好的分类器，是不是意味着检测性能更高？\n     * 通过观察得到的结论：\n       * 分类的准确率跟检测的准确率没有必然的关系\n       * 对于一个检测器，如果能够给每个物体周围给一个框一个非常高的score，这个框通常就是比较准的框，然后能够保证所有的物体都被cover到，那么这个检测器就应该是一个好的检测器，所以它跟分类的评价是不一样的，分类要求所有样本的平均性能尽可能高。\n       * 检测通常有两个分支，一个是分类的分支，一个是回归或者定位的分支，这两个分支是相互关联的。\n     * 关键结论\n       * 重新回顾 mAP 的计算过程发现：对所有的ground truth来说，IoU越高的框越重要。\n       * 重新回顾 FP 的计算过程发现：局部来看，分数最高的框是最重要的；全局来看，分数越高的负样本越重要。\n     * 提出了 Hierarchical Local Rank (HLR)\n       * 基于最终框的位置来衡量的，回归之后再衡量其重要性\n       * 正样本的 IoU HLR\n         * 第一步：会先计算Local Rank（这里都是以正样本为例），仍然有｛A，B，C，D，E｝5个正样本，对于A，B，C来说它们cover的是同一个人，所以它们是一组，然后D和E是一组，在每一组中，按它们的IoU进行排序，经过排序之后是C>A>B，D＞E，这样我们就得到了一个局部的排序，针对每个ground truth附近的框的排序。\n         * 第二步：把每一组的TOP 1拿出来进行排序，比如ABC中的TOP 1是C，DE中的TOP 1是D，把C和D拿出来做一个排序，然后把TOP 2A和E也拿出来做一个排序，同样TOP 3也做一个排序，这样我们就得到了一个Hierarchical Local Rank，最终得到的排序是DCAEB。假设这张图中有K个ground truth，TOP K代表每一个ground truth周围IoU最大的框，接下来K个元素代表每个ground truth TOP 2 IoU的框。这就保证了分别跟之前对mAP进行分析时的两条结论相对应。第一条是对于每个样本IoU高的会排在前面，第二条是对于不同的ground truth IoU高的会排在前面。所以最终得到的HierarchicalLocal Rank刚好符合之前对正样本重要性的分析。\n       * 负样本的 Score HLR\n         * 对于负样本来说，选择的就是Score的Hierarchical Local Rank，这也是基于之前对负样本的分析，它的方式基本和正样本一样，把IoU换成Score。\n     * 提出了 PrIme Sample Attention（PISA） 方法\n       * Importance-based SampleReweighting（ISR）\n       * 把 Rank 映射成一个 loss weight\n       * 里面的loss还是用的Positive，对每个Sample都给它一个wi’或者wj’这么一个loss weight，wi’或者wj’是通过上面得到的wi做了一个归一化，因为如果随便加了一个loss weight，最后总的loss的数值变化可能会比较大，为了消除其它因素的影响，就会保证在加了loss weight之后，loss的数值跟不加它是一样的，所以就乘了一个归一化的项。\n       * Classification-Aware Regression Loss（CARL）\n       * 基于我们的思考分类和回归这两个branch不是完全独立的，而是耦合的。分类的分支会有一个cross entropy loss，回归的分支大部分会用一个smoothL1的loss，那么如何把这两支融合起来呢？\n       * 再加一个classifification-awareregression loss，就是carl，这个loss会把分类的Score经过一个函数的映射（这个函数跟刚才Sample reweight指数函数形式完全一样的），跟原始的regression loss相乘，这就是classifification-awareregression loss，为什么叫classifification-aware ，因为这个 regression loss是把分类的Score乘了上去，这里的si是指正样本label分类的Score。\n   \n   * 总结\n   \n   * Sample Imbalance，IoU-balanced Negative Sampling和Instance-balanced Positive Sampling，在不带来overhead的情况下，有接近1个点的提升。\n     \n     Sample importance，我们重新思考了什么样的sample才是重要的sample，也思考了分类任务和检测任务的区别，提出了Prime Sample的概念，来对检测器的训练进行优化。",normalizedContent:"# 参考资料\n\n * https://www.techbeat.net/article-info?id=743\n\n * 针对目标检测的正负样本采样问题，也就是挑哪些 bbox 送进 bbox head 中进行训练\n\n * 主要有两个方面\n   \n   * 样本的不平衡性\n     \n     * ohme 只考虑了样本的难易程度，可能会对噪声样本更加敏感\n     * focal loss 在单阶段检测器中非常work，而在两阶段检测器中的提升有限\n     * 文章探寻了 iou 和 困难度之间的关系，结论是：高 iou 的负样本是比较困难的负样本。所以对于负样本采样，提出了 iou-balanced negative sampling\n       * 第一步，把iou的区间均匀的分成k个区间\n       * 第二步，在每个区间我们会sample n/k个bboxes\n     * 对于正样本采样，提出了 instance-balanced positive sampling\n       * 如果整张图中有n个groundtruths的话，samples n个正样本，那么在k个ground truths附近，每个ground truth旁边sample n/k个正样本，同样，如果最后数量达不到要求，会用随机采样的框来补上。\n     * \n   \n   * 样本的重要性\n     \n     * 困难的样本是不是对于检测器来说是最重要的样本？\n     * 检测和分类分支对于样本的重要度评估是否一致？假定我有一个框，如果能够train出更好的分类器，是不是意味着检测性能更高？\n     * 通过观察得到的结论：\n       * 分类的准确率跟检测的准确率没有必然的关系\n       * 对于一个检测器，如果能够给每个物体周围给一个框一个非常高的score，这个框通常就是比较准的框，然后能够保证所有的物体都被cover到，那么这个检测器就应该是一个好的检测器，所以它跟分类的评价是不一样的，分类要求所有样本的平均性能尽可能高。\n       * 检测通常有两个分支，一个是分类的分支，一个是回归或者定位的分支，这两个分支是相互关联的。\n     * 关键结论\n       * 重新回顾 map 的计算过程发现：对所有的ground truth来说，iou越高的框越重要。\n       * 重新回顾 fp 的计算过程发现：局部来看，分数最高的框是最重要的；全局来看，分数越高的负样本越重要。\n     * 提出了 hierarchical local rank (hlr)\n       * 基于最终框的位置来衡量的，回归之后再衡量其重要性\n       * 正样本的 iou hlr\n         * 第一步：会先计算local rank（这里都是以正样本为例），仍然有｛a，b，c，d，e｝5个正样本，对于a，b，c来说它们cover的是同一个人，所以它们是一组，然后d和e是一组，在每一组中，按它们的iou进行排序，经过排序之后是c>a>b，d＞e，这样我们就得到了一个局部的排序，针对每个ground truth附近的框的排序。\n         * 第二步：把每一组的top 1拿出来进行排序，比如abc中的top 1是c，de中的top 1是d，把c和d拿出来做一个排序，然后把top 2a和e也拿出来做一个排序，同样top 3也做一个排序，这样我们就得到了一个hierarchical local rank，最终得到的排序是dcaeb。假设这张图中有k个ground truth，top k代表每一个ground truth周围iou最大的框，接下来k个元素代表每个ground truth top 2 iou的框。这就保证了分别跟之前对map进行分析时的两条结论相对应。第一条是对于每个样本iou高的会排在前面，第二条是对于不同的ground truth iou高的会排在前面。所以最终得到的hierarchicallocal rank刚好符合之前对正样本重要性的分析。\n       * 负样本的 score hlr\n         * 对于负样本来说，选择的就是score的hierarchical local rank，这也是基于之前对负样本的分析，它的方式基本和正样本一样，把iou换成score。\n     * 提出了 prime sample attention（pisa） 方法\n       * importance-based samplereweighting（isr）\n       * 把 rank 映射成一个 loss weight\n       * 里面的loss还是用的positive，对每个sample都给它一个wi’或者wj’这么一个loss weight，wi’或者wj’是通过上面得到的wi做了一个归一化，因为如果随便加了一个loss weight，最后总的loss的数值变化可能会比较大，为了消除其它因素的影响，就会保证在加了loss weight之后，loss的数值跟不加它是一样的，所以就乘了一个归一化的项。\n       * classification-aware regression loss（carl）\n       * 基于我们的思考分类和回归这两个branch不是完全独立的，而是耦合的。分类的分支会有一个cross entropy loss，回归的分支大部分会用一个smoothl1的loss，那么如何把这两支融合起来呢？\n       * 再加一个classifification-awareregression loss，就是carl，这个loss会把分类的score经过一个函数的映射（这个函数跟刚才sample reweight指数函数形式完全一样的），跟原始的regression loss相乘，这就是classifification-awareregression loss，为什么叫classifification-aware ，因为这个 regression loss是把分类的score乘了上去，这里的si是指正样本label分类的score。\n   \n   * 总结\n   \n   * sample imbalance，iou-balanced negative sampling和instance-balanced positive sampling，在不带来overhead的情况下，有接近1个点的提升。\n     \n     sample importance，我们重新思考了什么样的sample才是重要的sample，也思考了分类任务和检测任务的区别，提出了prime sample的概念，来对检测器的训练进行优化。",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"A Tutorial of Transformers",frontmatter:{title:"A Tutorial of Transformers",date:"2021-06-30T22:54:33.000Z",permalink:"/pages/cdcabd/",categories:["计算机视觉","Transformer"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/06.%E8%AE%B2%E5%BA%A7%E8%AE%B0%E5%BD%95-%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E6%96%87%E7%AB%A0%E9%9B%86%E5%90%88/04.VALSE%20Tutorial%20A%20Tutorial%20of%20Transformers.html",relativePath:"02.学习笔记/06.讲座记录-有意思的文章集合/04.VALSE Tutorial A Tutorial of Transformers.md",key:"v-0bde3425",path:"/pages/cdcabd/",headers:[{level:2,title:"A Tutorial of Transformers",slug:"a-tutorial-of-transformers",normalizedTitle:"a tutorial of transformers",charIndex:2}],headersStr:"A Tutorial of Transformers",content:'# A Tutorial of Transformers\n\n# 1、前言\n\n语言表示学习指的是如何表示语言的语义，发展历程从知识图谱->分布式表示。表示学习将词映射为一个向量，这种向量一般被称为词嵌入（Embeddings）\n\n\n\n上下文编码器将上下文编码进词嵌入中，更准确地去体现词的语义。上下文编码器即为 Model 架构的设计，是模型驱动的，而如何基于数据将特征提取得更好，是数据驱动的\n\n\n\n这里是机器翻译的一个例子，Decoder是一个自回归模型\n\n# 2、如何建立远距离的依赖关系？\n\n全连接是一个非常简单的方式，但是计算量大以及不够灵活\n\n注意力机制，主要过程有两步\n\n * 计算注意力分布，并做归一化\n * 对所有的信息进行加权，根据这个注意力分布做输入做期望\n\n\n\n如何建模词语之间的依赖关系？上图是一个例子，也被成为 self-attention\n\n * 如果我们要查询The 的注意力\n * “The” 这个单词的 Embedding作为q，句子中其他所有词作为v，\n * 将 q 和 v 计算相似度，归一化之后得到权重\n * 最终 “The” 便可以由其他词表示\n\n# 3、建模 Query-Key-Value（QKV）Model\n\n\n\nQKV 有三个可学习的矩阵，使得其模型容量更大，可学习能力也更强\n\n# 4、Multi-head Self-Attention\n\n\n\n在多个高维空间中去建模他们的关系，类似于卷积中的多通道\n\n# 5、Multi-Layer Self-Attention\n\n\n\n# 6、Transformer\n\n\n\n关键模块：Self-Attention\n\n改进 Self-Attention\n\n * 传统Self-Attention只和内容相关，和位置没有关系，加入位置信息的编码\n * Layer Normalization\n * Skip connection\n * Position-wise FFN\n\n# 参考资料：\n\n * Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems. 2017. arXiv link\n * Lin, Tianyang, et al. "A Survey of Transformers." arXiv preprint arXiv:2106.04554 (2021). arXiv link',normalizedContent:'# a tutorial of transformers\n\n# 1、前言\n\n语言表示学习指的是如何表示语言的语义，发展历程从知识图谱->分布式表示。表示学习将词映射为一个向量，这种向量一般被称为词嵌入（embeddings）\n\n\n\n上下文编码器将上下文编码进词嵌入中，更准确地去体现词的语义。上下文编码器即为 model 架构的设计，是模型驱动的，而如何基于数据将特征提取得更好，是数据驱动的\n\n\n\n这里是机器翻译的一个例子，decoder是一个自回归模型\n\n# 2、如何建立远距离的依赖关系？\n\n全连接是一个非常简单的方式，但是计算量大以及不够灵活\n\n注意力机制，主要过程有两步\n\n * 计算注意力分布，并做归一化\n * 对所有的信息进行加权，根据这个注意力分布做输入做期望\n\n\n\n如何建模词语之间的依赖关系？上图是一个例子，也被成为 self-attention\n\n * 如果我们要查询the 的注意力\n * “the” 这个单词的 embedding作为q，句子中其他所有词作为v，\n * 将 q 和 v 计算相似度，归一化之后得到权重\n * 最终 “the” 便可以由其他词表示\n\n# 3、建模 query-key-value（qkv）model\n\n\n\nqkv 有三个可学习的矩阵，使得其模型容量更大，可学习能力也更强\n\n# 4、multi-head self-attention\n\n\n\n在多个高维空间中去建模他们的关系，类似于卷积中的多通道\n\n# 5、multi-layer self-attention\n\n\n\n# 6、transformer\n\n\n\n关键模块：self-attention\n\n改进 self-attention\n\n * 传统self-attention只和内容相关，和位置没有关系，加入位置信息的编码\n * layer normalization\n * skip connection\n * position-wise ffn\n\n# 参考资料：\n\n * vaswani, ashish, et al. "attention is all you need." advances in neural information processing systems. 2017. arxiv link\n * lin, tianyang, et al. "a survey of transformers." arxiv preprint arxiv:2106.04554 (2021). arxiv link',charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"20210411 控制输入，才能更好地控制输出",frontmatter:{title:"20210411 控制输入，才能更好地控制输出",date:"2021-04-14T22:00:46.000Z",permalink:"/pages/bc39b9/",categories:["更多","产品沉思录观后有感"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/07.%E4%BD%93%E4%BC%9A%E6%84%9F%E6%82%9F-%E4%BA%A7%E5%93%81%E6%B2%89%E6%80%9D%E5%BD%95%E8%A7%82%E5%90%8E%E6%9C%89%E6%84%9F/00.%E6%8E%A7%E5%88%B6%E8%BE%93%E5%85%A5%E6%89%8D%E8%83%BD%E6%9B%B4%E5%A5%BD%E5%9C%B0%E6%8E%A7%E5%88%B6%E8%BE%93%E5%87%BA.html",relativePath:"02.学习笔记/07.体会感悟-产品沉思录观后有感/00.控制输入才能更好地控制输出.md",key:"v-380a3df9",path:"/pages/bc39b9/",headersStr:null,content:"# Vol.20210404 少数人的智慧，像管理投资组合一样来管理知识\n\n# 像管理投资组合一样来管理知识\n\n 1. 定期投资 —— 养成坚持学习的习惯\n 2. 长期投资，多元化是关键 —— 专注眼下的事情，但也要注意更广泛的范围\n 3. 管理风险 —— 不要把「鸡蛋都放在一个篮子里」，也也要避免低风险低回报\n 4. 设法低买高卖，获取最大利润 —— 学习新兴技术或者适用面逐渐广的技术\n 5. 周期性的评估和平衡 —— 定期评估哪些是值得学习和关注的\n\n将阅读、思考和写作结合到一起，才能成为一个更好的思考者。如果想要正确地思考，还是得动手记录。不要练习浅层思考，而是深层思考。\n\n# Vol.20210411 控制输入，才能更好地控制输出\n\n# 关于信息时代的教育\n\n这一期有朋友分享了一个观点，关于教育和人工智能，我觉得思路真的特别清晰，这种分类思考的方式值得我去学习。这个观点将教育者教授的知识分为了四类：\n\n * 事实类知识：例如历史知识，鸦片战争多久爆发的？\n * 概念类知识：例如历史知识的溯源，鸦片战争为什么爆发？\n * 程序类知识：例如历史知识的搜集方式，从哪里能够找到鸦片战争的资料？\n * 元认知知识：例如我过往的认知和当下的认知有差异，这种差异是我自我在哪些地方成长了？\n\n人工智能擅长讲授事实类知识，而教育者对学生提供的帮助是概念类的知识、程序类的知识，那么教育机构不会被颠覆，并且如果能和人工智能互补能够强强联手，我们的教育将会更加的全面，我们在接触和处理信息时会更加准确和理性。\n\n但如果教育者仅仅提供事实类知识，而人工智能又把海量的知识经过推荐算法堆到你面前，人类是很难做出回应的，种种观点的冲突和混乱争吵的声音导致迷茫和焦虑。\n\n# 不要低估逆向思考\n\n其实逆向思考是一种强大的思维工具，因为它将初看起来并不明显的错误和障碍暴露在聚光灯下。如果事实恰恰相反呢？如果我把注意力集中在这种情况的不同方面会怎样？与其问如何做某事成功，不如问如何不去做某事而导致失败。\n\n * 管理者可以用来思考「糟糕的管理者每天都在做什么」，然后避免自己重复\n * 企业家可以用来思考「怎么才能让公司不那么创新呢」，避免原地踏步\n * 营销者可以用来思考「做什么事会让我们核心的客户离开我们」\n\n我最近在想一些如何运营学生技术社区的事情，得开始想想这个话题了“怎么才能让技术社区失去吸引力呢？”",normalizedContent:"# vol.20210404 少数人的智慧，像管理投资组合一样来管理知识\n\n# 像管理投资组合一样来管理知识\n\n 1. 定期投资 —— 养成坚持学习的习惯\n 2. 长期投资，多元化是关键 —— 专注眼下的事情，但也要注意更广泛的范围\n 3. 管理风险 —— 不要把「鸡蛋都放在一个篮子里」，也也要避免低风险低回报\n 4. 设法低买高卖，获取最大利润 —— 学习新兴技术或者适用面逐渐广的技术\n 5. 周期性的评估和平衡 —— 定期评估哪些是值得学习和关注的\n\n将阅读、思考和写作结合到一起，才能成为一个更好的思考者。如果想要正确地思考，还是得动手记录。不要练习浅层思考，而是深层思考。\n\n# vol.20210411 控制输入，才能更好地控制输出\n\n# 关于信息时代的教育\n\n这一期有朋友分享了一个观点，关于教育和人工智能，我觉得思路真的特别清晰，这种分类思考的方式值得我去学习。这个观点将教育者教授的知识分为了四类：\n\n * 事实类知识：例如历史知识，鸦片战争多久爆发的？\n * 概念类知识：例如历史知识的溯源，鸦片战争为什么爆发？\n * 程序类知识：例如历史知识的搜集方式，从哪里能够找到鸦片战争的资料？\n * 元认知知识：例如我过往的认知和当下的认知有差异，这种差异是我自我在哪些地方成长了？\n\n人工智能擅长讲授事实类知识，而教育者对学生提供的帮助是概念类的知识、程序类的知识，那么教育机构不会被颠覆，并且如果能和人工智能互补能够强强联手，我们的教育将会更加的全面，我们在接触和处理信息时会更加准确和理性。\n\n但如果教育者仅仅提供事实类知识，而人工智能又把海量的知识经过推荐算法堆到你面前，人类是很难做出回应的，种种观点的冲突和混乱争吵的声音导致迷茫和焦虑。\n\n# 不要低估逆向思考\n\n其实逆向思考是一种强大的思维工具，因为它将初看起来并不明显的错误和障碍暴露在聚光灯下。如果事实恰恰相反呢？如果我把注意力集中在这种情况的不同方面会怎样？与其问如何做某事成功，不如问如何不去做某事而导致失败。\n\n * 管理者可以用来思考「糟糕的管理者每天都在做什么」，然后避免自己重复\n * 企业家可以用来思考「怎么才能让公司不那么创新呢」，避免原地踏步\n * 营销者可以用来思考「做什么事会让我们核心的客户离开我们」\n\n我最近在想一些如何运营学生技术社区的事情，得开始想想这个话题了“怎么才能让技术社区失去吸引力呢？”",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"VALSE Webinar 20-02 元学习与小样本学习",frontmatter:{title:"VALSE Webinar 20-02 元学习与小样本学习",date:"2021-09-13T14:33:07.000Z",permalink:"/pages/7f9724/",categories:["学习笔记","讲座记录-有意思的文章集合"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/06.%E8%AE%B2%E5%BA%A7%E8%AE%B0%E5%BD%95-%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E6%96%87%E7%AB%A0%E9%9B%86%E5%90%88/06.VALSE%20Webinar%2020-02%20%E5%85%83%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0.html",relativePath:"02.学习笔记/06.讲座记录-有意思的文章集合/06.VALSE Webinar 20-02 元学习与小样本学习.md",key:"v-7030c545",path:"/pages/7f9724/",headers:[{level:2,title:"VALSE Webinar 20-02 元学习与小样本学习",slug:"valse-webinar-20-02-元学习与小样本学习",normalizedTitle:"valse webinar 20-02 元学习与小样本学习",charIndex:2},{level:3,title:"01、Siyuan Qiao JHU",slug:"_01、siyuan-qiao-jhu",normalizedTitle:"01、siyuan qiao jhu",charIndex:36},{level:3,title:"02、Deyu Meng XJTU",slug:"_02、deyu-meng-xjtu",normalizedTitle:"02、deyu meng xjtu",charIndex:560}],headersStr:"VALSE Webinar 20-02 元学习与小样本学习 01、Siyuan Qiao JHU 02、Deyu Meng XJTU",content:"# VALSE Webinar 20-02 元学习与小样本学习\n\n\n# 01、Siyuan Qiao JHU\n\n# Few-Shot Image Recognition by Predictiong Parameters from Activations (CVPR 2018)\n\nm-shot n-way image recognition\n\n * m-shot: each new class has m training images\n * n-way: predict the class of test images from n classes\n\nFew-shot + large-scale image recognition\n\n * pre-training on large-scale datasets(black) and few-shot adaptation to new classes(green).\n * 有点像 open-set 问题设置\n\n两个数据集：Few-shot DfewD_{few}Dfew 和 DlargeD_{large}Dlarge\n\n目标：在两个数据集上效果都比较优异\n\n两个小tricks:\n\n * Multi-View：\n\n * 集成学习， 学两个映射\n\n\n# 02、Deyu Meng XJTU\n\n鲁棒深度学习与元学习\n\n * 深度学习：标注质量很高的数据集，但是现实情况中数据是存在偏差的（Data bias），例如小样本、弱监督、标签带噪\n   \n   * Label Noise：标签是带有噪声的\n   * Data Noise：数据本身带有噪声的\n   * Class imbalance：类别不平衡\n\n * 鲁棒学习\n   \n   * 设计不同的鲁棒优化目标，例如鲁棒的损失函数\n   * 化腐朽为神奇：从质量很差的数据集中依然能抽取出我们想要的信息\n   * 损失函数\n     * Label Noise\n       * Generalized CE (NeurIPS 2018)\n       * Symmetric CE (ICCV 2019)\n       * Bi-Tempered logistic loss (NeurIPS 2019)\n       * Polynomial Softweighting loss (AAAI 2015)\n     * Focal loss (TPAMI 2018)：class imbalance\n     * CT loss (TMI 2018): data noise\n     * 问题：需要设置超参数，非凸优化\n\n * 元学习\n\n * 验证数据集和训练数据集的区别\n   \n   * 验证数据集用于超参数的调整，训练数据集用于分类器参数的学习\n\n * 设计一个 Meta loss\n   \n   * Optimization instead of search\n   * Intelligent instead of heuristic (partially)\n\n * Sample Reweighting methods\n   \n   * Self-paced\n   * Linear weighting\n   * Focal Loss\n   * Hard example mining：采样\n   * Prediction variance\n\n * 对样本的损失前加一个权重，放大/缩小一些样本损失\n   \n   * 出现了截然不同的两种加权策略\n     * 误差大权重小，误差小权重大：认为误差大的样本是噪声样本（label noise）\n     * 误差大权重大，误差小权重小：认为误差大的样本是难样本（class imbalance $ ohem）\n   * 使用 Meta Learning，将样本权重当做超参数去学出来\n     * MentorNet：V太多了，不能有效地利用前序信息，训练起来不稳定\n     * Meng组的改进：Meta-Weight-Net\n       * Meta-Weight-Net: Learning an Explict Mapping for Sample Weighting. (NeuIPS 2019)\n       * 将V变成一个函数，输入是Loss，输出是weight，函数希望既能够拟合单调递增又能够拟合单调递减，所以就用MLP来实现\n       * 从下图中可以看出，学的确实不错\n       * ",normalizedContent:"# valse webinar 20-02 元学习与小样本学习\n\n\n# 01、siyuan qiao jhu\n\n# few-shot image recognition by predictiong parameters from activations (cvpr 2018)\n\nm-shot n-way image recognition\n\n * m-shot: each new class has m training images\n * n-way: predict the class of test images from n classes\n\nfew-shot + large-scale image recognition\n\n * pre-training on large-scale datasets(black) and few-shot adaptation to new classes(green).\n * 有点像 open-set 问题设置\n\n两个数据集：few-shot dfewd_{few}dfew 和 dlarged_{large}dlarge\n\n目标：在两个数据集上效果都比较优异\n\n两个小tricks:\n\n * multi-view：\n\n * 集成学习， 学两个映射\n\n\n# 02、deyu meng xjtu\n\n鲁棒深度学习与元学习\n\n * 深度学习：标注质量很高的数据集，但是现实情况中数据是存在偏差的（data bias），例如小样本、弱监督、标签带噪\n   \n   * label noise：标签是带有噪声的\n   * data noise：数据本身带有噪声的\n   * class imbalance：类别不平衡\n\n * 鲁棒学习\n   \n   * 设计不同的鲁棒优化目标，例如鲁棒的损失函数\n   * 化腐朽为神奇：从质量很差的数据集中依然能抽取出我们想要的信息\n   * 损失函数\n     * label noise\n       * generalized ce (neurips 2018)\n       * symmetric ce (iccv 2019)\n       * bi-tempered logistic loss (neurips 2019)\n       * polynomial softweighting loss (aaai 2015)\n     * focal loss (tpami 2018)：class imbalance\n     * ct loss (tmi 2018): data noise\n     * 问题：需要设置超参数，非凸优化\n\n * 元学习\n\n * 验证数据集和训练数据集的区别\n   \n   * 验证数据集用于超参数的调整，训练数据集用于分类器参数的学习\n\n * 设计一个 meta loss\n   \n   * optimization instead of search\n   * intelligent instead of heuristic (partially)\n\n * sample reweighting methods\n   \n   * self-paced\n   * linear weighting\n   * focal loss\n   * hard example mining：采样\n   * prediction variance\n\n * 对样本的损失前加一个权重，放大/缩小一些样本损失\n   \n   * 出现了截然不同的两种加权策略\n     * 误差大权重小，误差小权重大：认为误差大的样本是噪声样本（label noise）\n     * 误差大权重大，误差小权重小：认为误差大的样本是难样本（class imbalance $ ohem）\n   * 使用 meta learning，将样本权重当做超参数去学出来\n     * mentornet：v太多了，不能有效地利用前序信息，训练起来不稳定\n     * meng组的改进：meta-weight-net\n       * meta-weight-net: learning an explict mapping for sample weighting. (neuips 2019)\n       * 将v变成一个函数，输入是loss，输出是weight，函数希望既能够拟合单调递增又能够拟合单调递减，所以就用mlp来实现\n       * 从下图中可以看出，学的确实不错\n       * ",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"20210426 差异化才能生存，泯然众生是宇宙引力",frontmatter:{title:"20210426 差异化才能生存，泯然众生是宇宙引力",date:"2021-04-26T22:03:04.000Z",permalink:"/pages/8ee315/",categories:["更多","产品沉思录观后有感"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/07.%E4%BD%93%E4%BC%9A%E6%84%9F%E6%82%9F-%E4%BA%A7%E5%93%81%E6%B2%89%E6%80%9D%E5%BD%95%E8%A7%82%E5%90%8E%E6%9C%89%E6%84%9F/01.%E5%B7%AE%E5%BC%82%E5%8C%96%E6%89%8D%E8%83%BD%E7%94%9F%E5%AD%98%EF%BC%8C%E6%B3%AF%E7%84%B6%E4%BC%97%E7%94%9F%E6%98%AF%E5%AE%87%E5%AE%99%E5%BC%95%E5%8A%9B.html",relativePath:"02.学习笔记/07.体会感悟-产品沉思录观后有感/01.差异化才能生存，泯然众生是宇宙引力.md",key:"v-8c039fd8",path:"/pages/8ee315/",headersStr:null,content:'# VOL. 20210425 差异化才能生存，泯然众生是宇宙引力\n\n这期里有朋友分享了《局外人》这本书，看到简介挺感兴趣，准备读读\n\n"我们能「真实」感受的其实就是身边的世界，你的家庭，朋友，同事等等，而多数时候，我们都是通过「舆论」来认识这个世界的。"\n\n"舆论是弱者的武器，强者可以剥夺弱者的一切，但剥夺不了弱者的评价。"\n\n这期摘抄了几句话，都很有意思。**差异化才能生存，泯然众生是宇宙引力。**读起来是真的晦涩，但是读完解析后豁然开朗。动物为了对抗环境，所以在活着的时候努力维持自己的体温来与环境抗衡，直到死去才会被环境温度左右；历史中混乱是常态，和平是偶然，所以一代代政权为了对抗历史洪流的必然做出了极大的努力；宇宙里熵增是必然，所以我们现在处处听到推崇低熵生活的声音，来避免被宇宙同化。宇宙是否是拟人态的不重要，但它给予我们阻力让我们进化和成长的过程很有意思，与天斗真是其乐无穷呵。持续不断地付出努力避免被同化是我们进化的唯一途径，被宇宙同化，任引力摆布，终是会把人拉扯成奇形怪状，不是人的模样。\n\n而关于舆论的部分也是好玩，舆论是弱者的武器，不过这个武器容易被强者滥用。我们真实感知到的舆论构成了我们对周遭环境的局部认识，而这些认识会引导我们产生新的舆论，最终整个空间都充斥着不同的声音。最近在看《觉醒年代》，我觉得只有当周遭环境的认识归一，大家都被压迫得抬不起身子，有真切的一致的感受后才会形成一股力量，而这股力量必将野蛮生长。从思想启蒙做起，仲甫先生实乃高人。',normalizedContent:'# vol. 20210425 差异化才能生存，泯然众生是宇宙引力\n\n这期里有朋友分享了《局外人》这本书，看到简介挺感兴趣，准备读读\n\n"我们能「真实」感受的其实就是身边的世界，你的家庭，朋友，同事等等，而多数时候，我们都是通过「舆论」来认识这个世界的。"\n\n"舆论是弱者的武器，强者可以剥夺弱者的一切，但剥夺不了弱者的评价。"\n\n这期摘抄了几句话，都很有意思。**差异化才能生存，泯然众生是宇宙引力。**读起来是真的晦涩，但是读完解析后豁然开朗。动物为了对抗环境，所以在活着的时候努力维持自己的体温来与环境抗衡，直到死去才会被环境温度左右；历史中混乱是常态，和平是偶然，所以一代代政权为了对抗历史洪流的必然做出了极大的努力；宇宙里熵增是必然，所以我们现在处处听到推崇低熵生活的声音，来避免被宇宙同化。宇宙是否是拟人态的不重要，但它给予我们阻力让我们进化和成长的过程很有意思，与天斗真是其乐无穷呵。持续不断地付出努力避免被同化是我们进化的唯一途径，被宇宙同化，任引力摆布，终是会把人拉扯成奇形怪状，不是人的模样。\n\n而关于舆论的部分也是好玩，舆论是弱者的武器，不过这个武器容易被强者滥用。我们真实感知到的舆论构成了我们对周遭环境的局部认识，而这些认识会引导我们产生新的舆论，最终整个空间都充斥着不同的声音。最近在看《觉醒年代》，我觉得只有当周遭环境的认识归一，大家都被压迫得抬不起身子，有真切的一致的感受后才会形成一股力量，而这股力量必将野蛮生长。从思想启蒙做起，仲甫先生实乃高人。',charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"20210502 做加法易，而做减法需要更多的认知努力",frontmatter:{title:"20210502 做加法易，而做减法需要更多的认知努力",date:"2021-05-07T16:46:53.000Z",permalink:"/pages/e6bc11/",categories:["更多","产品沉思录观后有感"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/07.%E4%BD%93%E4%BC%9A%E6%84%9F%E6%82%9F-%E4%BA%A7%E5%93%81%E6%B2%89%E6%80%9D%E5%BD%95%E8%A7%82%E5%90%8E%E6%9C%89%E6%84%9F/02.%E5%81%9A%E5%8A%A0%E6%B3%95%E6%98%93%EF%BC%8C%E8%80%8C%E5%81%9A%E5%87%8F%E6%B3%95%E9%9C%80%E8%A6%81%E6%9B%B4%E5%A4%9A%E7%9A%84%E8%AE%A4%E7%9F%A5%E5%8A%AA%E5%8A%9B.html",relativePath:"02.学习笔记/07.体会感悟-产品沉思录观后有感/02.做加法易，而做减法需要更多的认知努力.md",key:"v-5afadf0a",path:"/pages/e6bc11/",headersStr:null,content:"# Vol.20210502 做加法易，而做减法需要更多的认知努力\n\n只希望我们都要好好睡觉、该充电的时候好好充电、明白自己的一天是如何构成的，然后可以踏实地入睡～ Be water，my friends！（一直在追求生活的绵柔感\n\n * **Courage. 勇气：**未曾经历不幸，是一件更不幸的事情。因为你无法知道你真实的能力。而所谓的不幸，只不过是世界给你提的问题，你应该从中看到机会和成长的空间，这其中便是勇气重要的原因。\n * **Temperance. 节制：**不要做过分的事，以正确的方式做正确的事情。\n * Justice. 正义：出发的原点，认命不认输。\n * **Wisdom. 智慧：**驾驭上述三个美德的核心，在于拥有智慧。而拥有智慧，核心在于头脑开放。你不能学习你认为已经知道的知识\n\n3. 思维：人们系统性地忽略了减法带来的变化\n\n再次证明「少即是多」「如无必要，勿增实体」等说法具有的智慧。简单来说就是：感性默认做加法，而减法需要理性思考。\n\n「不」是一个决定，「是」是一个责任。\n\n**每次我们对一个请求说是，我们也就是在对我们可能在时间上完成的任何其他事情说不。**背后原因，从经济学的视角来看来自于「双曲折现」，即我们在对未来的收益评估其价值时，倾向于对较近的时期采用更低的折现率，对较远的时期采用更高的折现率 —— 比如要今天确定的 100 元，而不是一年后的 200 元。\n\n说「不」，帮你保存了生命中最重要的资产「时间」。对任何不能引导你朝着目标前进的事情说不，它是唯一高效的方法；说不，其实是以专注的方式说是，尤其是探索阶段，掌握主动权。\n\n如何说「不」呢？解决之道在于：尝试「立即说是」后立即就执行，是否会带来痛苦。这就是避免双曲贴现的心理效应，也能让你不会对未来做出许多愚蠢的承诺。（下周也准备试试这种决策方式，挺有趣的）\n\n使用「不」时，我们是在和机会成本做斗争。所谓机会成本，任何事物的机会成本都是你为了得到它而不得不放弃的东西。我们日常看到很多请求，孤立的看都是正确的，但是放到机会成本的角度来看，利害关系才更明显 —— 我们说是，也意味着用时间完成其他的事情，那么想想这些时间干什么事情值得。",normalizedContent:"# vol.20210502 做加法易，而做减法需要更多的认知努力\n\n只希望我们都要好好睡觉、该充电的时候好好充电、明白自己的一天是如何构成的，然后可以踏实地入睡～ be water，my friends！（一直在追求生活的绵柔感\n\n * **courage. 勇气：**未曾经历不幸，是一件更不幸的事情。因为你无法知道你真实的能力。而所谓的不幸，只不过是世界给你提的问题，你应该从中看到机会和成长的空间，这其中便是勇气重要的原因。\n * **temperance. 节制：**不要做过分的事，以正确的方式做正确的事情。\n * justice. 正义：出发的原点，认命不认输。\n * **wisdom. 智慧：**驾驭上述三个美德的核心，在于拥有智慧。而拥有智慧，核心在于头脑开放。你不能学习你认为已经知道的知识\n\n3. 思维：人们系统性地忽略了减法带来的变化\n\n再次证明「少即是多」「如无必要，勿增实体」等说法具有的智慧。简单来说就是：感性默认做加法，而减法需要理性思考。\n\n「不」是一个决定，「是」是一个责任。\n\n**每次我们对一个请求说是，我们也就是在对我们可能在时间上完成的任何其他事情说不。**背后原因，从经济学的视角来看来自于「双曲折现」，即我们在对未来的收益评估其价值时，倾向于对较近的时期采用更低的折现率，对较远的时期采用更高的折现率 —— 比如要今天确定的 100 元，而不是一年后的 200 元。\n\n说「不」，帮你保存了生命中最重要的资产「时间」。对任何不能引导你朝着目标前进的事情说不，它是唯一高效的方法；说不，其实是以专注的方式说是，尤其是探索阶段，掌握主动权。\n\n如何说「不」呢？解决之道在于：尝试「立即说是」后立即就执行，是否会带来痛苦。这就是避免双曲贴现的心理效应，也能让你不会对未来做出许多愚蠢的承诺。（下周也准备试试这种决策方式，挺有趣的）\n\n使用「不」时，我们是在和机会成本做斗争。所谓机会成本，任何事物的机会成本都是你为了得到它而不得不放弃的东西。我们日常看到很多请求，孤立的看都是正确的，但是放到机会成本的角度来看，利害关系才更明显 —— 我们说是，也意味着用时间完成其他的事情，那么想想这些时间干什么事情值得。",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"时间的河入海流",frontmatter:{title:"时间的河入海流",date:"2021-07-09T16:12:38.000Z",permalink:"/pages/18a53e/",categories:["更多","产品沉思录观后有感"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/07.%E4%BD%93%E4%BC%9A%E6%84%9F%E6%82%9F-%E4%BA%A7%E5%93%81%E6%B2%89%E6%80%9D%E5%BD%95%E8%A7%82%E5%90%8E%E6%9C%89%E6%84%9F/03.%E6%97%B6%E9%97%B4%E7%9A%84%E6%B2%B3%E5%85%A5%E6%B5%B7%E6%B5%81.html",relativePath:"02.学习笔记/07.体会感悟-产品沉思录观后有感/03.时间的河入海流.md",key:"v-bf597a0c",path:"/pages/18a53e/",headersStr:null,content:"# Vol.20210704：除了书之外，观察世界的另一组镜头\n\n一些基本的立论\n\n * 一本书给你的知识比这个星球上所有其他人都多，因为大多数人不读书。\n * 两本关于同主题的书也比任何人都能给你更多的知识，因为大多数人不会同时读两本关于同一事物的书。\n * 同主题的两本书，从竞争优势的角度来看，是拐点，第 3 本书的效用开始下降。\n\n这也是为什么我很推崇一种行动指南：「完成比完美重要」，因为完成代表了一次事件的发生，尔后获取反馈，可以准备下一次进化；而追求完美，看似是希望能全局最优，但是由于自身见识的局限性，以及环境的不断变化加上竞争者不断地追赶，最终会让本来还有的局部优势消失殆尽。\n\n较适者生存，而非最适者生存：原因在于任何时期都在不停地产生新的有效竞争者，且大量潜在竞争者尚未出现，因而永远达不到永久性的物种平衡。\n\n领先一步是先进，领先三步是先烈。\n\n前些日子终于开始读《万历十五年》，因为看完了《明朝那些事儿》，对万历年间的前后事情和重要人物，都有所了解后，开始激发了新的好奇心，那个年份到底发生了什么，以及当时的社会、经济、政治是什么样的。\n \n\n和麦克卢汉在《理解媒介》中的冷媒介、热媒介有些相似，西蒙 · 赫伯特认为，人们有两种认知的方式：\n\n冷认知：解决问题缓慢阶段的搜索，相对不容易陷入强烈的情感。\n热认知：突然间发现或者某个「啊哈」时刻，能唤起强烈的情感共鸣。\n \n直接读历史书，就像是上面的冷认知，虽然能理性的学习到很多具体的知识，但是很容易疲倦（当时上历史课都是这种感觉），而《明朝那些事儿》这种充满了浓烈的情绪的著作，很容易让人欲罢不能。书中举的是《寂静的春天》的例子，在这本书出版之前，行业内就已经知道了，但是这本书产生强大的影响，在于激发出来了情感，引发了人们的注意力，并且这种情感被激发出来就不会消失。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n参考资料：\n\n * 抽象一步是理想和现实之间一座坚实的桥梁",normalizedContent:"# vol.20210704：除了书之外，观察世界的另一组镜头\n\n一些基本的立论\n\n * 一本书给你的知识比这个星球上所有其他人都多，因为大多数人不读书。\n * 两本关于同主题的书也比任何人都能给你更多的知识，因为大多数人不会同时读两本关于同一事物的书。\n * 同主题的两本书，从竞争优势的角度来看，是拐点，第 3 本书的效用开始下降。\n\n这也是为什么我很推崇一种行动指南：「完成比完美重要」，因为完成代表了一次事件的发生，尔后获取反馈，可以准备下一次进化；而追求完美，看似是希望能全局最优，但是由于自身见识的局限性，以及环境的不断变化加上竞争者不断地追赶，最终会让本来还有的局部优势消失殆尽。\n\n较适者生存，而非最适者生存：原因在于任何时期都在不停地产生新的有效竞争者，且大量潜在竞争者尚未出现，因而永远达不到永久性的物种平衡。\n\n领先一步是先进，领先三步是先烈。\n\n前些日子终于开始读《万历十五年》，因为看完了《明朝那些事儿》，对万历年间的前后事情和重要人物，都有所了解后，开始激发了新的好奇心，那个年份到底发生了什么，以及当时的社会、经济、政治是什么样的。\n \n\n和麦克卢汉在《理解媒介》中的冷媒介、热媒介有些相似，西蒙 · 赫伯特认为，人们有两种认知的方式：\n\n冷认知：解决问题缓慢阶段的搜索，相对不容易陷入强烈的情感。\n热认知：突然间发现或者某个「啊哈」时刻，能唤起强烈的情感共鸣。\n \n直接读历史书，就像是上面的冷认知，虽然能理性的学习到很多具体的知识，但是很容易疲倦（当时上历史课都是这种感觉），而《明朝那些事儿》这种充满了浓烈的情绪的著作，很容易让人欲罢不能。书中举的是《寂静的春天》的例子，在这本书出版之前，行业内就已经知道了，但是这本书产生强大的影响，在于激发出来了情感，引发了人们的注意力，并且这种情感被激发出来就不会消失。\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n参考资料：\n\n * 抽象一步是理想和现实之间一座坚实的桥梁",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"摄影文章收藏",frontmatter:{title:"摄影文章收藏",date:"2021-08-02T15:49:49.000Z",permalink:"/pages/60f54c/",categories:["更多","摄影"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/08.%E4%BD%93%E4%BC%9A%E6%84%9F%E6%82%9F-%E6%91%84%E5%BD%B1/01.%E6%91%84%E5%BD%B1%E6%96%87%E7%AB%A0%E6%94%B6%E8%97%8F.html",relativePath:"02.学习笔记/08.体会感悟-摄影/01.摄影文章收藏.md",key:"v-7b4fdfde",path:"/pages/60f54c/",headers:[{level:3,title:"器材党最喜欢的文章",slug:"器材党最喜欢的文章",normalizedTitle:"器材党最喜欢的文章",charIndex:2},{level:3,title:"游记文章",slug:"游记文章",normalizedTitle:"游记文章",charIndex:101},{level:3,title:"滤镜资源",slug:"滤镜资源",normalizedTitle:"滤镜资源",charIndex:258}],headersStr:"器材党最喜欢的文章 游记文章 滤镜资源",content:"# 器材党最喜欢的文章\n\n * 相机好觅，镜头难挑：看看这些「老师傅」是如何推荐自己最爱的那一枚\n\n * https://www.bilibili.com/video/BV1PL4y1e7Lu\n\n\n# 游记文章\n\n * 新疆 16 天自驾攻略——草甸雪山落日星空，在路上，遇见大美的新疆\n\n滤镜文章\n\n * 泛用、简单、易出片——Fuji Classic Negative 风格滤镜\n * Re-creating the Fuji Classic Negative Look for Any Camera!\n\n\n# 滤镜资源\n\n * I Re-created Fuji’s Classic Negative as a preset for any Camera\n * https://fujixweekly.com/\n * https://weibo.com/ttarticle/p/show?id=2309404448281001328979#_0\n\n如何处理照片\n\n * 拿拍完的照片，做本摄影集吧\n\nhttps://www.bilibili.com/video/BV1vU4y1N7Ld\n\n# 评价画面优劣的标准\n\n * 表达普遍性主题\n\n * 画面形象表现力强\n\n * 具有丰富或强烈的情感因素\n\n * 画面形象简洁，主体对象突出\n\n * 画面具有形式美",normalizedContent:"# 器材党最喜欢的文章\n\n * 相机好觅，镜头难挑：看看这些「老师傅」是如何推荐自己最爱的那一枚\n\n * https://www.bilibili.com/video/bv1pl4y1e7lu\n\n\n# 游记文章\n\n * 新疆 16 天自驾攻略——草甸雪山落日星空，在路上，遇见大美的新疆\n\n滤镜文章\n\n * 泛用、简单、易出片——fuji classic negative 风格滤镜\n * re-creating the fuji classic negative look for any camera!\n\n\n# 滤镜资源\n\n * i re-created fuji’s classic negative as a preset for any camera\n * https://fujixweekly.com/\n * https://weibo.com/ttarticle/p/show?id=2309404448281001328979#_0\n\n如何处理照片\n\n * 拿拍完的照片，做本摄影集吧\n\nhttps://www.bilibili.com/video/bv1vu4y1n7ld\n\n# 评价画面优劣的标准\n\n * 表达普遍性主题\n\n * 画面形象表现力强\n\n * 具有丰富或强烈的情感因素\n\n * 画面形象简洁，主体对象突出\n\n * 画面具有形式美",charsets:{cjk:!0},lastUpdated:"2021/12/14, 00:10:07"},{title:"mmsegmentation框架解析（上）",frontmatter:{title:"mmsegmentation框架解析（上）",date:"2021-04-13T10:16:09.000Z",permalink:"/pages/3fb5c1/",categories:["计算机视觉","图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/05.mmsegmentation%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90%EF%BC%88%E4%B8%8A%EF%BC%89.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/05.mmsegmentation框架解析（上）.md",key:"v-7cba1716",path:"/pages/3fb5c1/",headers:[{level:2,title:"mmsegmentation 框架解析",slug:"mmsegmentation-框架解析",normalizedTitle:"mmsegmentation 框架解析",charIndex:2},{level:3,title:"Tutorial 1：配置文件",slug:"tutorial-1-配置文件",normalizedTitle:"tutorial 1：配置文件",charIndex:47},{level:3,title:"Tutorial 2：自定义数据集",slug:"tutorial-2-自定义数据集",normalizedTitle:"tutorial 2：自定义数据集",charIndex:14100}],headersStr:"mmsegmentation 框架解析 Tutorial 1：配置文件 Tutorial 2：自定义数据集",content:"# mmsegmentation 框架解析\n\n按照官方的 tutorial 先过一遍\n\n\n# Tutorial 1：配置文件\n\n# 1、配置文件的基本知识\n\n1、python tools/print_config.py /PATH/TO/CONFIG 可以用来可视化完整的config\n\n2、配置文件可以通过 --options xxx.yyy=zzz 来更新 config\n\npython tools/print_config.py mmsegmentation/configs/deeplabv3plus/deeplabv3plus_r18-d8_512x1024_80k_cityscapes.py\n\n\n1\n\n\n输出：\n\nConfig:\nnorm_cfg = dict(type='SyncBN', requires_grad=True)\nmodel = dict(\n    type='EncoderDecoder',\n    pretrained='open-mmlab://resnet18_v1c',\n    backbone=dict(\n        type='ResNetV1c',\n        depth=18,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        dilations=(1, 1, 2, 4),\n        strides=(1, 2, 1, 1),\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\n        norm_eval=False,\n        style='pytorch',\n        contract_dilation=True),\n    decode_head=dict(\n        type='DepthwiseSeparableASPPHead',\n        in_channels=512,\n        in_index=3,\n        channels=128,\n        dilations=(1, 12, 24, 36),\n        c1_in_channels=64,\n        c1_channels=12,\n        dropout_ratio=0.1,\n        num_classes=19,\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\n        align_corners=False,\n        loss_decode=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),\n    auxiliary_head=dict(\n        type='FCNHead',\n        in_channels=256,\n        in_index=2,\n        channels=64,\n        num_convs=1,\n        concat_input=False,\n        dropout_ratio=0.1,\n        num_classes=19,\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\n        align_corners=False,\n        loss_decode=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=0.4)),\n    train_cfg=dict(),\n    test_cfg=dict(mode='whole'))\ndataset_type = 'CityscapesDataset'\ndata_root = 'data/cityscapes/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ncrop_size = (512, 1024)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations'),\n    dict(type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),\n    dict(type='RandomCrop', crop_size=(512, 1024), cat_max_ratio=0.75),\n    dict(type='RandomFlip', prob=0.5),\n    dict(type='PhotoMetricDistortion'),\n    dict(\n        type='Normalize',\n        mean=[123.675, 116.28, 103.53],\n        std=[58.395, 57.12, 57.375],\n        to_rgb=True),\n    dict(type='Pad', size=(512, 1024), pad_val=0, seg_pad_val=255),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(2048, 1024),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(\n                type='Normalize',\n                mean=[123.675, 116.28, 103.53],\n                std=[58.395, 57.12, 57.375],\n                to_rgb=True),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img'])\n        ])\n]\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type='CityscapesDataset',\n        data_root='data/cityscapes/',\n        img_dir='leftImg8bit/train',\n        ann_dir='gtFine/train',\n        pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(type='LoadAnnotations'),\n            dict(\n                type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),\n            dict(type='RandomCrop', crop_size=(512, 1024), cat_max_ratio=0.75),\n            dict(type='RandomFlip', prob=0.5),\n            dict(type='PhotoMetricDistortion'),\n            dict(\n                type='Normalize',\n                mean=[123.675, 116.28, 103.53],\n                std=[58.395, 57.12, 57.375],\n                to_rgb=True),\n            dict(type='Pad', size=(512, 1024), pad_val=0, seg_pad_val=255),\n            dict(type='DefaultFormatBundle'),\n            dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n        ]),\n    val=dict(\n        type='CityscapesDataset',\n        data_root='data/cityscapes/',\n        img_dir='leftImg8bit/val',\n        ann_dir='gtFine/val',\n        pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(2048, 1024),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='ImageToTensor', keys=['img']),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ]),\n    test=dict(\n        type='CityscapesDataset',\n        data_root='data/cityscapes/',\n        img_dir='leftImg8bit/val',\n        ann_dir='gtFine/val',\n        pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(2048, 1024),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='ImageToTensor', keys=['img']),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ]))\nlog_config = dict(\n    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\ncudnn_benchmark = True\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0005)\noptimizer_config = dict()\nlr_config = dict(policy='poly', power=0.9, min_lr=0.0001, by_epoch=False)\nrunner = dict(type='IterBasedRunner', max_iters=80000)\ncheckpoint_config = dict(by_epoch=False, interval=8000)\nevaluation = dict(interval=8000, metric='mIoU')\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n\n\n# 2、配置文件的结构\n\n有4个基础的配置文件，分别是dataset、model、schedule、default_runtime\n\n * dataset 用于解析每个数据集\n * model 用作配置每个模型的基类\n * schedule 是对 lr 进行调整\n * default_runtime 是训练时的 log 以及 tensorboard、以及 nccl 、cudnn_benchmark等处理\n\nmmsegmentation 遵循这样的命名 {model}_{backbone}_[misc]_[gpu x batch_per_gpu]_{resolution}_{schedule}_{dataset}\n\n * {model}: model type like psp, deeplabv3, etc.\n * {backbone}: backbone type like r50 (ResNet-50), x101 (ResNeXt-101).\n * [misc]: miscellaneous setting/plugins of model, e.g. dconv, gcb, attention, mstrain.\n * [gpu x batch_per_gpu]: GPUs and samples per GPU, 8x2 is used by default.\n * {schedule}: training schedule, 20ki means 20k iterations.\n * {dataset}: dataset like cityscapes, voc12aug, ade.\n\nmmsegmentation 建议使用者对基类进行继承，如果需要开发全新的模型，就重写一个 model 基类\n\n * 例如这是 deeplabv3plus_r50-d8_512x1024_40k_cityscapes.py 的内容\n   \n   * _base_ = [\n         '../_base_/models/deeplabv3plus_r50-d8.py',\n         '../_base_/datasets/cityscapes.py', '../_base_/default_runtime.py',\n         '../_base_/schedules/schedule_40k.py'\n     ]\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n * 这是 deeplabv3plus_r101-d8_512x1024_40k_cityscapes.py 的内容\n   \n   * _base_ = './deeplabv3plus_r50-d8_512x1024_40k_cityscapes.py'\n     model = dict(pretrained='open-mmlab://resnet101_v1c', backbone=dict(depth=101))\n     \n     \n     1\n     2\n     \n\n可见 r50-d8 的全部pipeline 都是继承于基类，也就是说基类就是定义了一个 deeplabv3plus_r50-d8。r101-d8 的配置文件直接继承 r50-d8 并把 backbone 修改下即可，这种通过配置文件定义整个 pipeline 的方式值得学习\n\n接下来我们对 deeplabv3plus_r50-d8_512x1024_40k_cityscapes.py 配置文件的内容做一个解析，其实也就是对相应的基类做解析。\n\n# 2.1、deeplabv3plus 的 model 基类\n\n# model settings\nnorm_cfg = dict(type='SyncBN', requires_grad=True)\nmodel = dict(\n    type='EncoderDecoder',\n    pretrained='open-mmlab://resnet50_v1c',\n    backbone=dict(\n        type='ResNetV1c', \t# backbone 的类型\n        depth=50,\t\t\t# backbone 的深度\n        num_stages=4,\t\t# backbone 的stage\n        out_indices=(0, 1, 2, 3),\t# 在每个阶段中生成的输出特征图的index。\n        dilations=(1, 1, 2, 4),\t\t# 每层的 dilation rate\n        strides=(1, 2, 1, 1),\t\t# 每层的 stride \n        norm_cfg=norm_cfg,\t\t\t# 配置归一化的方式，一般使用 SyncBN\n        norm_eval=False,\t\t\t# 在评估和测试的的时候是否打开 BN\n        style='pytorch',\t\t\t\n        contract_dilation=True), \t \n    decode_head=dict(\n        type='DepthwiseSeparableASPPHead',\t# decode head 的类型\n        in_channels=2048,\t\t\t\t\t# auxiliary head 的输入通道数\n        in_index=3,\t\t\t\t\t\t\t# 选择的特征图的 index\n        channels=512,\t\t\t\t\t\t# decode head 的通道数\n        dilations=(1, 12, 24, 36),\t\t\t# 空洞卷积的尺度\n        c1_in_channels=256,\t\t\t\t\t# c1 输入的通道数\n        c1_channels=48,\t\t\t\t\t\t# c1 输出的通道数\n        dropout_ratio=0.1,\t\t\t\t\t# dropout 的概率\n        num_classes=19,\t\t\t\t\t\t# 输出的类别数量\n        norm_cfg=norm_cfg,\t\t\t\t\t# 归一化的方式\n        align_corners=False,\t\t\t\t# \n        loss_decode=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),\t# decode head 的损失\n    auxiliary_head=dict(\n        type='FCNHead',\t\t\t\t\t\t# auxiliary head 的类型\n        in_channels=1024,\t\t\t\t\t# 输入的通道数\n        in_index=2,\t\t\t\t\t\t\t# 输入的特征图的 index\n        channels=256,\t\t\t\t\t\t# 输出的通道数\n        num_convs=1,\t\t\t\t\t\t# auxiliary head 卷积的数量\n        concat_input=False,\t\t\t\t\t# 在分类前是否 concat 输入以及卷积的输出\n        dropout_ratio=0.1,\t\t\t\t\t# dropout 的概率\n        num_classes=19,\t\t\t\t\t\t# 输出的类别数量\n        norm_cfg=norm_cfg,\t\t\t\t\t# 归一化的方式\n        align_corners=False,\n        loss_decode=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=0.4)),\t# auxiliary head 的损失\n    # model training and testing settings\n    train_cfg=dict(),\t\t\t\t\t\t# 这里仅仅是占位符\n    test_cfg=dict(mode='whole'))\t\t\t# 测试的方式, 'whole' 和 'sliding'，'whole': 整张图测试， 'sliding': 以滑窗的方式测试\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n# 2.2、cityscapes 的 dataset 基类\n\n# dataset settings\ndataset_type = 'CityscapesDataset'\t\t\t# 定义数据集的类型\ndata_root = 'data/cityscapes/'\t\t\t\t# 数据集的根目录\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], \n    std=[58.395, 57.12, 57.375], \n    to_rgb=True)\t\t\t\t\t\t\t# 图像的均值以及方差，用作归一化\ncrop_size = (512, 1024)\t\t\t\t\t\t# resize 的尺寸\ntrain_pipeline = [\t\t\t\t\t\t\t# 训练的 pipeline\n    dict(type='LoadImageFromFile'),\t\t\t# 首先从给定的文件路径读取图像\n    dict(type='LoadAnnotations'),\t\t\t# 第二步是加载标注\n    dict(type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),\t# 第三步 Resize\n    dict(type='RandomCrop', crop_size=crop_size, cat_max_ratio=0.75),\t\t# 第四步 随机裁剪\n    dict(type='RandomFlip', prob=0.5),\t\t# 第五步 随机翻转\n    dict(type='PhotoMetricDistortion'),\t\t# 第六步 PhotoMetricDistortion 数据增强\n    dict(type='Normalize', **img_norm_cfg),\t# 第七步 归一化 \n    dict(type='Pad', size=crop_size, pad_val=0, seg_pad_val=255),\t# 第八步 crop 图像\n    dict(type='DefaultFormatBundle'),\t\t\t\t\t\t\t\t# 第九步 \n    dict(type='Collect', keys=['img', 'gt_semantic_seg']),\t\t\t# collect keys里面的数据\n]\ntest_pipeline = [\t\t\t\t\t\t\t# 测试的 pipeline \n    dict(type='LoadImageFromFile'),\t\t\t# 首先从给定的文件路径读取图像\n    dict(\n        type='MultiScaleFlipAug',\t\t\t# 多尺度翻转增强\n        img_scale=(2048, 1024),\t\t\t\t# 图像尺度\n        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\t# 是否多尺度测试\n        flip=False,\t\t\t\t\t\t\t# 是否翻转\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\t\t# Resize\n            dict(type='RandomFlip'),\t\t\t\t\t# 随机翻转\n            dict(type='Normalize', **img_norm_cfg),\t\t# 归一化\n            dict(type='ImageToTensor', keys=['img']),\t# 图像转Tensor\n            dict(type='Collect', keys=['img']),\t\t\t#\t \n        ])\n]\ndata = dict(\n    samples_per_gpu=2,\t\t\t\t\t\t# 每个 gpu 的 \n    workers_per_gpu=2,\t\t\t\t\t\t# 每个 gpu 的 worker\n    train=dict(\n        type=dataset_type,\t\t\t\t\t# 数据集类型\n        data_root=data_root,\t\t\t\t# 数据根目录\n        img_dir='leftImg8bit/train',\t\t# 训练图像文件的目录\n        ann_dir='gtFine/train',\t\t\t\t# 训练标注文件的目录\n        pipeline=train_pipeline),\t\t\t# train_pipeline \n    val=dict(\n        type=dataset_type,\t\t\t\t\t# 数据集类型\n        data_root=data_root,\t\t\t\t# 数据根目录\n        img_dir='leftImg8bit/val',\t\t\t# 验证图像文件的目录\n        ann_dir='gtFine/val',\t\t\t\t# 验证标注文件的目录\n        pipeline=test_pipeline),\t\t\t# test_pipeline\n    test=dict(\n        type=dataset_type,\t\t\t\t\t# 数据集类型\n        data_root=data_root,\t\t\t\t# 数据根目录\n        img_dir='leftImg8bit/val',\t\t\t# 测试图像文件的目录\n        ann_dir='gtFine/val',\t\t\t\t# 测试标注文件的目录\n        pipeline=test_pipeline))\t\t\t# test_pipeline\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n# 2.3、deeplabv3plus 的 schedules 基类\n\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0005)\t# 优化器的配置\noptimizer_config = dict()\n# learning policy\nlr_config = dict(policy='poly', power=0.9, min_lr=1e-4, by_epoch=False)\t\t# 学习率的配置\n# runtime settings\nrunner = dict(type='IterBasedRunner', max_iters=40000)\ncheckpoint_config = dict(by_epoch=False, interval=4000)\nevaluation = dict(interval=4000, metric='mIoU')\t\t\t\t\t\t\t\t# 评估间隔和metric的设置\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 2.4、deeplabv3plus 的 default_runtime 基类\n\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook', by_epoch=False),\t\t# TextLogger\n        # dict(type='TensorboardLoggerHook')\t\t\t\t# TensorboardLogger\n    ])\n# yapf:enable\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\ncudnn_benchmark = True\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# Tutorial 2：自定义数据集\n\n# 1、通过调整数据格式来自定义数据集（最简易的方式）\n\n├── data\n│   ├── my_dataset\n│   │   ├── img_dir\n│   │   │   ├── train\n│   │   │   │   ├── xxx{img_suffix}\n│   │   │   │   ├── yyy{img_suffix}\n│   │   │   │   ├── zzz{img_suffix}\n│   │   │   ├── val\n│   │   ├── ann_dir\n│   │   │   ├── train\n│   │   │   │   ├── xxx{seg_map_suffix}\n│   │   │   │   ├── yyy{seg_map_suffix}\n│   │   │   │   ├── zzz{seg_map_suffix}\n│   │   │   ├── val\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n * 一个训练对有相同的后缀 suffix\n * 标注文件是（H, W）的图像，像素值的取值范围应当是【0，num_classes - 1】，可以通过 'P' 模式使用pillow 加载彩色标注图像\n\n# 2、通过混合数据集来自定义数据集\n\nmmsegmentation 也支持混合数据集用于训练\n\n# 2.1、重复数据集\n\ndataset_A_train = dict(\n        type='RepeatDataset',\n        times=N,\n        dataset=dict(  # This is the original config of Dataset_A\n            type='Dataset_A',\n            ...\n            pipeline=train_pipeline\n        )\n    )\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 2.2、连接数据集\n\n可以连接两个标注目录\n\ndataset_A_train = dict(\n    type='Dataset_A',\n    img_dir = 'img_dir',\n    ann_dir = ['anno_dir_1', 'anno_dir_2'],\n    pipeline=train_pipeline\n)\n\n\n1\n2\n3\n4\n5\n6\n\n\n可以连接两个数据集划分\n\ndataset_A_train = dict(\n    type='Dataset_A',\n    img_dir = 'img_dir',\n    ann_dir = 'anno_dir',\n    split = ['split_1.txt', 'split_2.txt'],\n    pipeline=train_pipeline\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n也可以同时连接两个标注目录以及数据集划分，anno_dir_1 对应 split_1.txt，anno_dir_2对应 split_2.txt，\n\ndataset_A_train = dict(\n    type='Dataset_A',\n    img_dir = 'img_dir',\n    ann_dir = ['anno_dir_1', 'anno_dir_2'],\n    split = ['split_1.txt', 'split_2.txt'],\n    pipeline=train_pipeline\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 参考资料\n\n * https://mmsegmentation.readthedocs.io/en/latest/tutorials/",normalizedContent:"# mmsegmentation 框架解析\n\n按照官方的 tutorial 先过一遍\n\n\n# tutorial 1：配置文件\n\n# 1、配置文件的基本知识\n\n1、python tools/print_config.py /path/to/config 可以用来可视化完整的config\n\n2、配置文件可以通过 --options xxx.yyy=zzz 来更新 config\n\npython tools/print_config.py mmsegmentation/configs/deeplabv3plus/deeplabv3plus_r18-d8_512x1024_80k_cityscapes.py\n\n\n1\n\n\n输出：\n\nconfig:\nnorm_cfg = dict(type='syncbn', requires_grad=true)\nmodel = dict(\n    type='encoderdecoder',\n    pretrained='open-mmlab://resnet18_v1c',\n    backbone=dict(\n        type='resnetv1c',\n        depth=18,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        dilations=(1, 1, 2, 4),\n        strides=(1, 2, 1, 1),\n        norm_cfg=dict(type='syncbn', requires_grad=true),\n        norm_eval=false,\n        style='pytorch',\n        contract_dilation=true),\n    decode_head=dict(\n        type='depthwiseseparableaspphead',\n        in_channels=512,\n        in_index=3,\n        channels=128,\n        dilations=(1, 12, 24, 36),\n        c1_in_channels=64,\n        c1_channels=12,\n        dropout_ratio=0.1,\n        num_classes=19,\n        norm_cfg=dict(type='syncbn', requires_grad=true),\n        align_corners=false,\n        loss_decode=dict(\n            type='crossentropyloss', use_sigmoid=false, loss_weight=1.0)),\n    auxiliary_head=dict(\n        type='fcnhead',\n        in_channels=256,\n        in_index=2,\n        channels=64,\n        num_convs=1,\n        concat_input=false,\n        dropout_ratio=0.1,\n        num_classes=19,\n        norm_cfg=dict(type='syncbn', requires_grad=true),\n        align_corners=false,\n        loss_decode=dict(\n            type='crossentropyloss', use_sigmoid=false, loss_weight=0.4)),\n    train_cfg=dict(),\n    test_cfg=dict(mode='whole'))\ndataset_type = 'cityscapesdataset'\ndata_root = 'data/cityscapes/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=true)\ncrop_size = (512, 1024)\ntrain_pipeline = [\n    dict(type='loadimagefromfile'),\n    dict(type='loadannotations'),\n    dict(type='resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),\n    dict(type='randomcrop', crop_size=(512, 1024), cat_max_ratio=0.75),\n    dict(type='randomflip', prob=0.5),\n    dict(type='photometricdistortion'),\n    dict(\n        type='normalize',\n        mean=[123.675, 116.28, 103.53],\n        std=[58.395, 57.12, 57.375],\n        to_rgb=true),\n    dict(type='pad', size=(512, 1024), pad_val=0, seg_pad_val=255),\n    dict(type='defaultformatbundle'),\n    dict(type='collect', keys=['img', 'gt_semantic_seg'])\n]\ntest_pipeline = [\n    dict(type='loadimagefromfile'),\n    dict(\n        type='multiscaleflipaug',\n        img_scale=(2048, 1024),\n        flip=false,\n        transforms=[\n            dict(type='resize', keep_ratio=true),\n            dict(type='randomflip'),\n            dict(\n                type='normalize',\n                mean=[123.675, 116.28, 103.53],\n                std=[58.395, 57.12, 57.375],\n                to_rgb=true),\n            dict(type='imagetotensor', keys=['img']),\n            dict(type='collect', keys=['img'])\n        ])\n]\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type='cityscapesdataset',\n        data_root='data/cityscapes/',\n        img_dir='leftimg8bit/train',\n        ann_dir='gtfine/train',\n        pipeline=[\n            dict(type='loadimagefromfile'),\n            dict(type='loadannotations'),\n            dict(\n                type='resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),\n            dict(type='randomcrop', crop_size=(512, 1024), cat_max_ratio=0.75),\n            dict(type='randomflip', prob=0.5),\n            dict(type='photometricdistortion'),\n            dict(\n                type='normalize',\n                mean=[123.675, 116.28, 103.53],\n                std=[58.395, 57.12, 57.375],\n                to_rgb=true),\n            dict(type='pad', size=(512, 1024), pad_val=0, seg_pad_val=255),\n            dict(type='defaultformatbundle'),\n            dict(type='collect', keys=['img', 'gt_semantic_seg'])\n        ]),\n    val=dict(\n        type='cityscapesdataset',\n        data_root='data/cityscapes/',\n        img_dir='leftimg8bit/val',\n        ann_dir='gtfine/val',\n        pipeline=[\n            dict(type='loadimagefromfile'),\n            dict(\n                type='multiscaleflipaug',\n                img_scale=(2048, 1024),\n                flip=false,\n                transforms=[\n                    dict(type='resize', keep_ratio=true),\n                    dict(type='randomflip'),\n                    dict(\n                        type='normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=true),\n                    dict(type='imagetotensor', keys=['img']),\n                    dict(type='collect', keys=['img'])\n                ])\n        ]),\n    test=dict(\n        type='cityscapesdataset',\n        data_root='data/cityscapes/',\n        img_dir='leftimg8bit/val',\n        ann_dir='gtfine/val',\n        pipeline=[\n            dict(type='loadimagefromfile'),\n            dict(\n                type='multiscaleflipaug',\n                img_scale=(2048, 1024),\n                flip=false,\n                transforms=[\n                    dict(type='resize', keep_ratio=true),\n                    dict(type='randomflip'),\n                    dict(\n                        type='normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=true),\n                    dict(type='imagetotensor', keys=['img']),\n                    dict(type='collect', keys=['img'])\n                ])\n        ]))\nlog_config = dict(\n    interval=50, hooks=[dict(type='textloggerhook', by_epoch=false)])\ndist_params = dict(backend='nccl')\nlog_level = 'info'\nload_from = none\nresume_from = none\nworkflow = [('train', 1)]\ncudnn_benchmark = true\noptimizer = dict(type='sgd', lr=0.01, momentum=0.9, weight_decay=0.0005)\noptimizer_config = dict()\nlr_config = dict(policy='poly', power=0.9, min_lr=0.0001, by_epoch=false)\nrunner = dict(type='iterbasedrunner', max_iters=80000)\ncheckpoint_config = dict(by_epoch=false, interval=8000)\nevaluation = dict(interval=8000, metric='miou')\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n\n\n# 2、配置文件的结构\n\n有4个基础的配置文件，分别是dataset、model、schedule、default_runtime\n\n * dataset 用于解析每个数据集\n * model 用作配置每个模型的基类\n * schedule 是对 lr 进行调整\n * default_runtime 是训练时的 log 以及 tensorboard、以及 nccl 、cudnn_benchmark等处理\n\nmmsegmentation 遵循这样的命名 {model}_{backbone}_[misc]_[gpu x batch_per_gpu]_{resolution}_{schedule}_{dataset}\n\n * {model}: model type like psp, deeplabv3, etc.\n * {backbone}: backbone type like r50 (resnet-50), x101 (resnext-101).\n * [misc]: miscellaneous setting/plugins of model, e.g. dconv, gcb, attention, mstrain.\n * [gpu x batch_per_gpu]: gpus and samples per gpu, 8x2 is used by default.\n * {schedule}: training schedule, 20ki means 20k iterations.\n * {dataset}: dataset like cityscapes, voc12aug, ade.\n\nmmsegmentation 建议使用者对基类进行继承，如果需要开发全新的模型，就重写一个 model 基类\n\n * 例如这是 deeplabv3plus_r50-d8_512x1024_40k_cityscapes.py 的内容\n   \n   * _base_ = [\n         '../_base_/models/deeplabv3plus_r50-d8.py',\n         '../_base_/datasets/cityscapes.py', '../_base_/default_runtime.py',\n         '../_base_/schedules/schedule_40k.py'\n     ]\n     \n     \n     1\n     2\n     3\n     4\n     5\n     \n\n * 这是 deeplabv3plus_r101-d8_512x1024_40k_cityscapes.py 的内容\n   \n   * _base_ = './deeplabv3plus_r50-d8_512x1024_40k_cityscapes.py'\n     model = dict(pretrained='open-mmlab://resnet101_v1c', backbone=dict(depth=101))\n     \n     \n     1\n     2\n     \n\n可见 r50-d8 的全部pipeline 都是继承于基类，也就是说基类就是定义了一个 deeplabv3plus_r50-d8。r101-d8 的配置文件直接继承 r50-d8 并把 backbone 修改下即可，这种通过配置文件定义整个 pipeline 的方式值得学习\n\n接下来我们对 deeplabv3plus_r50-d8_512x1024_40k_cityscapes.py 配置文件的内容做一个解析，其实也就是对相应的基类做解析。\n\n# 2.1、deeplabv3plus 的 model 基类\n\n# model settings\nnorm_cfg = dict(type='syncbn', requires_grad=true)\nmodel = dict(\n    type='encoderdecoder',\n    pretrained='open-mmlab://resnet50_v1c',\n    backbone=dict(\n        type='resnetv1c', \t# backbone 的类型\n        depth=50,\t\t\t# backbone 的深度\n        num_stages=4,\t\t# backbone 的stage\n        out_indices=(0, 1, 2, 3),\t# 在每个阶段中生成的输出特征图的index。\n        dilations=(1, 1, 2, 4),\t\t# 每层的 dilation rate\n        strides=(1, 2, 1, 1),\t\t# 每层的 stride \n        norm_cfg=norm_cfg,\t\t\t# 配置归一化的方式，一般使用 syncbn\n        norm_eval=false,\t\t\t# 在评估和测试的的时候是否打开 bn\n        style='pytorch',\t\t\t\n        contract_dilation=true), \t \n    decode_head=dict(\n        type='depthwiseseparableaspphead',\t# decode head 的类型\n        in_channels=2048,\t\t\t\t\t# auxiliary head 的输入通道数\n        in_index=3,\t\t\t\t\t\t\t# 选择的特征图的 index\n        channels=512,\t\t\t\t\t\t# decode head 的通道数\n        dilations=(1, 12, 24, 36),\t\t\t# 空洞卷积的尺度\n        c1_in_channels=256,\t\t\t\t\t# c1 输入的通道数\n        c1_channels=48,\t\t\t\t\t\t# c1 输出的通道数\n        dropout_ratio=0.1,\t\t\t\t\t# dropout 的概率\n        num_classes=19,\t\t\t\t\t\t# 输出的类别数量\n        norm_cfg=norm_cfg,\t\t\t\t\t# 归一化的方式\n        align_corners=false,\t\t\t\t# \n        loss_decode=dict(\n            type='crossentropyloss', use_sigmoid=false, loss_weight=1.0)),\t# decode head 的损失\n    auxiliary_head=dict(\n        type='fcnhead',\t\t\t\t\t\t# auxiliary head 的类型\n        in_channels=1024,\t\t\t\t\t# 输入的通道数\n        in_index=2,\t\t\t\t\t\t\t# 输入的特征图的 index\n        channels=256,\t\t\t\t\t\t# 输出的通道数\n        num_convs=1,\t\t\t\t\t\t# auxiliary head 卷积的数量\n        concat_input=false,\t\t\t\t\t# 在分类前是否 concat 输入以及卷积的输出\n        dropout_ratio=0.1,\t\t\t\t\t# dropout 的概率\n        num_classes=19,\t\t\t\t\t\t# 输出的类别数量\n        norm_cfg=norm_cfg,\t\t\t\t\t# 归一化的方式\n        align_corners=false,\n        loss_decode=dict(\n            type='crossentropyloss', use_sigmoid=false, loss_weight=0.4)),\t# auxiliary head 的损失\n    # model training and testing settings\n    train_cfg=dict(),\t\t\t\t\t\t# 这里仅仅是占位符\n    test_cfg=dict(mode='whole'))\t\t\t# 测试的方式, 'whole' 和 'sliding'，'whole': 整张图测试， 'sliding': 以滑窗的方式测试\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n\n\n# 2.2、cityscapes 的 dataset 基类\n\n# dataset settings\ndataset_type = 'cityscapesdataset'\t\t\t# 定义数据集的类型\ndata_root = 'data/cityscapes/'\t\t\t\t# 数据集的根目录\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], \n    std=[58.395, 57.12, 57.375], \n    to_rgb=true)\t\t\t\t\t\t\t# 图像的均值以及方差，用作归一化\ncrop_size = (512, 1024)\t\t\t\t\t\t# resize 的尺寸\ntrain_pipeline = [\t\t\t\t\t\t\t# 训练的 pipeline\n    dict(type='loadimagefromfile'),\t\t\t# 首先从给定的文件路径读取图像\n    dict(type='loadannotations'),\t\t\t# 第二步是加载标注\n    dict(type='resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),\t# 第三步 resize\n    dict(type='randomcrop', crop_size=crop_size, cat_max_ratio=0.75),\t\t# 第四步 随机裁剪\n    dict(type='randomflip', prob=0.5),\t\t# 第五步 随机翻转\n    dict(type='photometricdistortion'),\t\t# 第六步 photometricdistortion 数据增强\n    dict(type='normalize', **img_norm_cfg),\t# 第七步 归一化 \n    dict(type='pad', size=crop_size, pad_val=0, seg_pad_val=255),\t# 第八步 crop 图像\n    dict(type='defaultformatbundle'),\t\t\t\t\t\t\t\t# 第九步 \n    dict(type='collect', keys=['img', 'gt_semantic_seg']),\t\t\t# collect keys里面的数据\n]\ntest_pipeline = [\t\t\t\t\t\t\t# 测试的 pipeline \n    dict(type='loadimagefromfile'),\t\t\t# 首先从给定的文件路径读取图像\n    dict(\n        type='multiscaleflipaug',\t\t\t# 多尺度翻转增强\n        img_scale=(2048, 1024),\t\t\t\t# 图像尺度\n        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\t# 是否多尺度测试\n        flip=false,\t\t\t\t\t\t\t# 是否翻转\n        transforms=[\n            dict(type='resize', keep_ratio=true),\t\t# resize\n            dict(type='randomflip'),\t\t\t\t\t# 随机翻转\n            dict(type='normalize', **img_norm_cfg),\t\t# 归一化\n            dict(type='imagetotensor', keys=['img']),\t# 图像转tensor\n            dict(type='collect', keys=['img']),\t\t\t#\t \n        ])\n]\ndata = dict(\n    samples_per_gpu=2,\t\t\t\t\t\t# 每个 gpu 的 \n    workers_per_gpu=2,\t\t\t\t\t\t# 每个 gpu 的 worker\n    train=dict(\n        type=dataset_type,\t\t\t\t\t# 数据集类型\n        data_root=data_root,\t\t\t\t# 数据根目录\n        img_dir='leftimg8bit/train',\t\t# 训练图像文件的目录\n        ann_dir='gtfine/train',\t\t\t\t# 训练标注文件的目录\n        pipeline=train_pipeline),\t\t\t# train_pipeline \n    val=dict(\n        type=dataset_type,\t\t\t\t\t# 数据集类型\n        data_root=data_root,\t\t\t\t# 数据根目录\n        img_dir='leftimg8bit/val',\t\t\t# 验证图像文件的目录\n        ann_dir='gtfine/val',\t\t\t\t# 验证标注文件的目录\n        pipeline=test_pipeline),\t\t\t# test_pipeline\n    test=dict(\n        type=dataset_type,\t\t\t\t\t# 数据集类型\n        data_root=data_root,\t\t\t\t# 数据根目录\n        img_dir='leftimg8bit/val',\t\t\t# 测试图像文件的目录\n        ann_dir='gtfine/val',\t\t\t\t# 测试标注文件的目录\n        pipeline=test_pipeline))\t\t\t# test_pipeline\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n\n\n# 2.3、deeplabv3plus 的 schedules 基类\n\n# optimizer\noptimizer = dict(type='sgd', lr=0.01, momentum=0.9, weight_decay=0.0005)\t# 优化器的配置\noptimizer_config = dict()\n# learning policy\nlr_config = dict(policy='poly', power=0.9, min_lr=1e-4, by_epoch=false)\t\t# 学习率的配置\n# runtime settings\nrunner = dict(type='iterbasedrunner', max_iters=40000)\ncheckpoint_config = dict(by_epoch=false, interval=4000)\nevaluation = dict(interval=4000, metric='miou')\t\t\t\t\t\t\t\t# 评估间隔和metric的设置\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# 2.4、deeplabv3plus 的 default_runtime 基类\n\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='textloggerhook', by_epoch=false),\t\t# textlogger\n        # dict(type='tensorboardloggerhook')\t\t\t\t# tensorboardlogger\n    ])\n# yapf:enable\ndist_params = dict(backend='nccl')\nlog_level = 'info'\nload_from = none\nresume_from = none\nworkflow = [('train', 1)]\ncudnn_benchmark = true\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# tutorial 2：自定义数据集\n\n# 1、通过调整数据格式来自定义数据集（最简易的方式）\n\n├── data\n│   ├── my_dataset\n│   │   ├── img_dir\n│   │   │   ├── train\n│   │   │   │   ├── xxx{img_suffix}\n│   │   │   │   ├── yyy{img_suffix}\n│   │   │   │   ├── zzz{img_suffix}\n│   │   │   ├── val\n│   │   ├── ann_dir\n│   │   │   ├── train\n│   │   │   │   ├── xxx{seg_map_suffix}\n│   │   │   │   ├── yyy{seg_map_suffix}\n│   │   │   │   ├── zzz{seg_map_suffix}\n│   │   │   ├── val\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n * 一个训练对有相同的后缀 suffix\n * 标注文件是（h, w）的图像，像素值的取值范围应当是【0，num_classes - 1】，可以通过 'p' 模式使用pillow 加载彩色标注图像\n\n# 2、通过混合数据集来自定义数据集\n\nmmsegmentation 也支持混合数据集用于训练\n\n# 2.1、重复数据集\n\ndataset_a_train = dict(\n        type='repeatdataset',\n        times=n,\n        dataset=dict(  # this is the original config of dataset_a\n            type='dataset_a',\n            ...\n            pipeline=train_pipeline\n        )\n    )\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 2.2、连接数据集\n\n可以连接两个标注目录\n\ndataset_a_train = dict(\n    type='dataset_a',\n    img_dir = 'img_dir',\n    ann_dir = ['anno_dir_1', 'anno_dir_2'],\n    pipeline=train_pipeline\n)\n\n\n1\n2\n3\n4\n5\n6\n\n\n可以连接两个数据集划分\n\ndataset_a_train = dict(\n    type='dataset_a',\n    img_dir = 'img_dir',\n    ann_dir = 'anno_dir',\n    split = ['split_1.txt', 'split_2.txt'],\n    pipeline=train_pipeline\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n也可以同时连接两个标注目录以及数据集划分，anno_dir_1 对应 split_1.txt，anno_dir_2对应 split_2.txt，\n\ndataset_a_train = dict(\n    type='dataset_a',\n    img_dir = 'img_dir',\n    ann_dir = ['anno_dir_1', 'anno_dir_2'],\n    split = ['split_1.txt', 'split_2.txt'],\n    pipeline=train_pipeline\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 参考资料\n\n * https://mmsegmentation.readthedocs.io/en/latest/tutorials/",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"胶片相机泛谈",frontmatter:{title:"胶片相机泛谈",date:"2021-09-09T23:20:26.000Z",permalink:"/pages/0611eb/",categories:["学习笔记","体会感悟-摄影"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/08.%E4%BD%93%E4%BC%9A%E6%84%9F%E6%82%9F-%E6%91%84%E5%BD%B1/03.%E8%83%B6%E7%89%87%E7%9B%B8%E6%9C%BA%E6%B3%9B%E8%B0%88.html",relativePath:"02.学习笔记/08.体会感悟-摄影/03.胶片相机泛谈.md",key:"v-60c8d04a",path:"/pages/0611eb/",headers:[{level:2,title:"01、胶卷冲洗和扫描",slug:"_01、胶卷冲洗和扫描",normalizedTitle:"01、胶卷冲洗和扫描",charIndex:2},{level:2,title:"02、胶片相机型号",slug:"_02、胶片相机型号",normalizedTitle:"02、胶片相机型号",charIndex:96},{level:3,title:"2.1、一次性相机",slug:"_2-1、一次性相机",normalizedTitle:"2.1、一次性相机",charIndex:128},{level:3,title:"2.2、傻瓜相机",slug:"_2-2、傻瓜相机",normalizedTitle:"2.2、傻瓜相机",charIndex:210},{level:2,title:"03、胶卷型号",slug:"_03、胶卷型号",normalizedTitle:"03、胶卷型号",charIndex:962},{level:3,title:"3.1、135胶卷",slug:"_3-1、135胶卷",normalizedTitle:"3.1、135胶卷",charIndex:1346},{level:2,title:"04、参考资料",slug:"_04、参考资料",normalizedTitle:"04、参考资料",charIndex:2920},{level:3,title:"4.1、 胶卷参考资料",slug:"_4-1、-胶卷参考资料",normalizedTitle:"4.1、 胶卷参考资料",charIndex:2932}],headersStr:"01、胶卷冲洗和扫描 02、胶片相机型号 2.1、一次性相机 2.2、傻瓜相机 03、胶卷型号 3.1、135胶卷 04、参考资料 4.1、 胶卷参考资料",content:"# 01、胶卷冲洗和扫描\n\n胶卷冲扫店推荐\n\n * sp3000：千禾胶片工作室。\n\n * 爱普生or哈苏：Film Feeling菲林感觉、海猫照相馆。\n\n * 顶级：菲林日记馆。\n\n\n# 02、胶片相机型号\n\n先列举几款型号较为热门的胶片相机\n\n\n# 2.1、一次性相机\n\n# 2.1.1、FuJIFILM X-TRA 400\n\n一次性傻瓜相机 800 度带闪光灯\n\n# 2.1.2、Kodak 一次性相机\n\n\n# 2.2、傻瓜相机\n\n对焦方式：自动对焦、泛焦\n\n测光方式：自动测光、无测光\n\n镜头种类：定焦、变焦\n\n特殊镀膜：蔡司t镀膜、宾得smc镀膜、富士ebc镀膜等\n\n光圈：2.8、3.2、3.5、4.0等等\n\n最高购买自动对焦、自动测光的机器，变焦/定焦 则看个人需求\n\n# 2.2.1、奥林巴斯 μ2\\mu2μ2 （35mm f2.8）\n\n * 价格：600-700 左右比较合理\n * 自动对焦 自动测光 定焦机（结构天梯图第一档）\n * 最大光圈可达2.8（上游）\n * 镀膜不详，可能是普通镜片\n * 塑料机身，手感一般\n * 无特殊功能\n\n# 2.2.2、富士 Silvi F2.8 (24-50)\n\n# 2.2.3、奥林巴斯 OM-1\n\n# 2.2.4、宾得 espio 140\n\n# 2.2.5、宾得 espio 160\n\n# 2.2.6、宾得 espio 928\n\n# 2.2.7、康泰时 tvs\n\n * 价格：2000+，较贵\n * 自动对焦，自动测光变焦机，最广角端可达28mm。（结构天梯图第二档）\n * 最大光圈可达3.5（中上游水准）\n * 蔡司t镀膜（镜头素质上游）\n * 拥有a档，可手动调焦（功能不错）\n * 金属机身（手感不错）\n\n# 2.2.8 理光Gr1v\n\n# 2.2.9 豆瓣四大神机之一 尼康 FM2\n\n * 50 1.4 套机价格：2000 左右\n\n# 2.2.10 豆瓣四大神机之一 佳能 AE1\n\n * 50 1.4 套机价格：1000 左右\n\n# 2.2.11 豆瓣四大神机之一 奥林巴斯 OM1\n\n * 50 1.4 套机价格：1000 左右\n\n# 2.2.12 豆瓣四大神机之一 美能达 X700\n\n * 50 1.4 套机价格：1000 左右\n\n\n# 03、胶卷型号\n\n分为120胶卷和135胶卷\n\n * 经常用的是135胶卷，135胶卷适应于各种型号的135照相机。这种胶卷宽35毫米，长160～170厘米，胶卷两边有按规则排列的片孔。一般可拍摄3.6厘米×2.4厘米的底片36张，也有可拍摄20、24、72张的135胶卷。135胶卷是最普遍的胶卷，相对成本要低。\n\n * 120胶卷根据不同的120照相机可拍摄出大小不同的画面，其中有拍摄16张底片的（画幅为4.5×6厘米）；拍摄12张底片的（6×6厘米）；还有拍摄10张底片的（6×7厘米）与8张底片的（6×9厘米）。120胶卷的长度一般为81-82.5厘米，宽度为6.1-6.5厘米。120胶卷拍摄几张底片，取决于相机的型号而各不相同。120胶卷成本比较高，很难找到能冲洗的店。\n\n柯达 E100\n\n富士 C200\n\n富士 业务100\n\n柯达 C200\n\n\n# 3.1、135胶卷\n\n# 3.1.1、Kodak ColorPlus 200\n\n * 国内市场上最廉价的基础胶卷。**发色是典雅的暖色，不是浓郁的色彩风格，**在我的作品里多次与诺日士扫描仪配合时出现电影色。颗粒是较为粗大的类型（当然颗粒和药剂扫面的关系更大）对亚洲人肤色表现不算友好，偏暖黄。晴天表现上佳，阴天体现出一股冷紫色彩。\n * 2019 年 1 月国内价格：16元\n\n# 3.1.2、Fuji Color C200\n\n * 还是绿绿的富士味道，和富士扫描仪搭配真腹肌味。色彩鲜艳活泼，在暗光和室内环境下阴影的绿色尤其明显。它洗出来的效果层次一般，对比度也较强，容易出现亮的亮死，黑的黑死的情况（当然还是不能只让胶卷背锅）。\n\n * 2019 年 1月 国内价格：20元\n\n# 3.1.3、Kodak Gold 200\n\n * 童年回忆金柯达，爸爸记录你成长，不让你玩的相机里装的就是这个。**但是新升级的金柯达在包装上就可以看出粉紫色有些端倪。**阳光明媚拍的色彩会显得过度，童年印象里的小黄人都是“金柯拉”的味道。但是颗粒细腻，反差明显就是它能配得上金装柯达称号的原因。\n\n * 2019 年 1 月国内价格：25元\n\n# 3.1.4、Fuji 记录用/业务用100\n\n * **从它的名字里可以看出是业务用——它的定位是施工现场监督，记录拍摄用的。可以说是superia100的廉价低配版，**在日本器材店里它和国内小店里的旺仔小包的馒头一样没有资格上主货架，只能挂在店门口的铁网上。但是到了国内变得十分火爆。主要还是大部分人对日系图片的印象是过曝和低对比度。这种特性让它在拍摄景物和体现色彩时十分辣鸡，但是对小清新人像来说真的是出奇的友好了。\n\n * 2019年1月国内价格：27元\n\n# 3.1.5、Kodak ekstar 100\n\n * **ektar 100可能是世界上颗粒最细分辨率最好的135彩色负片，但是同样的它对冲洗扫描的要求相当之高。**我只拍摄过一次这种卷但我感觉得到的效果并不让人满意，但是所谓的专业冲扫可能得到的效果没有你想象中的胶味十足。**室内暗光用这卷往往会让你得到漆黑一片的废片。**我既不想贬低它又不想批评冲扫师傅，可能它就是一个忧郁王子吧！\n\n * 2019 年 1 月国内价格：42元\n\n# 3.1.6、Kodak proimage 100\n\n * **名称是专业婚礼卷也就是西方国家专拍人像用的。**在父辈使用的年代这种卷也是十分廉价的。但它的缺点也十分明显，有着和富士业务十分相似的特性，**口味清淡拍景物也不会吃香。可以在大光照时用来追求虚化而选用，**但是不过分追求大光圈，胶片的氛围感远远优于数码。无需多大的光圈主体就会被烘托得十分明显，我依然更推崇廉价好用的200度胶卷。\n\n * 2019 年 1 月国内价格：25元\n\n# 3.1.7、Kodak Portra 160/400\n\n * **滨田英明老师最爱的炮塔160胶卷，**当然滨田英明老师用的是120版本。无论是135还是120版本用过的人都墙裂推荐的炮塔160/400系类。杰出的影调、迷人的色彩、细腻的颗粒。总之炮塔是每一个选择胶片摄影的人值得一试的胶卷。（快毒死了）\n\n * 2019 年 1 月国内价格：48元\n\n# 3.1.8、Fuji proH 400\n\n * 400度可以选择的胶卷其实不多，比pro400相对廉价的还有柯达万用400和fujixtra400，只需要二三十左右的价格。**但是我推荐pro400是因为它相比前两者的出色许多！**秉承腹肌家族优秀的色彩表现真的每次拍摄都让我心花怒放。保证较高的快门速度，又可以当夜拍卷用，锐度也相当不错。\n\n * 2019 年 1 月国内价格：65元\n\n\n# 04、参考资料\n\n\n# 4.1、 胶卷参考资料\n\n * https://zhuanlan.zhihu.com/p/55868351\n * https://zhuanlan.zhihu.com/p/79176719",normalizedContent:"# 01、胶卷冲洗和扫描\n\n胶卷冲扫店推荐\n\n * sp3000：千禾胶片工作室。\n\n * 爱普生or哈苏：film feeling菲林感觉、海猫照相馆。\n\n * 顶级：菲林日记馆。\n\n\n# 02、胶片相机型号\n\n先列举几款型号较为热门的胶片相机\n\n\n# 2.1、一次性相机\n\n# 2.1.1、fujifilm x-tra 400\n\n一次性傻瓜相机 800 度带闪光灯\n\n# 2.1.2、kodak 一次性相机\n\n\n# 2.2、傻瓜相机\n\n对焦方式：自动对焦、泛焦\n\n测光方式：自动测光、无测光\n\n镜头种类：定焦、变焦\n\n特殊镀膜：蔡司t镀膜、宾得smc镀膜、富士ebc镀膜等\n\n光圈：2.8、3.2、3.5、4.0等等\n\n最高购买自动对焦、自动测光的机器，变焦/定焦 则看个人需求\n\n# 2.2.1、奥林巴斯 μ2\\mu2μ2 （35mm f2.8）\n\n * 价格：600-700 左右比较合理\n * 自动对焦 自动测光 定焦机（结构天梯图第一档）\n * 最大光圈可达2.8（上游）\n * 镀膜不详，可能是普通镜片\n * 塑料机身，手感一般\n * 无特殊功能\n\n# 2.2.2、富士 silvi f2.8 (24-50)\n\n# 2.2.3、奥林巴斯 om-1\n\n# 2.2.4、宾得 espio 140\n\n# 2.2.5、宾得 espio 160\n\n# 2.2.6、宾得 espio 928\n\n# 2.2.7、康泰时 tvs\n\n * 价格：2000+，较贵\n * 自动对焦，自动测光变焦机，最广角端可达28mm。（结构天梯图第二档）\n * 最大光圈可达3.5（中上游水准）\n * 蔡司t镀膜（镜头素质上游）\n * 拥有a档，可手动调焦（功能不错）\n * 金属机身（手感不错）\n\n# 2.2.8 理光gr1v\n\n# 2.2.9 豆瓣四大神机之一 尼康 fm2\n\n * 50 1.4 套机价格：2000 左右\n\n# 2.2.10 豆瓣四大神机之一 佳能 ae1\n\n * 50 1.4 套机价格：1000 左右\n\n# 2.2.11 豆瓣四大神机之一 奥林巴斯 om1\n\n * 50 1.4 套机价格：1000 左右\n\n# 2.2.12 豆瓣四大神机之一 美能达 x700\n\n * 50 1.4 套机价格：1000 左右\n\n\n# 03、胶卷型号\n\n分为120胶卷和135胶卷\n\n * 经常用的是135胶卷，135胶卷适应于各种型号的135照相机。这种胶卷宽35毫米，长160～170厘米，胶卷两边有按规则排列的片孔。一般可拍摄3.6厘米×2.4厘米的底片36张，也有可拍摄20、24、72张的135胶卷。135胶卷是最普遍的胶卷，相对成本要低。\n\n * 120胶卷根据不同的120照相机可拍摄出大小不同的画面，其中有拍摄16张底片的（画幅为4.5×6厘米）；拍摄12张底片的（6×6厘米）；还有拍摄10张底片的（6×7厘米）与8张底片的（6×9厘米）。120胶卷的长度一般为81-82.5厘米，宽度为6.1-6.5厘米。120胶卷拍摄几张底片，取决于相机的型号而各不相同。120胶卷成本比较高，很难找到能冲洗的店。\n\n柯达 e100\n\n富士 c200\n\n富士 业务100\n\n柯达 c200\n\n\n# 3.1、135胶卷\n\n# 3.1.1、kodak colorplus 200\n\n * 国内市场上最廉价的基础胶卷。**发色是典雅的暖色，不是浓郁的色彩风格，**在我的作品里多次与诺日士扫描仪配合时出现电影色。颗粒是较为粗大的类型（当然颗粒和药剂扫面的关系更大）对亚洲人肤色表现不算友好，偏暖黄。晴天表现上佳，阴天体现出一股冷紫色彩。\n * 2019 年 1 月国内价格：16元\n\n# 3.1.2、fuji color c200\n\n * 还是绿绿的富士味道，和富士扫描仪搭配真腹肌味。色彩鲜艳活泼，在暗光和室内环境下阴影的绿色尤其明显。它洗出来的效果层次一般，对比度也较强，容易出现亮的亮死，黑的黑死的情况（当然还是不能只让胶卷背锅）。\n\n * 2019 年 1月 国内价格：20元\n\n# 3.1.3、kodak gold 200\n\n * 童年回忆金柯达，爸爸记录你成长，不让你玩的相机里装的就是这个。**但是新升级的金柯达在包装上就可以看出粉紫色有些端倪。**阳光明媚拍的色彩会显得过度，童年印象里的小黄人都是“金柯拉”的味道。但是颗粒细腻，反差明显就是它能配得上金装柯达称号的原因。\n\n * 2019 年 1 月国内价格：25元\n\n# 3.1.4、fuji 记录用/业务用100\n\n * **从它的名字里可以看出是业务用——它的定位是施工现场监督，记录拍摄用的。可以说是superia100的廉价低配版，**在日本器材店里它和国内小店里的旺仔小包的馒头一样没有资格上主货架，只能挂在店门口的铁网上。但是到了国内变得十分火爆。主要还是大部分人对日系图片的印象是过曝和低对比度。这种特性让它在拍摄景物和体现色彩时十分辣鸡，但是对小清新人像来说真的是出奇的友好了。\n\n * 2019年1月国内价格：27元\n\n# 3.1.5、kodak ekstar 100\n\n * **ektar 100可能是世界上颗粒最细分辨率最好的135彩色负片，但是同样的它对冲洗扫描的要求相当之高。**我只拍摄过一次这种卷但我感觉得到的效果并不让人满意，但是所谓的专业冲扫可能得到的效果没有你想象中的胶味十足。**室内暗光用这卷往往会让你得到漆黑一片的废片。**我既不想贬低它又不想批评冲扫师傅，可能它就是一个忧郁王子吧！\n\n * 2019 年 1 月国内价格：42元\n\n# 3.1.6、kodak proimage 100\n\n * **名称是专业婚礼卷也就是西方国家专拍人像用的。**在父辈使用的年代这种卷也是十分廉价的。但它的缺点也十分明显，有着和富士业务十分相似的特性，**口味清淡拍景物也不会吃香。可以在大光照时用来追求虚化而选用，**但是不过分追求大光圈，胶片的氛围感远远优于数码。无需多大的光圈主体就会被烘托得十分明显，我依然更推崇廉价好用的200度胶卷。\n\n * 2019 年 1 月国内价格：25元\n\n# 3.1.7、kodak portra 160/400\n\n * **滨田英明老师最爱的炮塔160胶卷，**当然滨田英明老师用的是120版本。无论是135还是120版本用过的人都墙裂推荐的炮塔160/400系类。杰出的影调、迷人的色彩、细腻的颗粒。总之炮塔是每一个选择胶片摄影的人值得一试的胶卷。（快毒死了）\n\n * 2019 年 1 月国内价格：48元\n\n# 3.1.8、fuji proh 400\n\n * 400度可以选择的胶卷其实不多，比pro400相对廉价的还有柯达万用400和fujixtra400，只需要二三十左右的价格。**但是我推荐pro400是因为它相比前两者的出色许多！**秉承腹肌家族优秀的色彩表现真的每次拍摄都让我心花怒放。保证较高的快门速度，又可以当夜拍卷用，锐度也相当不错。\n\n * 2019 年 1 月国内价格：65元\n\n\n# 04、参考资料\n\n\n# 4.1、 胶卷参考资料\n\n * https://zhuanlan.zhihu.com/p/55868351\n * https://zhuanlan.zhihu.com/p/79176719",charsets:{cjk:!0},lastUpdated:"2021/09/12, 20:42:58"},{title:"mmsegmentation框架解析（中）",frontmatter:{title:"mmsegmentation框架解析（中）",date:"2021-04-13T16:05:42.000Z",permalink:"/pages/2c9bb8/",categories:["计算机视觉","图像分割"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/05.%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90-mmlab%E7%B3%BB%E5%88%97/06.mmsegmentation%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90%EF%BC%88%E4%B8%AD%EF%BC%89.html",relativePath:"02.学习笔记/05.框架解析-mmlab系列/06.mmsegmentation框架解析（中）.md",key:"v-8f837ea0",path:"/pages/2c9bb8/",headers:[{level:2,title:"mmsegmentation 框架解析",slug:"mmsegmentation-框架解析",normalizedTitle:"mmsegmentation 框架解析",charIndex:2},{level:3,title:"Tutorial 3：自定义数据 Pipeline",slug:"tutorial-3-自定义数据-pipeline",normalizedTitle:"tutorial 3：自定义数据 pipeline",charIndex:26},{level:3,title:"Tutorial 4：自定义模型",slug:"tutorial-4-自定义模型",normalizedTitle:"tutorial 4：自定义模型",charIndex:2938}],headersStr:"mmsegmentation 框架解析 Tutorial 3：自定义数据 Pipeline Tutorial 4：自定义模型",content:"# mmsegmentation 框架解析\n\n\n# Tutorial 3：自定义数据 Pipeline\n\n因为语义分割的数据集可能不是相同的尺寸，所以在 MMCV 引入了一个新的 DataContainer 数据类型，来帮助组织不同尺寸的数据。\n\ndataset 和 data preparation pipeline 是解耦的，dataset 定义的是如何处理图像和标注，data pipeline 定义了所有准备数据的步骤。一个 pipeline 包含了一系列的操作，每个操作接收一个 dict 作为输入，并输出一个 dict 用作下一步的transform。下面是一个 Cityscapes 数据集的dataset pipeline\n\n# dataset settings\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ncrop_size = (512, 1024)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations'),\n    dict(type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),\n    dict(type='RandomCrop', crop_size=crop_size, cat_max_ratio=0.75),\n    dict(type='RandomFlip', prob=0.5),\n    dict(type='PhotoMetricDistortion'),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size=crop_size, pad_val=0, seg_pad_val=255),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(2048, 1024),\n        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n# 1、data pipeline 的组成部分\n\n# 1.1 数据加载\n\n * LoadImageFromFile\n * LoadAnnotations\n\n# 1.2 预处理\n\n * Resize\n * RandomFlip\n * Pad\n * RandomCrop\n * Normalize\n * SegRescale\n * PhotoMetricDistortion\n\n# 1.3 格式化\n\n * ToTensor\n\n * ImageToTensor\n\n * Transpose\n\n * ToDataContainer\n\n * DefaultFormatBundle\n\n * Collect\n\n# 1.4 测试时增强（TTA）\n\n * MultiScaleFlipAug\n\n# 2、如何自定义 pipeline\n\n首先实现自己的 MyTransform 类\n\nfrom mmseg.datasets import PIPELINES\n\n@PIPELINES.register_module()\nclass MyTransform:\n\n    def __call__(self, results):\n        results['dummy'] = True\n        return results\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n然后在 _init_.py 中 import 该类\n\nfrom .my_pipeline import MyTransform\n\n\n1\n\n\n在 config 中使用\n\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ncrop_size = (512, 1024)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations'),\n    dict(type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),\n    dict(type='RandomCrop', crop_size=crop_size, cat_max_ratio=0.75),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='PhotoMetricDistortion'),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size=crop_size, pad_val=0, seg_pad_val=255),\n    dict(type='MyTransform'),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# Tutorial 4：自定义模型\n\n# 1、自定义 optimizer\n\n先定义自己的优化器 MyOptimizer，并注册在 _init_.py 中 import 该优化器\n\nfrom mmcv.runner import OPTIMIZERS\nfrom torch.optim import Optimizer\n\n\n@OPTIMIZERS.register_module\nclass MyOptimizer(Optimizer):\n\n    def __init__(self, a, b, c)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nfrom .my_optimizer import MyOptimizer\n\n\n1\n\n\n2、在 config 中使用自定义的优化器\n\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n\n\n1\n\n\noptimizer = dict(type='Adam', lr=0.0003, weight_decay=0.0001)\n\n\n1\n\n\noptimizer = dict(type='MyOptimizer', a=a_value, b=b_value, c=c_value)\n\n\n1\n\n\n# 2、自定义 optimizer constructor\n\nfrom mmcv.utils import build_from_cfg\n\nfrom mmcv.runner import OPTIMIZER_BUILDERS\nfrom .cocktail_optimizer import CocktailOptimizer\n\n\n@OPTIMIZER_BUILDERS.register_module\nclass CocktailOptimizerConstructor(object):\n\n    def __init__(self, optimizer_cfg, paramwise_cfg=None):\n\n    def __call__(self, model):\n\n        return my_optimizer\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 3、开发一个新组件\n\n# 3.1 添加新的 backbone\n\nimport torch.nn as nn\n\nfrom ..registry import BACKBONES\n\n@BACKBONES.register_module\nclass MobileNet(nn.Module):\n    def __init__(self, arg1, arg2):\n        pass\n\n    def forward(self, x):  # should return a tuple\n        pass\n\n    def init_weights(self, pretrained=None):\n        pass\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nfrom .mobilenet import MobileNet\n\n\n1\n\n\nmodel = dict(\n    ...\n    backbone=dict(\n        type='MobileNet',\n        arg1=xxx,\n        arg2=xxx),\n    ...\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 3.2 添加新的 head\n\n@HEADS.register_module()\nclass PSPHead(BaseDecodeHead):\n\n    def __init__(self, pool_scales=(1, 2, 3, 6), **kwargs):\n        super(PSPHead, self).__init__(**kwargs)\n\n    def init_weights(self):\n\n    def forward(self, inputs):\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nnorm_cfg = dict(type='SyncBN', requires_grad=True)\nmodel = dict(\n    type='EncoderDecoder',\n    pretrained='pretrain_model/resnet50_v1c_trick-2cccc1ad.pth',\n    backbone=dict(\n        type='ResNetV1c',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        dilations=(1, 1, 2, 4),\n        strides=(1, 2, 1, 1),\n        norm_cfg=norm_cfg,\n        norm_eval=False,\n        style='pytorch',\n        contract_dilation=True),\n    decode_head=dict(\n        type='PSPHead',\n        in_channels=2048,\n        in_index=3,\n        channels=512,\n        pool_scales=(1, 2, 3, 6),\n        dropout_ratio=0.1,\n        num_classes=19,\n        norm_cfg=norm_cfg,\n        align_corners=False,\n        loss_decode=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n# 3.3 添加新的 loss\n\nimport torch\nimport torch.nn as nn\n\nfrom ..builder import LOSSES\nfrom .utils import weighted_loss\n\n@weighted_loss\ndef my_loss(pred, target):\n    assert pred.size() == target.size() and target.numel() > 0\n    loss = torch.abs(pred - target)\n    return loss\n\n@LOSSES.register_module\nclass MyLoss(nn.Module):\n\n    def __init__(self, reduction='mean', loss_weight=1.0):\n        super(MyLoss, self).__init__()\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None):\n        assert reduction_override in (None, 'none', 'mean', 'sum')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        loss = self.loss_weight * my_loss(\n            pred, target, weight, reduction=reduction, avg_factor=avg_factor)\n        return loss\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nfrom .my_loss import MyLoss, my_loss\n\n\n1\n\n\nloss_decode=dict(type='MyLoss', loss_weight=1.0))\n\n\n1\n",normalizedContent:"# mmsegmentation 框架解析\n\n\n# tutorial 3：自定义数据 pipeline\n\n因为语义分割的数据集可能不是相同的尺寸，所以在 mmcv 引入了一个新的 datacontainer 数据类型，来帮助组织不同尺寸的数据。\n\ndataset 和 data preparation pipeline 是解耦的，dataset 定义的是如何处理图像和标注，data pipeline 定义了所有准备数据的步骤。一个 pipeline 包含了一系列的操作，每个操作接收一个 dict 作为输入，并输出一个 dict 用作下一步的transform。下面是一个 cityscapes 数据集的dataset pipeline\n\n# dataset settings\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=true)\ncrop_size = (512, 1024)\ntrain_pipeline = [\n    dict(type='loadimagefromfile'),\n    dict(type='loadannotations'),\n    dict(type='resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),\n    dict(type='randomcrop', crop_size=crop_size, cat_max_ratio=0.75),\n    dict(type='randomflip', prob=0.5),\n    dict(type='photometricdistortion'),\n    dict(type='normalize', **img_norm_cfg),\n    dict(type='pad', size=crop_size, pad_val=0, seg_pad_val=255),\n    dict(type='defaultformatbundle'),\n    dict(type='collect', keys=['img', 'gt_semantic_seg']),\n]\ntest_pipeline = [\n    dict(type='loadimagefromfile'),\n    dict(\n        type='multiscaleflipaug',\n        img_scale=(2048, 1024),\n        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n        flip=false,\n        transforms=[\n            dict(type='resize', keep_ratio=true),\n            dict(type='randomflip'),\n            dict(type='normalize', **img_norm_cfg),\n            dict(type='imagetotensor', keys=['img']),\n            dict(type='collect', keys=['img']),\n        ])\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n# 1、data pipeline 的组成部分\n\n# 1.1 数据加载\n\n * loadimagefromfile\n * loadannotations\n\n# 1.2 预处理\n\n * resize\n * randomflip\n * pad\n * randomcrop\n * normalize\n * segrescale\n * photometricdistortion\n\n# 1.3 格式化\n\n * totensor\n\n * imagetotensor\n\n * transpose\n\n * todatacontainer\n\n * defaultformatbundle\n\n * collect\n\n# 1.4 测试时增强（tta）\n\n * multiscaleflipaug\n\n# 2、如何自定义 pipeline\n\n首先实现自己的 mytransform 类\n\nfrom mmseg.datasets import pipelines\n\n@pipelines.register_module()\nclass mytransform:\n\n    def __call__(self, results):\n        results['dummy'] = true\n        return results\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n然后在 _init_.py 中 import 该类\n\nfrom .my_pipeline import mytransform\n\n\n1\n\n\n在 config 中使用\n\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=true)\ncrop_size = (512, 1024)\ntrain_pipeline = [\n    dict(type='loadimagefromfile'),\n    dict(type='loadannotations'),\n    dict(type='resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),\n    dict(type='randomcrop', crop_size=crop_size, cat_max_ratio=0.75),\n    dict(type='randomflip', flip_ratio=0.5),\n    dict(type='photometricdistortion'),\n    dict(type='normalize', **img_norm_cfg),\n    dict(type='pad', size=crop_size, pad_val=0, seg_pad_val=255),\n    dict(type='mytransform'),\n    dict(type='defaultformatbundle'),\n    dict(type='collect', keys=['img', 'gt_semantic_seg']),\n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# tutorial 4：自定义模型\n\n# 1、自定义 optimizer\n\n先定义自己的优化器 myoptimizer，并注册在 _init_.py 中 import 该优化器\n\nfrom mmcv.runner import optimizers\nfrom torch.optim import optimizer\n\n\n@optimizers.register_module\nclass myoptimizer(optimizer):\n\n    def __init__(self, a, b, c)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nfrom .my_optimizer import myoptimizer\n\n\n1\n\n\n2、在 config 中使用自定义的优化器\n\noptimizer = dict(type='sgd', lr=0.02, momentum=0.9, weight_decay=0.0001)\n\n\n1\n\n\noptimizer = dict(type='adam', lr=0.0003, weight_decay=0.0001)\n\n\n1\n\n\noptimizer = dict(type='myoptimizer', a=a_value, b=b_value, c=c_value)\n\n\n1\n\n\n# 2、自定义 optimizer constructor\n\nfrom mmcv.utils import build_from_cfg\n\nfrom mmcv.runner import optimizer_builders\nfrom .cocktail_optimizer import cocktailoptimizer\n\n\n@optimizer_builders.register_module\nclass cocktailoptimizerconstructor(object):\n\n    def __init__(self, optimizer_cfg, paramwise_cfg=none):\n\n    def __call__(self, model):\n\n        return my_optimizer\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 3、开发一个新组件\n\n# 3.1 添加新的 backbone\n\nimport torch.nn as nn\n\nfrom ..registry import backbones\n\n@backbones.register_module\nclass mobilenet(nn.module):\n    def __init__(self, arg1, arg2):\n        pass\n\n    def forward(self, x):  # should return a tuple\n        pass\n\n    def init_weights(self, pretrained=none):\n        pass\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nfrom .mobilenet import mobilenet\n\n\n1\n\n\nmodel = dict(\n    ...\n    backbone=dict(\n        type='mobilenet',\n        arg1=xxx,\n        arg2=xxx),\n    ...\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 3.2 添加新的 head\n\n@heads.register_module()\nclass psphead(basedecodehead):\n\n    def __init__(self, pool_scales=(1, 2, 3, 6), **kwargs):\n        super(psphead, self).__init__(**kwargs)\n\n    def init_weights(self):\n\n    def forward(self, inputs):\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nnorm_cfg = dict(type='syncbn', requires_grad=true)\nmodel = dict(\n    type='encoderdecoder',\n    pretrained='pretrain_model/resnet50_v1c_trick-2cccc1ad.pth',\n    backbone=dict(\n        type='resnetv1c',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        dilations=(1, 1, 2, 4),\n        strides=(1, 2, 1, 1),\n        norm_cfg=norm_cfg,\n        norm_eval=false,\n        style='pytorch',\n        contract_dilation=true),\n    decode_head=dict(\n        type='psphead',\n        in_channels=2048,\n        in_index=3,\n        channels=512,\n        pool_scales=(1, 2, 3, 6),\n        dropout_ratio=0.1,\n        num_classes=19,\n        norm_cfg=norm_cfg,\n        align_corners=false,\n        loss_decode=dict(\n            type='crossentropyloss', use_sigmoid=false, loss_weight=1.0)))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n# 3.3 添加新的 loss\n\nimport torch\nimport torch.nn as nn\n\nfrom ..builder import losses\nfrom .utils import weighted_loss\n\n@weighted_loss\ndef my_loss(pred, target):\n    assert pred.size() == target.size() and target.numel() > 0\n    loss = torch.abs(pred - target)\n    return loss\n\n@losses.register_module\nclass myloss(nn.module):\n\n    def __init__(self, reduction='mean', loss_weight=1.0):\n        super(myloss, self).__init__()\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=none,\n                avg_factor=none,\n                reduction_override=none):\n        assert reduction_override in (none, 'none', 'mean', 'sum')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        loss = self.loss_weight * my_loss(\n            pred, target, weight, reduction=reduction, avg_factor=avg_factor)\n        return loss\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nfrom .my_loss import myloss, my_loss\n\n\n1\n\n\nloss_decode=dict(type='myloss', loss_weight=1.0))\n\n\n1\n",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"后期泛谈",frontmatter:{title:"后期泛谈",date:"2021-09-10T15:08:25.000Z",permalink:"/pages/736ec2/",categories:["学习笔记","体会感悟-摄影"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/08.%E4%BD%93%E4%BC%9A%E6%84%9F%E6%82%9F-%E6%91%84%E5%BD%B1/04.%E5%90%8E%E6%9C%9F%E6%B3%9B%E8%B0%88.html",relativePath:"02.学习笔记/08.体会感悟-摄影/04.后期泛谈.md",key:"v-1973eb08",path:"/pages/736ec2/",headersStr:null,content:"黄蓝色调、哈苏色调、青橙色调\n\n * https://zhuanlan.zhihu.com/p/142072802",normalizedContent:"黄蓝色调、哈苏色调、青橙色调\n\n * https://zhuanlan.zhihu.com/p/142072802",charsets:{cjk:!0},lastUpdated:"2021/09/12, 20:42:58"},{title:"特征可视化简介",frontmatter:{title:"特征可视化简介",date:"2021-03-30T14:59:48.000Z",permalink:"/pages/405167/",categories:["计算机视觉","特征可视化"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/09.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-.%E7%89%B9%E5%BE%81%E5%8F%AF%E8%A7%86%E5%8C%96/00.%E7%89%B9%E5%BE%81%E5%8F%AF%E8%A7%86%E5%8C%96%E7%AE%80%E4%BB%8B.html",relativePath:"02.学习笔记/09.系列笔记-.特征可视化/00.特征可视化简介.md",key:"v-74c89e7c",path:"/pages/405167/",headers:[{level:2,title:"特征可视化简介",slug:"特征可视化简介",normalizedTitle:"特征可视化简介",charIndex:2}],headersStr:"特征可视化简介",content:"# 特征可视化简介\n\n特征可视化有助于我们更好的理解深度网络，是深度学习可解释性的重要组成部分。常见的特征可视化方法主要分为三类：特征层的可视化、卷积核的可视化、类激活图的可视化。\n\n而实现这些特征的可视化主要的思路分为前向计算和反向计算。前向计算是指：我们对于一张输入图像，直接进行前向计算，可以得到网络每层的feature map，可以直接对 feature map 进行可视化。后向计算是指：根据网络最后一层最大的激活值，利用感受野关系映射到原始输入图像的某些区域，可以观察到是哪些区域激活了网络，也就是类激活图的可视化。卷积核的可视化就是对于某个输入图像，将卷积核\n\n# 1、类激活图（Class Activation Map）\n\nLearning deep features for discriminative localization\n\nGrad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization\n\nGrad-CAM++: Generalized Gradient-based Visual Explanations for Deep Convolutional Networks\n\n# 2、特征层的可视化\n\nVisualizing and Understanding Convolutional Networks\n\n# 3、卷积核的可视化\n\n# 参考资料\n\n * CNN的一些可视化方法\n * CNN可视化技术总结（一）--特征图可视化\n * CNN可视化技术总结（二）--卷积核可视化\n * CNN可视化技术总结（三）--类可视化\n * 最便捷的神经网络可视化工具之一--Flashtorch\n\n# 参考代码\n\n * Neural network visualization toolkit for keras\n * Visualization toolkit for neural networks in PyTorch!\n * PyTorch_Tutorial",normalizedContent:"# 特征可视化简介\n\n特征可视化有助于我们更好的理解深度网络，是深度学习可解释性的重要组成部分。常见的特征可视化方法主要分为三类：特征层的可视化、卷积核的可视化、类激活图的可视化。\n\n而实现这些特征的可视化主要的思路分为前向计算和反向计算。前向计算是指：我们对于一张输入图像，直接进行前向计算，可以得到网络每层的feature map，可以直接对 feature map 进行可视化。后向计算是指：根据网络最后一层最大的激活值，利用感受野关系映射到原始输入图像的某些区域，可以观察到是哪些区域激活了网络，也就是类激活图的可视化。卷积核的可视化就是对于某个输入图像，将卷积核\n\n# 1、类激活图（class activation map）\n\nlearning deep features for discriminative localization\n\ngrad-cam:visual explanations from deep networks via gradient-based localization\n\ngrad-cam++: generalized gradient-based visual explanations for deep convolutional networks\n\n# 2、特征层的可视化\n\nvisualizing and understanding convolutional networks\n\n# 3、卷积核的可视化\n\n# 参考资料\n\n * cnn的一些可视化方法\n * cnn可视化技术总结（一）--特征图可视化\n * cnn可视化技术总结（二）--卷积核可视化\n * cnn可视化技术总结（三）--类可视化\n * 最便捷的神经网络可视化工具之一--flashtorch\n\n# 参考代码\n\n * neural network visualization toolkit for keras\n * visualization toolkit for neural networks in pytorch!\n * pytorch_tutorial",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"特征图可视化",frontmatter:{title:"特征图可视化",date:"2021-03-30T15:30:24.000Z",permalink:"/pages/7cc0fb/",categories:["计算机视觉","特征可视化"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/09.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-.%E7%89%B9%E5%BE%81%E5%8F%AF%E8%A7%86%E5%8C%96/02.%E7%89%B9%E5%BE%81%E5%9B%BE%E5%8F%AF%E8%A7%86%E5%8C%96.html",relativePath:"02.学习笔记/09.系列笔记-.特征可视化/02.特征图可视化.md",key:"v-56003e12",path:"/pages/7cc0fb/",headers:[{level:2,title:"特征图可视化",slug:"特征图可视化",normalizedTitle:"特征图可视化",charIndex:2}],headersStr:"特征图可视化",content:"# 特征图可视化\n\n特征图可视化主要有两类方法，一类是将某一层的feature map 映射到0-255 的范围转化为图像直接进行可视化，另一类是使用反卷积方法（反卷积、反池化）将feature map转化为图像，达到可视化feature map的目的。\n\n\n\n# 1、单通道特征图的可视化\n\n# 2、多通道特征图的可视化\n\n# 参考资料\n\nPyTorch模型训练特征图可视化（TensorboardX）",normalizedContent:"# 特征图可视化\n\n特征图可视化主要有两类方法，一类是将某一层的feature map 映射到0-255 的范围转化为图像直接进行可视化，另一类是使用反卷积方法（反卷积、反池化）将feature map转化为图像，达到可视化feature map的目的。\n\n\n\n# 1、单通道特征图的可视化\n\n# 2、多通道特征图的可视化\n\n# 参考资料\n\npytorch模型训练特征图可视化（tensorboardx）",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"卷积核可视化",frontmatter:{title:"卷积核可视化",date:"2021-03-30T15:30:34.000Z",permalink:"/pages/78c106/",categories:["计算机视觉","特征可视化"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/09.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-.%E7%89%B9%E5%BE%81%E5%8F%AF%E8%A7%86%E5%8C%96/03.%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%8F%AF%E8%A7%86%E5%8C%96.html",relativePath:"02.学习笔记/09.系列笔记-.特征可视化/03.卷积核可视化.md",key:"v-5c9bb8c7",path:"/pages/78c106/",headers:[{level:2,title:"卷积核可视化",slug:"卷积核可视化",normalizedTitle:"卷积核可视化",charIndex:2}],headersStr:"卷积核可视化",content:'# 卷积核可视化\n\n\n\n早在 AlexNet 论文中就已经有卷积核可视化了，主要目的是为了说明卷积核到底学到了什么内容\n\n# 参考文献\n\n * Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." Advances in neural information processing systems 25 (2012): 1097-1105.',normalizedContent:'# 卷积核可视化\n\n\n\n早在 alexnet 论文中就已经有卷积核可视化了，主要目的是为了说明卷积核到底学到了什么内容\n\n# 参考文献\n\n * krizhevsky, alex, ilya sutskever, and geoffrey e. hinton. "imagenet classification with deep convolutional neural networks." advances in neural information processing systems 25 (2012): 1097-1105.',charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"类激活热力图可视化",frontmatter:{title:"类激活热力图可视化",date:"2021-03-30T14:45:07.000Z",permalink:"/pages/26899e/",categories:["计算机视觉","特征可视化"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/09.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-.%E7%89%B9%E5%BE%81%E5%8F%AF%E8%A7%86%E5%8C%96/01.%E7%B1%BB%E6%BF%80%E6%B4%BB%E7%83%AD%E5%8A%9B%E5%9B%BE%E5%8F%AF%E8%A7%86%E5%8C%96.html",relativePath:"02.学习笔记/09.系列笔记-.特征可视化/01.类激活热力图可视化.md",key:"v-05ab1571",path:"/pages/26899e/",headers:[{level:2,title:"类激活热力图可视化",slug:"类激活热力图可视化",normalizedTitle:"类激活热力图可视化",charIndex:2}],headersStr:"类激活热力图可视化",content:"# 类激活热力图可视化\n\n# 参考资料\n\n * Zhou B, Khosla A, Lapedriza A, et al. Learning deep features for discriminative localization[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 2921-2929.\n * Selvaraju R R, Cogswell M, Das A, et al. Grad-cam: Visual explanations from deep networks via gradient-based localization[C]//Proceedings of the IEEE international conference on computer vision. 2017: 618-626.\n * Chattopadhay A, Sarkar A, Howlader P, et al. Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks[C]//2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2018: 839-847.\n\n# 参考代码\n\n * Activation Map Visualization\n\n自监督来做弱监督的语义分割\n\nlogits 的loss 怎么算呢\n\n因果推理的参考资料\n\n自监督的spatial attention",normalizedContent:"# 类激活热力图可视化\n\n# 参考资料\n\n * zhou b, khosla a, lapedriza a, et al. learning deep features for discriminative localization[c]//proceedings of the ieee conference on computer vision and pattern recognition. 2016: 2921-2929.\n * selvaraju r r, cogswell m, das a, et al. grad-cam: visual explanations from deep networks via gradient-based localization[c]//proceedings of the ieee international conference on computer vision. 2017: 618-626.\n * chattopadhay a, sarkar a, howlader p, et al. grad-cam++: generalized gradient-based visual explanations for deep convolutional networks[c]//2018 ieee winter conference on applications of computer vision (wacv). ieee, 2018: 839-847.\n\n# 参考代码\n\n * activation map visualization\n\n自监督来做弱监督的语义分割\n\nlogits 的loss 怎么算呢\n\n因果推理的参考资料\n\n自监督的spatial attention",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"五线谱基础（上）",frontmatter:{title:"五线谱基础（上）",date:"2021-03-31T20:43:03.000Z",permalink:"/pages/9867d4/",categories:["更多","乐理和五线谱"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-%E4%B9%90%E7%90%86%E5%92%8C%E4%BA%94%E7%BA%BF%E8%B0%B1/00.%E4%BA%94%E7%BA%BF%E8%B0%B1%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%8A%EF%BC%89.html",relativePath:"02.学习笔记/10.系列笔记-乐理和五线谱/00.五线谱基础（上）.md",key:"v-cc9eb5f0",path:"/pages/9867d4/",headers:[{level:2,title:"五线谱基础（上）",slug:"五线谱基础-上",normalizedTitle:"五线谱基础（上）",charIndex:2},{level:3,title:"01 什么是线与间",slug:"_01-什么是线与间",normalizedTitle:"01 什么是线与间",charIndex:15},{level:3,title:"02 拍号与音符时值",slug:"_02-拍号与音符时值",normalizedTitle:"02 拍号与音符时值",charIndex:130},{level:3,title:"03 五线谱上的各类音名",slug:"_03-五线谱上的各类音名",normalizedTitle:"03 五线谱上的各类音名",charIndex:434},{level:3,title:"04 符干与连音线",slug:"_04-符干与连音线",normalizedTitle:"04 符干与连音线",charIndex:544},{level:3,title:"05 变化音与还原记号",slug:"_05-变化音与还原记号",normalizedTitle:"05 变化音与还原记号",charIndex:615}],headersStr:"五线谱基础（上） 01 什么是线与间 02 拍号与音符时值 03 五线谱上的各类音名 04 符干与连音线 05 变化音与还原记号",content:"# 五线谱基础（上）\n\n\n# 01 什么是线与间\n\n\n\n\n\n * 花连谱号、低音谱号、高音谱号\n\n * 黑色的长方形叫做休止符\n\n * 高音谱号和低音谱号，五线四间\n\n * 上加一线，上加一间\n\n * 高音谱号的下加一线和低音谱号的上加一线是同一个音\n\n\n# 02 拍号与音符时值\n\n# 五线谱如何表示拍号\n\n\n\n\n\n * C 表示 4/4 拍、6/8拍\n * 4分音符的休止符、2分音符休止符、全休止符、八分音符休止符、十六分音符休止符\n   * 全休止符：黑色长方形 贴在线下方\n   * 二分音符休止符：黑色长方形 贴在线上方\n   * 四分音符休止符：一个花休止符\n   * 八分音符休止符：一个小尾巴\n   * 十六分音符休止符：两个小尾巴（以此类推）\n\n# 音符的时值\n\n\n\n * 空心的带符干的音符代表一个二分音符\n\n * 实心的带符干的音符代表一个四分音符\n\n * 单独的八分音符有一个小尾巴\n\n * 单独的十六分音符有两个小尾巴（依此类推）\n\n\n# 03 五线谱上的各类音名\n\n\n\n上图是高音谱号的C的位置\n\n * C3 是下加一线，C4 是第三间，C5 是上加二线\n\n\n\n上图是低音谱号的C 的位置\n\n * C1 是下加二线，C2 是第二间，C3 是上加一线\n\n\n# 04 符干与连音线\n\n * 三线以下符干向上，三线以上符干向下\n * 多音的时候符干朝向一致\n * 连音线的时值是多个音符的时值相加\n\n\n# 05 变化音与还原记号\n\n\n\n# 什么是变音记号？\n\n * 变音记号：升号、降号、重升、重降，都写在音符左边\n * 升号：类似于井号键，升一次半音\n * 降号：类似于小写b，降一次半音\n * 重升号：类似于X符号，升两次半音（升一个全音）\n * 重降号：两个降号并排写，降两次半音（降一个全音）\n\n注：如果在同一小节如果一个音进行了升降，如果后面再出现这个音就不需要标升降了\n\n还原记号就是对升降的音进行还原\n\n# 参考资料\n\n * 《从零轻松学习五线谱》全套课程【10集全】",normalizedContent:"# 五线谱基础（上）\n\n\n# 01 什么是线与间\n\n\n\n\n\n * 花连谱号、低音谱号、高音谱号\n\n * 黑色的长方形叫做休止符\n\n * 高音谱号和低音谱号，五线四间\n\n * 上加一线，上加一间\n\n * 高音谱号的下加一线和低音谱号的上加一线是同一个音\n\n\n# 02 拍号与音符时值\n\n# 五线谱如何表示拍号\n\n\n\n\n\n * c 表示 4/4 拍、6/8拍\n * 4分音符的休止符、2分音符休止符、全休止符、八分音符休止符、十六分音符休止符\n   * 全休止符：黑色长方形 贴在线下方\n   * 二分音符休止符：黑色长方形 贴在线上方\n   * 四分音符休止符：一个花休止符\n   * 八分音符休止符：一个小尾巴\n   * 十六分音符休止符：两个小尾巴（以此类推）\n\n# 音符的时值\n\n\n\n * 空心的带符干的音符代表一个二分音符\n\n * 实心的带符干的音符代表一个四分音符\n\n * 单独的八分音符有一个小尾巴\n\n * 单独的十六分音符有两个小尾巴（依此类推）\n\n\n# 03 五线谱上的各类音名\n\n\n\n上图是高音谱号的c的位置\n\n * c3 是下加一线，c4 是第三间，c5 是上加二线\n\n\n\n上图是低音谱号的c 的位置\n\n * c1 是下加二线，c2 是第二间，c3 是上加一线\n\n\n# 04 符干与连音线\n\n * 三线以下符干向上，三线以上符干向下\n * 多音的时候符干朝向一致\n * 连音线的时值是多个音符的时值相加\n\n\n# 05 变化音与还原记号\n\n\n\n# 什么是变音记号？\n\n * 变音记号：升号、降号、重升、重降，都写在音符左边\n * 升号：类似于井号键，升一次半音\n * 降号：类似于小写b，降一次半音\n * 重升号：类似于x符号，升两次半音（升一个全音）\n * 重降号：两个降号并排写，降两次半音（降一个全音）\n\n注：如果在同一小节如果一个音进行了升降，如果后面再出现这个音就不需要标升降了\n\n还原记号就是对升降的音进行还原\n\n# 参考资料\n\n * 《从零轻松学习五线谱》全套课程【10集全】",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"五线谱基础（下）",frontmatter:{title:"五线谱基础（下）",date:"2021-04-14T11:42:51.000Z",permalink:"/pages/4ac034/",categories:["更多","乐理和五线谱"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-%E4%B9%90%E7%90%86%E5%92%8C%E4%BA%94%E7%BA%BF%E8%B0%B1/01.%E4%BA%94%E7%BA%BF%E8%B0%B1%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%8B%EF%BC%89.html",relativePath:"02.学习笔记/10.系列笔记-乐理和五线谱/01.五线谱基础（下）.md",key:"v-232d0ce8",path:"/pages/4ac034/",headers:[{level:2,title:"五线谱基础（下）",slug:"五线谱基础-下",normalizedTitle:"五线谱基础（下）",charIndex:2},{level:3,title:"06 调号的认识",slug:"_06-调号的认识",normalizedTitle:"06 调号的认识",charIndex:15},{level:3,title:"07 高低八度标记",slug:"_07-高低八度标记",normalizedTitle:"07 高低八度标记",charIndex:146},{level:3,title:"08 踏板标记的认识",slug:"_08-踏板标记的认识",normalizedTitle:"08 踏板标记的认识",charIndex:239},{level:3,title:"09 渐强、渐弱以及重音记号",slug:"_09-渐强、渐弱以及重音记号",normalizedTitle:"09 渐强、渐弱以及重音记号",charIndex:267},{level:3,title:"10 琶音",slug:"_10-琶音",normalizedTitle:"10 琶音",charIndex:290}],headersStr:"五线谱基础（下） 06 调号的认识 07 高低八度标记 08 踏板标记的认识 09 渐强、渐弱以及重音记号 10 琶音",content:"# 五线谱基础（下）\n\n\n# 06 调号的认识\n\n五线谱的调号\n\n如果后面的音在F、C、G几个音上面的话，都会升半音，就不用再使用升号标记了\n\n# 号类：4152637\n\nb 号类：7362514\n\n\n\n例如在萤火虫这首歌里面，在男低的谱子中，就对F、C这两个音做了升半音的操作\n\n\n\n\n# 07 高低八度标记\n\n\n\n上面的高八度的标记会直接高八度，C3 族的音在实际弹奏的时候会是 C4 族的\n\n下面的低八度的标记会直接低八度，C3族的音在实际弹奏时会是 C2 族的\n\n\n# 08 踏板标记的认识\n\n\n\n踩下踏板是有延音的\n\n\n# 09 渐强、渐弱以及重音记号\n\n\n\n\n\n\n# 10 琶音\n\n\n\n是多个音一起弹，每个音的长短和强弱要自己把握\n\n * # 参考资料\n   \n   * 《从零轻松学习五线谱》全套课程【10集全】",normalizedContent:"# 五线谱基础（下）\n\n\n# 06 调号的认识\n\n五线谱的调号\n\n如果后面的音在f、c、g几个音上面的话，都会升半音，就不用再使用升号标记了\n\n# 号类：4152637\n\nb 号类：7362514\n\n\n\n例如在萤火虫这首歌里面，在男低的谱子中，就对f、c这两个音做了升半音的操作\n\n\n\n\n# 07 高低八度标记\n\n\n\n上面的高八度的标记会直接高八度，c3 族的音在实际弹奏的时候会是 c4 族的\n\n下面的低八度的标记会直接低八度，c3族的音在实际弹奏时会是 c2 族的\n\n\n# 08 踏板标记的认识\n\n\n\n踩下踏板是有延音的\n\n\n# 09 渐强、渐弱以及重音记号\n\n\n\n\n\n\n# 10 琶音\n\n\n\n是多个音一起弹，每个音的长短和强弱要自己把握\n\n * # 参考资料\n   \n   * 《从零轻松学习五线谱》全套课程【10集全】",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"富士相机泛谈",frontmatter:{title:"富士相机泛谈",date:"2021-09-08T12:28:58.000Z",permalink:"/pages/5f0c87/",categories:["学习笔记","体会感悟-摄影"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/08.%E4%BD%93%E4%BC%9A%E6%84%9F%E6%82%9F-%E6%91%84%E5%BD%B1/02.%E5%AF%8C%E5%A3%AB%E7%9B%B8%E6%9C%BA%E6%B3%9B%E8%B0%88.html",relativePath:"02.学习笔记/08.体会感悟-摄影/02.富士相机泛谈.md",key:"v-0fbc1026",path:"/pages/5f0c87/",headersStr:null,content:"富士目前在售的及其没有单反，没有全画幅，所以基本它的全部精力都在 APS-C 和中画幅\n\n# 01、APS-C 画幅、全画幅、中画幅的区别\n\n * APS-C 画幅（Advanced Photo System-classic）：长宽比为3:2，边长近似为24.9×16.6mm\n\n * 全画幅（Full Frame）：一般指 135 胶卷的成像面积，感光面积为 36x24 mm 尺寸大小的规格\n\n * 中画幅（Medium Format）：一般指 120 胶卷的成像面积，传感器尺寸介于 36×24mm 与 4×5 英寸之间。\n\n\n\n# 02、焦距换算率\n\n * 通常各个不同相机厂商都有自己的转换系数\n * 比如佳能的转换系数是1.6，索尼、尼康、富士的转换系数是1.5等等\n * 用 50mm APS-C 画幅相机的等效焦距约为 75mm\n\n# 03、富士相机型号一览\n\n * GFX 系列：中画幅微单，分为 GFX100 和 GFX50 两个系列\n   \n   * GFX 100 代表富士相机的最高画质水平，大约一亿像素\n   * GFX 50 大约五千万像素\n\n * X 系列：APS-C 微单\n   \n   * 类单反系列\n     * X-H 系列：中高端产品线，目前只有 2018 年发布的 X-H1 这一款机型\n     * X-S 系列：2020年10月开启的全新系列，目前只有 2020 年发布的 X-S10 一款产品\n     * X-T 系列：富士微单主力军\n   * 类旁轴系列\n     * X-Pro 系列：类旁轴造型中的高端产品系列\n       * 对标 X-T 一位数机型\n     * X-E 系列：类旁轴造型中的中端产品系列\n       * 过去对标X-T二位数机型，现在主要对标X-S机型\n     * X-A 系列：类旁轴造型中的低端/入门产品系列\n       * X-A系列的一位数型号对标X-T三位数型号\n       * X-A系列的两位数型号是富士最低端的微单系列\n   * 不可更换镜头相机\n   \n   \n\n下面着重介绍下 X-T 系列的相机，对以后购买相机做个参考\n\n# 04、APS-C 微单 X-T 系列\n\n截止目前（2021年9月），富士官网在售的 X-T 系列机器还有 7 个型号\n\n# 4.1 X-T100\n\n官网链接：https://fujifilm-x.com/zh-cn/products/cameras/x-t100/\n\n相机参数：\n\n * 有效像素：2420 万\n\n * 镜头卡口：富士 X 卡口\n\n * 存储卡最大支持容量：16GB\n\n# 4.2 X-T200\n\n官网链接：https://fujifilm-x.com/zh-cn/products/cameras/x-t200/\n\n# 4.3 X-T20\n\n官网链接：https://fujifilm-x.com/zh-cn/products/cameras/x-t20/\n\n# 4.4 X-T30\n\n发售时间：2019年03月\n\n官网链接：https://fujifilm-x.com/global/products/cameras/x-t30/\n\n闲鱼价格：15-45 套机 5800，单机身 4500\n\n京东价格：15-45 套机 6790，单机身 5999\n\n淘宝次新品价格：单机身 5290\n\n相机参数\n\n * 相机重量：383 g\n * \n\n# 4.5 X-T30II\n\n官网链接：https://fujifilm-x.com/zh-cn/products/cameras/x-t30-ii/\n\n# 4.5 X-T2\n\n发售时间：2016 年09月\n\n闲鱼价格：2021 年 9 月 3000 左右，2020 年 9 月箱说全 3500\n\n京东价格：\n\n# 4.6 X-T3\n\n发售时间：2018年09月\n\n官网链接：https://fujifilm-x.com/zh-cn/products/cameras/x-t3/\n\n闲鱼价格：机身 5500 左右\n\n京东价格：机身 8990 左右，35mm F1.4 套机 10900 左右\n\n淘宝次新品价格：单机身 7299\n\n相机参数：\n\n * 传感器类型：X-Trans CMOS 4\n * 有效像素：2610 万\n * 电池性能：NP-W126S锂离子电池\n\n# 4.7 X-T4\n\n发售时间：2020年\n\n官网链接：https://fujifilm-x.com/zh-cn/products/cameras/x-t4/\n\n相机参数：\n\n * 传感器类型：X-Trans CMOS 4\n\n * 有效像素：2610 万\n\n * 镜头卡口：富士 X 卡口\n\n * 快门速度：1/8000s, 1/32000s\n\n * 连拍功能：30张/s\n\n * 产品重量：仅机身 526g，含电池和存储卡 607g\n\n# 05、APS-C 微单 X-S10\n\n单独介绍以下 X-S10\n\n * 相机参数\n\n * 价格：\n   \n   * 京东价格：全新机身 6999，15-45 套机 7699\n   * 淘宝次新品价格：单机身 7299\n\n * 优点\n   \n   * 五轴防抖\n   * 翻转屏\n   * 握持手感\n\n * 缺点\n   \n   * 续航一般，一块电池 300 张出头\n   * 单卡槽\n   * 机身发热\n\n# 06、镜头群介绍\n\n6.0 一些缩写介绍\n\n * OIS：Optical Image Stabilization 光学图像防抖，通过陀螺仪来获取当前设备的抖动幅度进行反向补偿，从而获得防抖效果\n\n6.1、 国产\n\n * 唯卓仕：https://item.jd.com/10021830637202.html\n   \n   * 56mm F/1.4\n\n * 七工匠：\n   \n   * 35mm F1.2：https://post.smzdm.com/p/az5934ko/\n\n6.1、原厂\n\n变焦镜头\n\n * XC 15-45mm F3.5-5.6\n   * 重量：135g\n   * 价格：800左右\n   * 缺点：对焦声音大\n * XF 18-55mm F/2.8-4\n   * 重量：310g\n   * 价格：2000-2500左右\n   * 缺点：太重\n * XF 16-55mm F2.8 R LM WR（恒定光圈）\n   * 重量：655g\n   * 价格：6990\n * XF 10-24mm F4 R OIS WR\n   * 重量：410g\n   * 价格：6690\n   * 用途：超广角，拍摄Vlog 镜头\n * XF 16-80mm F4 R OIS WR\n   * 重量：440g\n   * 价格：5580\n   * 用途：\n\n定焦镜头\n\n（澳洲镜头打折贼便宜）\n\n * XF 27mm F2.8\n   \n   * 重量：78g\n   * 价格：国外500，国内1200（？？？）\n   * 优点：饼干头，小巧\n   * 评价：出了二代，多了全天候设计，多了光圈环。\n\n * XC 35mm F/2\n   \n   * 重量：130g\n   * 价格：199 USD\n   * 缺点：无全天候防护，卡口周围没有防尘封闭\n   * 评价：nifty fifty：直译为“漂亮的50mm”，可以认为是小痰盂\n\n * XF 35mm F/2 WR\n   \n   * 重量：170g\n   * 价格：399 USD\n   * 优点：金属材质，外观的金属滚花纹更复古，有全天候防护，附赠一个遮光罩\n\n * XC 35mm F1.4\n   \n   * 光学上很出色，但是其对焦较慢\n\n * XF 23mm F1.4\n\n * XF 16mm F2.8\n   \n   * 广角镜头\n\n * XF 18mm F2\n\n * XF 50mm F2\n   \n   * 人像镜头\n\n# 06、后期软件\n\n * Alien Skin Exposure\n * VSCO\n * Lightroom\n\n# 参考资料\n\n * 从型号了解一台相机·富士篇 | 2021新版",normalizedContent:"富士目前在售的及其没有单反，没有全画幅，所以基本它的全部精力都在 aps-c 和中画幅\n\n# 01、aps-c 画幅、全画幅、中画幅的区别\n\n * aps-c 画幅（advanced photo system-classic）：长宽比为3:2，边长近似为24.9×16.6mm\n\n * 全画幅（full frame）：一般指 135 胶卷的成像面积，感光面积为 36x24 mm 尺寸大小的规格\n\n * 中画幅（medium format）：一般指 120 胶卷的成像面积，传感器尺寸介于 36×24mm 与 4×5 英寸之间。\n\n\n\n# 02、焦距换算率\n\n * 通常各个不同相机厂商都有自己的转换系数\n * 比如佳能的转换系数是1.6，索尼、尼康、富士的转换系数是1.5等等\n * 用 50mm aps-c 画幅相机的等效焦距约为 75mm\n\n# 03、富士相机型号一览\n\n * gfx 系列：中画幅微单，分为 gfx100 和 gfx50 两个系列\n   \n   * gfx 100 代表富士相机的最高画质水平，大约一亿像素\n   * gfx 50 大约五千万像素\n\n * x 系列：aps-c 微单\n   \n   * 类单反系列\n     * x-h 系列：中高端产品线，目前只有 2018 年发布的 x-h1 这一款机型\n     * x-s 系列：2020年10月开启的全新系列，目前只有 2020 年发布的 x-s10 一款产品\n     * x-t 系列：富士微单主力军\n   * 类旁轴系列\n     * x-pro 系列：类旁轴造型中的高端产品系列\n       * 对标 x-t 一位数机型\n     * x-e 系列：类旁轴造型中的中端产品系列\n       * 过去对标x-t二位数机型，现在主要对标x-s机型\n     * x-a 系列：类旁轴造型中的低端/入门产品系列\n       * x-a系列的一位数型号对标x-t三位数型号\n       * x-a系列的两位数型号是富士最低端的微单系列\n   * 不可更换镜头相机\n   \n   \n\n下面着重介绍下 x-t 系列的相机，对以后购买相机做个参考\n\n# 04、aps-c 微单 x-t 系列\n\n截止目前（2021年9月），富士官网在售的 x-t 系列机器还有 7 个型号\n\n# 4.1 x-t100\n\n官网链接：https://fujifilm-x.com/zh-cn/products/cameras/x-t100/\n\n相机参数：\n\n * 有效像素：2420 万\n\n * 镜头卡口：富士 x 卡口\n\n * 存储卡最大支持容量：16gb\n\n# 4.2 x-t200\n\n官网链接：https://fujifilm-x.com/zh-cn/products/cameras/x-t200/\n\n# 4.3 x-t20\n\n官网链接：https://fujifilm-x.com/zh-cn/products/cameras/x-t20/\n\n# 4.4 x-t30\n\n发售时间：2019年03月\n\n官网链接：https://fujifilm-x.com/global/products/cameras/x-t30/\n\n闲鱼价格：15-45 套机 5800，单机身 4500\n\n京东价格：15-45 套机 6790，单机身 5999\n\n淘宝次新品价格：单机身 5290\n\n相机参数\n\n * 相机重量：383 g\n * \n\n# 4.5 x-t30ii\n\n官网链接：https://fujifilm-x.com/zh-cn/products/cameras/x-t30-ii/\n\n# 4.5 x-t2\n\n发售时间：2016 年09月\n\n闲鱼价格：2021 年 9 月 3000 左右，2020 年 9 月箱说全 3500\n\n京东价格：\n\n# 4.6 x-t3\n\n发售时间：2018年09月\n\n官网链接：https://fujifilm-x.com/zh-cn/products/cameras/x-t3/\n\n闲鱼价格：机身 5500 左右\n\n京东价格：机身 8990 左右，35mm f1.4 套机 10900 左右\n\n淘宝次新品价格：单机身 7299\n\n相机参数：\n\n * 传感器类型：x-trans cmos 4\n * 有效像素：2610 万\n * 电池性能：np-w126s锂离子电池\n\n# 4.7 x-t4\n\n发售时间：2020年\n\n官网链接：https://fujifilm-x.com/zh-cn/products/cameras/x-t4/\n\n相机参数：\n\n * 传感器类型：x-trans cmos 4\n\n * 有效像素：2610 万\n\n * 镜头卡口：富士 x 卡口\n\n * 快门速度：1/8000s, 1/32000s\n\n * 连拍功能：30张/s\n\n * 产品重量：仅机身 526g，含电池和存储卡 607g\n\n# 05、aps-c 微单 x-s10\n\n单独介绍以下 x-s10\n\n * 相机参数\n\n * 价格：\n   \n   * 京东价格：全新机身 6999，15-45 套机 7699\n   * 淘宝次新品价格：单机身 7299\n\n * 优点\n   \n   * 五轴防抖\n   * 翻转屏\n   * 握持手感\n\n * 缺点\n   \n   * 续航一般，一块电池 300 张出头\n   * 单卡槽\n   * 机身发热\n\n# 06、镜头群介绍\n\n6.0 一些缩写介绍\n\n * ois：optical image stabilization 光学图像防抖，通过陀螺仪来获取当前设备的抖动幅度进行反向补偿，从而获得防抖效果\n\n6.1、 国产\n\n * 唯卓仕：https://item.jd.com/10021830637202.html\n   \n   * 56mm f/1.4\n\n * 七工匠：\n   \n   * 35mm f1.2：https://post.smzdm.com/p/az5934ko/\n\n6.1、原厂\n\n变焦镜头\n\n * xc 15-45mm f3.5-5.6\n   * 重量：135g\n   * 价格：800左右\n   * 缺点：对焦声音大\n * xf 18-55mm f/2.8-4\n   * 重量：310g\n   * 价格：2000-2500左右\n   * 缺点：太重\n * xf 16-55mm f2.8 r lm wr（恒定光圈）\n   * 重量：655g\n   * 价格：6990\n * xf 10-24mm f4 r ois wr\n   * 重量：410g\n   * 价格：6690\n   * 用途：超广角，拍摄vlog 镜头\n * xf 16-80mm f4 r ois wr\n   * 重量：440g\n   * 价格：5580\n   * 用途：\n\n定焦镜头\n\n（澳洲镜头打折贼便宜）\n\n * xf 27mm f2.8\n   \n   * 重量：78g\n   * 价格：国外500，国内1200（？？？）\n   * 优点：饼干头，小巧\n   * 评价：出了二代，多了全天候设计，多了光圈环。\n\n * xc 35mm f/2\n   \n   * 重量：130g\n   * 价格：199 usd\n   * 缺点：无全天候防护，卡口周围没有防尘封闭\n   * 评价：nifty fifty：直译为“漂亮的50mm”，可以认为是小痰盂\n\n * xf 35mm f/2 wr\n   \n   * 重量：170g\n   * 价格：399 usd\n   * 优点：金属材质，外观的金属滚花纹更复古，有全天候防护，附赠一个遮光罩\n\n * xc 35mm f1.4\n   \n   * 光学上很出色，但是其对焦较慢\n\n * xf 23mm f1.4\n\n * xf 16mm f2.8\n   \n   * 广角镜头\n\n * xf 18mm f2\n\n * xf 50mm f2\n   \n   * 人像镜头\n\n# 06、后期软件\n\n * alien skin exposure\n * vsco\n * lightroom\n\n# 参考资料\n\n * 从型号了解一台相机·富士篇 | 2021新版",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"爬虫基础",frontmatter:{title:"爬虫基础",date:"2021-03-09T21:20:55.000Z",permalink:"/pages/2cfd0e/",categories:["技术文章","爬虫实践"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/11.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-%E7%88%AC%E8%99%AB%E5%AE%9E%E8%B7%B5/01.%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80.html",relativePath:"02.学习笔记/11.系列笔记-爬虫实践/01.爬虫基础.md",key:"v-20a0e6d4",path:"/pages/2cfd0e/",headers:[{level:3,title:"Python 爬虫 -01 爬虫基础",slug:"python-爬虫-01-爬虫基础",normalizedTitle:"python 爬虫 -01 爬虫基础",charIndex:2}],headersStr:"Python 爬虫 -01 爬虫基础",content:"# Python 爬虫 -01 爬虫基础\n\n# 1 HTTP\n\nHTTP是一个客户端（用户）和服务器端（网站）之间进行请求和应答的标准。通过使用网页浏览器、网络爬虫或者其他工具，客户端可以向服务器上的指定端口（默认端口为80）发起一个HTTP请求。客户端称为客户代理（user agent），应答服务器成为源服务器（origin server)。\n\nHTTP假定其下层协议能够提供可靠的传输，因此，任何能够提供这种保证的协议都可以使用。使用TCP/IP协议族时RCP(Remote Procedure Call，远程过程调用)作为传输层\n\n通常由HTTP客户端发起一个请求，创建一个到服务器指定端口（默认是80端口）的TCP链接。HTTP服务器则在该端口监听客户端的请求。一旦收到请求，服务器会向客户端返回一个状态（比如“THTTP/1.1 200 OK”），以及请求的文件、错误信息等响应内容。\n\nHTTP的请求方法\n\n * GET：像指定资源发出“显示”请求，GET方法应该只用于读取数据。GET可能会被爬虫等随意访问\n\n * HEAD：与GET方法一样，都是向服务器发去指定资源的请求，不过服务器不会传回资源的内容。好处在于不必传输内容，将获取到该资源的元数据\n\n * POST：向指定资源提交数据，请求服务器进行处理（例如提交表单或者上传文件）。数据被包含在请求文本中\n\n * PUT：向指定资源位置上传输最新内容\n\n * DELETE：请求服务器删除Request-URL所标识的资源\n\n * TRACE：回显服务器收到的请求，主要用于测试或诊断\n\n * OPTIONS：这个方法可使服务器传回该资源所支持的所有HTTP请求方法。用“*”来代表资源名称向Web服务器发送OPTIONS请求，可以测试服务器共能是否正常。\n\n * CONNECT：HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。通常用于SSL加密服务器的连接（经由非加密的HTTP代理服务器）。方法名称是区分大小写的。当某个请求所针对的资源不支持对应的请求方法的时候，服务器应当返回状态码405（Method Not Allowed），当服务器不认识或者不支持对应的请求方法的时候，应当返回状态码501（Not Implemented）。\n\n# 2 网页基础\n\n# 2.1 网页组成\n\n网页是由 HTML 、 CSS 、JavaScript 组成的。\n\n * HTML：F12开发者工具中的选项 Elements 中可以看到网页的源代码，这里展示的就是 HTML 代码。\n\n * CSS：在Style标签页中，显示的是当前选中的HTML代码标签的CSS层叠样式，\n\n * JavaScript：JavaScript 就厉害了，它在 HTML 代码中通常使用",normalizedContent:"# python 爬虫 -01 爬虫基础\n\n# 1 http\n\nhttp是一个客户端（用户）和服务器端（网站）之间进行请求和应答的标准。通过使用网页浏览器、网络爬虫或者其他工具，客户端可以向服务器上的指定端口（默认端口为80）发起一个http请求。客户端称为客户代理（user agent），应答服务器成为源服务器（origin server)。\n\nhttp假定其下层协议能够提供可靠的传输，因此，任何能够提供这种保证的协议都可以使用。使用tcp/ip协议族时rcp(remote procedure call，远程过程调用)作为传输层\n\n通常由http客户端发起一个请求，创建一个到服务器指定端口（默认是80端口）的tcp链接。http服务器则在该端口监听客户端的请求。一旦收到请求，服务器会向客户端返回一个状态（比如“thttp/1.1 200 ok”），以及请求的文件、错误信息等响应内容。\n\nhttp的请求方法\n\n * get：像指定资源发出“显示”请求，get方法应该只用于读取数据。get可能会被爬虫等随意访问\n\n * head：与get方法一样，都是向服务器发去指定资源的请求，不过服务器不会传回资源的内容。好处在于不必传输内容，将获取到该资源的元数据\n\n * post：向指定资源提交数据，请求服务器进行处理（例如提交表单或者上传文件）。数据被包含在请求文本中\n\n * put：向指定资源位置上传输最新内容\n\n * delete：请求服务器删除request-url所标识的资源\n\n * trace：回显服务器收到的请求，主要用于测试或诊断\n\n * options：这个方法可使服务器传回该资源所支持的所有http请求方法。用“*”来代表资源名称向web服务器发送options请求，可以测试服务器共能是否正常。\n\n * connect：http/1.1 协议中预留给能够将连接改为管道方式的代理服务器。通常用于ssl加密服务器的连接（经由非加密的http代理服务器）。方法名称是区分大小写的。当某个请求所针对的资源不支持对应的请求方法的时候，服务器应当返回状态码405（method not allowed），当服务器不认识或者不支持对应的请求方法的时候，应当返回状态码501（not implemented）。\n\n# 2 网页基础\n\n# 2.1 网页组成\n\n网页是由 html 、 css 、javascript 组成的。\n\n * html：f12开发者工具中的选项 elements 中可以看到网页的源代码，这里展示的就是 html 代码。\n\n * css：在style标签页中，显示的是当前选中的html代码标签的css层叠样式，\n\n * javascript：javascript 就厉害了，它在 html 代码中通常使用",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"Beautiful-soup4、Xpath、re",frontmatter:{title:"Beautiful-soup4、Xpath、re",date:"2021-03-09T21:21:23.000Z",permalink:"/pages/ad2d26/",categories:["技术文章","爬虫实践"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/11.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-%E7%88%AC%E8%99%AB%E5%AE%9E%E8%B7%B5/02.Beautiful-soup4%E3%80%81Xpath%E3%80%81re.html",relativePath:"02.学习笔记/11.系列笔记-爬虫实践/02.Beautiful-soup4、Xpath、re.md",key:"v-5154597e",path:"/pages/ad2d26/",headersStr:null,content:"# Python 爬虫 - 02 Beautiful soup4、Xpath、re\n\n# 1. Beautiful soup库\n\n# 1.1 Beautiful soup4 简介\n\n * Beautiful Soup 是一个HTML/XML 的解析器，主要用于解析和提取 HTML/XML 数据。\n * 它基于HTML DOM 的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多，所以性能要低于lxml。\n * BeautifulSoup 用来解析 HTML 比较简单，API非常人性化，支持CSS选择器、Python标准库中的HTML解析器，也支持 lxml 的 XML解析器。\n * BeautifulSoup4 简单容易比较上手，但其匹配效率还是远远不如正则以及xpath的，一般不推荐使用，推荐正则的使用。\n\n# 1.2 从零创建一个bs对象\n\n * pip install beautifulsoup4\n * from bs4 import BeautifulSoup\n * soup = BeautifulSoup(html, 'html.parser')\n\n# 1.3 BS 库的基本元素\n\n元素名               释义\nTag               标签，最基本的信息组织单元\nName              标签的名字\nAttributes        标签的属性，字典形式组织数据\nNavigableString   标签内非属性字符串\nComment           标签内字符串的注释部分\n\n# 导入bs4库\nfrom bs4 import BeautifulSoup\nimport requests # 抓取页面\n\nr = requests.get('https://python123.io/ws/demo.html') # Demo网址\ndemo = r.text  # 抓取的数据\nprint(demo)\n# 解析HTML页面\nsoup = BeautifulSoup(demo, 'html.parser')  # 抓取的页面数据；bs4的解析器\n# 有层次感的输出解析后的HTML页面\nprint(soup.prettify())\n\ndef test1():\n    print(soup.a)\n    print(soup.title)\n    print(soup.a.name)\n    print(soup.a.parent.name)\n    print(soup.p.parent.name)\n\n    tag = soup.a\n    print(tag.attrs)\n    print(tag.attrs['class'])\n    print(type(tag.attrs))\n\n    print(soup.a.string)\n    print(type(soup.a.string))\n    print(type(soup.p.string))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n * bs4库将任何HTML输入都变成utf‐8编码，解析无障碍\n\n# 1.4 基于 bs4 库的 HTML 内容遍历方法\n\n> HTML基本格式:<>…构成了所属关系，形成了标签的树形结构\n> \n>  * 标签树的下行遍历\n>    * .contents 子节点的列表，将``所有儿子节点存入列表\n>    * .children 子节点的迭代类型，与.contents类似，用于循环遍历儿子节点\n>    * .descendants 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历\n>  * 标签树的上行遍\n>    * .parent 节点的父亲标签\n>    * .parents 节点先辈标签的迭代类型，用于循环遍历先辈节点\n>  * 标签树的平行遍历\n>    * .next_sibling 返回按照HTML文本顺序的下一个平行节点标签\n>    * .previous_sibling 返回按照HTML文本顺序的上一个平行节点标签\n>    * .next_siblings 迭代类型，返回按照HTML文本顺序的后续所有平行节点标签\n>    * .previous_siblings 迭代类型，返回按照HTML文本顺序的前续所有平行节点标签\n\n# 标签的下行遍历\ndef test2():\n    print(soup.body.content)#返回标签树的body标签下的节点\n    print(soup.head)#返回head标签\n    for child in soup.body.children:#遍历儿子节点\n        print(child)\n    for child in soup.body.descendants:#遍历子孙节点\n        print(child)\n\n# 标签树的上行遍历\ndef test3():\n    print(soup.title.parent)\n    print(soup.parent)\n    for parent in soup.a.parents: # 遍历先辈的信息\n        if parent is None:\n            print(parent)\n        else:\n            print(parent.name)\n\n# 标签树的平行遍历\ndef test4():\n    print(soup.a.next_sibling)#a标签的下一个标签\n    print(soup.a.next_sibling.next_sibling)#a标签的下一个标签的下一个标签\n    print(soup.a.previous_sibling)#a标签的前一个标签\n    print(soup.a.previous_sibling.previous_sibling)#a标签的前一个标签的前一个标签\n    for sibling in soup.a.next_siblings:#遍历后续节点\n        print(sibling)\n    for sibling in soup.a.previous_sibling:#遍历之前的节点\n        print(sibling)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# 1.5 基于bs4 库的HTML 内容的查找方法\n\n>  * <>.find_all(name, attrs, recursive, string, **kwargs)\n>    * 参数：\n>    * ∙ name : 对标签名称的检索字符串\n>    * ∙ attrs: 对标签属性值的检索字符串，可标注属性检索\n>    * ∙ recursive: 是否对子孙全部检索，默认True\n>    * ∙ string: <>…</>中字符串区域的检索字符串\n>      * 简写：\n>      * (..) 等价于.find_all(..)\n>      * soup(..) 等价于 soup.find_all(..)\n>  * 扩展方法：\n>    * <>.find() 搜索且只返回一个结果，同.find_all()参数\n>    * <>.find_parents() 在先辈节点中搜索，返回列表类型，同.find_all()参数\n>    * <>.find_parent() 在先辈节点中返回一个结果，同.find()参数\n>    * <>.find_next_siblings() 在后续平行节点中搜索，返回列表类型，同.find_all()参数\n>    * <>.find_next_sibling() 在后续平行节点中返回一个结果，同.find()参数\n>    * <>.find_previous_siblings() 在前序平行节点中搜索，返回列表类型，同.find_all()参数\n>    * <>.find_previous_sibling() 在前序平行节点中返回一个结果，同.find()参数\n\n# 爬取中国最好大学排名\n\n\n1\n\n\n# 2. Xpath 解析\n\n# 2.1 Xpath 简介\n\n * XPath即为XML路径语言（XML Path Language），它是一种用来确定XML文档中某部分位置的语言。\n * 在XPath中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档（根）节点。\n * XML文档是被作为节点树来对待的。\n\n> XPath使用路径表达式在XML文档中选取节点。节点是通过沿着路径选取的。下面列出了最常用的路径表达式：\n> \n>  * nodename 选取此节点的所有子节点。\n>  * / 从根节点选取。\n>  * // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。\n>  * . 选取当前节点。\n>  * .. 选取当前节点的父节点。\n>  * @ 选取属性。\n>  * /text() 提取标签下面的文本内容\n>    * 如：\n>    * /标签名 逐层提取\n>    * /标签名 提取所有名为<>的标签\n>    * //标签名[@属性=“属性值”](vscode-resource://file///d%3A/From Github/team-learning-master/Python爬虫编程实践/task2/) 提取包含属性为属性值的标签\n>    * @属性名 代表取某个属性名的属性值\n>  * 详细学习：https://www.cnblogs.com/gaojun/archive/2012/08/11/2633908.html\n\n# 2.2 使用 lxml 解析（尚未完成）\n\n\n\n\n1\n\n\n# 3. Re 库解析(尚未完成)",normalizedContent:"# python 爬虫 - 02 beautiful soup4、xpath、re\n\n# 1. beautiful soup库\n\n# 1.1 beautiful soup4 简介\n\n * beautiful soup 是一个html/xml 的解析器，主要用于解析和提取 html/xml 数据。\n * 它基于html dom 的，会载入整个文档，解析整个dom树，因此时间和内存开销都会大很多，所以性能要低于lxml。\n * beautifulsoup 用来解析 html 比较简单，api非常人性化，支持css选择器、python标准库中的html解析器，也支持 lxml 的 xml解析器。\n * beautifulsoup4 简单容易比较上手，但其匹配效率还是远远不如正则以及xpath的，一般不推荐使用，推荐正则的使用。\n\n# 1.2 从零创建一个bs对象\n\n * pip install beautifulsoup4\n * from bs4 import beautifulsoup\n * soup = beautifulsoup(html, 'html.parser')\n\n# 1.3 bs 库的基本元素\n\n元素名               释义\ntag               标签，最基本的信息组织单元\nname              标签的名字\nattributes        标签的属性，字典形式组织数据\nnavigablestring   标签内非属性字符串\ncomment           标签内字符串的注释部分\n\n# 导入bs4库\nfrom bs4 import beautifulsoup\nimport requests # 抓取页面\n\nr = requests.get('https://python123.io/ws/demo.html') # demo网址\ndemo = r.text  # 抓取的数据\nprint(demo)\n# 解析html页面\nsoup = beautifulsoup(demo, 'html.parser')  # 抓取的页面数据；bs4的解析器\n# 有层次感的输出解析后的html页面\nprint(soup.prettify())\n\ndef test1():\n    print(soup.a)\n    print(soup.title)\n    print(soup.a.name)\n    print(soup.a.parent.name)\n    print(soup.p.parent.name)\n\n    tag = soup.a\n    print(tag.attrs)\n    print(tag.attrs['class'])\n    print(type(tag.attrs))\n\n    print(soup.a.string)\n    print(type(soup.a.string))\n    print(type(soup.p.string))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n * bs4库将任何html输入都变成utf‐8编码，解析无障碍\n\n# 1.4 基于 bs4 库的 html 内容遍历方法\n\n> html基本格式:<>…构成了所属关系，形成了标签的树形结构\n> \n>  * 标签树的下行遍历\n>    * .contents 子节点的列表，将``所有儿子节点存入列表\n>    * .children 子节点的迭代类型，与.contents类似，用于循环遍历儿子节点\n>    * .descendants 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历\n>  * 标签树的上行遍\n>    * .parent 节点的父亲标签\n>    * .parents 节点先辈标签的迭代类型，用于循环遍历先辈节点\n>  * 标签树的平行遍历\n>    * .next_sibling 返回按照html文本顺序的下一个平行节点标签\n>    * .previous_sibling 返回按照html文本顺序的上一个平行节点标签\n>    * .next_siblings 迭代类型，返回按照html文本顺序的后续所有平行节点标签\n>    * .previous_siblings 迭代类型，返回按照html文本顺序的前续所有平行节点标签\n\n# 标签的下行遍历\ndef test2():\n    print(soup.body.content)#返回标签树的body标签下的节点\n    print(soup.head)#返回head标签\n    for child in soup.body.children:#遍历儿子节点\n        print(child)\n    for child in soup.body.descendants:#遍历子孙节点\n        print(child)\n\n# 标签树的上行遍历\ndef test3():\n    print(soup.title.parent)\n    print(soup.parent)\n    for parent in soup.a.parents: # 遍历先辈的信息\n        if parent is none:\n            print(parent)\n        else:\n            print(parent.name)\n\n# 标签树的平行遍历\ndef test4():\n    print(soup.a.next_sibling)#a标签的下一个标签\n    print(soup.a.next_sibling.next_sibling)#a标签的下一个标签的下一个标签\n    print(soup.a.previous_sibling)#a标签的前一个标签\n    print(soup.a.previous_sibling.previous_sibling)#a标签的前一个标签的前一个标签\n    for sibling in soup.a.next_siblings:#遍历后续节点\n        print(sibling)\n    for sibling in soup.a.previous_sibling:#遍历之前的节点\n        print(sibling)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n# 1.5 基于bs4 库的html 内容的查找方法\n\n>  * <>.find_all(name, attrs, recursive, string, **kwargs)\n>    * 参数：\n>    * ∙ name : 对标签名称的检索字符串\n>    * ∙ attrs: 对标签属性值的检索字符串，可标注属性检索\n>    * ∙ recursive: 是否对子孙全部检索，默认true\n>    * ∙ string: <>…</>中字符串区域的检索字符串\n>      * 简写：\n>      * (..) 等价于.find_all(..)\n>      * soup(..) 等价于 soup.find_all(..)\n>  * 扩展方法：\n>    * <>.find() 搜索且只返回一个结果，同.find_all()参数\n>    * <>.find_parents() 在先辈节点中搜索，返回列表类型，同.find_all()参数\n>    * <>.find_parent() 在先辈节点中返回一个结果，同.find()参数\n>    * <>.find_next_siblings() 在后续平行节点中搜索，返回列表类型，同.find_all()参数\n>    * <>.find_next_sibling() 在后续平行节点中返回一个结果，同.find()参数\n>    * <>.find_previous_siblings() 在前序平行节点中搜索，返回列表类型，同.find_all()参数\n>    * <>.find_previous_sibling() 在前序平行节点中返回一个结果，同.find()参数\n\n# 爬取中国最好大学排名\n\n\n1\n\n\n# 2. xpath 解析\n\n# 2.1 xpath 简介\n\n * xpath即为xml路径语言（xml path language），它是一种用来确定xml文档中某部分位置的语言。\n * 在xpath中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档（根）节点。\n * xml文档是被作为节点树来对待的。\n\n> xpath使用路径表达式在xml文档中选取节点。节点是通过沿着路径选取的。下面列出了最常用的路径表达式：\n> \n>  * nodename 选取此节点的所有子节点。\n>  * / 从根节点选取。\n>  * // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。\n>  * . 选取当前节点。\n>  * .. 选取当前节点的父节点。\n>  * @ 选取属性。\n>  * /text() 提取标签下面的文本内容\n>    * 如：\n>    * /标签名 逐层提取\n>    * /标签名 提取所有名为<>的标签\n>    * //标签名[@属性=“属性值”](vscode-resource://file///d%3a/from github/team-learning-master/python爬虫编程实践/task2/) 提取包含属性为属性值的标签\n>    * @属性名 代表取某个属性名的属性值\n>  * 详细学习：https://www.cnblogs.com/gaojun/archive/2012/08/11/2633908.html\n\n# 2.2 使用 lxml 解析（尚未完成）\n\n\n\n\n1\n\n\n# 3. re 库解析(尚未完成)",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"session和cookie、代理、selenium自动化",frontmatter:{title:"session和cookie、代理、selenium自动化",date:"2021-03-09T21:21:36.000Z",permalink:"/pages/a36ee1/",categories:["技术文章","爬虫实践"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/11.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-%E7%88%AC%E8%99%AB%E5%AE%9E%E8%B7%B5/03.session%E5%92%8Ccookie%E3%80%81%E4%BB%A3%E7%90%86%E3%80%81selenium%E8%87%AA%E5%8A%A8%E5%8C%96.html",relativePath:"02.学习笔记/11.系列笔记-爬虫实践/03.session和cookie、代理、selenium自动化.md",key:"v-78fd300e",path:"/pages/a36ee1/",headers:[{level:3,title:"1. 动态网页和静态网页",slug:"_1-动态网页和静态网页",normalizedTitle:"1. 动态网页和静态网页",charIndex:51},{level:3,title:"2. session和cookies",slug:"_2-session和cookies",normalizedTitle:"2. session和cookies",charIndex:686}],headersStr:"1. 动态网页和静态网页 2. session和cookies",content:"# Python 爬虫 - 03 session和cookie、代理、selenium自动化\n\n\n# 1. 动态网页和静态网页\n\n# 1.1 静态网页\n\n就是我们上一篇写的那种 html 页面，后缀为 .html 的这种文件，直接部署到或者是放到某个 web 容器上，就可以在浏览器通过链接直接访问到了，常用的 web 容器有 Nginx 、 Apache 、 Tomcat 、Weblogic 、 Jboss 、 Resin 等。\n\n# 1.2 动态网页\n\n动态网页可以解析 URL 中的参数，或者是关联数据库中的数据，显示不同的网页内容。现在各位同学访问的网站大多数都是动态网站，它们不再简简单单是由 HTML 堆砌而成，可能是由 JSP 、 PHP 等语言编写的，当然，现在很多由前端框架编写而成的网页小编这里也归属为动态网页。\n\n# 1.3 http1.0\n\n * HTTP1.0的特点是无状态无链接的\n\n * 无状态就是指 HTTP 协议对于请求的发送处理是没有记忆功能的，也就是说每次 HTTP 请求到达服务端，服务端都不知道当前的客户端（浏览器）到底是一个什么状态。客户端向服务端发送请求后，服务端处理这个请求，然后将内容响应回客户端，完成一次交互，这个过程是完全相互独立的，服务端不会记录前后的状态变化，也就是缺少状态记录。这就产生了上面的问题，服务端如何知道当前在浏览器面前操作的这个人是谁？其实，在用户做登录操作的时候，服务端会下发一个类似于 token 凭证的东西返回至客户端（浏览器），有了这个凭证，才能保持登录状态。那么这个凭证是什么？\n\n\n# 2. session和cookies\n\n * Session 是会话的意思，会话是产生在服务端的，用来保存当前用户的会话信息\n\n * Cookies 是保存在客户端（浏览器），有了 Cookie 以后，客户端（浏览器）再次访问服务端的时候，会将这个 Cookie 带上，这时，服务端可以通过 Cookie 来识别本次请求到底是谁在访问。\n\n * 在爬虫中，有时候遇到需要登录才能访问的网页，只需要在登录后获取了 Cookies ，在下次访问的时候将登录后获取到的 Cookies 放在请求头中，这时，服务端就会认为我们的爬虫是一个正常登录用户。\n\n# 2.1 session\n\n在客户端（浏览器）第一次请求服务端的时候，服务端会返回一个请求头中带有 Set-Cookie 字段的响应给客户端（浏览器），用来标记是哪一个用户，客户端（浏览器）会把这个 Cookies 给保存起来。\n\n下图是 PostMan 访问京东登录页后的返回头\n\n\n\n当我们输入好用户名和密码时，客户端会将这个 Cookies 放在请求头一起发送给服务端，这时，服务端就知道是谁在进行登录操作，并且可以判断这个人输入的用户名和密码对不对，如果输入正确，则在服务端的 Session 记录一下这个人已经登录成功了，下次再请求的时候这个人就是登录状态了。\n\n如果客户端传给服务端的 Cookies 是无效的，或者这个 Cookies 根本不是由这个服务端下发的，或者这个 Cookies 已经过期了，那么接下里的请求将不再能访问需要登录后才能访问的页面。\n\n所以， Session 和 Cookies 之间是需要相互配合的，一个在服务端，一个在客户端。\n\n# 2.2 cookie\n\n我们还是打开某东的网站，看下这些 Cookies到底有哪些内容：\n\n\n\n具体操作方式还是在 Chrome 中按 F12 打开开发者工具，选择 Application 标签，点开 Cookies 这一栏。\n\n * Name：这个是 Cookie 的名字。一旦创建，该名称便不可更改。\n * Value：这个是 Cookie 的值。\n * Domain：这个是可以访问该 Cookie 的域名。例如，如果设置为 .jd.com ，则所有以 jd.com ，结尾的域名都可以访问该Cookie。\n * Max Age：Cookie 失效的时间，单位为秒，也常和 Expires 一起使用。 Max Age 如果为正数，则在 Max Age 秒之后失效，如果为负数，则关闭浏览器时 Cookie 即失效，浏览器也不会保存该 Cookie 。\n * Path：Cookie 的使用路径。如果设置为 /path/ ，则只有路径为 /path/ 的页面可以访问该 Cookie 。如果设置为 / ，则本域名下的所有页面都可以访问该 Cookie 。\n * Size：Cookie 的大小。\n * HTTPOnly：如果此项打勾，那么通过 JS 脚本将无法读取到 Cookie 信息，这样能有效的防止 XSS 攻击，窃取 Cookie 内容，可以增加 Cookie 的安全性。\n * Secure：如果此项打勾，那么这个 Cookie 只能用 HTTPS 协议发送给服务器，用 HTTP 协议是不发送的。\n\n那么有的网站为什么这次关闭了，下次打开的时候还是登录状态呢？\n\n这就要说到 Cookie 的持久化了，其实也不能说是持久化，就是 Cookie 失效的时间设置的长一点，比如直接设置到 2099 年失效，这样，在浏览器关闭后，这个 Cookie 是会保存在我们的硬盘中的，下次打开浏览器，会再从我们的硬盘中将这个 Cookie 读取出来，用来维持用户的会话状态。\n\n第二个问题产生了，服务端的会话也会无限的维持下去么，当然不会，这就要在 Cookie 和 Session 上做文章了， Cookie 中可以使用加密的方式将用户名记录下来，在下次将 Cookies 读取出来由请求发送到服务端后，服务端悄悄的自己创建一个用户已经登录的会话，这样我们在客户端看起来就好像这个登录会话是一直保持的。\n\n# 2.3 一个重要概念\n\n * 当我们关闭浏览器的时候会自动销毁服务端的会话，这个是错误的，因为在关闭浏览器的时候，浏览器并不会额外的通知服务端说，我要关闭了，你把和我的会话销毁掉吧。\n   \n   因为服务端的会话是保存在内存中的，虽然一个会话不会很大，但是架不住会话多啊，硬件毕竟是会有限制的，不能无限扩充下去的，所以在服务端设置会话的过期时间就非常有必要。\n   \n   当然，有没有方式能让浏览器在关闭的时候同步的关闭服务端的会话，当然是可以的，我们可以通过脚本语言 JS 来监听浏览器关闭的动作，当浏览器触发关闭动作的时候，由 JS 像服务端发起一个请求来通知服务端销毁会话。\n   \n   由于不同的浏览器对 JS 事件的实现机制不一致，不一定保证 JS 能监听到浏览器关闭的动作，所以现在常用的方式还是在服务端自己设置会话的过期时间",normalizedContent:"# python 爬虫 - 03 session和cookie、代理、selenium自动化\n\n\n# 1. 动态网页和静态网页\n\n# 1.1 静态网页\n\n就是我们上一篇写的那种 html 页面，后缀为 .html 的这种文件，直接部署到或者是放到某个 web 容器上，就可以在浏览器通过链接直接访问到了，常用的 web 容器有 nginx 、 apache 、 tomcat 、weblogic 、 jboss 、 resin 等。\n\n# 1.2 动态网页\n\n动态网页可以解析 url 中的参数，或者是关联数据库中的数据，显示不同的网页内容。现在各位同学访问的网站大多数都是动态网站，它们不再简简单单是由 html 堆砌而成，可能是由 jsp 、 php 等语言编写的，当然，现在很多由前端框架编写而成的网页小编这里也归属为动态网页。\n\n# 1.3 http1.0\n\n * http1.0的特点是无状态无链接的\n\n * 无状态就是指 http 协议对于请求的发送处理是没有记忆功能的，也就是说每次 http 请求到达服务端，服务端都不知道当前的客户端（浏览器）到底是一个什么状态。客户端向服务端发送请求后，服务端处理这个请求，然后将内容响应回客户端，完成一次交互，这个过程是完全相互独立的，服务端不会记录前后的状态变化，也就是缺少状态记录。这就产生了上面的问题，服务端如何知道当前在浏览器面前操作的这个人是谁？其实，在用户做登录操作的时候，服务端会下发一个类似于 token 凭证的东西返回至客户端（浏览器），有了这个凭证，才能保持登录状态。那么这个凭证是什么？\n\n\n# 2. session和cookies\n\n * session 是会话的意思，会话是产生在服务端的，用来保存当前用户的会话信息\n\n * cookies 是保存在客户端（浏览器），有了 cookie 以后，客户端（浏览器）再次访问服务端的时候，会将这个 cookie 带上，这时，服务端可以通过 cookie 来识别本次请求到底是谁在访问。\n\n * 在爬虫中，有时候遇到需要登录才能访问的网页，只需要在登录后获取了 cookies ，在下次访问的时候将登录后获取到的 cookies 放在请求头中，这时，服务端就会认为我们的爬虫是一个正常登录用户。\n\n# 2.1 session\n\n在客户端（浏览器）第一次请求服务端的时候，服务端会返回一个请求头中带有 set-cookie 字段的响应给客户端（浏览器），用来标记是哪一个用户，客户端（浏览器）会把这个 cookies 给保存起来。\n\n下图是 postman 访问京东登录页后的返回头\n\n\n\n当我们输入好用户名和密码时，客户端会将这个 cookies 放在请求头一起发送给服务端，这时，服务端就知道是谁在进行登录操作，并且可以判断这个人输入的用户名和密码对不对，如果输入正确，则在服务端的 session 记录一下这个人已经登录成功了，下次再请求的时候这个人就是登录状态了。\n\n如果客户端传给服务端的 cookies 是无效的，或者这个 cookies 根本不是由这个服务端下发的，或者这个 cookies 已经过期了，那么接下里的请求将不再能访问需要登录后才能访问的页面。\n\n所以， session 和 cookies 之间是需要相互配合的，一个在服务端，一个在客户端。\n\n# 2.2 cookie\n\n我们还是打开某东的网站，看下这些 cookies到底有哪些内容：\n\n\n\n具体操作方式还是在 chrome 中按 f12 打开开发者工具，选择 application 标签，点开 cookies 这一栏。\n\n * name：这个是 cookie 的名字。一旦创建，该名称便不可更改。\n * value：这个是 cookie 的值。\n * domain：这个是可以访问该 cookie 的域名。例如，如果设置为 .jd.com ，则所有以 jd.com ，结尾的域名都可以访问该cookie。\n * max age：cookie 失效的时间，单位为秒，也常和 expires 一起使用。 max age 如果为正数，则在 max age 秒之后失效，如果为负数，则关闭浏览器时 cookie 即失效，浏览器也不会保存该 cookie 。\n * path：cookie 的使用路径。如果设置为 /path/ ，则只有路径为 /path/ 的页面可以访问该 cookie 。如果设置为 / ，则本域名下的所有页面都可以访问该 cookie 。\n * size：cookie 的大小。\n * httponly：如果此项打勾，那么通过 js 脚本将无法读取到 cookie 信息，这样能有效的防止 xss 攻击，窃取 cookie 内容，可以增加 cookie 的安全性。\n * secure：如果此项打勾，那么这个 cookie 只能用 https 协议发送给服务器，用 http 协议是不发送的。\n\n那么有的网站为什么这次关闭了，下次打开的时候还是登录状态呢？\n\n这就要说到 cookie 的持久化了，其实也不能说是持久化，就是 cookie 失效的时间设置的长一点，比如直接设置到 2099 年失效，这样，在浏览器关闭后，这个 cookie 是会保存在我们的硬盘中的，下次打开浏览器，会再从我们的硬盘中将这个 cookie 读取出来，用来维持用户的会话状态。\n\n第二个问题产生了，服务端的会话也会无限的维持下去么，当然不会，这就要在 cookie 和 session 上做文章了， cookie 中可以使用加密的方式将用户名记录下来，在下次将 cookies 读取出来由请求发送到服务端后，服务端悄悄的自己创建一个用户已经登录的会话，这样我们在客户端看起来就好像这个登录会话是一直保持的。\n\n# 2.3 一个重要概念\n\n * 当我们关闭浏览器的时候会自动销毁服务端的会话，这个是错误的，因为在关闭浏览器的时候，浏览器并不会额外的通知服务端说，我要关闭了，你把和我的会话销毁掉吧。\n   \n   因为服务端的会话是保存在内存中的，虽然一个会话不会很大，但是架不住会话多啊，硬件毕竟是会有限制的，不能无限扩充下去的，所以在服务端设置会话的过期时间就非常有必要。\n   \n   当然，有没有方式能让浏览器在关闭的时候同步的关闭服务端的会话，当然是可以的，我们可以通过脚本语言 js 来监听浏览器关闭的动作，当浏览器触发关闭动作的时候，由 js 像服务端发起一个请求来通知服务端销毁会话。\n   \n   由于不同的浏览器对 js 事件的实现机制不一致，不一定保证 js 能监听到浏览器关闭的动作，所以现在常用的方式还是在服务端自己设置会话的过期时间",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"Django学习笔记（一）--简单入门",frontmatter:{title:"Django学习笔记（一）--简单入门",date:"2021-03-09T21:25:07.000Z",permalink:"/pages/597550/",categories:["技术文章","Django学习笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/12.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Django%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/01.Django%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89--%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8.html",relativePath:"02.学习笔记/12.系列笔记-Django学习笔记/01.Django学习笔记（一）--简单入门.md",key:"v-1fb62b40",path:"/pages/597550/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/08/17, 18:07:06"},{title:"安装云环境的mysql",frontmatter:{title:"安装云环境的mysql",date:"2021-06-03T12:45:20.000Z",permalink:"/pages/359c09/",categories:["技术文章","Django学习笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/12.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Django%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/00.%E5%AE%89%E8%A3%85%E4%BA%91%E7%8E%AF%E5%A2%83%E7%9A%84mysql.html",relativePath:"02.学习笔记/12.系列笔记-Django学习笔记/00.安装云环境的mysql.md",key:"v-52e3e8cf",path:"/pages/359c09/",headers:[{level:3,title:"在Ubuntu 18.04上卸载MariaDB",slug:"在ubuntu-18-04上卸载mariadb",normalizedTitle:"在ubuntu 18.04上卸载mariadb",charIndex:2},{level:3,title:"在Ubuntu 18.04上安装MariaDB",slug:"在ubuntu-18-04上安装mariadb",normalizedTitle:"在ubuntu 18.04上安装mariadb",charIndex:167},{level:3,title:"设置密码",slug:"设置密码",normalizedTitle:"设置密码",charIndex:390},{level:3,title:"使用命令行连接 MariaDB",slug:"使用命令行连接-mariadb",normalizedTitle:"使用命令行连接 mariadb",charIndex:1235},{level:3,title:"设置 MySQL 远程访问权限",slug:"设置-mysql-远程访问权限",normalizedTitle:"设置 mysql 远程访问权限",charIndex:1371},{level:3,title:"查看监听端口",slug:"查看监听端口",normalizedTitle:"查看监听端口",charIndex:1593},{level:3,title:"关闭系统防火墙（可选）",slug:"关闭系统防火墙-可选",normalizedTitle:"关闭系统防火墙（可选）",charIndex:1680},{level:3,title:"华为云安全组放行",slug:"华为云安全组放行",normalizedTitle:"华为云安全组放行",charIndex:1776},{level:3,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:1789}],headersStr:"在Ubuntu 18.04上卸载MariaDB 在Ubuntu 18.04上安装MariaDB 设置密码 使用命令行连接 MariaDB 设置 MySQL 远程访问权限 查看监听端口 关闭系统防火墙（可选） 华为云安全组放行 参考资料",content:"# 在Ubuntu 18.04上卸载MariaDB\n\n#\nsudo apt-get remove mysql-\\*\n\n#\nsudo apt-get purge mariadb-*\n\ndpkg -l |grep ^rc|awk '{print $2}' |sudo xargs dpkg -P\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 在Ubuntu 18.04上安装MariaDB\n\n# 安装 MariaDB\nsudo apt update\nsudo apt install mariadb-server\n\n# 验证MariaDB服务是否自动启动\nsudo systemctl status mariadb\n\nservice mysqld status\n\n# 检查MariaDB版本\nmysql -V\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 设置密码\n\n\n# 禁用 root 用户的插件身份验证\nuse mysql;\nupdate user set plugin='' where User='root';\nflush privileges;\nexit\n# 重新启动 MariaDB\nsudo systemctl restart mariadb.service \n\n# 重启 MariaDB 如果出现 Failed to allocate directory watch: Too many open files\nsudo vim /etc/sysctl.conf\nfs.inotify.max_user_instances=512\nfs.inotify.max_user_watches=262144\nsudo sysctl -p\n\n# 运行 mysql_secure_installation 以提升 MariaDB 的安全性\nsudo mysql_secure_installation\n\nEnter current password for root (enter for none): 直接敲回车\nSet root password? [Y/n]: Y\nNew password: 输入你的密码\nRe-enter new password: 重复你输入的密码\nRemove anonymous users? [Y/n]: Y\nDisallow root login remotely? [Y/n]: Y\nRemove test database and access to it? [Y/n]:  Y\nReload privilege tables now? [Y/n]:  Y\n\n# 使用密码身份验证登录\nsudo mysql -u root -p\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 使用命令行连接 MariaDB\n\n# 连接 mysql\nmysql -u root -p \n\n# 显示所有数据库\nshow databases;\n\n# 显示数据库中的数据表\nuse mysql;\nshow tables;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 设置 MySQL 远程访问权限\n\n# 设置 MySQL 远程访问权限\nupdate user set host =\"%\" where user =\"root\";\nflush privileges;\n\n# 查看 root 当前的 host 属性\nselect host from user where user = 'root';\n\nselect user,host from mysql.user\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 查看监听端口\n\nnetstat -an|grep 3306\n\n# 开启 skip-name-resolve\nskip-name-resolve\n\n\n1\n2\n3\n4\n\n\n\n# 关闭系统防火墙（可选）\n\n# 查看防火墙当前状态\nsudo ufw status\n\n# 关闭防火墙\nsudo ufw disable\n\n\n1\n2\n3\n4\n5\n\n\n检查iptables\n\n\n# 华为云安全组放行\n\n\n# 参考资料\n\n * https://my.oschina.net/u/4381258/blog/4327780",normalizedContent:"# 在ubuntu 18.04上卸载mariadb\n\n#\nsudo apt-get remove mysql-\\*\n\n#\nsudo apt-get purge mariadb-*\n\ndpkg -l |grep ^rc|awk '{print $2}' |sudo xargs dpkg -p\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 在ubuntu 18.04上安装mariadb\n\n# 安装 mariadb\nsudo apt update\nsudo apt install mariadb-server\n\n# 验证mariadb服务是否自动启动\nsudo systemctl status mariadb\n\nservice mysqld status\n\n# 检查mariadb版本\nmysql -v\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n# 设置密码\n\n\n# 禁用 root 用户的插件身份验证\nuse mysql;\nupdate user set plugin='' where user='root';\nflush privileges;\nexit\n# 重新启动 mariadb\nsudo systemctl restart mariadb.service \n\n# 重启 mariadb 如果出现 failed to allocate directory watch: too many open files\nsudo vim /etc/sysctl.conf\nfs.inotify.max_user_instances=512\nfs.inotify.max_user_watches=262144\nsudo sysctl -p\n\n# 运行 mysql_secure_installation 以提升 mariadb 的安全性\nsudo mysql_secure_installation\n\nenter current password for root (enter for none): 直接敲回车\nset root password? [y/n]: y\nnew password: 输入你的密码\nre-enter new password: 重复你输入的密码\nremove anonymous users? [y/n]: y\ndisallow root login remotely? [y/n]: y\nremove test database and access to it? [y/n]:  y\nreload privilege tables now? [y/n]:  y\n\n# 使用密码身份验证登录\nsudo mysql -u root -p\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n# 使用命令行连接 mariadb\n\n# 连接 mysql\nmysql -u root -p \n\n# 显示所有数据库\nshow databases;\n\n# 显示数据库中的数据表\nuse mysql;\nshow tables;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 设置 mysql 远程访问权限\n\n# 设置 mysql 远程访问权限\nupdate user set host =\"%\" where user =\"root\";\nflush privileges;\n\n# 查看 root 当前的 host 属性\nselect host from user where user = 'root';\n\nselect user,host from mysql.user\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n# 查看监听端口\n\nnetstat -an|grep 3306\n\n# 开启 skip-name-resolve\nskip-name-resolve\n\n\n1\n2\n3\n4\n\n\n\n# 关闭系统防火墙（可选）\n\n# 查看防火墙当前状态\nsudo ufw status\n\n# 关闭防火墙\nsudo ufw disable\n\n\n1\n2\n3\n4\n5\n\n\n检查iptables\n\n\n# 华为云安全组放行\n\n\n# 参考资料\n\n * https://my.oschina.net/u/4381258/blog/4327780",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"常见错误",frontmatter:{title:"常见错误",date:"2021-05-07T16:48:21.000Z",permalink:"/pages/3dcbbc/",categories:["技术文章","Git 使用笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Git%20%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/00.%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF.html",relativePath:"02.学习笔记/13.系列笔记-Git 使用笔记/00.常见错误.md",key:"v-16aa8d4a",path:"/pages/3dcbbc/",headersStr:null,content:"0、Git的奇技淫巧\n\n * Git常用命令集合\n\n1、Please commit your changes or stash them before you merge.\n\nUpdating d419eb2..3525a9c\nerror: Your local changes to the following files would be overwritten by merge:\n\tdocs/xxx.md\nPlease commit your changes or stash them before you merge.\n\n\n1\n2\n3\n4\n\n\n解决方案：按照提示来，使用git stash 指令\n\ngit stash\ngit pull\ngit stash pop\n\n\n1\n2\n3\n\n * git stash:来备份当前工作区内容， 从最近的一次提交中读取相关内容，让工作区保证与上次提交的内容一致，同 时将当前的工作区内容保存到 Git 栈上\n * git stash pop: 从 Git 栈中读取最近一次保存的内容，恢复工作区的相关内容\n\n参考资料\n\n * https://wkevin.github.io/GitChat/gitchat.html",normalizedContent:"0、git的奇技淫巧\n\n * git常用命令集合\n\n1、please commit your changes or stash them before you merge.\n\nupdating d419eb2..3525a9c\nerror: your local changes to the following files would be overwritten by merge:\n\tdocs/xxx.md\nplease commit your changes or stash them before you merge.\n\n\n1\n2\n3\n4\n\n\n解决方案：按照提示来，使用git stash 指令\n\ngit stash\ngit pull\ngit stash pop\n\n\n1\n2\n3\n\n * git stash:来备份当前工作区内容， 从最近的一次提交中读取相关内容，让工作区保证与上次提交的内容一致，同 时将当前的工作区内容保存到 git 栈上\n * git stash pop: 从 git 栈中读取最近一次保存的内容，恢复工作区的相关内容\n\n参考资料\n\n * https://wkevin.github.io/gitchat/gitchat.html",charsets:{cjk:!0},lastUpdated:"2021/09/12, 20:42:58"},{title:"Django学习笔记（二）-- 连接MySQL数据库的小应用",frontmatter:{title:"Django学习笔记（二）-- 连接MySQL数据库的小应用",date:"2021-03-09T21:25:51.000Z",permalink:"/pages/fc23c1/",categories:["技术文章","Django学习笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/12.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Django%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.Django%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89--%20%E8%BF%9E%E6%8E%A5MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%B0%8F%E5%BA%94%E7%94%A8.html",relativePath:"02.学习笔记/12.系列笔记-Django学习笔记/02.Django学习笔记（二）-- 连接MySQL数据库的小应用.md",key:"v-2cb0147f",path:"/pages/fc23c1/",headers:[{level:2,title:"1. 设计一个表单接收用户发送的数据",slug:"_1-设计一个表单接收用户发送的数据",normalizedTitle:"1. 设计一个表单接收用户发送的数据",charIndex:20},{level:2,title:"2. 返回动态页面",slug:"_2-返回动态页面",normalizedTitle:"2. 返回动态页面",charIndex:637},{level:2,title:"3. 使用数据库",slug:"_3-使用数据库",normalizedTitle:"3. 使用数据库",charIndex:715}],headersStr:"1. 设计一个表单接收用户发送的数据 2. 返回动态页面 3. 使用数据库",content:'# Django学习笔记（二）\n\n\n# 1. 设计一个表单接收用户发送的数据\n\n我们在html文件里写好一个表单用来搜集数据，然后运行时发现报错，所以我们加上了如下一行代码\n\n{% csrf_token %} #用来解决跨域问题\n\n\n1\n\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>首页</title>\n</head>\n<body>\n    <h1>用户输入</h1>\n    <form action="/index/" method="POST">\n        {% csrf_token %}\n        用户名：<input type="text" name="username" /><br/>\n        密码： <input type="password" name="password" /><br/>\n        <input type="submit" value="提交" />\n    </form>\n\n    <h1 style="background-color: antiquewhite;color:black">Hello World!</h1>\n</body>\n</html>\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n\n\n\n\n# 2. 返回动态页面\n\n我们将每次用户输入的username和password都写到一个列表中，并进行返回.然后修改HTML代码进行展示\n\n\n\n\n\n\n\n\n# 3. 使用数据库\n\n我们先注册App，让数据库知道是为哪个App所建立的数据库，然后我们这里不使用自带的sqlite3,我们转用Mysql，所以我们将setting里的DATABATES做相应的改动，并且在创建的app下的migrations文件夹下的init.py中加入以下两行代码\n\nimport pymysql\npymysql.install_as_MySQLdb()\n\n\n1\n2\n\n\n然后在models中创建两个字段，在命令行中使用下面两条命令就可以自动创建\n\npython manage.py makemigrations\npython manage.py migrate\n\n\n1\n2\n\n\n输入之后会在migrations目录生成一个0001_initial.py的迁移记录文件\n\n然后执行\n\npython manage.py makemigrations \npython manage.py migrate\n\n\n1\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n最后我们能够在数据库中看到结果就不用每次开启服务时都需要重新记录了。\n\n',normalizedContent:'# django学习笔记（二）\n\n\n# 1. 设计一个表单接收用户发送的数据\n\n我们在html文件里写好一个表单用来搜集数据，然后运行时发现报错，所以我们加上了如下一行代码\n\n{% csrf_token %} #用来解决跨域问题\n\n\n1\n\n\n<!doctype html>\n<html lang="en">\n<head>\n    <meta charset="utf-8">\n    <title>首页</title>\n</head>\n<body>\n    <h1>用户输入</h1>\n    <form action="/index/" method="post">\n        {% csrf_token %}\n        用户名：<input type="text" name="username" /><br/>\n        密码： <input type="password" name="password" /><br/>\n        <input type="submit" value="提交" />\n    </form>\n\n    <h1 style="background-color: antiquewhite;color:black">hello world!</h1>\n</body>\n</html>\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n\n\n\n\n# 2. 返回动态页面\n\n我们将每次用户输入的username和password都写到一个列表中，并进行返回.然后修改html代码进行展示\n\n\n\n\n\n\n\n\n# 3. 使用数据库\n\n我们先注册app，让数据库知道是为哪个app所建立的数据库，然后我们这里不使用自带的sqlite3,我们转用mysql，所以我们将setting里的databates做相应的改动，并且在创建的app下的migrations文件夹下的init.py中加入以下两行代码\n\nimport pymysql\npymysql.install_as_mysqldb()\n\n\n1\n2\n\n\n然后在models中创建两个字段，在命令行中使用下面两条命令就可以自动创建\n\npython manage.py makemigrations\npython manage.py migrate\n\n\n1\n2\n\n\n输入之后会在migrations目录生成一个0001_initial.py的迁移记录文件\n\n然后执行\n\npython manage.py makemigrations \npython manage.py migrate\n\n\n1\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n最后我们能够在数据库中看到结果就不用每次开启服务时都需要重新记录了。\n\n',charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"本地分支的新建",frontmatter:{title:"本地分支的新建",date:"2021-05-17T22:29:26.000Z",permalink:"/pages/c9c01b/",categories:["技术文章","Git 使用笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Git%20%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/01.%E6%9C%AC%E5%9C%B0%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA.html",relativePath:"02.学习笔记/13.系列笔记-Git 使用笔记/01.本地分支的新建.md",key:"v-5388bbd0",path:"/pages/c9c01b/",headers:[{level:2,title:"本地分支的新建",slug:"本地分支的新建",normalizedTitle:"本地分支的新建",charIndex:2}],headersStr:"本地分支的新建",content:"# 本地分支的新建\n\n1、新建分支\n\ngit branch develop\n\n\n1\n\n\n2、切换到 develop 分支\n\ngit checkout develop\n\n\n1\n\n\n3、查看当前分支及所有分支\n\ngit branch\n\n\n1\n\n\n4、新建并切换到 develop 分支\n\ngit checkout -b develop\n\n\n1\n\n\n# 参考资料\n\n * 3.2 Git 分支 - 分支的新建与合并",normalizedContent:"# 本地分支的新建\n\n1、新建分支\n\ngit branch develop\n\n\n1\n\n\n2、切换到 develop 分支\n\ngit checkout develop\n\n\n1\n\n\n3、查看当前分支及所有分支\n\ngit branch\n\n\n1\n\n\n4、新建并切换到 develop 分支\n\ngit checkout -b develop\n\n\n1\n\n\n# 参考资料\n\n * 3.2 git 分支 - 分支的新建与合并",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"fork 代码后想要同步至最新版",frontmatter:{title:"fork 代码后想要同步至最新版",date:"2021-08-14T15:00:57.000Z",permalink:"/pages/27a54a/",categories:["学习笔记","系列笔记-Git 使用笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Git%20%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/02.fork%20%E4%BB%A3%E7%A0%81%E5%90%8E%E6%83%B3%E8%A6%81%E5%90%8C%E6%AD%A5%E8%87%B3%E6%9C%80%E6%96%B0%E7%89%88.html",relativePath:"02.学习笔记/13.系列笔记-Git 使用笔记/02.fork 代码后想要同步至最新版.md",key:"v-bfbdbcee",path:"/pages/27a54a/",headersStr:null,content:"fork了别人的代码，原作者改动了，自己的github想要同步到最新版。\n\n * 参考资料：https://blog.csdn.net/wzy4072/article/details/79628659",normalizedContent:"fork了别人的代码，原作者改动了，自己的github想要同步到最新版。\n\n * 参考资料：https://blog.csdn.net/wzy4072/article/details/79628659",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"撤销 git commit",frontmatter:{title:"撤销 git commit",date:"2021-08-17T18:20:25.000Z",permalink:"/pages/1e41a6/",categories:["学习笔记","系列笔记-Git 使用笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Git%20%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/05.%E6%92%A4%E9%94%80%20git%20commit.html",relativePath:"02.学习笔记/13.系列笔记-Git 使用笔记/05.撤销 git commit.md",key:"v-47a2a4fb",path:"/pages/1e41a6/",headers:[{level:2,title:"撤销 git commit",slug:"撤销-git-commit",normalizedTitle:"撤销 git commit",charIndex:2}],headersStr:"撤销 git commit",content:"# 撤销 git commit\n\n# 回到上一个版本（撤销上一个commit）\n\ngit reset --soft HEAD^\n\n\n1\n\n\n# commit 注释写错了\n\ngit commit --amend\n\n\n1\n",normalizedContent:"# 撤销 git commit\n\n# 回到上一个版本（撤销上一个commit）\n\ngit reset --soft head^\n\n\n1\n\n\n# commit 注释写错了\n\ngit commit --amend\n\n\n1\n",charsets:{cjk:!0},lastUpdated:"2021/08/29, 21:59:07"},{title:"remote Support for password authentication was removed Please use a personal access token instead",frontmatter:{title:"remote Support for password authentication was removed Please use a personal access token instead",date:"2021-08-14T17:37:02.000Z",permalink:"/pages/83cc49/",categories:["学习笔记","系列笔记-Git 使用笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Git%20%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/03.remote%20Support%20for%20password%20authentication%20was%20removed%20Please%20use%20a%20personal%20access%20token%20instead.html",relativePath:"02.学习笔记/13.系列笔记-Git 使用笔记/03.remote Support for password authentication was removed Please use a personal access token instead.md",key:"v-0e34e394",path:"/pages/83cc49/",headers:[{level:2,title:"remote: Support for password authentication was removed on August 13, 2021. Please use a personal access token instead.",slug:"remote-support-for-password-authentication-was-removed-on-august-13-2021-please-use-a-personal-access-token-instead",normalizedTitle:"remote: support for password authentication was removed on august 13, 2021. please use a personal access token instead.",charIndex:2}],headersStr:"remote: Support for password authentication was removed on August 13, 2021. Please use a personal access token instead.",content:"# remote: Support for password authentication was removed on August 13, 2021. Please use a personal access token instead.\n\nremote: Please see https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/ for more information.\n\nhttps://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/#what-you-need-to-do-today",normalizedContent:"# remote: support for password authentication was removed on august 13, 2021. please use a personal access token instead.\n\nremote: please see https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/ for more information.\n\nhttps://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/#what-you-need-to-do-today",charsets:{},lastUpdated:"2021/08/17, 18:07:06"},{title:"ssh 与 Git 登录",frontmatter:{title:"ssh 与 Git 登录",date:"2021-09-04T22:26:30.000Z",permalink:"/pages/02e47d/",categories:["学习笔记","系列笔记-Git 使用笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Git%20%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/06.ssh%20%E4%B8%8E%20Git%20%E7%99%BB%E5%BD%95.html",relativePath:"02.学习笔记/13.系列笔记-Git 使用笔记/06.ssh 与 Git 登录.md",key:"v-8a7101b2",path:"/pages/02e47d/",headersStr:null,content:'1、生成 SSH公钥-私钥对，生成的 SSH 秘钥默认位于 ~/.ssh 路径下，公钥为 id_rsa.pub ，私钥为 id_rsa：\n\nssh-keygen -t rsa -b 4096 -C "your_email@example.com"\n\n\n1\n\n\n参考资料\n\n * https://dowww.spencerwoo.com/2-cli/2-3-cli-tools.html#%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6',normalizedContent:'1、生成 ssh公钥-私钥对，生成的 ssh 秘钥默认位于 ~/.ssh 路径下，公钥为 id_rsa.pub ，私钥为 id_rsa：\n\nssh-keygen -t rsa -b 4096 -c "your_email@example.com"\n\n\n1\n\n\n参考资料\n\n * https://dowww.spencerwoo.com/2-cli/2-3-cli-tools.html#%e7%89%88%e6%9c%ac%e6%8e%a7%e5%88%b6',charsets:{cjk:!0},lastUpdated:"2021/09/05, 19:05:19"},{title:"撤销 git add",frontmatter:{title:"撤销 git add",date:"2021-08-15T23:09:27.000Z",permalink:"/pages/ecdf24/",categories:["学习笔记","系列笔记-Git 使用笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Git%20%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/04.%E6%92%A4%E9%94%80%20git%20add.html",relativePath:"02.学习笔记/13.系列笔记-Git 使用笔记/04.撤销 git add.md",key:"v-95b7e762",path:"/pages/ecdf24/",headers:[{level:2,title:"撤销 git add",slug:"撤销-git-add",normalizedTitle:"撤销 git add",charIndex:2}],headersStr:"撤销 git add",content:"# 撤销 git add\n\n如果是撤销所有的已经add的文件:\n\ngit reset HEAD .\n\n\n1\n\n\n如果是撤销某个文件或文件夹：\n\ngit reset HEAD  -filename\n\n\n1\n\n * 参考资料：https://segmentfault.com/q/1010000006864939",normalizedContent:"# 撤销 git add\n\n如果是撤销所有的已经add的文件:\n\ngit reset head .\n\n\n1\n\n\n如果是撤销某个文件或文件夹：\n\ngit reset head  -filename\n\n\n1\n\n * 参考资料：https://segmentfault.com/q/1010000006864939",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"Git 设置代理",frontmatter:{title:"Git 设置代理",date:"2022-01-04T10:42:41.000Z",permalink:"/pages/4000c9/",categories:["学习笔记","系列笔记-Git 使用笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Git%20%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/07.Git%20%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86.html",relativePath:"02.学习笔记/13.系列笔记-Git 使用笔记/07.Git 设置代理.md",key:"v-d675efea",path:"/pages/4000c9/",headersStr:null,content:"# 设置代理\ngit config --global http.proxy 'socks5://127.0.0.1:7890'\ngit config --global https.proxy 'socks5://127.0.0.1:7890'\n\n# 查看代理\ngit config --global --get http.proxy\ngit config --global --get https.proxy\n\n# 取消代理\ngit config --global --unset http.proxy\ngit config --global --unset https.proxy\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n",normalizedContent:"# 设置代理\ngit config --global http.proxy 'socks5://127.0.0.1:7890'\ngit config --global https.proxy 'socks5://127.0.0.1:7890'\n\n# 查看代理\ngit config --global --get http.proxy\ngit config --global --get https.proxy\n\n# 取消代理\ngit config --global --unset http.proxy\ngit config --global --unset https.proxy\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"flarum 论坛搭建",frontmatter:{title:"flarum 论坛搭建",date:"2021-03-30T09:33:21.000Z",permalink:"/pages/2da48e/",categories:["更多","网站搭建"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/14.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/00.flarum%20%E8%AE%BA%E5%9D%9B%E6%90%AD%E5%BB%BA.html",relativePath:"02.学习笔记/14.系列笔记-网站搭建/00.flarum 论坛搭建.md",key:"v-90d4e4ee",path:"/pages/2da48e/",headers:[{level:3,title:"Flarum Github仓库地址",slug:"flarum-github仓库地址",normalizedTitle:"flarum github仓库地址",charIndex:2}],headersStr:"Flarum Github仓库地址",content:"# Flarum Github仓库地址\n\n# 安装步骤\n\nBefore you install Flarum, it's important to check that your server meets the requirements. To run Flarum, you will need:\n\n * Apache (with mod_rewrite enabled) or Nginx\n * PHP 7.3+ with the following extensions: curl, dom, gd, json, mbstring, openssl, pdo_mysql, tokenizer, zip\n * MySQL 5.6+ or MariaDB 10.0.5+\n * SSH (command-line) access to run Composer\n\n按照官方文档，我们应该拥有这些环境，所以先检查服务器这些环境是否满足\n\n# 查看操作系统版本\ncat /etc/redhat-release\n# 查看 Nginx 版本\nnginx -V\n# 查看 PHP 版本\nphp -v\n\n\n1\n2\n3\n4\n5\n6\n\n\nFlarum 的安装与配置\n\n宝塔 Linux 面板安装 Flarum\n\nFlarum 插件（测试有效的）\n\n * Flarum 简体中文语言包 2020\n\n * Emoji 表情选择器（Emoji Picker）\n\n * 主题贴浏览量\n\n * FoF 会员名录\n\n * FoF 文件上传器\n\n * FoF 网站地图\n\n * FancyBox 图片灯箱（beta）\n\n * FoF 导航栏链接\n\n * Flarum Blog",normalizedContent:"# flarum github仓库地址\n\n# 安装步骤\n\nbefore you install flarum, it's important to check that your server meets the requirements. to run flarum, you will need:\n\n * apache (with mod_rewrite enabled) or nginx\n * php 7.3+ with the following extensions: curl, dom, gd, json, mbstring, openssl, pdo_mysql, tokenizer, zip\n * mysql 5.6+ or mariadb 10.0.5+\n * ssh (command-line) access to run composer\n\n按照官方文档，我们应该拥有这些环境，所以先检查服务器这些环境是否满足\n\n# 查看操作系统版本\ncat /etc/redhat-release\n# 查看 nginx 版本\nnginx -v\n# 查看 php 版本\nphp -v\n\n\n1\n2\n3\n4\n5\n6\n\n\nflarum 的安装与配置\n\n宝塔 linux 面板安装 flarum\n\nflarum 插件（测试有效的）\n\n * flarum 简体中文语言包 2020\n\n * emoji 表情选择器（emoji picker）\n\n * 主题贴浏览量\n\n * fof 会员名录\n\n * fof 文件上传器\n\n * fof 网站地图\n\n * fancybox 图片灯箱（beta）\n\n * fof 导航栏链接\n\n * flarum blog",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"README 美化",frontmatter:{title:"README 美化",date:"2022-05-20T15:00:48.000Z",permalink:"/pages/83977c/",categories:["学习笔记","系列笔记-Git 使用笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Git%20%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/09.README%20%E7%BE%8E%E5%8C%96.html",relativePath:"02.学习笔记/13.系列笔记-Git 使用笔记/09.README 美化.md",key:"v-da9fda54",path:"/pages/83977c/",headersStr:null,content:"添加 commit 的贪吃蛇：\n\n * https://ericagrundy.medium.com/how-to-add-a-snake-game-to-your-contribution-graph-on-github-e4b5fd295775",normalizedContent:"添加 commit 的贪吃蛇：\n\n * https://ericagrundy.medium.com/how-to-add-a-snake-game-to-your-contribution-graph-on-github-e4b5fd295775",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"Unverify",frontmatter:{title:"Unverify",date:"2022-01-04T12:27:50.000Z",permalink:"/pages/950767/",categories:["学习笔记","系列笔记-Git 使用笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Git%20%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/08.Unverify.html",relativePath:"02.学习笔记/13.系列笔记-Git 使用笔记/08.Unverify.md",key:"v-167dd495",path:"/pages/950767/",headersStr:null,content:"提交后出现\n\nUnverified:The email in this signature doesn’t match the committer email.\n\n删除 GPG 密钥\n\ngpg --delete-secret-keys 1748DD4A5B0ED441\n\n\n1\n\n\n使用 GitHub 官方提供的下述解决方案\n\n * https://docs.github.com/cn/authentication/managing-commit-signature-verification/checking-for-existing-gpg-keys\n\n * https://docs.github.com/cn/authentication/managing-commit-signature-verification/checking-for-existing-gpg-keys\n\n注意：\n\n * 如果你的 GitHub 使用匿名邮箱，在配置 GPG 密钥时也需要使用匿名邮箱",normalizedContent:"提交后出现\n\nunverified:the email in this signature doesn’t match the committer email.\n\n删除 gpg 密钥\n\ngpg --delete-secret-keys 1748dd4a5b0ed441\n\n\n1\n\n\n使用 github 官方提供的下述解决方案\n\n * https://docs.github.com/cn/authentication/managing-commit-signature-verification/checking-for-existing-gpg-keys\n\n * https://docs.github.com/cn/authentication/managing-commit-signature-verification/checking-for-existing-gpg-keys\n\n注意：\n\n * 如果你的 github 使用匿名邮箱，在配置 gpg 密钥时也需要使用匿名邮箱",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"图游走类模型",frontmatter:{title:"图游走类模型",date:"2021-07-05T16:07:36.000Z",permalink:"/pages/eef8e4/",categories:["计算机视觉","图卷积网络"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/15.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/01.%E5%9B%BE%E6%B8%B8%E8%B5%B0%E7%B1%BB%E6%A8%A1%E5%9E%8B.html",relativePath:"02.学习笔记/15.系列笔记-图卷积网络/01.图游走类模型.md",key:"v-103ba0a2",path:"/pages/eef8e4/",headers:[{level:3,title:"1、图游走类算法简介",slug:"_1、图游走类算法简介",normalizedTitle:"1、图游走类算法简介",charIndex:2},{level:3,title:"2、Word2vec",slug:"_2、word2vec",normalizedTitle:"2、word2vec",charIndex:125},{level:3,title:"3、Word2vec->图浅入领域",slug:"_3、word2vec-图浅入领域",normalizedTitle:"3、word2vec-&gt;图浅入领域",charIndex:null}],headersStr:"1、图游走类算法简介 2、Word2vec 3、Word2vec->图浅入领域",content:"# 1、图游走类算法简介\n\n目标：Node embeddings。得到节点的低维表示，学习到节点与邻居的关系，更好地表示节点信息，再用于下游任务\n\n方法：多次游走，得到游走序列，类似于NLP领域的Word2vec模型：词的语义由其上下文决定\n\n\n# 2、Word2vec\n\n# 2.1 Word2vec：Skip Gram\n\n通过给定中心词预测上下文这个任务，得到Hidden layer的参数，通过Hidden Layer即可得到词的Embedding。\n\n\n\n# 2.2 Word2vec：Negative Sampling\n\nsoftmax需要对上下文的每个词都预测一个概率，计算量很大\n\n提出负采样（Negative Sampling），得到正样本和负样本，做一个分类任务，将 Softmax 变为 Multiple sigmoid\n\n\n# 3、Word2vec->图浅入领域\n\n# 3.1 图游走类模型-DeepWalk\n\n通过随机游走得到NLP领域中的“句子”，得到多个游走序列，本质就是可以回头的DFS\n\n15：50",normalizedContent:"# 1、图游走类算法简介\n\n目标：node embeddings。得到节点的低维表示，学习到节点与邻居的关系，更好地表示节点信息，再用于下游任务\n\n方法：多次游走，得到游走序列，类似于nlp领域的word2vec模型：词的语义由其上下文决定\n\n\n# 2、word2vec\n\n# 2.1 word2vec：skip gram\n\n通过给定中心词预测上下文这个任务，得到hidden layer的参数，通过hidden layer即可得到词的embedding。\n\n\n\n# 2.2 word2vec：negative sampling\n\nsoftmax需要对上下文的每个词都预测一个概率，计算量很大\n\n提出负采样（negative sampling），得到正样本和负样本，做一个分类任务，将 softmax 变为 multiple sigmoid\n\n\n# 3、word2vec->图浅入领域\n\n# 3.1 图游走类模型-deepwalk\n\n通过随机游走得到nlp领域中的“句子”，得到多个游走序列，本质就是可以回头的dfs\n\n15：50",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"节点分类是如何训练的",frontmatter:{title:"节点分类是如何训练的",date:"2021-11-03T23:14:11.000Z",permalink:"/pages/a12467/",categories:["学习笔记","系列笔记-图卷积网络"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/15.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/02.%E8%8A%82%E7%82%B9%E5%88%86%E7%B1%BB%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E7%9A%84.html",relativePath:"02.学习笔记/15.系列笔记-图卷积网络/02.节点分类是如何训练的.md",key:"v-8a1baa90",path:"/pages/a12467/",headers:[{level:2,title:"图节点分类如何训练？",slug:"图节点分类如何训练",normalizedTitle:"图节点分类如何训练？",charIndex:2}],headersStr:"图节点分类如何训练？",content:"# 图节点分类如何训练？",normalizedContent:"# 图节点分类如何训练？",charsets:{cjk:!0},lastUpdated:"2021/12/14, 00:10:07"},{title:"图学习初印象",frontmatter:{title:"图学习初印象",date:"2021-06-27T14:06:37.000Z",permalink:"/pages/1a1c0e/",categories:["计算机视觉","图卷积网络"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/15.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/00.%E5%9B%BE%E5%AD%A6%E4%B9%A0%E5%88%9D%E5%8D%B0%E8%B1%A1.html",relativePath:"02.学习笔记/15.系列笔记-图卷积网络/00.图学习初印象.md",key:"v-0e6d37a8",path:"/pages/1a1c0e/",headers:[{level:3,title:"图学习初印象",slug:"图学习初印象",normalizedTitle:"图学习初印象",charIndex:2},{level:3,title:"图嵌入（Graph Embedding）",slug:"图嵌入-graph-embedding",normalizedTitle:"图嵌入（graph embedding）",charIndex:72},{level:3,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:288}],headersStr:"图学习初印象 图嵌入（Graph Embedding） 参考资料",content:"# 图学习初印象\n\n图的基本概念\n\nG=(V,E,v,e)G=(V,E,v,e)G=(V,E,v,e)\n\n图学习的基本概念\n\n图的应用\n\n\n# 图嵌入（Graph Embedding）\n\n在处理NLP或计算机视觉问题时，我们习惯在深度神经网络中对图像或文本进行嵌入(embedding)。到目前为止，我们所看到的图的一个局限性是没有向量特征。但是，我们可以学习图的嵌入！图有不同几个级别的嵌入：\n\n * 对图的组件进行嵌入（节点，边，特征…）(Node2Vec)\n * 对图的子图或整个图进行嵌入(Graph2Vec)\n\n这部分将在后续案例中结合PGL代码实现讲解。\n\n\n# 参考资料\n\n * PGL系列前置教程：图与图学习\n\nG1\n\n * 节点：各个类别的多个聚类中心\n * ",normalizedContent:"# 图学习初印象\n\n图的基本概念\n\ng=(v,e,v,e)g=(v,e,v,e)g=(v,e,v,e)\n\n图学习的基本概念\n\n图的应用\n\n\n# 图嵌入（graph embedding）\n\n在处理nlp或计算机视觉问题时，我们习惯在深度神经网络中对图像或文本进行嵌入(embedding)。到目前为止，我们所看到的图的一个局限性是没有向量特征。但是，我们可以学习图的嵌入！图有不同几个级别的嵌入：\n\n * 对图的组件进行嵌入（节点，边，特征…）(node2vec)\n * 对图的子图或整个图进行嵌入(graph2vec)\n\n这部分将在后续案例中结合pgl代码实现讲解。\n\n\n# 参考资料\n\n * pgl系列前置教程：图与图学习\n\ng1\n\n * 节点：各个类别的多个聚类中心\n * ",charsets:{cjk:!0},lastUpdated:"2021/11/03, 23:35:28"},{title:"Course overview and the shell",frontmatter:{title:"Course overview and the shell",date:"2021-06-28T18:44:40.000Z",permalink:"/pages/79bd3f/",categories:["技术文章","MIT-NULL"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/16.%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-MIT-NULL/00.Course%20overview%20and%20the%20shell.html",relativePath:"02.学习笔记/16.课程笔记-MIT-NULL/00.Course overview and the shell.md",key:"v-0a57134c",path:"/pages/79bd3f/",headers:[{level:2,title:"Course overview + the shell",slug:"course-overview-the-shell",normalizedTitle:"course overview + the shell",charIndex:2},{level:3,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:748}],headersStr:"Course overview + the shell 参考资料",content:"# Course overview + the shell\n\ndate\necho hello\necho \"Hello world\"\necho $PATH\n\nwhich echo\npwd(present working directory)\n\ncd /home\ncd .\ncd ..\ncd ~\ncd -\n\n\nls\nls ..\nls --help\nls -l\n\n# 权限控制\nr--read\nw--write\nx--execute\n\n# 移动/重命名\nmv dotefiles.md foo.md\n\ncp dotfiles.md foo.md\n\nrm ../food.md\n\nmkdir photos\nrmdir photos\n\nman ls\nctrl+L\n\n# input streams \n# output streams\n\necho hello > hello.txt\ncat hello.txt\ncat < hello.txt\n\ncat < hello.txt > hello2.txt\ncat < hello.txt >> hello2.txt\n\nls -l / | tail n1\n\ncurl --head --silent google.com | grep -i content-length | cut --delimiter=' ' -f2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\nshell 通过环境变量来找程序\n\n\n# 参考资料\n\n * https://missing.csail.mit.edu/\n\n * https://missing-semester-cn.github.io/\n\n * https://www.bilibili.com/video/BV14E411J7n2\n\n * https://www.youtube.com/watch?v=&list=PLyzOVJj3bHQuloKGG59rS43e29ro7I57J\n\n * https://missing-semester-cn.github.io/2020/course-shell/",normalizedContent:"# course overview + the shell\n\ndate\necho hello\necho \"hello world\"\necho $path\n\nwhich echo\npwd(present working directory)\n\ncd /home\ncd .\ncd ..\ncd ~\ncd -\n\n\nls\nls ..\nls --help\nls -l\n\n# 权限控制\nr--read\nw--write\nx--execute\n\n# 移动/重命名\nmv dotefiles.md foo.md\n\ncp dotfiles.md foo.md\n\nrm ../food.md\n\nmkdir photos\nrmdir photos\n\nman ls\nctrl+l\n\n# input streams \n# output streams\n\necho hello > hello.txt\ncat hello.txt\ncat < hello.txt\n\ncat < hello.txt > hello2.txt\ncat < hello.txt >> hello2.txt\n\nls -l / | tail n1\n\ncurl --head --silent google.com | grep -i content-length | cut --delimiter=' ' -f2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n\n\nshell 通过环境变量来找程序\n\n\n# 参考资料\n\n * https://missing.csail.mit.edu/\n\n * https://missing-semester-cn.github.io/\n\n * https://www.bilibili.com/video/bv14e411j7n2\n\n * https://www.youtube.com/watch?v=&list=plyzovjj3bhqulokgg59rs43e29ro7i57j\n\n * https://missing-semester-cn.github.io/2020/course-shell/",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"图像增强-几何及灰度变换",frontmatter:{title:"图像增强-几何及灰度变换",date:"2021-03-09T21:16:46.000Z",permalink:"/pages/a70988/",categories:["技术文章","OpenCV-Python"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/17.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-OpenCV-Python/02.%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E5%87%A0%E4%BD%95%E5%8F%8A%E7%81%B0%E5%BA%A6%E5%8F%98%E6%8D%A2.html",relativePath:"02.学习笔记/17.系列笔记-OpenCV-Python/02.图像增强-几何及灰度变换.md",key:"v-2a51b9b5",path:"/pages/a70988/",headers:[{level:2,title:"Opencv-Python 02 几何及灰度变换、图像增强",slug:"opencv-python-02-几何及灰度变换、图像增强",normalizedTitle:"opencv-python 02 几何及灰度变换、图像增强",charIndex:2},{level:3,title:"1 简介",slug:"_1-简介",normalizedTitle:"1 简介",charIndex:36},{level:3,title:"2 算法理论介绍",slug:"_2-算法理论介绍",normalizedTitle:"2 算法理论介绍",charIndex:216}],headersStr:"Opencv-Python 02 几何及灰度变换、图像增强 1 简介 2 算法理论介绍",content:"# Opencv-Python 02 几何及灰度变换、图像增强\n\n\n# 1 简介\n\n图像的几何及灰度变换操作较为常用，图像增强操作不仅仅只在深度学习中用作数据增广、在实际应用中图像增强技术也能改善图像质量，给人以更好的体验。在深度学习领域，我们常用平移、旋转、镜像等操作进行数据增广；在传统CV领域，由于某些拍摄角度的问题，我们需要对图像进行矫正处理，而几何变换正是这个处理过程的基础，因此了解和学习几何变换也是有必要的。\n\n\n# 2 算法理论介绍\n\n# 2.1 仿射变换形式\n\n需要一个仿射变换矩阵，与原坐标(v.w) 做仿射变换(点乘)后，得到变换后的坐标(x,y)。常用的变换矩阵及作用如下\n\n\n\n\n\n# 2.2 坐标系变换\n\n变换中心，对于缩放、平移可以以图像坐标原点（图像左上角为原点）为中心变换，这不用坐标系变换，直接按照一般形式计算即可。而对于旋转和偏移，一般是以图像中心为原点，那么这就涉及坐标系转换了。\n\n图像坐标的原点在图像左上角，水平向右为 X 轴，垂直向下为 Y 轴。数学课本中常见的坐标系是以图像中心为原点，水平向右为 X 轴，垂直向上为 Y 轴，称为笛卡尔坐标系。\n\n因此，对于旋转和偏移，就需要3步（3次变换）：\n\n * 将输入原图图像坐标转换为笛卡尔坐标系；\n * 进行旋转计算。旋转矩阵前面已经给出了；\n * 将旋转后的图像的笛卡尔坐标转回图像坐标。\n\n# 2.3 反向映射\n\n前向映射就是根据原图用变换公式直接算出输出图像相应像素的空间位置，那么这会导致一个问题：可能会有多个像素坐标映射到输出图像的同一位置，也可能输出图像的某些位置完全没有相应的输入图像像素与它匹配，也就是没有被映射到，造成有规律的空洞（黑色的蜂窝状）。更好的一种方式是采用 反向映射（Inverse Mapping）：扫描输出图像的位置(x,y)，通过\n\n（为T的逆矩阵）计算输入图像对应的位置 (v,w)，通过插值方法决定输出图像该位置的灰度值。\n\n# 2.4 插值\n\n采用反向映射后，需通过插值方法决定输出图像该位置的值，因此需要选择插值算法。通常有最近邻插值、双线性插值，双三次插值等，OpencV默认采用双线性插值，我们也就采用双线性插值。",normalizedContent:"# opencv-python 02 几何及灰度变换、图像增强\n\n\n# 1 简介\n\n图像的几何及灰度变换操作较为常用，图像增强操作不仅仅只在深度学习中用作数据增广、在实际应用中图像增强技术也能改善图像质量，给人以更好的体验。在深度学习领域，我们常用平移、旋转、镜像等操作进行数据增广；在传统cv领域，由于某些拍摄角度的问题，我们需要对图像进行矫正处理，而几何变换正是这个处理过程的基础，因此了解和学习几何变换也是有必要的。\n\n\n# 2 算法理论介绍\n\n# 2.1 仿射变换形式\n\n需要一个仿射变换矩阵，与原坐标(v.w) 做仿射变换(点乘)后，得到变换后的坐标(x,y)。常用的变换矩阵及作用如下\n\n\n\n\n\n# 2.2 坐标系变换\n\n变换中心，对于缩放、平移可以以图像坐标原点（图像左上角为原点）为中心变换，这不用坐标系变换，直接按照一般形式计算即可。而对于旋转和偏移，一般是以图像中心为原点，那么这就涉及坐标系转换了。\n\n图像坐标的原点在图像左上角，水平向右为 x 轴，垂直向下为 y 轴。数学课本中常见的坐标系是以图像中心为原点，水平向右为 x 轴，垂直向上为 y 轴，称为笛卡尔坐标系。\n\n因此，对于旋转和偏移，就需要3步（3次变换）：\n\n * 将输入原图图像坐标转换为笛卡尔坐标系；\n * 进行旋转计算。旋转矩阵前面已经给出了；\n * 将旋转后的图像的笛卡尔坐标转回图像坐标。\n\n# 2.3 反向映射\n\n前向映射就是根据原图用变换公式直接算出输出图像相应像素的空间位置，那么这会导致一个问题：可能会有多个像素坐标映射到输出图像的同一位置，也可能输出图像的某些位置完全没有相应的输入图像像素与它匹配，也就是没有被映射到，造成有规律的空洞（黑色的蜂窝状）。更好的一种方式是采用 反向映射（inverse mapping）：扫描输出图像的位置(x,y)，通过\n\n（为t的逆矩阵）计算输入图像对应的位置 (v,w)，通过插值方法决定输出图像该位置的灰度值。\n\n# 2.4 插值\n\n采用反向映射后，需通过插值方法决定输出图像该位置的值，因此需要选择插值算法。通常有最近邻插值、双线性插值，双三次插值等，opencv默认采用双线性插值，我们也就采用双线性插值。",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"图像插值",frontmatter:{title:"图像插值",date:"2021-03-09T21:15:22.000Z",permalink:"/pages/8cab5c/",categories:["技术文章","OpenCV-Python"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/17.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-OpenCV-Python/01.%E5%9B%BE%E5%83%8F%E6%8F%92%E5%80%BC.html",relativePath:"02.学习笔记/17.系列笔记-OpenCV-Python/01.图像插值.md",key:"v-29693aaf",path:"/pages/8cab5c/",headers:[{level:3,title:"Opencv-Python 01 图像插值",slug:"opencv-python-01-图像插值",normalizedTitle:"opencv-python 01 图像插值",charIndex:2}],headersStr:"Opencv-Python 01 图像插值",content:'# Opencv-Python 01 图像插值\n\n# 1 简介\n\n图像插值比较常用，是图像缩放背后的数学逻辑。Mask RCNN将RoI pooling 改进为 RoI Align背后的理论支撑就是双线性插值。在图像中，整数坐标上的灰度值才有意义，但图像进入算法模型中往往需要进行缩放，这就会考虑到非整数坐标上的灰度值。图像插值就是用来解决这一问题，通过整数坐标上的灰度值来估计非整数坐标上的灰度值。常见的插值算法有最近邻插值、双线性插值和三次样条插值\n\n * 值得一提的是，这些插值方法在数学建模中也常常使用。需要分清楚插值和拟合的区别\n\n# 2 插值方法\n\n# 2.1 最近邻插值\n\n顾名思义、最近邻插值算法中，非整数坐标的点的灰度值由离他最近的整数坐标点的灰度值来确定。在2维图像中，距离度量一般使用欧氏距离。在RoI Pooling中使用的插值算法即为最近邻插值。下面使用一些例子来说明\n\n * 例子：将一个 3x3 的图像 resize 成4x4\n * 算法缺陷：最近邻插值算法常常出现块状效应，会在一定程度上损失 空间对称性（Alignment）\n\nf(dstX,dstY)=h(dstXsrcWidthdstWidth,dstYsrcHeightdstHeight)f(dst_{X}, dst_{Y}) = h(\\frac{dst_{X}src_{Width}} {dst_{Width}}, \\frac{dst_{Y}src_{Height}} {dst_{Height}}) f(dstX ,dstY )=h(dstWidth dstX srcWidth ,dstHeight dstY srcHeight )\n\nf(0,0)=h(0,0)f(0,1)=h(0,0.75)=h(0,1)f(0,2)=h(0,1.50)=h(0,2)f(0,3)=h(0,2.25)=h(0,2)f(0,0)=h(0,0) \\\\ f(0,1)=h(0,0.75)=h(0,1) \\\\ f(0,2)=h(0,1.50)=h(0,2) \\\\ f(0,3)=h(0,2.25)=h(0,2) \\\\ f(0,0)=h(0,0)f(0,1)=h(0,0.75)=h(0,1)f(0,2)=h(0,1.50)=h(0,2)f(0,3)=h(0,2.25)=h(0,2)\n\n# 2.2 双线性插值\n\n顾名思义、双线性插值是做两次线性插值。在二维图像中，两次插值是指在横坐标和纵坐标上进行线性插值。线性插值的过程十分简单\n\n * 算法缺陷：双线性灰度插值的平滑作用可能使得图像的细节产生退化，这种现象在进行图像放大时尤其明显\n * 算法优势：保证了 空间对称性（Alignment），在RoIAlign中采用\n\ny=y0+(x−x0)y1−y0x1−x0=y0+(x−x0)y1−(x−x0)y0x1−x0y=y_{0}+\\left(x-x_{0}\\right) \\frac{y_{1}-y_{0}}{x_{1}-x_{0}}=y_{0}+\\frac{\\left(x-x_{0}\\right) y_{1}-\\left(x-x_{0}\\right) y_{0}}{x_{1}-x_{0}} y=y0 +(x−x0 )x1 −x0 y1 −y0 =y0 +x1 −x0 (x−x0 )y1 −(x−x0 )y0\n\n# 3 Opencv-Python代码实践\n\n * 参考资料中有这样的阐述：在缩放时推荐cv2.INTER_AREA，在扩展时推荐cv2.INTER_CUBIC（慢）和cv2.INTER_LINEAR。\n\n * cv2.resize(src, dsize, dst=None, fx=None, fy=None, interpolation=None)\n   \n   \n   1\n   \n\n * src:输入图像\n   dsize:输出图像尺寸\n   fx、fy:x,y方向上的缩放因子\n   interpolation：插值方法，总共五种\n   \t1. cv.INTER_NEAREST - 最近邻插值法\n   \t2. cv.INTER_LINEAR - 双线性插值法（默认）\n       3. cv.INTER_AREA - 基于局部像素的重采样(resampling using pixel area relation)。对于图像抽取(image decimation)来说，这可能是一个更好的方法。但如果是放大图像时，它和最近邻法的效果类似。\n       4. cv.INTER_CUBIC - 基于4x4像素邻域的3次插值法\n       5. cv.INTER_LANCZOS4 - 在x，y方向分别对相邻的八个点进行插值，也就是计算加权和，所以它是一个8x8的描述子。\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   \n\nimport cv2\n \nif __name__ == "__main__":\n    img = cv2.imread(\'../datasets/lena.jpg\', cv2.IMREAD_UNCHANGED)\n    \n    print(\'Original Dimensions : \',img.shape)\n    \n    scale_percent = 30       # percent of original size\n    width = int(img.shape[1] * scale_percent / 100)\n    height = int(img.shape[0] * scale_percent / 100)\n    dim = (width, height)\n    # resize image\n    resized = cv2.resize(img, dim, interpolation = cv2.INTER_LINEAR)\n\n    fx = 1.5\n    fy = 1.5\n\n    resized1 = cv2.resize(resized, dsize=None, fx=fx, fy=fy, interpolation = cv2.INTER_NEAREST)\n    \n    resized2 = cv2.resize(resized, dsize=None, fx=fx, fy=fy, interpolation = cv2.INTER_LINEAR)\n    print(\'Resized Dimensions : \',resized.shape)\n    \n    cv2.imshow("Resized image", resized)\n    cv2.imshow("INTER_NEAREST image", resized1)\n    cv2.imshow("INTER_LINEAR image", resized2)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n# 4 参考资料\n\n * https://www.kancloud.cn/aollo/aolloopencv/259610\n * https://blog.csdn.net/weixin_39940512/article/details/105343418',normalizedContent:'# opencv-python 01 图像插值\n\n# 1 简介\n\n图像插值比较常用，是图像缩放背后的数学逻辑。mask rcnn将roi pooling 改进为 roi align背后的理论支撑就是双线性插值。在图像中，整数坐标上的灰度值才有意义，但图像进入算法模型中往往需要进行缩放，这就会考虑到非整数坐标上的灰度值。图像插值就是用来解决这一问题，通过整数坐标上的灰度值来估计非整数坐标上的灰度值。常见的插值算法有最近邻插值、双线性插值和三次样条插值\n\n * 值得一提的是，这些插值方法在数学建模中也常常使用。需要分清楚插值和拟合的区别\n\n# 2 插值方法\n\n# 2.1 最近邻插值\n\n顾名思义、最近邻插值算法中，非整数坐标的点的灰度值由离他最近的整数坐标点的灰度值来确定。在2维图像中，距离度量一般使用欧氏距离。在roi pooling中使用的插值算法即为最近邻插值。下面使用一些例子来说明\n\n * 例子：将一个 3x3 的图像 resize 成4x4\n * 算法缺陷：最近邻插值算法常常出现块状效应，会在一定程度上损失 空间对称性（alignment）\n\nf(dstx,dsty)=h(dstxsrcwidthdstwidth,dstysrcheightdstheight)f(dst_{x}, dst_{y}) = h(\\frac{dst_{x}src_{width}} {dst_{width}}, \\frac{dst_{y}src_{height}} {dst_{height}}) f(dstx ,dsty )=h(dstwidth dstx srcwidth ,dstheight dsty srcheight )\n\nf(0,0)=h(0,0)f(0,1)=h(0,0.75)=h(0,1)f(0,2)=h(0,1.50)=h(0,2)f(0,3)=h(0,2.25)=h(0,2)f(0,0)=h(0,0) \\\\ f(0,1)=h(0,0.75)=h(0,1) \\\\ f(0,2)=h(0,1.50)=h(0,2) \\\\ f(0,3)=h(0,2.25)=h(0,2) \\\\ f(0,0)=h(0,0)f(0,1)=h(0,0.75)=h(0,1)f(0,2)=h(0,1.50)=h(0,2)f(0,3)=h(0,2.25)=h(0,2)\n\n# 2.2 双线性插值\n\n顾名思义、双线性插值是做两次线性插值。在二维图像中，两次插值是指在横坐标和纵坐标上进行线性插值。线性插值的过程十分简单\n\n * 算法缺陷：双线性灰度插值的平滑作用可能使得图像的细节产生退化，这种现象在进行图像放大时尤其明显\n * 算法优势：保证了 空间对称性（alignment），在roialign中采用\n\ny=y0+(x−x0)y1−y0x1−x0=y0+(x−x0)y1−(x−x0)y0x1−x0y=y_{0}+\\left(x-x_{0}\\right) \\frac{y_{1}-y_{0}}{x_{1}-x_{0}}=y_{0}+\\frac{\\left(x-x_{0}\\right) y_{1}-\\left(x-x_{0}\\right) y_{0}}{x_{1}-x_{0}} y=y0 +(x−x0 )x1 −x0 y1 −y0 =y0 +x1 −x0 (x−x0 )y1 −(x−x0 )y0\n\n# 3 opencv-python代码实践\n\n * 参考资料中有这样的阐述：在缩放时推荐cv2.inter_area，在扩展时推荐cv2.inter_cubic（慢）和cv2.inter_linear。\n\n * cv2.resize(src, dsize, dst=none, fx=none, fy=none, interpolation=none)\n   \n   \n   1\n   \n\n * src:输入图像\n   dsize:输出图像尺寸\n   fx、fy:x,y方向上的缩放因子\n   interpolation：插值方法，总共五种\n   \t1. cv.inter_nearest - 最近邻插值法\n   \t2. cv.inter_linear - 双线性插值法（默认）\n       3. cv.inter_area - 基于局部像素的重采样(resampling using pixel area relation)。对于图像抽取(image decimation)来说，这可能是一个更好的方法。但如果是放大图像时，它和最近邻法的效果类似。\n       4. cv.inter_cubic - 基于4x4像素邻域的3次插值法\n       5. cv.inter_lanczos4 - 在x，y方向分别对相邻的八个点进行插值，也就是计算加权和，所以它是一个8x8的描述子。\n   \n   \n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   \n\nimport cv2\n \nif __name__ == "__main__":\n    img = cv2.imread(\'../datasets/lena.jpg\', cv2.imread_unchanged)\n    \n    print(\'original dimensions : \',img.shape)\n    \n    scale_percent = 30       # percent of original size\n    width = int(img.shape[1] * scale_percent / 100)\n    height = int(img.shape[0] * scale_percent / 100)\n    dim = (width, height)\n    # resize image\n    resized = cv2.resize(img, dim, interpolation = cv2.inter_linear)\n\n    fx = 1.5\n    fy = 1.5\n\n    resized1 = cv2.resize(resized, dsize=none, fx=fx, fy=fy, interpolation = cv2.inter_nearest)\n    \n    resized2 = cv2.resize(resized, dsize=none, fx=fx, fy=fy, interpolation = cv2.inter_linear)\n    print(\'resized dimensions : \',resized.shape)\n    \n    cv2.imshow("resized image", resized)\n    cv2.imshow("inter_nearest image", resized1)\n    cv2.imshow("inter_linear image", resized2)\n    cv2.waitkey(0)\n    cv2.destroyallwindows()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n# 4 参考资料\n\n * https://www.kancloud.cn/aollo/aolloopencv/259610\n * https://blog.csdn.net/weixin_39940512/article/details/105343418',charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"图像滤波",frontmatter:{title:"图像滤波",date:"2021-03-09T21:17:09.000Z",permalink:"/pages/846a1d/",categories:["技术文章","OpenCV-Python"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/17.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-OpenCV-Python/04.%E5%9B%BE%E5%83%8F%E6%BB%A4%E6%B3%A2.html",relativePath:"02.学习笔记/17.系列笔记-OpenCV-Python/04.图像滤波.md",key:"v-416df733",path:"/pages/846a1d/",headers:[{level:2,title:"Opencv-Python 04 图像滤波",slug:"opencv-python-04-图像滤波",normalizedTitle:"opencv-python 04 图像滤波",charIndex:2},{level:3,title:"1 简介",slug:"_1-简介",normalizedTitle:"1 简介",charIndex:28},{level:3,title:"2 算法理论介绍",slug:"_2-算法理论介绍",normalizedTitle:"2 算法理论介绍",charIndex:151}],headersStr:"Opencv-Python 04 图像滤波 1 简介 2 算法理论介绍",content:"# Opencv-Python 04 图像滤波\n\n\n# 1 简介\n\n图像的实质是一种二维信号，滤波是信号处理中的一个重要概念。在图像处理中，滤波是一种非常常见的技术，它们的原理非常简单，但是其思想却十分值得借鉴，滤波是很多图像算法的前置步骤或基础，掌握图像滤波对理解卷积神经网络也有一定帮助。\n\n\n# 2 算法理论介绍\n\n# 2.1 均值滤波、方框滤波\n\n# 2.1.1 滤波分类\n\n线性滤波： 对邻域中的像素的计算为线性运算时，如利用窗口函数进行平滑加权求和的运算，或者某种卷积运算，都可以称为线性滤波。常见的线性滤波有：均值滤波、高斯滤波、盒子滤波、拉普拉斯滤波等等，通常线性滤波器之间只是模版系数不同。\n\n非线性滤波： 非线性滤波利用原始图像跟模版之间的一种逻辑关系得到结果，如最值滤波器，中值滤波器。比较常用的有中值滤波器和双边滤波器。\n\n# 2.1.2 方框滤波\n\n方框滤波是一种非常有用的线性滤波，也叫盒子滤波，均值滤波就是盒子滤波归一化的特殊情况。\n\n应用： 可以说，一切需要求某个邻域内像素之和的场合，都有方框滤波的用武之地，比如：均值滤波、引导滤波、计算Haar特征等等。\n\n优势： 就一个字：快！它可以使复杂度为O(MN)的求和，求方差等运算降低到O(1)或近似于O(1)的复杂度，也就是说与邻域尺寸无关了，有点类似积分图吧，但是比积分图更快（与它的实现方式有关）。\n\n2.1.3 均值滤波\n\n均值滤波的应用场合： 根据冈萨雷斯书中的描述，均值模糊可以模糊图像以便得到感兴趣物体的粗略描述，也就是说，去除图像中的不相关细节，其中“不相关”是指与滤波器模板尺寸相比较小的像素区域，从而对图像有一个整体的认知。即为了对感兴趣的物体得到一个大致的整体的描述而模糊一幅图像，忽略细小的细节。\n\n均值滤波的缺陷： 均值滤波本身存在着固有的缺陷，即它不能很好地保护图像细节，在图像去噪的同时也破坏了图像的细节部分，从而使图像变得模糊，不能很好地去除噪声点。特别是椒盐噪声。\n\n均值滤波是上述方框滤波的特殊情况，均值滤波方法是：对待处理的当前像素，选择一个模板，该模板为其邻近的若干个像素组成，用模板的均值（方框滤波归一化）来替代原像素的值。公式表示为：\n\n\n\ng(x,y)为该邻域的中心像素，n跟系数模版大小有关，一般3*3邻域的模板，n取为9，如：\n\n\n\n当然，模板是可变的，一般取奇数，如5 * 5 , 7 * 7等等。\n\n注：在实际处理过程中可对图像边界进行扩充，扩充为0或扩充为邻近的像素值。",normalizedContent:"# opencv-python 04 图像滤波\n\n\n# 1 简介\n\n图像的实质是一种二维信号，滤波是信号处理中的一个重要概念。在图像处理中，滤波是一种非常常见的技术，它们的原理非常简单，但是其思想却十分值得借鉴，滤波是很多图像算法的前置步骤或基础，掌握图像滤波对理解卷积神经网络也有一定帮助。\n\n\n# 2 算法理论介绍\n\n# 2.1 均值滤波、方框滤波\n\n# 2.1.1 滤波分类\n\n线性滤波： 对邻域中的像素的计算为线性运算时，如利用窗口函数进行平滑加权求和的运算，或者某种卷积运算，都可以称为线性滤波。常见的线性滤波有：均值滤波、高斯滤波、盒子滤波、拉普拉斯滤波等等，通常线性滤波器之间只是模版系数不同。\n\n非线性滤波： 非线性滤波利用原始图像跟模版之间的一种逻辑关系得到结果，如最值滤波器，中值滤波器。比较常用的有中值滤波器和双边滤波器。\n\n# 2.1.2 方框滤波\n\n方框滤波是一种非常有用的线性滤波，也叫盒子滤波，均值滤波就是盒子滤波归一化的特殊情况。\n\n应用： 可以说，一切需要求某个邻域内像素之和的场合，都有方框滤波的用武之地，比如：均值滤波、引导滤波、计算haar特征等等。\n\n优势： 就一个字：快！它可以使复杂度为o(mn)的求和，求方差等运算降低到o(1)或近似于o(1)的复杂度，也就是说与邻域尺寸无关了，有点类似积分图吧，但是比积分图更快（与它的实现方式有关）。\n\n2.1.3 均值滤波\n\n均值滤波的应用场合： 根据冈萨雷斯书中的描述，均值模糊可以模糊图像以便得到感兴趣物体的粗略描述，也就是说，去除图像中的不相关细节，其中“不相关”是指与滤波器模板尺寸相比较小的像素区域，从而对图像有一个整体的认知。即为了对感兴趣的物体得到一个大致的整体的描述而模糊一幅图像，忽略细小的细节。\n\n均值滤波的缺陷： 均值滤波本身存在着固有的缺陷，即它不能很好地保护图像细节，在图像去噪的同时也破坏了图像的细节部分，从而使图像变得模糊，不能很好地去除噪声点。特别是椒盐噪声。\n\n均值滤波是上述方框滤波的特殊情况，均值滤波方法是：对待处理的当前像素，选择一个模板，该模板为其邻近的若干个像素组成，用模板的均值（方框滤波归一化）来替代原像素的值。公式表示为：\n\n\n\ng(x,y)为该邻域的中心像素，n跟系数模版大小有关，一般3*3邻域的模板，n取为9，如：\n\n\n\n当然，模板是可变的，一般取奇数，如5 * 5 , 7 * 7等等。\n\n注：在实际处理过程中可对图像边界进行扩充，扩充为0或扩充为邻近的像素值。",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"彩色空间互转",frontmatter:{title:"彩色空间互转",date:"2021-03-09T21:16:58.000Z",permalink:"/pages/adde71/",categories:["技术文章","OpenCV-Python"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/17.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-OpenCV-Python/03.%E5%BD%A9%E8%89%B2%E7%A9%BA%E9%97%B4%E4%BA%92%E8%BD%AC.html",relativePath:"02.学习笔记/17.系列笔记-OpenCV-Python/03.彩色空间互转.md",key:"v-966faedc",path:"/pages/adde71/",headers:[{level:2,title:"Opencv-Python 03 彩色空间互转",slug:"opencv-python-03-彩色空间互转",normalizedTitle:"opencv-python 03 彩色空间互转",charIndex:2},{level:3,title:"1 简介",slug:"_1-简介",normalizedTitle:"1 简介",charIndex:30},{level:3,title:"2 算法理论",slug:"_2-算法理论",normalizedTitle:"2 算法理论",charIndex:152},{level:3,title:"3 代码实现",slug:"_3-代码实现",normalizedTitle:"3 代码实现",charIndex:872}],headersStr:"Opencv-Python 03 彩色空间互转 1 简介 2 算法理论 3 代码实现",content:"# Opencv-Python 03 彩色空间互转\n\n\n# 1 简介\n\n颜色空间在数字图像领域是一个基础问题，有一些算法也尝试将图像映射到不同的颜色空间中进行处理。相比RGB，其他颜色空间（例如HSV、HSI）更具可分离性和可操作性，所以很多图像算法需要转换颜色空间，我们也有必要进行学习和掌握\n\n\n# 2 算法理论\n\n# 2.1 RGB 与 灰度图互转\n\nRGB是人眼识别的颜色定义出的空间，克表示大部分颜色。它是最通用的面向硬件的彩色模型。RGB颜色空间基于颜色的加法混色原理，从黑色不断叠加Red，Green，Blue的颜色，最终可以得到白色\n\n对于彩色转灰度，有一个很著名的心理学公式：\n\nGray=R∗0.299+G∗0.587+B∗0.114Gray = R*0.299 + G*0.587 + B*0.114 Gray=R∗0.299+G∗0.587+B∗0.114\n\n直接计算因为是浮点型计算，所以复杂度较高，速度较低。所以我们考虑优化，可以将小数转为整数，除法变为移位，乘法也变为移位（整数计算比浮点型快，移位运算和加减法比乘除法快），但是这种方法也会带来一定的精度损失，我们可以根据实际情况选择需要保留的精度位数。\n\n# 2.2 RGB 与 HSV互转\n\nHSV是一种将RGB色彩空间中的点在倒圆锥体中的表示方法。HSV即色相(Hue)、饱和度(Saturation)、明度(Value)，又称HSB(B即Brightness)。色相是色彩的基本属性，就是平常说的颜色的名称，如红色、黄色等。饱和度（S）是指色彩的纯度，越高色彩越纯，低则逐渐变灰，取0-100%的数值。明度（V），取0-max(计算机中HSV取值范围和存储的长度有关)。HSV颜色空间可以用一个圆锥空间模型来描述。圆锥的顶点处，V=0，H和S无定义，代表黑色。圆锥的顶面中心处V=max，S=0，H无定义，代表白色。\n\n * H是色彩；\n\n * S是深浅， S = 0时，只有灰度；\n\n * V是明暗，表示色彩的明亮程度，但与光强无直接联系。\n\n\n\n\n# 3 代码实现",normalizedContent:"# opencv-python 03 彩色空间互转\n\n\n# 1 简介\n\n颜色空间在数字图像领域是一个基础问题，有一些算法也尝试将图像映射到不同的颜色空间中进行处理。相比rgb，其他颜色空间（例如hsv、hsi）更具可分离性和可操作性，所以很多图像算法需要转换颜色空间，我们也有必要进行学习和掌握\n\n\n# 2 算法理论\n\n# 2.1 rgb 与 灰度图互转\n\nrgb是人眼识别的颜色定义出的空间，克表示大部分颜色。它是最通用的面向硬件的彩色模型。rgb颜色空间基于颜色的加法混色原理，从黑色不断叠加red，green，blue的颜色，最终可以得到白色\n\n对于彩色转灰度，有一个很著名的心理学公式：\n\ngray=r∗0.299+g∗0.587+b∗0.114gray = r*0.299 + g*0.587 + b*0.114 gray=r∗0.299+g∗0.587+b∗0.114\n\n直接计算因为是浮点型计算，所以复杂度较高，速度较低。所以我们考虑优化，可以将小数转为整数，除法变为移位，乘法也变为移位（整数计算比浮点型快，移位运算和加减法比乘除法快），但是这种方法也会带来一定的精度损失，我们可以根据实际情况选择需要保留的精度位数。\n\n# 2.2 rgb 与 hsv互转\n\nhsv是一种将rgb色彩空间中的点在倒圆锥体中的表示方法。hsv即色相(hue)、饱和度(saturation)、明度(value)，又称hsb(b即brightness)。色相是色彩的基本属性，就是平常说的颜色的名称，如红色、黄色等。饱和度（s）是指色彩的纯度，越高色彩越纯，低则逐渐变灰，取0-100%的数值。明度（v），取0-max(计算机中hsv取值范围和存储的长度有关)。hsv颜色空间可以用一个圆锥空间模型来描述。圆锥的顶点处，v=0，h和s无定义，代表黑色。圆锥的顶面中心处v=max，s=0，h无定义，代表白色。\n\n * h是色彩；\n\n * s是深浅， s = 0时，只有灰度；\n\n * v是明暗，表示色彩的明亮程度，但与光强无直接联系。\n\n\n\n\n# 3 代码实现",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"阈值分割及二值化",frontmatter:{title:"阈值分割及二值化",date:"2021-03-09T21:17:21.000Z",permalink:"/pages/b41db5/",categories:["技术文章","OpenCV-Python"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/17.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-OpenCV-Python/05.%E9%98%88%E5%80%BC%E5%88%86%E5%89%B2%E5%8F%8A%E4%BA%8C%E5%80%BC%E5%8C%96.html",relativePath:"02.学习笔记/17.系列笔记-OpenCV-Python/05.阈值分割及二值化.md",key:"v-1beb24d8",path:"/pages/b41db5/",headers:[{level:2,title:"Opencv-Python 05 阈值分割及二值化",slug:"opencv-python-05-阈值分割及二值化",normalizedTitle:"opencv-python 05 阈值分割及二值化",charIndex:2},{level:3,title:"1 简介",slug:"_1-简介",normalizedTitle:"1 简介",charIndex:32},{level:3,title:"2 算法简介",slug:"_2-算法简介",normalizedTitle:"2 算法简介",charIndex:220},{level:3,title:"5.4.1 最大类间方差法（大津阈值法）",slug:"_5-4-1-最大类间方差法-大津阈值法",normalizedTitle:"5.4.1 最大类间方差法（大津阈值法）",charIndex:231},{level:3,title:"5.4.2 自适应阈值",slug:"_5-4-2-自适应阈值",normalizedTitle:"5.4.2 自适应阈值",charIndex:752}],headersStr:"Opencv-Python 05 阈值分割及二值化 1 简介 2 算法简介 5.4.1 最大类间方差法（大津阈值法） 5.4.2 自适应阈值",content:"# Opencv-Python 05 阈值分割及二值化\n\n\n# 1 简介\n\n在没有深度学习的年代，图像分割依然是个热门的话题。图像阈值化分割是传统的最常用的图像分割方法，其具有实现简单、计算量小、性能较稳定等优点。图像阈值化的目的是要按照灰度级，对像素集合进行一个划分，得到的每个子集形成一个与现实景物相对应的区域，各个区域内部具有一致的属性，而相邻区域不具有这种一致属性。这样的划分可以通过从灰度级出发选取一个或多个阈值来实现。\n\n\n# 2 算法简介\n\n\n# 5.4.1 最大类间方差法（大津阈值法）\n\n大津法（OTSU）是一种确定图像二值化分割阈值的算法，由日本学者大津于1979年提出。从大津法的原理上来讲，该方法又称作最大类间方差法，因为按照大津法求得的阈值进行图像二值化分割后，前景与背景图像的类间方差最大。\n\n它被认为是图像分割中阈值选取的最佳算法，计算简单，不受图像亮度和对比度的影响，因此在数字图像处理上得到了广泛的应用。它是按图像的灰度特性，将图像分成背景和前景两部分。因方差是灰度分布均匀性的一种度量,背景和前景之间的类间方差越大,说明构成图像的两部分的差别越大,当部分前景错分为背景或部分背景错分为前景都会导致两部分差别变小。因此,使类间方差最大的分割意味着错分概率最小。\n\n应用： 是求图像全局阈值的最佳方法，应用不言而喻，适用于大部分需要求图像全局阈值的场合。\n\n优点： 计算简单快速，不受图像亮度和对比度的影响。\n\n缺点： 对图像噪声敏感；只能针对单一目标分割；当目标和背景大小比例悬殊、类间方差函数可能呈现双峰或者多峰，这个时候效果不好。\n\n原理非常简单，涉及的知识点就是均值、方差等概念和一些公式推导。为了便于理解，我们从目的入手，反推一下这著名的OTSU算法。\n\n\n# 5.4.2 自适应阈值\n\n前面介绍了OTSU算法，但这算法属于全局阈值法，所以对于某些光照不均的图像，这种全局阈值分割的方法会显得苍白无力，如下图：\n\n\n\n\n显然，这样的阈值处理结果不是我们想要的，那么就需要一种方法来应对这样的情况。\n\n这种办法就是自适应阈值法(adaptiveThreshold)，它的思想不是计算全局图像的阈值，而是根据图像不同区域亮度分布，计算其局部阈值，所以对于图像不同区域，能够自适应计算不同的阈值，因此被称为自适应阈值法。(其实就是局部阈值法)\n\n如何确定局部阈值呢？可以计算某个邻域(局部)的均值、中值、高斯加权平均(高斯滤波)来确定阈值。值得说明的是：如果用局部的均值作为局部的阈值，就是常说的移动平均法。",normalizedContent:"# opencv-python 05 阈值分割及二值化\n\n\n# 1 简介\n\n在没有深度学习的年代，图像分割依然是个热门的话题。图像阈值化分割是传统的最常用的图像分割方法，其具有实现简单、计算量小、性能较稳定等优点。图像阈值化的目的是要按照灰度级，对像素集合进行一个划分，得到的每个子集形成一个与现实景物相对应的区域，各个区域内部具有一致的属性，而相邻区域不具有这种一致属性。这样的划分可以通过从灰度级出发选取一个或多个阈值来实现。\n\n\n# 2 算法简介\n\n\n# 5.4.1 最大类间方差法（大津阈值法）\n\n大津法（otsu）是一种确定图像二值化分割阈值的算法，由日本学者大津于1979年提出。从大津法的原理上来讲，该方法又称作最大类间方差法，因为按照大津法求得的阈值进行图像二值化分割后，前景与背景图像的类间方差最大。\n\n它被认为是图像分割中阈值选取的最佳算法，计算简单，不受图像亮度和对比度的影响，因此在数字图像处理上得到了广泛的应用。它是按图像的灰度特性，将图像分成背景和前景两部分。因方差是灰度分布均匀性的一种度量,背景和前景之间的类间方差越大,说明构成图像的两部分的差别越大,当部分前景错分为背景或部分背景错分为前景都会导致两部分差别变小。因此,使类间方差最大的分割意味着错分概率最小。\n\n应用： 是求图像全局阈值的最佳方法，应用不言而喻，适用于大部分需要求图像全局阈值的场合。\n\n优点： 计算简单快速，不受图像亮度和对比度的影响。\n\n缺点： 对图像噪声敏感；只能针对单一目标分割；当目标和背景大小比例悬殊、类间方差函数可能呈现双峰或者多峰，这个时候效果不好。\n\n原理非常简单，涉及的知识点就是均值、方差等概念和一些公式推导。为了便于理解，我们从目的入手，反推一下这著名的otsu算法。\n\n\n# 5.4.2 自适应阈值\n\n前面介绍了otsu算法，但这算法属于全局阈值法，所以对于某些光照不均的图像，这种全局阈值分割的方法会显得苍白无力，如下图：\n\n\n\n\n显然，这样的阈值处理结果不是我们想要的，那么就需要一种方法来应对这样的情况。\n\n这种办法就是自适应阈值法(adaptivethreshold)，它的思想不是计算全局图像的阈值，而是根据图像不同区域亮度分布，计算其局部阈值，所以对于图像不同区域，能够自适应计算不同的阈值，因此被称为自适应阈值法。(其实就是局部阈值法)\n\n如何确定局部阈值呢？可以计算某个邻域(局部)的均值、中值、高斯加权平均(高斯滤波)来确定阈值。值得说明的是：如果用局部的均值作为局部的阈值，就是常说的移动平均法。",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"利用 GitHub 以及商家的批量导入记账数据",frontmatter:{title:"利用 GitHub 以及商家的批量导入记账数据",date:"2021-08-02T21:40:28.000Z",permalink:"/pages/3a4147/",categories:["学习笔记","系列笔记-使用 Beancount 记账"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/18.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-%E4%BD%BF%E7%94%A8%20Beancount%20%E8%AE%B0%E8%B4%A6/01.%E5%88%A9%E7%94%A8%20GitHub%20%E4%BB%A5%E5%8F%8A%E5%95%86%E5%AE%B6%E7%9A%84%E6%89%B9%E9%87%8F%E5%AF%BC%E5%85%A5%E8%AE%B0%E8%B4%A6%E6%95%B0%E6%8D%AE.html",relativePath:"02.学习笔记/18.系列笔记-使用 Beancount 记账/01.利用 GitHub 以及商家的批量导入记账数据.md",key:"v-ed2b6be2",path:"/pages/3a4147/",headers:[{level:2,title:"利用 GitHub 以及商家的批量导入记账数据",slug:"利用-github-以及商家的批量导入记账数据",normalizedTitle:"利用 github 以及商家的批量导入记账数据",charIndex:2}],headersStr:"利用 GitHub 以及商家的批量导入记账数据",content:"# 利用 GitHub 以及商家的批量导入记账数据\n\n1、\n\n导入 Wechat 账单\n\ndouble-entry-generator translate --config alipay_config.yaml --output alipay.beancount alipay-20210623-20210801.csv\n\n\n1\n\n\n导入 Alipay 账单\n\ndouble-entry-generator translate --config wechat_config.yaml --provider wechat --output alipay.beancount wechat-20210623-20210801.csv\n\n\n1\n\n\nmain\n\n2、首先在 Github 创建自己的账单仓库，",normalizedContent:"# 利用 github 以及商家的批量导入记账数据\n\n1、\n\n导入 wechat 账单\n\ndouble-entry-generator translate --config alipay_config.yaml --output alipay.beancount alipay-20210623-20210801.csv\n\n\n1\n\n\n导入 alipay 账单\n\ndouble-entry-generator translate --config wechat_config.yaml --provider wechat --output alipay.beancount wechat-20210623-20210801.csv\n\n\n1\n\n\nmain\n\n2、首先在 github 创建自己的账单仓库，",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"入门 Beancount",frontmatter:{title:"入门 Beancount",date:"2021-06-28T14:50:17.000Z",permalink:"/pages/ac712c/",categories:["更多","环境搭建"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/18.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-%E4%BD%BF%E7%94%A8%20Beancount%20%E8%AE%B0%E8%B4%A6/00.%E5%85%A5%E9%97%A8%20Beancount.html",relativePath:"02.学习笔记/18.系列笔记-使用 Beancount 记账/00.入门 Beancount.md",key:"v-59d4b975",path:"/pages/ac712c/",headers:[{level:2,title:"入门 Beancount",slug:"入门-beancount",normalizedTitle:"入门 beancount",charIndex:2},{level:3,title:"1、什么是复式记账",slug:"_1、什么是复式记账",normalizedTitle:"1、什么是复式记账",charIndex:19},{level:3,title:"2、安装 Beancount及简单操作",slug:"_2、安装-beancount及简单操作",normalizedTitle:"2、安装 beancount及简单操作",charIndex:594},{level:3,title:"3、参考资料",slug:"_3、参考资料",normalizedTitle:"3、参考资料",charIndex:1875}],headersStr:"入门 Beancount 1、什么是复式记账 2、安装 Beancount及简单操作 3、参考资料",content:'# 入门 Beancount\n\n\n# 1、什么是复式记账\n\n# 1.1 会计恒等式\n\n资产 = 债务 + 所有者权益\n\n# 1.2 复式记账法（Double Entry Accounting）\n\n账户（Account）：Debit + Credit\n\n复式记账法中，每一笔商业交易都会引发至少两次账户的变动\n\n回到会计恒等式：资产 = 债务 + 所有者权益\n\n资产的 T-Account 主要包含：现金、设备、应收账款、库存、预付租金\n\n债务的 T-Account 主要包含：贷款、应付账款、预付款账款\n\n所有者权益的 T-Account 主要包含：所有者资本、收入、支出、撤资\n\n# 1.3 复式记账法的流程\n\n日记账（General Journal）：日期、账户、Debit、Credit、描述\n\n分类账户（Ledger Account）：\n\n核对和纠错：对记账进行核对与纠错\n\n1、根据时间顺序记录到 General Journal中\n\n2、将日记账的每一笔交易再转移到对应的分类账户中\n\n3、完成记录后通过试算表进行核对和纠错\n\n注：如何查错？\n\n * 算出试算表左右总和的差值，根据差值去日记账去查找（可能忘记录入某个分类账户）\n * 无法从日记账找到该金额：除以2再去查找（可能记录到了同一侧）\n * 无法从日记账找到该金额：除以9再去查找（可能记录的数字位数出错）\n\n\n# 2、安装 Beancount及简单操作\n\n# 2.1 安装 beancount\n\npip install beancount\npip install fava\n\n\n1\n2\n\n\n# 2.2 试运行\n\n(1) 创建一个 moneybook.bean 的文件，将如下内容录入\n\n;【一、账本信息】\noption "title" "我的账本" ;账本名称\noption "operating_currency" "CNY" ;账本主货币\n\n;【二、账户设置】\n;1、开设账户\n1990-01-01 open Assets:Card:1234 CNY, USD ;尾号1234的银行卡，支持CNY和USD\n1990-01-01 open Liabilities:CreditCard:5678 CNY, USD ;双币信用卡\n1990-01-01 open Income:Salary CNY ;工资收入\n1990-01-01 open Expenses:Tax CNY ;交税\n1990-01-01 open Expenses:Traffic:Taxi CNY ;打车消费，只支持CNY\n1990-01-01 open Equity:OpenBalance ;用于账户初始化，支持任意货币\n\n;2、账户初始化\n2019-08-27 * "" "银行卡，初始余额10000元"\n    Assets:Card:1234           10000.00 CNY\n    Equity:OpenBalance        -10000.00 CNY\n\n;【三、交易记录】\n2019-08-28 * "杭州出租车公司" "打车到公司，银行卡支付"\n    Expenses:Traffic:Taxi        200.00 CNY\n    Assets:Card:1234            -200.00 CNY\n\n2019-08-29 * "" "餐饮"\n    Assets:Card:1234           -1100.00 CNY\n    Liabilities:CreditCard:5678 1100.00 CNY\n\n2019-08-31 * "XX公司" "工资收入"\n    Assets:Card:1234           12000.00 CNY\n    Expenses:Tax                1000.00 CNY\n    Income:Salary\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n(2) 命令行执行 fava moneybook.bean\n\n$ fava moneybook.bean \nRunning Fava on http://localhost:5000\n\n\n1\n2\n\n\n(3) 浏览器打开 http://localhost:5000 即可\n\n\n\n\n# 3、参考资料\n\n * 会计：会计恒等式\n * 会计：复式记账法\n * 记账神器Beancount\n * beancount 简易入门指南\n * beancount 股票实践教程\n * 开始使用 Beancount',normalizedContent:'# 入门 beancount\n\n\n# 1、什么是复式记账\n\n# 1.1 会计恒等式\n\n资产 = 债务 + 所有者权益\n\n# 1.2 复式记账法（double entry accounting）\n\n账户（account）：debit + credit\n\n复式记账法中，每一笔商业交易都会引发至少两次账户的变动\n\n回到会计恒等式：资产 = 债务 + 所有者权益\n\n资产的 t-account 主要包含：现金、设备、应收账款、库存、预付租金\n\n债务的 t-account 主要包含：贷款、应付账款、预付款账款\n\n所有者权益的 t-account 主要包含：所有者资本、收入、支出、撤资\n\n# 1.3 复式记账法的流程\n\n日记账（general journal）：日期、账户、debit、credit、描述\n\n分类账户（ledger account）：\n\n核对和纠错：对记账进行核对与纠错\n\n1、根据时间顺序记录到 general journal中\n\n2、将日记账的每一笔交易再转移到对应的分类账户中\n\n3、完成记录后通过试算表进行核对和纠错\n\n注：如何查错？\n\n * 算出试算表左右总和的差值，根据差值去日记账去查找（可能忘记录入某个分类账户）\n * 无法从日记账找到该金额：除以2再去查找（可能记录到了同一侧）\n * 无法从日记账找到该金额：除以9再去查找（可能记录的数字位数出错）\n\n\n# 2、安装 beancount及简单操作\n\n# 2.1 安装 beancount\n\npip install beancount\npip install fava\n\n\n1\n2\n\n\n# 2.2 试运行\n\n(1) 创建一个 moneybook.bean 的文件，将如下内容录入\n\n;【一、账本信息】\noption "title" "我的账本" ;账本名称\noption "operating_currency" "cny" ;账本主货币\n\n;【二、账户设置】\n;1、开设账户\n1990-01-01 open assets:card:1234 cny, usd ;尾号1234的银行卡，支持cny和usd\n1990-01-01 open liabilities:creditcard:5678 cny, usd ;双币信用卡\n1990-01-01 open income:salary cny ;工资收入\n1990-01-01 open expenses:tax cny ;交税\n1990-01-01 open expenses:traffic:taxi cny ;打车消费，只支持cny\n1990-01-01 open equity:openbalance ;用于账户初始化，支持任意货币\n\n;2、账户初始化\n2019-08-27 * "" "银行卡，初始余额10000元"\n    assets:card:1234           10000.00 cny\n    equity:openbalance        -10000.00 cny\n\n;【三、交易记录】\n2019-08-28 * "杭州出租车公司" "打车到公司，银行卡支付"\n    expenses:traffic:taxi        200.00 cny\n    assets:card:1234            -200.00 cny\n\n2019-08-29 * "" "餐饮"\n    assets:card:1234           -1100.00 cny\n    liabilities:creditcard:5678 1100.00 cny\n\n2019-08-31 * "xx公司" "工资收入"\n    assets:card:1234           12000.00 cny\n    expenses:tax                1000.00 cny\n    income:salary\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n\n\n(2) 命令行执行 fava moneybook.bean\n\n$ fava moneybook.bean \nrunning fava on http://localhost:5000\n\n\n1\n2\n\n\n(3) 浏览器打开 http://localhost:5000 即可\n\n\n\n\n# 3、参考资料\n\n * 会计：会计恒等式\n * 会计：复式记账法\n * 记账神器beancount\n * beancount 简易入门指南\n * beancount 股票实践教程\n * 开始使用 beancount',charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"将 Cashwarden 的数据导入 Beancount",frontmatter:{title:"将 Cashwarden 的数据导入 Beancount",date:"2021-08-02T23:31:02.000Z",permalink:"/pages/f53f1f/",categories:["学习笔记","系列笔记-使用 Beancount 记账"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/18.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-%E4%BD%BF%E7%94%A8%20Beancount%20%E8%AE%B0%E8%B4%A6/02.%E5%B0%86%20Cashwarden%20%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%20Beancount.html",relativePath:"02.学习笔记/18.系列笔记-使用 Beancount 记账/02.将 Cashwarden 的数据导入 Beancount.md",key:"v-05774214",path:"/pages/f53f1f/",headers:[{level:2,title:"将 Cashwarden 的数据导入 Beancount",slug:"将-cashwarden-的数据导入-beancount",normalizedTitle:"将 cashwarden 的数据导入 beancount",charIndex:2}],headersStr:"将 Cashwarden 的数据导入 Beancount",content:"# 将 Cashwarden 的数据导入 Beancount\n\n",normalizedContent:"# 将 cashwarden 的数据导入 beancount\n\n",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"工厂方法模式",frontmatter:{title:"工厂方法模式",date:"2021-08-05T14:28:03.000Z",permalink:"/pages/eee0ed/",categories:["学习笔记","系列笔记-Python设计模式"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/19.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/00.%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F.html",relativePath:"02.学习笔记/19.系列笔记-Python设计模式/00.工厂方法模式.md",key:"v-b8a31364",path:"/pages/eee0ed/",headersStr:null,content:"# 参考资料\n\n * 阿里云云栖社区 Python与设计模式：https://yq.aliyun.com/topic/122\n\n * A collection of design patterns/idioms in Python：https://github.com/faif/python-patterns",normalizedContent:"# 参考资料\n\n * 阿里云云栖社区 python与设计模式：https://yq.aliyun.com/topic/122\n\n * a collection of design patterns/idioms in python：https://github.com/faif/python-patterns",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"(Notes) A Chat with Andrew on MLOps From Model-centric to Data-centric AI",frontmatter:{title:"(Notes) A Chat with Andrew on MLOps From Model-centric to Data-centric AI",date:"2021-08-08T15:31:38.000Z",permalink:"/pages/2bcb1d/",categories:["学习笔记","系列笔记-MLOps"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/20.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-MLOps/01.(Notes)%20A%20Chat%20with%20Andrew%20on%20MLOps%20From%20Model-centric%20to%20Data-centric%20AI.html",relativePath:"02.学习笔记/20.系列笔记-MLOps/01.(Notes) A Chat with Andrew on MLOps From Model-centric to Data-centric AI.md",key:"v-39505a4f",path:"/pages/2bcb1d/",headers:[{level:2,title:"讲座笔记：A Chat with Andrew: on MLOps From Model-centric to Data-centric AI",slug:"讲座笔记-a-chat-with-andrew-on-mlops-from-model-centric-to-data-centric-ai",normalizedTitle:"讲座笔记：a chat with andrew: on mlops from model-centric to data-centric ai",charIndex:2}],headersStr:"讲座笔记：A Chat with Andrew: on MLOps From Model-centric to Data-centric AI",content:"# 讲座笔记：A Chat with Andrew: on MLOps From Model-centric to Data-centric AI\n\nAI system = Code + Data\n\n改进 Data 会帮助我们让算法达到我们的期望\n\n对于39类的瑕疵检测，有76%的精度，目标是90%的精度\n\n如果你是leader，你会更注重改善数据还是更注重改善模型/算法呢\n\n * 80%：code\n\n * 20%：data\n\nData is Food for AI\n\nPrepare high quality data：80%\n\nTrain a model：20%\n\nThe Lifecycle of an ML Project\n\n大数据集下也有长尾数据，所以对于小数据集的处理方法是通用的\n\n迭代地改进数据质量\n\n * 训练模型\n * 进行错误分析，以识别那些算法表现差的样本\n * 通过数据增强获得更多样本，数据生成扩增数据量，给出更多标签一致性的定义\n\n部署生产模型\n\n * 监控所部署模型的性能，并且为了持续更新模型，需要迭代更新的数据\n\n * 系统地检查概念漂移（concept drift）以及数据漂移（data drift），这是检查性能衰退（performance degradation）的两个方式\n   \n   * 概念漂移指的是 label 的分布或者定义发生了变化\n   * 数据漂移表示特征的分布发生了变化\n\n * 将数据回滚，重新训练更新模型",normalizedContent:"# 讲座笔记：a chat with andrew: on mlops from model-centric to data-centric ai\n\nai system = code + data\n\n改进 data 会帮助我们让算法达到我们的期望\n\n对于39类的瑕疵检测，有76%的精度，目标是90%的精度\n\n如果你是leader，你会更注重改善数据还是更注重改善模型/算法呢\n\n * 80%：code\n\n * 20%：data\n\ndata is food for ai\n\nprepare high quality data：80%\n\ntrain a model：20%\n\nthe lifecycle of an ml project\n\n大数据集下也有长尾数据，所以对于小数据集的处理方法是通用的\n\n迭代地改进数据质量\n\n * 训练模型\n * 进行错误分析，以识别那些算法表现差的样本\n * 通过数据增强获得更多样本，数据生成扩增数据量，给出更多标签一致性的定义\n\n部署生产模型\n\n * 监控所部署模型的性能，并且为了持续更新模型，需要迭代更新的数据\n\n * 系统地检查概念漂移（concept drift）以及数据漂移（data drift），这是检查性能衰退（performance degradation）的两个方式\n   \n   * 概念漂移指的是 label 的分布或者定义发生了变化\n   * 数据漂移表示特征的分布发生了变化\n\n * 将数据回滚，重新训练更新模型",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"关于MLOps",frontmatter:{title:"关于MLOps",date:"2021-08-05T16:18:26.000Z",permalink:"/pages/8a695b/",categories:["学习笔记","系列笔记-MLOps"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/20.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-MLOps/00.%E5%85%B3%E4%BA%8EMLOps.html",relativePath:"02.学习笔记/20.系列笔记-MLOps/00.关于MLOps.md",key:"v-552c5d34",path:"/pages/8a695b/",headersStr:null,content:"MLOps 更快地交付机器学习模型\n\n一系列设计、构建和管理可重现、可测试和可持续的基于 ML 的软件实践。对于大数据 / 机器学习团队，MLOps 包含了大多数 DataOps 的任务以及其他特定于 ML 的任务，例如模型版本控制、测试、验证和监控\n\nFrom Big Data to Good Data\n\nTakeaways: Data-centric AI\n\n百度架构：\n\n训练：kubeflow+airflow+自己的工作流系统\n\n预测：CRD + istio\n\n调度器：volcano\n\n# 参考资料\n\n * A Chat with Andrew on MLOps: From Model-centric to Data-centric AI：[video] [slides]\n\n * Practical MLOPS: HOW TO GET READY FORPRODUCTION MODELS: [blog] [eBook]\n\n * What the Ops are you talking about? [Blog (English)] [Blog (中译版)]\n\n * MLOps：机器学习中的持续交付和自动化流水线 [Blog]\n\n * 从小作坊到智能中枢: MLOps简介：https://zhuanlan.zhihu.com/p/357897337\n\n * 人类早期驯服野生机器学习模型的珍贵资料：https://zhuanlan.zhihu.com/p/330577488\n\n * Full Stack Deep Learning：https://zhuanlan.zhihu.com/p/218468169\n\n * 深入学习AI：https://stevenjokess.github.io/2bPM/chapter_AI_dive/MLOps.html\n\n * fullstackdeeplearning：https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2021-labs\n\n * MLOps 简介：https://segmentfault.com/a/1190000039957405\n\n * 使用Kubeflow和Volcano实现典型AI训练任务：\n\n * https://support.huaweicloud.com/bestpractice-cce/cce_bestpractice_0075.html",normalizedContent:"mlops 更快地交付机器学习模型\n\n一系列设计、构建和管理可重现、可测试和可持续的基于 ml 的软件实践。对于大数据 / 机器学习团队，mlops 包含了大多数 dataops 的任务以及其他特定于 ml 的任务，例如模型版本控制、测试、验证和监控\n\nfrom big data to good data\n\ntakeaways: data-centric ai\n\n百度架构：\n\n训练：kubeflow+airflow+自己的工作流系统\n\n预测：crd + istio\n\n调度器：volcano\n\n# 参考资料\n\n * a chat with andrew on mlops: from model-centric to data-centric ai：[video] [slides]\n\n * practical mlops: how to get ready forproduction models: [blog] [ebook]\n\n * what the ops are you talking about? [blog (english)] [blog (中译版)]\n\n * mlops：机器学习中的持续交付和自动化流水线 [blog]\n\n * 从小作坊到智能中枢: mlops简介：https://zhuanlan.zhihu.com/p/357897337\n\n * 人类早期驯服野生机器学习模型的珍贵资料：https://zhuanlan.zhihu.com/p/330577488\n\n * full stack deep learning：https://zhuanlan.zhihu.com/p/218468169\n\n * 深入学习ai：https://stevenjokess.github.io/2bpm/chapter_ai_dive/mlops.html\n\n * fullstackdeeplearning：https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2021-labs\n\n * mlops 简介：https://segmentfault.com/a/1190000039957405\n\n * 使用kubeflow和volcano实现典型ai训练任务：\n\n * https://support.huaweicloud.com/bestpractice-cce/cce_bestpractice_0075.html",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"常用的数据治理手段",frontmatter:{title:"常用的数据治理手段",date:"2021-09-02T22:33:48.000Z",permalink:"/pages/ac6238/",categories:["学习笔记","系列笔记-MLOps"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/20.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-MLOps/02.%E5%B8%B8%E7%94%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86%E6%89%8B%E6%AE%B5.html",relativePath:"02.学习笔记/20.系列笔记-MLOps/02.常用的数据治理手段.md",key:"v-6ce7636a",path:"/pages/ac6238/",headers:[{level:2,title:"深度学习中常用的数据治理手段",slug:"深度学习中常用的数据治理手段",normalizedTitle:"深度学习中常用的数据治理手段",charIndex:2}],headersStr:"深度学习中常用的数据治理手段",content:"# 深度学习中常用的数据治理手段\n\n维护一个数据版本\n\n# 工程方面：\n\n1、统一的数据读取接口\n\n# 特性方面：\n\n1、数据离线增强\n\n * 数据增广\n\n * 亮度调整\n\n * 图像去噪\n\n * 以及从数据中挖掘到的应当用的增强方式\n\n2、标注是否存在噪声\n\n * NLKD\n * Label Smooth\n * O2U-Net\n\n3、是否存在类别不平衡的现象\n\n * 过采样/降采样\n * 平均采样（例如 ReID 中在一个 batch 中需要按照 id 来训练，效果会好一些）\n * \n\n4、是否存在类别之间难易程度不均衡的问题\n\n * Focal loss\n\n * OHEM\n\n * S-OHEM\n\n * GHM",normalizedContent:"# 深度学习中常用的数据治理手段\n\n维护一个数据版本\n\n# 工程方面：\n\n1、统一的数据读取接口\n\n# 特性方面：\n\n1、数据离线增强\n\n * 数据增广\n\n * 亮度调整\n\n * 图像去噪\n\n * 以及从数据中挖掘到的应当用的增强方式\n\n2、标注是否存在噪声\n\n * nlkd\n * label smooth\n * o2u-net\n\n3、是否存在类别不平衡的现象\n\n * 过采样/降采样\n * 平均采样（例如 reid 中在一个 batch 中需要按照 id 来训练，效果会好一些）\n * \n\n4、是否存在类别之间难易程度不均衡的问题\n\n * focal loss\n\n * ohem\n\n * s-ohem\n\n * ghm",charsets:{cjk:!0},lastUpdated:"2021/09/05, 19:05:19"},{title:"Apollo 核心模块",frontmatter:{title:"Apollo 核心模块",date:"2021-08-09T13:47:16.000Z",permalink:"/pages/899a9e/",categories:["学习笔记","系列笔记-Apollo自动驾驶"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/21.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Apollo%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/00.Apollo%20%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97.html",relativePath:"02.学习笔记/21.系列笔记-Apollo自动驾驶/00.Apollo 核心模块.md",key:"v-1f470076",path:"/pages/899a9e/",headers:[{level:2,title:"Apollo 核心模块",slug:"apollo-核心模块",normalizedTitle:"apollo 核心模块",charIndex:2},{level:3,title:"01、定位",slug:"_01、定位",normalizedTitle:"01、定位",charIndex:18},{level:3,title:"02、感知",slug:"_02、感知",normalizedTitle:"02、感知",charIndex:322}],headersStr:"Apollo 核心模块 01、定位 02、感知",content:"# Apollo 核心模块\n\n\n# 01、定位\n\n默认情况下定位模块有两种方法，一种是结合 GPS 以及 IMU 信息的 RTK （Real Time Kinematic 实时运动）方法，另一种是融合 GPS、IMU 以及 激光雷达信息的多传感器融合方法。\n\n# 1.1 定位模块输入介绍\n\n * RTK方法的输入是\n   * GPS - 全球定位系统\n   * IMU - 惯性测量单元\n * 多传感器融合定位方法的输入是\n   * GPS - 全球定位系统\n   * IMU - 惯性测量单元\n   * 激光雷达 - 光探测与测距传感器\n\n# 1.2 定位模块输出介绍\n\n * 一个 LocalizationEstimate 实例\n\n\n# 02、感知\n\n感知模块用于检测和分类障碍物，其在检测组件中实现。感知模块与多相机、雷达以及激光雷达协作识别障碍物，以及融合他们各自的轨迹以获得最终的轨迹。障碍物子模块检测、分类以及跟踪障碍物，这个子模块也预测障碍物的运动以及姿态信息。对于车道线，通过对车道进行像素级别的解析的后处理来构建车道实例，并且计算车道与本车的相对位置。\n\n\n\n\n\n# 2.1、定位模块输入介绍\n\n * 128 通道的 LiDAR 数据 (cyber channel /apollo/sensor/velodyne128)\n * 16 通道的 LiDAR 数据 (cyber channel /apollo/sensor/lidar_front, lidar_rear_left, lidar_rear_right)\n * Radar 数据 (cyber channel /apollo/sensor/radar_front, radar_rear)\n * 图像数据 (cyber channel /apollo/sensor/camera/front_6mm, front_12mm)\n * 雷达传感器标定的外参 (from YAML files)\n * 前置摄像头标定的内参和外参 (from YAML files)\n * 本车的速度和角速度 (cyber channel /apollo/localization/pose)\n\n# 2.2、定位模块输出介绍\n\n * 障碍物的 3D 轨迹，并且带有朝向，速度以及分类信息 (cyber channel /apollo/perception/obstacles)\n * 交通信号灯的检测识别输出 (cyber channel /apollo/perception/traffic_light)",normalizedContent:"# apollo 核心模块\n\n\n# 01、定位\n\n默认情况下定位模块有两种方法，一种是结合 gps 以及 imu 信息的 rtk （real time kinematic 实时运动）方法，另一种是融合 gps、imu 以及 激光雷达信息的多传感器融合方法。\n\n# 1.1 定位模块输入介绍\n\n * rtk方法的输入是\n   * gps - 全球定位系统\n   * imu - 惯性测量单元\n * 多传感器融合定位方法的输入是\n   * gps - 全球定位系统\n   * imu - 惯性测量单元\n   * 激光雷达 - 光探测与测距传感器\n\n# 1.2 定位模块输出介绍\n\n * 一个 localizationestimate 实例\n\n\n# 02、感知\n\n感知模块用于检测和分类障碍物，其在检测组件中实现。感知模块与多相机、雷达以及激光雷达协作识别障碍物，以及融合他们各自的轨迹以获得最终的轨迹。障碍物子模块检测、分类以及跟踪障碍物，这个子模块也预测障碍物的运动以及姿态信息。对于车道线，通过对车道进行像素级别的解析的后处理来构建车道实例，并且计算车道与本车的相对位置。\n\n\n\n\n\n# 2.1、定位模块输入介绍\n\n * 128 通道的 lidar 数据 (cyber channel /apollo/sensor/velodyne128)\n * 16 通道的 lidar 数据 (cyber channel /apollo/sensor/lidar_front, lidar_rear_left, lidar_rear_right)\n * radar 数据 (cyber channel /apollo/sensor/radar_front, radar_rear)\n * 图像数据 (cyber channel /apollo/sensor/camera/front_6mm, front_12mm)\n * 雷达传感器标定的外参 (from yaml files)\n * 前置摄像头标定的内参和外参 (from yaml files)\n * 本车的速度和角速度 (cyber channel /apollo/localization/pose)\n\n# 2.2、定位模块输出介绍\n\n * 障碍物的 3d 轨迹，并且带有朝向，速度以及分类信息 (cyber channel /apollo/perception/obstacles)\n * 交通信号灯的检测识别输出 (cyber channel /apollo/perception/traffic_light)",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"自动驾驶简介",frontmatter:{title:"自动驾驶简介",date:"2021-08-09T14:19:06.000Z",permalink:"/pages/799e91/",categories:["学习笔记","系列笔记-Apollo自动驾驶"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/21.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Apollo%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/01.%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E7%AE%80%E4%BB%8B.html",relativePath:"02.学习笔记/21.系列笔记-Apollo自动驾驶/01.自动驾驶简介.md",key:"v-2e51ed34",path:"/pages/799e91/",headersStr:null,content:"# 01、从L0-L5的分类\n\n# 02、自动驾驶的各个模块\n\n\n\n03、参考车辆以及硬件平台\n\n线控驾驶车辆\n\n传感器参数由参考硬件规格定义\n\n控制器区域网络（Controller Area Network or CAN）是车辆的内部通信网络\n\nGPS 获取位置，IMU 测量车辆的运动和位置\n\n雷达的优势在于便宜，并且适用于各种天气和照明条件，其特别擅长测量其他车辆的速度\n\n开源软件栈\n\n * 实时操作系统（RTOS）：确保在给定时间内完成任务\n   \n   * Apollo RTOS 是 Ubuntu OS + Apollo kernel\n\n * 运行时框架：是 ROS （Robot Operating System）的定制版\n\n * 应用程序模块层：MAP引擎、定位模块、感知模块、规划、控制、人机接口等等\n\n云服务\n\n云服务可以提供 HD Map，Simulation，Data Platform，Security，OTA，DuerOS\n\n仿真场景数据：记录场景以及虚拟场景\n\nTraffic Lights Data、Obstacles with bbox、Segmentation Data\n\nApolloScape Dataset 最复杂又最精确的无人驾驶数据集",normalizedContent:"# 01、从l0-l5的分类\n\n# 02、自动驾驶的各个模块\n\n\n\n03、参考车辆以及硬件平台\n\n线控驾驶车辆\n\n传感器参数由参考硬件规格定义\n\n控制器区域网络（controller area network or can）是车辆的内部通信网络\n\ngps 获取位置，imu 测量车辆的运动和位置\n\n雷达的优势在于便宜，并且适用于各种天气和照明条件，其特别擅长测量其他车辆的速度\n\n开源软件栈\n\n * 实时操作系统（rtos）：确保在给定时间内完成任务\n   \n   * apollo rtos 是 ubuntu os + apollo kernel\n\n * 运行时框架：是 ros （robot operating system）的定制版\n\n * 应用程序模块层：map引擎、定位模块、感知模块、规划、控制、人机接口等等\n\n云服务\n\n云服务可以提供 hd map，simulation，data platform，security，ota，dueros\n\n仿真场景数据：记录场景以及虚拟场景\n\ntraffic lights data、obstacles with bbox、segmentation data\n\napolloscape dataset 最复杂又最精确的无人驾驶数据集",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"高精度地图",frontmatter:{title:"高精度地图",date:"2021-08-09T14:46:26.000Z",permalink:"/pages/e423da/",categories:["学习笔记","系列笔记-Apollo自动驾驶"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/21.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Apollo%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/02.%E9%AB%98%E7%B2%BE%E5%BA%A6%E5%9C%B0%E5%9B%BE.html",relativePath:"02.学习笔记/21.系列笔记-Apollo自动驾驶/02.高精度地图.md",key:"v-6ea858af",path:"/pages/e423da/",headersStr:null,content:"高精度地图的作用：定位、预先规划\n\n高精度地图最重要的特征就是精度，其精度会达到厘米级别\n\n道路网的精确三维表征\n\n * 交叉路口的布局以及路标位置\n\n * 交通灯上不同颜色的含义\n\n * 指示车道速度限制以及左转车道开始的位置\n\n高精地图如何用于定位？\n\n * 首先确定自己的大概位置，类似于解拼图问题的过程\n\n * 接着查找地标\n\n * 与地标比较，看是否匹配\n\n高精地图如何用于感知？\n\n * 在恶劣天气下，传感器的能力可能会受到影响\n\n * 如果由于遮挡未检测到信号灯，高精地图可以将信号灯位置提供\n\n * 高精地图可以减小检测范围，在特定位置的附近检测相关信息即可（提供RoI）\n\n高精地图如何用于规划？\n\n * 帮助车辆识别中心线\n * 具有减速限速标志的区域，可以提前查看并预先减速\n * 如果需要变道，高精地图可以帮助其缩小选择范围以便选择最佳方案\n\nApollo 高精地图需要不断地更新，制定统一的高精地图标准有助于地图构建和维护\n\n\n\nApollo 高精地图构建\n\n * 维护车队维护和更新地图\n * 维护车队的传感器：GPS、Antenna、LiDAR、Camera",normalizedContent:"高精度地图的作用：定位、预先规划\n\n高精度地图最重要的特征就是精度，其精度会达到厘米级别\n\n道路网的精确三维表征\n\n * 交叉路口的布局以及路标位置\n\n * 交通灯上不同颜色的含义\n\n * 指示车道速度限制以及左转车道开始的位置\n\n高精地图如何用于定位？\n\n * 首先确定自己的大概位置，类似于解拼图问题的过程\n\n * 接着查找地标\n\n * 与地标比较，看是否匹配\n\n高精地图如何用于感知？\n\n * 在恶劣天气下，传感器的能力可能会受到影响\n\n * 如果由于遮挡未检测到信号灯，高精地图可以将信号灯位置提供\n\n * 高精地图可以减小检测范围，在特定位置的附近检测相关信息即可（提供roi）\n\n高精地图如何用于规划？\n\n * 帮助车辆识别中心线\n * 具有减速限速标志的区域，可以提前查看并预先减速\n * 如果需要变道，高精地图可以帮助其缩小选择范围以便选择最佳方案\n\napollo 高精地图需要不断地更新，制定统一的高精地图标准有助于地图构建和维护\n\n\n\napollo 高精地图构建\n\n * 维护车队维护和更新地图\n * 维护车队的传感器：gps、antenna、lidar、camera",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"感知",frontmatter:{title:"感知",date:"2021-09-04T22:05:32.000Z",permalink:"/pages/1d8a53/",categories:["学习笔记","系列笔记-Apollo自动驾驶"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/21.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Apollo%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/04.%E6%84%9F%E7%9F%A5.html",relativePath:"02.学习笔记/21.系列笔记-Apollo自动驾驶/04.感知.md",key:"v-97cae6c4",path:"/pages/1d8a53/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/09/05, 19:05:19"},{title:"预测",frontmatter:{title:"预测",date:"2021-09-04T22:05:49.000Z",permalink:"/pages/7dc507/",categories:["学习笔记","系列笔记-Apollo自动驾驶"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/21.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Apollo%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/05.%E9%A2%84%E6%B5%8B.html",relativePath:"02.学习笔记/21.系列笔记-Apollo自动驾驶/05.预测.md",key:"v-718e9ea6",path:"/pages/7dc507/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/09/05, 19:05:19"},{title:"规划",frontmatter:{title:"规划",date:"2021-09-04T22:05:56.000Z",permalink:"/pages/8beaa5/",categories:["学习笔记","系列笔记-Apollo自动驾驶"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/21.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Apollo%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/06.%E8%A7%84%E5%88%92.html",relativePath:"02.学习笔记/21.系列笔记-Apollo自动驾驶/06.规划.md",key:"v-24a8bf61",path:"/pages/8beaa5/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/09/05, 19:05:19"},{title:"控制",frontmatter:{title:"控制",date:"2021-09-04T22:06:02.000Z",permalink:"/pages/b43d07/",categories:["学习笔记","系列笔记-Apollo自动驾驶"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/21.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Apollo%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/07.%E6%8E%A7%E5%88%B6.html",relativePath:"02.学习笔记/21.系列笔记-Apollo自动驾驶/07.控制.md",key:"v-4ee69ba8",path:"/pages/b43d07/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/09/05, 19:05:19"},{title:"整体评分",frontmatter:{title:"整体评分",date:"2021-08-14T15:05:06.000Z",permalink:"/pages/0ba4e9/",categories:["学习笔记","系列笔记-PaddlePaddle"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/22.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-PaddlePaddle/00.%E6%95%B4%E4%BD%93%E8%AF%84%E5%88%86.html",relativePath:"02.学习笔记/22.系列笔记-PaddlePaddle/00.整体评分.md",key:"v-47f5d7b3",path:"/pages/0ba4e9/",headers:[{level:2,title:"服务器部署 — Paddle Inference",slug:"服务器部署-paddle-inference",normalizedTitle:"服务器部署 — paddle inference",charIndex:2},{level:2,title:"移动端/嵌入式部署 — Paddle Lite",slug:"移动端-嵌入式部署-paddle-lite",normalizedTitle:"移动端/嵌入式部署 — paddle lite",charIndex:1237},{level:2,title:"模型压缩 — PaddleSlim",slug:"模型压缩-paddleslim",normalizedTitle:"模型压缩 — paddleslim",charIndex:2021},{level:2,title:"分布式训练快速开始",slug:"分布式训练快速开始",normalizedTitle:"分布式训练快速开始",charIndex:4402},{level:2,title:"使用FleetAPI进行分布式训练",slug:"使用fleetapi进行分布式训练",normalizedTitle:"使用fleetapi进行分布式训练",charIndex:5386}],headersStr:"服务器部署 — Paddle Inference 移动端/嵌入式部署 — Paddle Lite 模型压缩 — PaddleSlim 分布式训练快速开始 使用FleetAPI进行分布式训练",content:"# 服务器部署 — Paddle Inference\n\n# 0、链接：服务器部署 — Paddle Inference\n\n# 1、整体评分\n\n内容对你了解 Paddle Inference 的帮助程度\n\n4分；基本上能有了解，\n\n内容对你使用 Paddle Inference 的帮助程度\n\n4分；使用起来基本没问题，也能够从 python示例 处上手，但在使用过程中发现三个问题，描述如下\n\n(1) 如下图所示，此处的解压命令错误，应当为 tar zxf resnet50.tgz\n\n\n\n(2) 当我运行 python_demo 程序后，得到的预测输出结果 output data size 与 output data shape与官方文档不一致\n\n\n\n\n\n(3) 希望在 预测程序开发环节 也能有对应直接能上手的 Demo 代码\n\n# 2、导航&标题\n\n导航目录和分类的清晰合理，便于查找\n\n4分；较为清晰\n\n文档标题是否清晰准确易懂\n\n4分；较为清晰\n\n# 3、文档内容\n\n语言表达的易读程度\n\n4分；比较容易理解，但有一些可以改进的地方，整理如下\n\n(1) 如下图所示，“同时对飞腾、鲲鹏、曙光、昆仑等国产CPU/NPU进行适配。。（多了一个句号）\n\n\n\n(2) 如下图所示，Paddle Inference 大小写不一致，中英文夹杂的情况下空格不一致\n\n\n\n内容中背景知识的覆盖程度\n\n4分；基本介绍了相关背景知识，流程图非常清晰，希望在 \"主框架model.predict区别\"这一小节也能有可视化（图片或者表格）的解释，这样会更加一目了然\n\n内容的内部段落结构的合理程度\n\n4分；较为合理\n\n操作过程的可指导性，步骤间连贯性\n\n3分；在我尝试想要上手使用 Paddle Inference 的时候，我个人应该是觉得要首先上来看 Inference Demo 的，但是当我进入 Inference Demo 仓库时，是建议我对 Paddle Inference 文档做了解，我个人觉得可以减少跳转，给用户一个顺序的指导，而不是直接给出几个常见的文档链接，不做引导\n\n * 对于刚刚接触的同学：直接去看文档的 快速上手 部分，这部分内容写的比较清晰。\n * 对于有了解的同学：可以去 Demo 仓库 ，但是仓库的 README 写的不足够清晰，对于使用样例没有跳转链接\n\n\n\n内容根据最新技术的更新程度\n\n4分；\n\n# 4、排版\n\n内容排版层次分明、重点突出\n\n4分；预测流程图中的文字与文档的字体大小差异过大，建议做成能够双击放大的形式，或者对不同的阅读场景做适配，或者参照 Paddle Lite--架构 的图做调整\n\n\n\n# 5、整体评价\n\n3.5分；整理阅读下来对于领域内的人应该能够比较清晰的了解，但仍有可以改进的地方。文档中 快速上手 的案例与代码仓库 中的 Demo 对应起来，并且代码仓库中的 README 比较不详细，有很大的改进空间。\n\n\n# 移动端/嵌入式部署 — Paddle Lite\n\n# 0、链接：移动端/嵌入式部署 — Paddle Lite\n\n# 1、整体评分\n\n内容对你了解XXX的帮助程度\n\n4分；基本上能看懂理解，有以下一个问题可以改进：\n\n(1) 使用教程 和 Paddle-Lite 中描述不一致，缺少 PyTorch 的支持，基于 PyTorch 庞大的用户群，我觉得支持 PyTorch 模型是很好的特性，可以突出一下。此外，使用教程中也有 markdown 渲染不成功的问题，多了一个空格。\n\n\n\n\n\n内容对你使用XXX的帮助程度\n\n4分；使用起来基本没问题，有以下两个方面可以改进：\n\n(1) 示例程序这里希望直接把关键代码贴出来（或者放到 baidu 的服务器上下载），而不是要去 GitHub 下载，去 GitHub 上需要 clone 或者复制代码，由于网速等问题会降低用户的使用好感。除此之外，下图有一个错别字，应为“优化前”。\n\n\n\n(2) 希望在 Python 应用开发说明 也能有对应直接能上手的 Demo 代码\n\n# 2、导航&标题\n\n导航目录和分类的清晰合理，便于查找\n\n4分；较为清晰\n\n(1) 小杠一下，我觉得 tag 这里可以统一改成英文 (强迫症)\n\n\n\n文档标题是否清晰准确易懂\n\n4分；较为清晰\n\n# 3、文档内容\n\n语言表达的易读程度\n\n4分；较为容易理解\n\n内容中背景知识的覆盖程度\n\n4分；基本介绍了相关背景知识\n\n内容的内部段落结构的合理程度\n\n4分；较为合理\n\n操作过程的可指导性，步骤间连贯性\n\n4分；可以从用户的角度让大家先上手示例程序，可以给个引导\n\n内容根据最新技术的更新程度\n\n4分；\n\n# 4、排版\n\n内容排版层次分明、重点突出\n\n4分；\n\n# 5、整体评价\n\n4分；文档较为清晰，排版也没有太大的问题，能够较好的上手\n\n\n# 模型压缩 — PaddleSlim\n\n# 0、链接：模型压缩 — PaddleSlim\n\n# 1、整体评分\n\n内容对你了解XXX的帮助程度\n\n4分；基本上能看懂理解\n\n内容对你使用XXX的帮助程度\n\n2分；使用起来出了很多 bug，猜测是重构之后有很多遗留问题，或许代码上已经修复，但是文档中的代码没有更新\n\n1、动态图\n\n(1) 卷积 Filter 剪裁\n\n在 5.2 对模型进行微调 小节报出维度不匹配：ValueError: (InvalidArgument) Param and Velocity of MomentumOp should have the same dimension，所以还是希望能够给一个 Demo 代码\n\n\n\n\n\n(2) 量化训练\n\n在 5. 导出预测模型 小节导出预测模型时报错：AttributeError: 'Assign' object has no attribute 'type_comment'，希望能够给出一个 Demo 代码\n\n\n\n(3) 离线量化\n\n在 3. 进行预训练 小节训练完成后导出预测模型时报错：AttributeError: 'Assign' object has no attribute 'type_comment'，希望能够给出一个 Demo 代码\n\n(4) 发现同样问题\n\n我在 paddlepaddle 仓库的 issue 以及 paddleslim 的 issue 也发现同样的问题，应该是bug，希望及时修复，不影响文档的使用，\n\n2、静态图\n\n(1) 卷积 Filter 剪裁\n\n报错：InvalidArgumentError: The start row index must be less than the end row index.But received the start index = 0, the end index = 0.\n\n(2) 知识蒸馏\n\n能够顺利运行，但是 loss 非常大\n\n\n\n(3) 量化训练\n\n在 保存量化后的模型 小节报错：NameError: name 'float_prog' is not defined\n\n(4) 离线量化\n\n无问题，顺利运行\n\n(5) 网络结构搜索\n\n无法顺利运行：8. 完整示例中报以下错\n\n * File \"static_nas.py\", line 87, in <module>\n       train_loader, eval_loader = input_data(inputs)\n   TypeError: input_data() missing 1 required positional argument: 'label'\n   \n   \n   1\n   2\n   3\n   \n\n修改为以下代码后依然无法运行\n\n * exe, train_program, eval_program, (image, label), avg_cost, acc_top1, acc_top5 = build_program(archs)\n   train_loader, eval_loader = input_data(image, label)\n   \n   \n   1\n   2\n   \n\n报错为：InvalidArgumentError: If Attr(soft_label) == false, the axis dimension of Input(Label) should be 1\n\n3、如何复现\n\n本地 Python 环境：\n\npaddlehub                  1.8.2\npaddlelite                 2.8\npaddlepaddle               2.0.2\npaddlepaddle-gpu           2.1.1\npaddleseg                  2.1.0\npaddleslim                 2.1.0\npaddlex                    2.0.0rc4\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n本地硬件环境：\n\n * GPU: TiTAN RTX\n * Driver API Version: 11.2, Runtime API Version: 10.2\n * cuDNN Version: 7.6.\n\n我复现代码的链接：https://github.com/Muyun99/PaddleSlim_demo\n\n# 2、导航&标题\n\n导航目录和分类的清晰合理，便于查找\n\n4分；较为清晰\n\n文档标题是否清晰准确易懂\n\n4分；较为清晰，加上了压缩效果的示例非常有吸引力\n\n# 3、文档内容\n\n语言表达的易读程度\n\n4分；比较容易理解\n\n内容中背景知识的覆盖程度\n\n4分；基本介绍了相关背景知识\n\n内容的内部段落结构的合理程度\n\n4分；\n\n操作过程的可指导性，步骤间连贯性\n\n4分；\n\n内容根据最新技术的更新程度\n\n2分；看起来很多代码都没有更新，出现了很多 bug\n\n# 4、排版\n\n内容排版层次分明、重点突出\n\n4分；\n\n# 5、整体评价\n\n2分；本来开始看文档的时候我觉得很好用，有性能对比，快速上手部分也有相当多的注释，但是代码质量堪忧，我个人跑示例的过程中出现了很多bug，希望工程师能够尽快更新。\n\n这是我复现文档代码的 github 仓库：https://github.com/Muyun99/PaddleSlim_demo，希望工程师看下是代码的问题还是环境不匹配，因为我所有代码都是从文档上复制过来的，有什么问题可以提 issue\n\n\n# 分布式训练快速开始\n\n# 0、链接：分布式训练快速开始\n\n# 1、整体评分\n\n内容对你了解XXX的帮助程度\n\n3分；未给出 FleetX 的官方文档链接，给出的上手代码不完整\n\n内容对你使用XXX的帮助程度\n\n3分；使用起来基本没问题\n\n(1) 1.3 和 1.4 代码不完整\n\n1.3 小节中有 from resnet_dygraph import ResNet，可以从 resnet_dygraph 链接中找到\n\n1.4 小节中有 import resnet_static as resnet ，可以从 resnet_static 链接中找到\n\n文档中未给出相应的代码链接，不完整\n\n(2) 1.5 运行示例都会报错\n\n以下两条语句都会报错\n\n * python3 -m paddle.distributed.launch --gpus=0,1 train_fleet_dygraph.py\n\n * python3 -m paddle.distributed.launch --gpus=0,1 train_fleet_static.py\n\n参考 运行示例，应当改为：\n\n * fleetrun --gpus=0,1 train_fleet_dygraph.py\n * fleetrun --gpus=0,1 train_fleet_static.py\n\n其中静态图依旧运行失败，动态图代码可以运行但是 loss 降不下来\n\n# 2、导航&标题\n\n导航目录和分类的清晰合理，便于查找\n\n4分；目录较为清晰\n\n文档标题是否清晰准确易懂\n\n4分；标题较为清晰\n\n# 3、文档内容\n\n语言表达的易读程度\n\n4分；比较容易理解\n\n内容中背景知识的覆盖程度\n\n3分；未介绍相关背景知识，直接就上手快速开始了\n\n内容的内部段落结构的合理程度\n\n4分；\n\n操作过程的可指导性，步骤间连贯性\n\n2分；代码不完整，运行语句存在bug\n\n内容根据最新技术的更新程度\n\n3分，猜测是未更新文档\n\n# 4、排版\n\n内容排版层次分明、重点突出\n\n4分\n\n# 5、整体评价\n\n2分，感觉飞桨官网 FleetX 文档和 FleetX 文档 是分开维护的，其代码和 README 并没有同步。导致代码不完整以及运行语句都不一致，并且飞桨官网并没有放 FleetX 文档链接，使用体验较差\n\n\n# 使用FleetAPI进行分布式训练\n\n# 0、链接：使用FleetAPI进行分布式训练\n\n# 1、整体评分\n\n内容对你了解XXX的帮助程度\n\n3分；对于不了解的人来讲未给出 FleetX 的官方文档链接，没有详细的背景介绍\n\n内容对你使用XXX的帮助程度\n\n3分；给出的快速上手示例代码使用有问题\n\n# 2、导航&标题\n\n导航目录和分类的清晰合理，便于查找\n\n4分；\n\n文档标题是否清晰准确易懂\n\n4分；\n\n# 3、文档内容\n\n语言表达的易读程度\n\n4分；比较容易理解\n\n内容中背景知识的覆盖程度\n\n3分；未介绍相关背景知识，直接就上手快速开始了\n\n内容的内部段落结构的合理程度\n\n4分；\n\n操作过程的可指导性，步骤间连贯性\n\n3分；代码运行语句有误，没有示例 py 文件下载途径，也没有相关的 GitHub 仓库链接\n\n内容根据最新技术的更新程度\n\n4分；\n\n# 4、排版\n\n内容排版层次分明、重点突出\n\n4分；\n\n# 5、整体评价\n\n3分，背景介绍太过简单，并且代码对于动态图和静态图报错，建议仔细检查，统一文档格式和代码示例的方式",normalizedContent:"# 服务器部署 — paddle inference\n\n# 0、链接：服务器部署 — paddle inference\n\n# 1、整体评分\n\n内容对你了解 paddle inference 的帮助程度\n\n4分；基本上能有了解，\n\n内容对你使用 paddle inference 的帮助程度\n\n4分；使用起来基本没问题，也能够从 python示例 处上手，但在使用过程中发现三个问题，描述如下\n\n(1) 如下图所示，此处的解压命令错误，应当为 tar zxf resnet50.tgz\n\n\n\n(2) 当我运行 python_demo 程序后，得到的预测输出结果 output data size 与 output data shape与官方文档不一致\n\n\n\n\n\n(3) 希望在 预测程序开发环节 也能有对应直接能上手的 demo 代码\n\n# 2、导航&标题\n\n导航目录和分类的清晰合理，便于查找\n\n4分；较为清晰\n\n文档标题是否清晰准确易懂\n\n4分；较为清晰\n\n# 3、文档内容\n\n语言表达的易读程度\n\n4分；比较容易理解，但有一些可以改进的地方，整理如下\n\n(1) 如下图所示，“同时对飞腾、鲲鹏、曙光、昆仑等国产cpu/npu进行适配。。（多了一个句号）\n\n\n\n(2) 如下图所示，paddle inference 大小写不一致，中英文夹杂的情况下空格不一致\n\n\n\n内容中背景知识的覆盖程度\n\n4分；基本介绍了相关背景知识，流程图非常清晰，希望在 \"主框架model.predict区别\"这一小节也能有可视化（图片或者表格）的解释，这样会更加一目了然\n\n内容的内部段落结构的合理程度\n\n4分；较为合理\n\n操作过程的可指导性，步骤间连贯性\n\n3分；在我尝试想要上手使用 paddle inference 的时候，我个人应该是觉得要首先上来看 inference demo 的，但是当我进入 inference demo 仓库时，是建议我对 paddle inference 文档做了解，我个人觉得可以减少跳转，给用户一个顺序的指导，而不是直接给出几个常见的文档链接，不做引导\n\n * 对于刚刚接触的同学：直接去看文档的 快速上手 部分，这部分内容写的比较清晰。\n * 对于有了解的同学：可以去 demo 仓库 ，但是仓库的 readme 写的不足够清晰，对于使用样例没有跳转链接\n\n\n\n内容根据最新技术的更新程度\n\n4分；\n\n# 4、排版\n\n内容排版层次分明、重点突出\n\n4分；预测流程图中的文字与文档的字体大小差异过大，建议做成能够双击放大的形式，或者对不同的阅读场景做适配，或者参照 paddle lite--架构 的图做调整\n\n\n\n# 5、整体评价\n\n3.5分；整理阅读下来对于领域内的人应该能够比较清晰的了解，但仍有可以改进的地方。文档中 快速上手 的案例与代码仓库 中的 demo 对应起来，并且代码仓库中的 readme 比较不详细，有很大的改进空间。\n\n\n# 移动端/嵌入式部署 — paddle lite\n\n# 0、链接：移动端/嵌入式部署 — paddle lite\n\n# 1、整体评分\n\n内容对你了解xxx的帮助程度\n\n4分；基本上能看懂理解，有以下一个问题可以改进：\n\n(1) 使用教程 和 paddle-lite 中描述不一致，缺少 pytorch 的支持，基于 pytorch 庞大的用户群，我觉得支持 pytorch 模型是很好的特性，可以突出一下。此外，使用教程中也有 markdown 渲染不成功的问题，多了一个空格。\n\n\n\n\n\n内容对你使用xxx的帮助程度\n\n4分；使用起来基本没问题，有以下两个方面可以改进：\n\n(1) 示例程序这里希望直接把关键代码贴出来（或者放到 baidu 的服务器上下载），而不是要去 github 下载，去 github 上需要 clone 或者复制代码，由于网速等问题会降低用户的使用好感。除此之外，下图有一个错别字，应为“优化前”。\n\n\n\n(2) 希望在 python 应用开发说明 也能有对应直接能上手的 demo 代码\n\n# 2、导航&标题\n\n导航目录和分类的清晰合理，便于查找\n\n4分；较为清晰\n\n(1) 小杠一下，我觉得 tag 这里可以统一改成英文 (强迫症)\n\n\n\n文档标题是否清晰准确易懂\n\n4分；较为清晰\n\n# 3、文档内容\n\n语言表达的易读程度\n\n4分；较为容易理解\n\n内容中背景知识的覆盖程度\n\n4分；基本介绍了相关背景知识\n\n内容的内部段落结构的合理程度\n\n4分；较为合理\n\n操作过程的可指导性，步骤间连贯性\n\n4分；可以从用户的角度让大家先上手示例程序，可以给个引导\n\n内容根据最新技术的更新程度\n\n4分；\n\n# 4、排版\n\n内容排版层次分明、重点突出\n\n4分；\n\n# 5、整体评价\n\n4分；文档较为清晰，排版也没有太大的问题，能够较好的上手\n\n\n# 模型压缩 — paddleslim\n\n# 0、链接：模型压缩 — paddleslim\n\n# 1、整体评分\n\n内容对你了解xxx的帮助程度\n\n4分；基本上能看懂理解\n\n内容对你使用xxx的帮助程度\n\n2分；使用起来出了很多 bug，猜测是重构之后有很多遗留问题，或许代码上已经修复，但是文档中的代码没有更新\n\n1、动态图\n\n(1) 卷积 filter 剪裁\n\n在 5.2 对模型进行微调 小节报出维度不匹配：valueerror: (invalidargument) param and velocity of momentumop should have the same dimension，所以还是希望能够给一个 demo 代码\n\n\n\n\n\n(2) 量化训练\n\n在 5. 导出预测模型 小节导出预测模型时报错：attributeerror: 'assign' object has no attribute 'type_comment'，希望能够给出一个 demo 代码\n\n\n\n(3) 离线量化\n\n在 3. 进行预训练 小节训练完成后导出预测模型时报错：attributeerror: 'assign' object has no attribute 'type_comment'，希望能够给出一个 demo 代码\n\n(4) 发现同样问题\n\n我在 paddlepaddle 仓库的 issue 以及 paddleslim 的 issue 也发现同样的问题，应该是bug，希望及时修复，不影响文档的使用，\n\n2、静态图\n\n(1) 卷积 filter 剪裁\n\n报错：invalidargumenterror: the start row index must be less than the end row index.but received the start index = 0, the end index = 0.\n\n(2) 知识蒸馏\n\n能够顺利运行，但是 loss 非常大\n\n\n\n(3) 量化训练\n\n在 保存量化后的模型 小节报错：nameerror: name 'float_prog' is not defined\n\n(4) 离线量化\n\n无问题，顺利运行\n\n(5) 网络结构搜索\n\n无法顺利运行：8. 完整示例中报以下错\n\n * file \"static_nas.py\", line 87, in <module>\n       train_loader, eval_loader = input_data(inputs)\n   typeerror: input_data() missing 1 required positional argument: 'label'\n   \n   \n   1\n   2\n   3\n   \n\n修改为以下代码后依然无法运行\n\n * exe, train_program, eval_program, (image, label), avg_cost, acc_top1, acc_top5 = build_program(archs)\n   train_loader, eval_loader = input_data(image, label)\n   \n   \n   1\n   2\n   \n\n报错为：invalidargumenterror: if attr(soft_label) == false, the axis dimension of input(label) should be 1\n\n3、如何复现\n\n本地 python 环境：\n\npaddlehub                  1.8.2\npaddlelite                 2.8\npaddlepaddle               2.0.2\npaddlepaddle-gpu           2.1.1\npaddleseg                  2.1.0\npaddleslim                 2.1.0\npaddlex                    2.0.0rc4\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n本地硬件环境：\n\n * gpu: titan rtx\n * driver api version: 11.2, runtime api version: 10.2\n * cudnn version: 7.6.\n\n我复现代码的链接：https://github.com/muyun99/paddleslim_demo\n\n# 2、导航&标题\n\n导航目录和分类的清晰合理，便于查找\n\n4分；较为清晰\n\n文档标题是否清晰准确易懂\n\n4分；较为清晰，加上了压缩效果的示例非常有吸引力\n\n# 3、文档内容\n\n语言表达的易读程度\n\n4分；比较容易理解\n\n内容中背景知识的覆盖程度\n\n4分；基本介绍了相关背景知识\n\n内容的内部段落结构的合理程度\n\n4分；\n\n操作过程的可指导性，步骤间连贯性\n\n4分；\n\n内容根据最新技术的更新程度\n\n2分；看起来很多代码都没有更新，出现了很多 bug\n\n# 4、排版\n\n内容排版层次分明、重点突出\n\n4分；\n\n# 5、整体评价\n\n2分；本来开始看文档的时候我觉得很好用，有性能对比，快速上手部分也有相当多的注释，但是代码质量堪忧，我个人跑示例的过程中出现了很多bug，希望工程师能够尽快更新。\n\n这是我复现文档代码的 github 仓库：https://github.com/muyun99/paddleslim_demo，希望工程师看下是代码的问题还是环境不匹配，因为我所有代码都是从文档上复制过来的，有什么问题可以提 issue\n\n\n# 分布式训练快速开始\n\n# 0、链接：分布式训练快速开始\n\n# 1、整体评分\n\n内容对你了解xxx的帮助程度\n\n3分；未给出 fleetx 的官方文档链接，给出的上手代码不完整\n\n内容对你使用xxx的帮助程度\n\n3分；使用起来基本没问题\n\n(1) 1.3 和 1.4 代码不完整\n\n1.3 小节中有 from resnet_dygraph import resnet，可以从 resnet_dygraph 链接中找到\n\n1.4 小节中有 import resnet_static as resnet ，可以从 resnet_static 链接中找到\n\n文档中未给出相应的代码链接，不完整\n\n(2) 1.5 运行示例都会报错\n\n以下两条语句都会报错\n\n * python3 -m paddle.distributed.launch --gpus=0,1 train_fleet_dygraph.py\n\n * python3 -m paddle.distributed.launch --gpus=0,1 train_fleet_static.py\n\n参考 运行示例，应当改为：\n\n * fleetrun --gpus=0,1 train_fleet_dygraph.py\n * fleetrun --gpus=0,1 train_fleet_static.py\n\n其中静态图依旧运行失败，动态图代码可以运行但是 loss 降不下来\n\n# 2、导航&标题\n\n导航目录和分类的清晰合理，便于查找\n\n4分；目录较为清晰\n\n文档标题是否清晰准确易懂\n\n4分；标题较为清晰\n\n# 3、文档内容\n\n语言表达的易读程度\n\n4分；比较容易理解\n\n内容中背景知识的覆盖程度\n\n3分；未介绍相关背景知识，直接就上手快速开始了\n\n内容的内部段落结构的合理程度\n\n4分；\n\n操作过程的可指导性，步骤间连贯性\n\n2分；代码不完整，运行语句存在bug\n\n内容根据最新技术的更新程度\n\n3分，猜测是未更新文档\n\n# 4、排版\n\n内容排版层次分明、重点突出\n\n4分\n\n# 5、整体评价\n\n2分，感觉飞桨官网 fleetx 文档和 fleetx 文档 是分开维护的，其代码和 readme 并没有同步。导致代码不完整以及运行语句都不一致，并且飞桨官网并没有放 fleetx 文档链接，使用体验较差\n\n\n# 使用fleetapi进行分布式训练\n\n# 0、链接：使用fleetapi进行分布式训练\n\n# 1、整体评分\n\n内容对你了解xxx的帮助程度\n\n3分；对于不了解的人来讲未给出 fleetx 的官方文档链接，没有详细的背景介绍\n\n内容对你使用xxx的帮助程度\n\n3分；给出的快速上手示例代码使用有问题\n\n# 2、导航&标题\n\n导航目录和分类的清晰合理，便于查找\n\n4分；\n\n文档标题是否清晰准确易懂\n\n4分；\n\n# 3、文档内容\n\n语言表达的易读程度\n\n4分；比较容易理解\n\n内容中背景知识的覆盖程度\n\n3分；未介绍相关背景知识，直接就上手快速开始了\n\n内容的内部段落结构的合理程度\n\n4分；\n\n操作过程的可指导性，步骤间连贯性\n\n3分；代码运行语句有误，没有示例 py 文件下载途径，也没有相关的 github 仓库链接\n\n内容根据最新技术的更新程度\n\n4分；\n\n# 4、排版\n\n内容排版层次分明、重点突出\n\n4分；\n\n# 5、整体评价\n\n3分，背景介绍太过简单，并且代码对于动态图和静态图报错，建议仔细检查，统一文档格式和代码示例的方式",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"极大似然函数",frontmatter:{title:"极大似然函数",date:"2021-09-14T14:46:13.000Z",permalink:"/pages/b4bbc7/",categories:["学习笔记","深度学习及机器学习理论知识学习笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/25.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/00.%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0.html",relativePath:"02.学习笔记/25.深度学习及机器学习理论知识学习笔记/00.极大似然函数.md",key:"v-c123a972",path:"/pages/b4bbc7/",headers:[{level:2,title:"极大似然估计",slug:"极大似然估计",normalizedTitle:"极大似然估计",charIndex:2}],headersStr:"极大似然估计",content:"# 极大似然估计\n\n# 0 前言\n\n这是高数下册里面学到的知识，我印象中当时是比较简单的，做题的话套公式就可以了，但是在近期和一位师妹的学习沟通过程中，她问到了我很多细节，例如为什么在全连接层后面要用 softmax 函数呢？为什么 softmax 函数需要用指数的形式呢？我发现我检索到的很多回答很多都用到了极大似然估计的内容，好好再回顾以下相关的知识，记录于此。\n\n值得注意的是，我们要把动机和优点区分开。动机是做这件事情的目的，优点是在完成这个目的时各个工具的优劣性，这两者不能混淆。例如：\n\n * 在全连接层后使用 softmax 的动机是为了将全连接层的输出映射成一个概率分布。\n * 那为什么现在常用的是 softmax 呢，其他的映射函数是否可以呢？\n * softmax 这么常用，其优点又是什么呢？\n * 在什么样的场合我们可以使用其他的映射函数呢？\n\n私以为，这确实是一个研究生应当弄清楚的深度学习的理论基础，这有助于我们在今后撰写代码时不仅仅是网格搜索式地调参及改模型，也感谢我的师妹给我提了个醒，和我沟通确实有助于我进行知识的查漏补缺，让基础更加扎实一些。\n\n# 01、什么是极大似然估计？\n\n极大似然估计（Maximum Likelihood Estimate, MLE），是参数估计的一种方法。\n\n * 参数估计的解释是：已知某个随机样本满足某种概率分布，但其中的参数不确定。参数估计就是通过若干次实验，观察其结果，利用结果推出参数的大概值\n * 极大似然估计的思想：已知某组参数能够使得这个样本出现的概率最大，我们就将该组概率作为该组估计的真实值。\n * 极大似然估计的优点\n   * 渐进正确性：随着样本量的增加，估计值会最终趋向于真实值\n   * 渐进正态性：估计的抽样分布服从正态分布\n   * 有效性：极大似然估计在所有无偏估计中具有最小方差\n * 极大似然估计的局限\n   * 是一种粗略的数学期望\n   * 一旦使用极大似然估计法，数据的产生过程必须严格完整地被假定并且描述，这意味着估计者需要对数据的产生过程有着较深的理解\n   * 极大似然估计一般不太适用于包含理性预期的结构模型，这类模型中的似然函数通常高度非线性化，这使得模型的估计应为搜索全局最优而变得及其困难复杂\n\n# 02、如何计算极大似然估计呢？\n\n求解极大似然函数估计的一般步骤，在印象中这也是高数里面的解题方法，套路性非常强，当时以为是送分题（\n\n * 写出似然函数\n * 对似然函数取对数并整理\n * 求导数\n * 解似然方程\n\n那么我们来解一道考研例题吧！\n\n\n\n似然：理念世界和现实世界\n\n似然值：从真实世界估计的参数下的真实事件发生的概率\n\n神经网络本质上就是计算神经网络里面的概率模型的似然值，找到那个极大似然值所对应的概率模型，应该就是最接近现实情况的那个概率模型",normalizedContent:"# 极大似然估计\n\n# 0 前言\n\n这是高数下册里面学到的知识，我印象中当时是比较简单的，做题的话套公式就可以了，但是在近期和一位师妹的学习沟通过程中，她问到了我很多细节，例如为什么在全连接层后面要用 softmax 函数呢？为什么 softmax 函数需要用指数的形式呢？我发现我检索到的很多回答很多都用到了极大似然估计的内容，好好再回顾以下相关的知识，记录于此。\n\n值得注意的是，我们要把动机和优点区分开。动机是做这件事情的目的，优点是在完成这个目的时各个工具的优劣性，这两者不能混淆。例如：\n\n * 在全连接层后使用 softmax 的动机是为了将全连接层的输出映射成一个概率分布。\n * 那为什么现在常用的是 softmax 呢，其他的映射函数是否可以呢？\n * softmax 这么常用，其优点又是什么呢？\n * 在什么样的场合我们可以使用其他的映射函数呢？\n\n私以为，这确实是一个研究生应当弄清楚的深度学习的理论基础，这有助于我们在今后撰写代码时不仅仅是网格搜索式地调参及改模型，也感谢我的师妹给我提了个醒，和我沟通确实有助于我进行知识的查漏补缺，让基础更加扎实一些。\n\n# 01、什么是极大似然估计？\n\n极大似然估计（maximum likelihood estimate, mle），是参数估计的一种方法。\n\n * 参数估计的解释是：已知某个随机样本满足某种概率分布，但其中的参数不确定。参数估计就是通过若干次实验，观察其结果，利用结果推出参数的大概值\n * 极大似然估计的思想：已知某组参数能够使得这个样本出现的概率最大，我们就将该组概率作为该组估计的真实值。\n * 极大似然估计的优点\n   * 渐进正确性：随着样本量的增加，估计值会最终趋向于真实值\n   * 渐进正态性：估计的抽样分布服从正态分布\n   * 有效性：极大似然估计在所有无偏估计中具有最小方差\n * 极大似然估计的局限\n   * 是一种粗略的数学期望\n   * 一旦使用极大似然估计法，数据的产生过程必须严格完整地被假定并且描述，这意味着估计者需要对数据的产生过程有着较深的理解\n   * 极大似然估计一般不太适用于包含理性预期的结构模型，这类模型中的似然函数通常高度非线性化，这使得模型的估计应为搜索全局最优而变得及其困难复杂\n\n# 02、如何计算极大似然估计呢？\n\n求解极大似然函数估计的一般步骤，在印象中这也是高数里面的解题方法，套路性非常强，当时以为是送分题（\n\n * 写出似然函数\n * 对似然函数取对数并整理\n * 求导数\n * 解似然方程\n\n那么我们来解一道考研例题吧！\n\n\n\n似然：理念世界和现实世界\n\n似然值：从真实世界估计的参数下的真实事件发生的概率\n\n神经网络本质上就是计算神经网络里面的概率模型的似然值，找到那个极大似然值所对应的概率模型，应该就是最接近现实情况的那个概率模型",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"ffmpeg 库的使用",frontmatter:{title:"ffmpeg 库的使用",date:"2021-08-19T22:29:54.000Z",permalink:"/pages/0f2c42/",categories:["学习笔记","系列笔记-视频操作"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/23.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-%E8%A7%86%E9%A2%91%E6%93%8D%E4%BD%9C/00.ffmpeg%20%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8.html",relativePath:"02.学习笔记/23.系列笔记-视频操作/00.ffmpeg 库的使用.md",key:"v-7181a7c3",path:"/pages/0f2c42/",headersStr:null,content:"# 参考资料\n\n高效的抽帧工具：https://bbs.huaweicloud.com/blogs/242453",normalizedContent:"# 参考资料\n\n高效的抽帧工具：https://bbs.huaweicloud.com/blogs/242453",charsets:{cjk:!0},lastUpdated:"2021/08/29, 21:59:07"},{title:"逻辑回归与sigmoid",frontmatter:{title:"逻辑回归与sigmoid",date:"2021-09-14T14:46:30.000Z",permalink:"/pages/fbd258/",categories:["学习笔记","深度学习及机器学习理论知识学习笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/25.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/01.%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8Esigmoid.html",relativePath:"02.学习笔记/25.深度学习及机器学习理论知识学习笔记/01.逻辑回归与sigmoid.md",key:"v-502a83ef",path:"/pages/fbd258/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2023/03/25, 19:58:09"},{title:"softmax与交叉熵",frontmatter:{title:"softmax与交叉熵",date:"2021-09-14T14:46:29.000Z",permalink:"/pages/bc5b38/",categories:["学习笔记","深度学习及机器学习理论知识学习笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/25.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02.softmax%E4%B8%8E%E4%BA%A4%E5%8F%89%E7%86%B5.html",relativePath:"02.学习笔记/25.深度学习及机器学习理论知识学习笔记/02.softmax与交叉熵.md",key:"v-4af706ce",path:"/pages/bc5b38/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/09/26, 00:09:41"},{title:"定位",frontmatter:{title:"定位",date:"2021-09-04T22:04:44.000Z",permalink:"/pages/638dfe/",categories:["学习笔记","系列笔记-Apollo自动驾驶"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/21.%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0-Apollo%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/03.%E5%AE%9A%E4%BD%8D.html",relativePath:"02.学习笔记/21.系列笔记-Apollo自动驾驶/03.定位.md",key:"v-6fd27afe",path:"/pages/638dfe/",headers:[{level:2,title:"定位",slug:"定位",normalizedTitle:"定位",charIndex:2}],headersStr:"定位",content:"# 定位",normalizedContent:"# 定位",charsets:{cjk:!0},lastUpdated:"2021/09/12, 20:42:58"},{title:"矩估计",frontmatter:{title:"矩估计",date:"2021-09-14T15:15:50.000Z",permalink:"/pages/38d46e/",categories:["学习笔记","深度学习及机器学习理论知识学习笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/25.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/03.%E7%9F%A9%E4%BC%B0%E8%AE%A1.html",relativePath:"02.学习笔记/25.深度学习及机器学习理论知识学习笔记/03.矩估计.md",key:"v-38b4cc66",path:"/pages/38d46e/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/09/26, 00:09:41"},{title:"损失函数的前置知识",frontmatter:{title:"损失函数的前置知识",date:"2021-09-14T15:43:53.000Z",permalink:"/pages/e646c8/",categories:["学习笔记","深度学习及机器学习理论知识学习笔记"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/25.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/04.%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86.html",relativePath:"02.学习笔记/25.深度学习及机器学习理论知识学习笔记/04.损失函数的前置知识.md",key:"v-19930428",path:"/pages/e646c8/",headersStr:null,content:"三个基本思路（其实感觉不是很准确哈）\n\n最小二乘法：用于计算损失值\n\n极大似然估计法：用于参数估计\n\n交叉熵：用于衡量信息量之间的差异\n\n如何定量地去衡量真实参数和所估计参数之间的差异？\n\n * \n\n为什么要用交叉熵？\n\n * 因为模型分布可能是异构的，一个分布是高斯分布，一个分布是泊松分布。\n * 异构的概率分布无法直接比较其差异，同构的概率分布可以比较其参数\n\n# 信息量（Information）\n\n如何计算信息量：\n\n * 定义：I(x)=−log(p(x))=log1p(x)I(x) = -log(p(x)) = log \\frac{1}{p(x)}I(x)=−log(p(x))=logp(x)1\n\n * 直观理解：刻画消除不确定性所需要的信息量，所发生的事情概率越大，所带来的信息量越小，反之其信息量越大\n\n * 用途：用于刻画某个事件的信息量\n\n# 熵（Entropy）\n\n熵既有热力学的概念，又有信息学的概念\n\n * 热力学概念：代表一个系统中的混乱程度\n * 信息学概念：用于衡量整体所带来的信息量的大小，也就是衡量一个系统消除不确定性的难度\n\n如何计算信息熵：\n\n * 定义：H(p):=E(Pf)H(p):=E(P_f)H(p):=E(Pf )，信息熵定义为对该系统的信息量求期望\n\n * 直观理解：一个系统消除不确定性的难度\n\n * 用途：衡量一个概率模型的不确定程度\n\n# 相对熵（KL 散度，KL Divergence）\n\n如何计算相对熵？\n\n * 定义：DKL(P∣∣Q):=H(p,q)−H(p)=∑1N[p(xi)log1p(xi)−p(xi)log1q(xi)]D_{KL}(P||Q):=H(p,q)-H(p) = \\sum_1^N[p(x_i)log\\frac{1}{p(x_i)}-p(x_i)log\\frac{1}{q(x_i)}]DKL (P∣∣Q):=H(p,q)−H(p)=∑1N [p(xi )logp(xi )1 −p(xi )logq(xi )1 ]$\n * 直观理解：如果分布 QQQ 想达到分布 PPP 的话，还差了多少信息量\n * 用途：衡量两个概率分布之间的差异\n * 特点\n   * 其不对称，分布 PPP 在前则是指以分布 PPP 当做基准\n   * 其大于等于0，当分布 PPP 与 QQQ 相等时等于0，不相等时大于0（吉布斯不等式证明）\n\n# 交叉熵（Cross Entropy）\n\n如何计算交叉熵？\n\n * 定义：H(p,q)=∑1Np(x)log1q(x)H(p,q) = \\sum_{1}^{N}p(x)log\\frac{1}{q(x)}H(p,q)=∑1N p(x)logq(x)1\n\n * 直观理解：想要让分布 QQQ 与 PPP 尽量地接近，就可以让 H(p，q)H(p，q)H(p，q) 尽量的小，所以 H(p,q)H(p,q)H(p,q) 本身即可作为损失函数，称之为交叉熵。交叉熵越小，代表两个概率模型间越相近\n\n * 用途：衡量两个概率分布直接的差异\n   \n   #\n\n参考资料\n\n * [1] “交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”",normalizedContent:"三个基本思路（其实感觉不是很准确哈）\n\n最小二乘法：用于计算损失值\n\n极大似然估计法：用于参数估计\n\n交叉熵：用于衡量信息量之间的差异\n\n如何定量地去衡量真实参数和所估计参数之间的差异？\n\n * \n\n为什么要用交叉熵？\n\n * 因为模型分布可能是异构的，一个分布是高斯分布，一个分布是泊松分布。\n * 异构的概率分布无法直接比较其差异，同构的概率分布可以比较其参数\n\n# 信息量（information）\n\n如何计算信息量：\n\n * 定义：i(x)=−log(p(x))=log1p(x)i(x) = -log(p(x)) = log \\frac{1}{p(x)}i(x)=−log(p(x))=logp(x)1\n\n * 直观理解：刻画消除不确定性所需要的信息量，所发生的事情概率越大，所带来的信息量越小，反之其信息量越大\n\n * 用途：用于刻画某个事件的信息量\n\n# 熵（entropy）\n\n熵既有热力学的概念，又有信息学的概念\n\n * 热力学概念：代表一个系统中的混乱程度\n * 信息学概念：用于衡量整体所带来的信息量的大小，也就是衡量一个系统消除不确定性的难度\n\n如何计算信息熵：\n\n * 定义：h(p):=e(pf)h(p):=e(p_f)h(p):=e(pf )，信息熵定义为对该系统的信息量求期望\n\n * 直观理解：一个系统消除不确定性的难度\n\n * 用途：衡量一个概率模型的不确定程度\n\n# 相对熵（kl 散度，kl divergence）\n\n如何计算相对熵？\n\n * 定义：dkl(p∣∣q):=h(p,q)−h(p)=∑1n[p(xi)log1p(xi)−p(xi)log1q(xi)]d_{kl}(p||q):=h(p,q)-h(p) = \\sum_1^n[p(x_i)log\\frac{1}{p(x_i)}-p(x_i)log\\frac{1}{q(x_i)}]dkl (p∣∣q):=h(p,q)−h(p)=∑1n [p(xi )logp(xi )1 −p(xi )logq(xi )1 ]$\n * 直观理解：如果分布 qqq 想达到分布 ppp 的话，还差了多少信息量\n * 用途：衡量两个概率分布之间的差异\n * 特点\n   * 其不对称，分布 ppp 在前则是指以分布 ppp 当做基准\n   * 其大于等于0，当分布 ppp 与 qqq 相等时等于0，不相等时大于0（吉布斯不等式证明）\n\n# 交叉熵（cross entropy）\n\n如何计算交叉熵？\n\n * 定义：h(p,q)=∑1np(x)log1q(x)h(p,q) = \\sum_{1}^{n}p(x)log\\frac{1}{q(x)}h(p,q)=∑1n p(x)logq(x)1\n\n * 直观理解：想要让分布 qqq 与 ppp 尽量地接近，就可以让 h(p，q)h(p，q)h(p，q) 尽量的小，所以 h(p,q)h(p,q)h(p,q) 本身即可作为损失函数，称之为交叉熵。交叉熵越小，代表两个概率模型间越相近\n\n * 用途：衡量两个概率分布直接的差异\n   \n   #\n\n参考资料\n\n * [1] “交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“kl散度”、“交叉熵”",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"PyTorch 常见代码片段",frontmatter:{title:"PyTorch 常见代码片段",date:"2022-05-11T16:23:52.000Z",permalink:"/pages/60e26f/",categories:["学习笔记","PyTorch Tricks"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/26.PyTorch%20Tricks/00.PyTorch%20%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5.html",relativePath:"02.学习笔记/26.PyTorch Tricks/00.PyTorch 常见代码片段.md",key:"v-06a7f062",path:"/pages/60e26f/",headers:[{level:3,title:"1、基本配置",slug:"_1、基本配置",normalizedTitle:"1、基本配置",charIndex:2},{level:3,title:"2、张量处理",slug:"_2、张量处理",normalizedTitle:"2、张量处理",charIndex:856},{level:3,title:"3、timm 模型提取特征流程",slug:"_3、timm-模型提取特征流程",normalizedTitle:"3、timm 模型提取特征流程",charIndex:4019}],headersStr:"1、基本配置 2、张量处理 3、timm 模型提取特征流程",content:"# 1、基本配置\n\n# 1.1 导入包和版本查询\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nprint(torch.__version__)\nprint(torch.cuda.is_available())\nprint(torch.version.cuda)\nprint(torch.backends.cudnn.version())\nprint(torch.cuda.get_device_name(0))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 1.2 可复现性以及设置随机种子\n\nnp.random.seed(0)  \ntorch.manual_seed(0)  \ntorch.cuda.manual_seed_all(0)  \n  \ntorch.backends.cudnn.deterministic = True  \ntorch.backends.cudnn.benchmark = False  \n\n\n1\n2\n3\n4\n5\n6\n\n\n# 1.3 显卡设置以及清除显存\n\n# 利用 PyTorch 的接口指定单张显卡\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n\n# 如果需要指定多张显卡，比如0，1号显卡。\nimport os  \nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'  \n\n# 也可以在命令行运行代码时设置显卡：\nCUDA_VISIBLE_DEVICES=0,1 python train.py  \n\n\n# 清除显存\ntorch.cuda.empty_cache()  \n\n# 也可以使用在命令行重置GPU的指令\nnvidia-smi --gpu-reset -i [gpu_id]  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 2、张量处理\n\n# 2.1 查看张量的基本信息\n\ntensor = torch.randn(3,4,5)\nprint(tensor.type())  # 数据类型\nprint(tensor.size())  # 张量的shape，是个元组\nprint(tensor.shape)  # 张量的shape，是个元组\nprint(tensor.dim())   # 维度的数量\n\n\n1\n2\n3\n4\n5\n\n\n# 2.2 torch.Tensor 和 np.ndarray 互相转换\n\n# np.ndarray -> torch.Tensor\nndarray = tensor.cpu().numpy()\ntensor = torch.from_numpy(ndarray).float()\ntensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride.\n\n# torch.Tensor -> np.ndarray\ntensor = torch.randn(3,4,5)\narray = tensor.cpu().detach().numpy()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 2.3 torch.tensor 与 PIL.Image 转换\n\n# pytorch中的张量默认采用[N, C, H, W]的顺序，并且数据范围在[0,1]，需要进行转置和规范化\n# torch.Tensor -> PIL.Image\nimage = PIL.Image.fromarray(\n    torch.clamp(tensor*255, min=0, max=255).byte().permute(1,2,0).cpu().numpy())\nimage = torchvision.transforms.functional.to_pil_image(tensor)  # Equivalently way\n\n# PIL.Image -> torch.Tensor\npath = 'figure.jpg'\ntensor = torch.from_numpy(\n    np.asarray(PIL.Image.open(path))\n).permute(2,0,1).float() / 255\n\n# PIL.Image -> torch.Tensor\ntensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path)) \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 2.4 张量的一些操作\n\n# 1、从只包含一个元素的张量中提取值\nvalue = torch.rand(1).item()\n\n# 2、张量形变\ntensor = torch.rand(2,3,4)\nshape = (6, 4)\ntensor = torch.reshape(tensor, shape)\n\n# 3、张量顺序重排\ntensor = torch.rand(2,3,4)\ntensor = tensor.permute(2,0,1)\n\n# 4、复制张量\n# Operation                 |  New/Shared memory | Still in computation graph |\ntensor.clone()            # |        New         |          Yes               |\ntensor.detach()           # |      Shared        |          No                |\ntensor.detach.clone()()   # |        New         |          No                |\n\n# 5、张量拼接\n# 注意 torch.cat 和 torch.stack 的区别在于 torch.cat 沿着给定的维度拼接，\n# 而 torch.stack 会新增一维。例如当参数是 3 个 10x5 的张量，torch.cat 的结果是 30x5 的张量，\n# 而 torch.stack 的结果是3x10x5的张量。\n\ntensor = torch.cat(list_of_tensors, dim=0)\ntensor = torch.stack(list_of_tensors, dim=0)\n\n# 6、得到非零元素\ntorch.nonzero(tensor)               # index of non-zero elements\ntorch.nonzero(tensor==0)            # index of zero elements\ntorch.nonzero(tensor).size(0)       # number of non-zero elements\ntorch.nonzero(tensor == 0).size(0)  # number of zero elements\n\n# 7、判断两个张量相等\ntorch.allclose(tensor1, tensor2)  # float tensor\ntorch.equal(tensor1, tensor2)     # int tensor\n\n# 8、张量扩展\n# Expand tensor of shape 64*512 to shape 64*512*7*7.\ntensor = torch.rand(64,512)\ntorch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7)\n\n# 9、矩阵乘法\n# Matrix multiplcation: (m*n) * (n*p) * -> (m*p).\nresult = torch.mm(tensor1, tensor2)\n\n# Batch matrix multiplication: (b*m*n) * (b*n*p) -> (b*m*p)\nresult = torch.bmm(tensor1, tensor2)\n\n# Element-wise multiplication.\nresult = tensor1 * tensor2\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n# 2.5 将整数标签转为 one-hot 编码\n\n# pytorch的标记默认从0开始\ntensor = torch.tensor([0, 2, 1, 3])\nN = tensor.size(0)\nnum_classes = 4\none_hot = torch.zeros(N, num_classes).long()\none_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long())\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 3、timm 模型提取特征流程\n\n# 3.1 timm 提取特征\n\ndef forward_features(self, x):\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.act1(x)\n    x = self.maxpool(x)\n\n    if self.grad_checkpointing and not torch.jit.is_scripting():\n        x = checkpoint_seq([self.layer1, self.layer2, self.layer3, self.layer4], x, flatten=True)\n    else:\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n    return x\n\ndef forward_head(self, x, pre_logits: bool = False):\n    x = self.global_pool(x)\n    if self.drop_rate:\n        x = F.dropout(x, p=float(self.drop_rate), training=self.training)\n    return x if pre_logits else self.fc(x)\n\ndef forward(self, x):\n    x = self.forward_features(x)\n    x = self.forward_head(x)\n    return x\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n# 3.2 timm 展示出所有的模型\n\n参考资料\n\n * 部分内容转载自：https://zhuanlan.zhihu.com/p/104019160\n * ",normalizedContent:"# 1、基本配置\n\n# 1.1 导入包和版本查询\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nprint(torch.__version__)\nprint(torch.cuda.is_available())\nprint(torch.version.cuda)\nprint(torch.backends.cudnn.version())\nprint(torch.cuda.get_device_name(0))\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 1.2 可复现性以及设置随机种子\n\nnp.random.seed(0)  \ntorch.manual_seed(0)  \ntorch.cuda.manual_seed_all(0)  \n  \ntorch.backends.cudnn.deterministic = true  \ntorch.backends.cudnn.benchmark = false  \n\n\n1\n2\n3\n4\n5\n6\n\n\n# 1.3 显卡设置以及清除显存\n\n# 利用 pytorch 的接口指定单张显卡\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n\n# 如果需要指定多张显卡，比如0，1号显卡。\nimport os  \nos.environ['cuda_visible_devices'] = '0,1'  \n\n# 也可以在命令行运行代码时设置显卡：\ncuda_visible_devices=0,1 python train.py  \n\n\n# 清除显存\ntorch.cuda.empty_cache()  \n\n# 也可以使用在命令行重置gpu的指令\nnvidia-smi --gpu-reset -i [gpu_id]  \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n\n# 2、张量处理\n\n# 2.1 查看张量的基本信息\n\ntensor = torch.randn(3,4,5)\nprint(tensor.type())  # 数据类型\nprint(tensor.size())  # 张量的shape，是个元组\nprint(tensor.shape)  # 张量的shape，是个元组\nprint(tensor.dim())   # 维度的数量\n\n\n1\n2\n3\n4\n5\n\n\n# 2.2 torch.tensor 和 np.ndarray 互相转换\n\n# np.ndarray -> torch.tensor\nndarray = tensor.cpu().numpy()\ntensor = torch.from_numpy(ndarray).float()\ntensor = torch.from_numpy(ndarray.copy()).float() # if ndarray has negative stride.\n\n# torch.tensor -> np.ndarray\ntensor = torch.randn(3,4,5)\narray = tensor.cpu().detach().numpy()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 2.3 torch.tensor 与 pil.image 转换\n\n# pytorch中的张量默认采用[n, c, h, w]的顺序，并且数据范围在[0,1]，需要进行转置和规范化\n# torch.tensor -> pil.image\nimage = pil.image.fromarray(\n    torch.clamp(tensor*255, min=0, max=255).byte().permute(1,2,0).cpu().numpy())\nimage = torchvision.transforms.functional.to_pil_image(tensor)  # equivalently way\n\n# pil.image -> torch.tensor\npath = 'figure.jpg'\ntensor = torch.from_numpy(\n    np.asarray(pil.image.open(path))\n).permute(2,0,1).float() / 255\n\n# pil.image -> torch.tensor\ntensor = torchvision.transforms.functional.to_tensor(pil.image.open(path)) \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 2.4 张量的一些操作\n\n# 1、从只包含一个元素的张量中提取值\nvalue = torch.rand(1).item()\n\n# 2、张量形变\ntensor = torch.rand(2,3,4)\nshape = (6, 4)\ntensor = torch.reshape(tensor, shape)\n\n# 3、张量顺序重排\ntensor = torch.rand(2,3,4)\ntensor = tensor.permute(2,0,1)\n\n# 4、复制张量\n# operation                 |  new/shared memory | still in computation graph |\ntensor.clone()            # |        new         |          yes               |\ntensor.detach()           # |      shared        |          no                |\ntensor.detach.clone()()   # |        new         |          no                |\n\n# 5、张量拼接\n# 注意 torch.cat 和 torch.stack 的区别在于 torch.cat 沿着给定的维度拼接，\n# 而 torch.stack 会新增一维。例如当参数是 3 个 10x5 的张量，torch.cat 的结果是 30x5 的张量，\n# 而 torch.stack 的结果是3x10x5的张量。\n\ntensor = torch.cat(list_of_tensors, dim=0)\ntensor = torch.stack(list_of_tensors, dim=0)\n\n# 6、得到非零元素\ntorch.nonzero(tensor)               # index of non-zero elements\ntorch.nonzero(tensor==0)            # index of zero elements\ntorch.nonzero(tensor).size(0)       # number of non-zero elements\ntorch.nonzero(tensor == 0).size(0)  # number of zero elements\n\n# 7、判断两个张量相等\ntorch.allclose(tensor1, tensor2)  # float tensor\ntorch.equal(tensor1, tensor2)     # int tensor\n\n# 8、张量扩展\n# expand tensor of shape 64*512 to shape 64*512*7*7.\ntensor = torch.rand(64,512)\ntorch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7)\n\n# 9、矩阵乘法\n# matrix multiplcation: (m*n) * (n*p) * -> (m*p).\nresult = torch.mm(tensor1, tensor2)\n\n# batch matrix multiplication: (b*m*n) * (b*n*p) -> (b*m*p)\nresult = torch.bmm(tensor1, tensor2)\n\n# element-wise multiplication.\nresult = tensor1 * tensor2\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\n# 2.5 将整数标签转为 one-hot 编码\n\n# pytorch的标记默认从0开始\ntensor = torch.tensor([0, 2, 1, 3])\nn = tensor.size(0)\nnum_classes = 4\none_hot = torch.zeros(n, num_classes).long()\none_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(n, num_classes).long())\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n# 3、timm 模型提取特征流程\n\n# 3.1 timm 提取特征\n\ndef forward_features(self, x):\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.act1(x)\n    x = self.maxpool(x)\n\n    if self.grad_checkpointing and not torch.jit.is_scripting():\n        x = checkpoint_seq([self.layer1, self.layer2, self.layer3, self.layer4], x, flatten=true)\n    else:\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n    return x\n\ndef forward_head(self, x, pre_logits: bool = false):\n    x = self.global_pool(x)\n    if self.drop_rate:\n        x = f.dropout(x, p=float(self.drop_rate), training=self.training)\n    return x if pre_logits else self.fc(x)\n\ndef forward(self, x):\n    x = self.forward_features(x)\n    x = self.forward_head(x)\n    return x\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n\n\n# 3.2 timm 展示出所有的模型\n\n参考资料\n\n * 部分内容转载自：https://zhuanlan.zhihu.com/p/104019160\n * ",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"敬告青年",frontmatter:{title:"敬告青年",date:"2021-05-07T16:46:53.000Z",permalink:"/pages/967c74/",categories:["更多","心情杂货"],tags:[null]},regularPath:"/03.%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88/01.%E5%BF%83%E6%83%85%E6%9D%82%E8%B4%A7/01.%E6%95%AC%E5%91%8A%E9%9D%92%E5%B9%B4.html",relativePath:"03.生活杂谈/01.心情杂货/01.敬告青年.md",key:"v-2df9e565",path:"/pages/967c74/",headersStr:null,content:"当up的经历告诉我： 每个人的经历人生思想千千万，很多问题和观点是没有唯一标准答案的，也并没有一个答案可以满足所有人；但似乎我们从小被告知的是，只有一个解决一切问题的标准答案，所以带着这个想法挑剔的去评判别人的表达，挑剔的认为某个视频某个回答没有想到xxxxxx，所以你说的不对； 但凡带着这样的角度去上网冲浪，我们不会体会到互联网带来的丰富世界，唯一能获得的，大概只有打完字之后，涌入心头【他们都是傻X，只有我人间清醒】的得意\n\n",normalizedContent:"当up的经历告诉我： 每个人的经历人生思想千千万，很多问题和观点是没有唯一标准答案的，也并没有一个答案可以满足所有人；但似乎我们从小被告知的是，只有一个解决一切问题的标准答案，所以带着这个想法挑剔的去评判别人的表达，挑剔的认为某个视频某个回答没有想到xxxxxx，所以你说的不对； 但凡带着这样的角度去上网冲浪，我们不会体会到互联网带来的丰富世界，唯一能获得的，大概只有打完字之后，涌入心头【他们都是傻x，只有我人间清醒】的得意\n\n",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"使用半精度训练",frontmatter:{title:"使用半精度训练",date:"2022-05-11T16:24:29.000Z",permalink:"/pages/21270d/",categories:["学习笔记","PyTorch Tricks"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/26.PyTorch%20Tricks/02.%E4%BD%BF%E7%94%A8%E5%8D%8A%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83.html",relativePath:"02.学习笔记/26.PyTorch Tricks/02.使用半精度训练.md",key:"v-1efee980",path:"/pages/21270d/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2023/03/25, 19:58:09"},{title:"数据增强",frontmatter:{title:"数据增强",date:"2022-05-11T20:35:05.000Z",permalink:"/pages/15e77c/",categories:["学习笔记","PyTorch Tricks"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/26.PyTorch%20Tricks/03.%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA.html",relativePath:"02.学习笔记/26.PyTorch Tricks/03.数据增强.md",key:"v-4120d242",path:"/pages/15e77c/",headersStr:null,content:"https://jishuin.proginn.com/p/763bfbd6c9bf\n\nAutoAugment\n\nRandAugment\n\nTrivalAugment\n\nAlbumentations\n\n * https://github.com/albumentations-team/albumentations\n * https://albumentations.ai/docs/\n\n基于深度学习的数据增广技术一览\n\n * https://www.cvmart.net/community/detail/2281",normalizedContent:"https://jishuin.proginn.com/p/763bfbd6c9bf\n\nautoaugment\n\nrandaugment\n\ntrivalaugment\n\nalbumentations\n\n * https://github.com/albumentations-team/albumentations\n * https://albumentations.ai/docs/\n\n基于深度学习的数据增广技术一览\n\n * https://www.cvmart.net/community/detail/2281",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"常见 Tricks 代码片段",frontmatter:{title:"常见 Tricks 代码片段",date:"2022-05-12T15:54:26.000Z",permalink:"/pages/53d32c/",categories:["学习笔记","PyTorch Tricks"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/26.PyTorch%20Tricks/04.%E5%B8%B8%E8%A7%81%20Tricks%20%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5.html",relativePath:"02.学习笔记/26.PyTorch Tricks/04.常见 Tricks 代码片段.md",key:"v-ae94161c",path:"/pages/53d32c/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2023/03/25, 19:58:09"},{title:"年度计划模板",frontmatter:{title:"年度计划模板",date:"2021-05-12T20:56:40.000Z",permalink:"/pages/fe5265/",categories:["更多","心情杂货"],tags:[null]},regularPath:"/03.%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88/01.%E5%BF%83%E6%83%85%E6%9D%82%E8%B4%A7/02.%E5%B9%B4%E5%BA%A6%E8%AE%A1%E5%88%92%E6%A8%A1%E6%9D%BF.html",relativePath:"03.生活杂谈/01.心情杂货/02.年度计划模板.md",key:"v-9f230ad8",path:"/pages/fe5265/",headers:[{level:2,title:"对未来的认识",slug:"对未来的认识",normalizedTitle:"对未来的认识",charIndex:2},{level:3,title:"对当下的要求",slug:"对当下的要求",normalizedTitle:"对当下的要求",charIndex:39}],headersStr:"对未来的认识 对当下的要求",content:"# 对未来的认识\n\n# 人生的方向\n\n# 环境与局势\n\n# 未曾设想\n\n\n# 对当下的要求\n\n# 想要保持进步的习惯/兴趣/爱好等\n\n# 想要戒掉的坏习惯\n\n# 想要尝试的新鲜事物",normalizedContent:"# 对未来的认识\n\n# 人生的方向\n\n# 环境与局势\n\n# 未曾设想\n\n\n# 对当下的要求\n\n# 想要保持进步的习惯/兴趣/爱好等\n\n# 想要戒掉的坏习惯\n\n# 想要尝试的新鲜事物",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"年度总结模板",frontmatter:{title:"年度总结模板",date:"2021-05-12T20:49:24.000Z",permalink:"/pages/32c4fa/",categories:["更多","心情杂货"],tags:[null]},regularPath:"/03.%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88/01.%E5%BF%83%E6%83%85%E6%9D%82%E8%B4%A7/03.%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93%E6%A8%A1%E6%9D%BF.html",relativePath:"03.生活杂谈/01.心情杂货/03.年度总结模板.md",key:"v-6ed459d2",path:"/pages/32c4fa/",headers:[{level:3,title:"学习&成长&突破方面",slug:"学习-成长-突破方面",normalizedTitle:"学习&amp;成长&amp;突破方面",charIndex:null},{level:3,title:"放松&体验&探索方面",slug:"放松-体验-探索方面",normalizedTitle:"放松&amp;体验&amp;探索方面",charIndex:null},{level:3,title:"家庭&生活&健康方面",slug:"家庭-生活-健康方面",normalizedTitle:"家庭&amp;生活&amp;健康方面",charIndex:null},{level:3,title:"工作&事业&梦想方面",slug:"工作-事业-梦想方面",normalizedTitle:"工作&amp;事业&amp;梦想方面",charIndex:null},{level:3,title:"储蓄&理财&保险方面",slug:"储蓄-理财-保险方面",normalizedTitle:"储蓄&amp;理财&amp;保险方面",charIndex:null},{level:3,title:"伙伴&人际&社交方面",slug:"伙伴-人际-社交方面",normalizedTitle:"伙伴&amp;人际&amp;社交方面",charIndex:null},{level:3,title:"评价与分析",slug:"评价与分析",normalizedTitle:"评价与分析",charIndex:440},{level:3,title:"对比去年",slug:"对比去年",normalizedTitle:"对比去年",charIndex:450},{level:3,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:459}],headersStr:"学习&成长&突破方面 放松&体验&探索方面 家庭&生活&健康方面 工作&事业&梦想方面 储蓄&理财&保险方面 伙伴&人际&社交方面 评价与分析 对比去年 总结",content:"感恩\n\n回顾\n\n\n# 学习&成长&突破方面\n\n# 学到最值的三项？\n\n# 最棒的三个成就？\n\n# 最大的三个突破？\n\n# 补充\n\n\n# 放松&体验&探索方面\n\n# 养成了什么新的习惯？\n\n# 最美好的三个经历与回忆？\n\n# 吃过最美味的三顿饭？\n\n# 去过最美好的三个地方，日常外的冒险？\n\n# 读过看过听过用过玩过用过最喜欢的书/电影/番剧/影视剧/纪录片/综艺/音乐\n\n# 补充\n\n\n# 家庭&生活&健康方面\n\n# 与大家的三个回忆镜头？\n\n# 置办的最喜欢的三个新玩意？\n\n# 大家健康的变化？\n\n# 补充\n\n\n# 工作&事业&梦想方面\n\n# 工作上的成就、经验、教训？\n\n# 自己想做的并帮到大家的？\n\n# 现在离梦想有多近，还有多远？\n\n# 补充\n\n\n# 储蓄&理财&保险方面\n\n# 最佳投资项目？\n\n# 收入与费用的梳理？\n\n# 保险的梳理？\n\n\n# 伙伴&人际&社交方面\n\n# 他们的闪光点和从他们身上学到的\n\n# 社区活动、志愿、公益的进展？\n\n# 补充\n\n\n# 评价与分析\n\n\n# 对比去年\n\n\n# 总结",normalizedContent:"感恩\n\n回顾\n\n\n# 学习&成长&突破方面\n\n# 学到最值的三项？\n\n# 最棒的三个成就？\n\n# 最大的三个突破？\n\n# 补充\n\n\n# 放松&体验&探索方面\n\n# 养成了什么新的习惯？\n\n# 最美好的三个经历与回忆？\n\n# 吃过最美味的三顿饭？\n\n# 去过最美好的三个地方，日常外的冒险？\n\n# 读过看过听过用过玩过用过最喜欢的书/电影/番剧/影视剧/纪录片/综艺/音乐\n\n# 补充\n\n\n# 家庭&生活&健康方面\n\n# 与大家的三个回忆镜头？\n\n# 置办的最喜欢的三个新玩意？\n\n# 大家健康的变化？\n\n# 补充\n\n\n# 工作&事业&梦想方面\n\n# 工作上的成就、经验、教训？\n\n# 自己想做的并帮到大家的？\n\n# 现在离梦想有多近，还有多远？\n\n# 补充\n\n\n# 储蓄&理财&保险方面\n\n# 最佳投资项目？\n\n# 收入与费用的梳理？\n\n# 保险的梳理？\n\n\n# 伙伴&人际&社交方面\n\n# 他们的闪光点和从他们身上学到的\n\n# 社区活动、志愿、公益的进展？\n\n# 补充\n\n\n# 评价与分析\n\n\n# 对比去年\n\n\n# 总结",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"2020年终总结备份",frontmatter:{title:"2020年终总结备份",date:"2021-05-12T21:11:05.000Z",permalink:"/pages/a32ea4/",categories:["更多","心情杂货"],tags:[null]},regularPath:"/03.%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88/01.%E5%BF%83%E6%83%85%E6%9D%82%E8%B4%A7/04.2020%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%E5%A4%87%E4%BB%BD.html",relativePath:"03.生活杂谈/01.心情杂货/04.2020年终总结备份.md",key:"v-49a90feb",path:"/pages/a32ea4/",headers:[{level:3,title:"流水账的日常",slug:"流水账的日常",normalizedTitle:"流水账的日常",charIndex:29},{level:3,title:"书影生活",slug:"书影生活",normalizedTitle:"书影生活",charIndex:1745}],headersStr:"流水账的日常 书影生活",content:"# 2020年终总结\n\n遗憾：未能成为一名创作者\n\n\n# 流水账的日常\n\n又到了写年终年中总结的时候了，一回想过去，就感到时间过的是真的快。2019年终总结 是在农历的腊月二十九写的，当时疫情已经爆发了，去年的年终写到 “今年的年关将至，社会不是很太平”，我没想到会持续到现在。今年的总结不像去年那样一月月的梳理了，今年大致分为四个阶段吧，宅在家--毕设和毕业季--暑假--正式研一。今年的自己感觉成长最大的是心态，心态相比于去年会柔和很多，不过还是一样容易受影响，最近已经到不敢去看那些消失的同学留下的遗书，每看一次我会失落很久，共情能力所带来的负面情绪会缠绕我很久，会让我不断追问自己生活的意义。感谢今年遇到的很多很多人，在年初以及前几天我也开了一个匿名提问箱，里面很多的问题都特别有意义，感谢这些提问的同学，让我在思考中梳理了自己的想法。下面开始以流水账的形式回忆以下魔幻的二零二零吧。\n\n2020年的寒假从一月份开始，一直放到了四月底（还在返校的火车上追了《四月是你的谎言》）。在家做毕设的日子是难捱的，我慢慢地做Keep上的有氧无氧训练，一方面是想给家里人看我也是偶尔会锻炼锻炼的，一方面也是真的太闲了。还幻想着可以回学校考六级刷分，认真地在墨墨上背了100多天的单词，到学校就断掉了。刚刚去翻自己的tg channel，发现疫情在家的生活过的很平淡，除了偶尔到楼下的沿河绿道走走还有在家做做运动什么的就是待在电脑前，和在学校的状态差不多。不过很好玩的是学会了做饭，还顺手写了篇博客哈哈哈哈：如何在家里做一顿好的。在家的时间探索了很多新东西，有一个 App 叫“给未来写封信” 还蛮有意思的。\n\n和家人在一起的时间变多了，相处还算融洽，不过总是有看不惯父母行为的地方，比如外放抖音，比如特别晚还不睡觉看电视（我爸比我还能熬夜🤣）。也在家调节了父母的矛盾，很高兴的是自己有勇气去做这些事情了，原来我姐经常会说我很容易哭鼻子，其实我是在家人面前挺脆弱的，因为我觉得我还可以依赖他们，可现在不一样了。二是在家我们一家经常和几位高中同学的家里聚餐，也是那时候被叔叔们劝了些酒。意识到自己真的说话很笨，连一些基本的祝酒词都不会说。也有一次家里请客吃饭，二三十人，我爸不喝酒并且要开车，所以就让我代祝酒，我什么都没有准备，硬着头皮说了些连当祝酒词都很俗气的话，说下来自己干了小杯白酒，脸一阵红，无地自容。好在老爸后面也没说啥，不过我意识到自己好像已经不小了，也没有人会觉得自己是个学生了。在家还有次是父母特别敬重的一位爷爷回来了，据说就是这个爷爷给我和姐姐取名，我也蛮喜欢我的云字，很想感谢这位爷爷，帮老妈老爸陪爷爷喝了几杯。没想到我大意了，我本来喝酒就会上脸，几杯下肚后还敬了几位叔叔阿姨，忍不住去洗手间吐了一次，离席后在路边等车又直接吐到了路旁。总之在家关于喝酒这个话题出了很多次事故，让我意识到随着年岁的增长，一些事情正在慢慢被揭开，我需要积极的去面对的。\n\n接下来的5月和6月就被关在学校做毕设了，想起今年的上半年自己还在念大四，就觉得一阵魔幻和晕眩，原来几个月之前那群人都还在一起待了这么久呀。身处其间真的感知不到和室友在一起的幸福，那段时间的 tg channel 发的都是在实验室熬夜做毕设，写代码跑实验没什么好讲的，挺无聊的。不过有个晚上突然想玩植物大战僵尸，就硬玩了一个通宵，第二天照镜子感觉自己可以去客串僵尸了hhh。毕设渐渐的过去，我感受的毕业季到来的特别突兀，想到一起生活四年的同学要各奔东西了还是有些感伤的。以及难过的是几个月过去了，我还是没能瘦下来，去拍个好看的毕业照。还好有可爱的方方给俺买了一束花，拿到毕业典礼时超级受人喜欢，借给了好多同学拍照，然后我就成为了学院最后一个拨穗的同学，很不好意思的请许建秋老师穿上刚刚脱下的衣服给我拨了最后的穗，然后和以前很多的同学拍了照。一起打数模比赛的小凡队长拿到了宾大的CS offer，贺键老哥去了同济，我留守南航搬砖，大家都有光明的未来。和室友以及班上同学吃了散伙饭，告别了我的本科四年。\n\n\n\n\n\n\n\n暑假的日子\n\n十月去到上海和欣忆看了人生第一场音乐会，是彩虹合唱团的上海回归首场，\n\n三月参加了合唱团\n\n\n# 书影生活\n\n豆瓣上大概总结了下自己的书影生活，照旧的是书籍没看多少，重读了《龙族》，新看了《一句顶一万句》和《恶意》。看刘震云的文字和我看余华的书感觉十分相似，读着读着就有非常强的不真实感，总感觉书里的故事是和我周遭的现实是脱离的，但是回到现实往往又会邂逅书中的故事，这种魅力令人着迷。\n\n至于影视作品则是一大堆，影视剧有《绝命毒师》《觉醒年代》《去他妈的世界》，以及跟着b站up主看了一堆《疑犯追踪》《暗黑》等等剧。之前十分热衷追b站up主的解读，我真切的感受到自己的思维能力有一些下降，现在会有意识地去排斥这些二手三手的解读信息了，过于舒适的获取信息容易形成思维惯性，让我思考的能力变弱。\n\n电影部分《心灵奇旅》《海上钢琴师》可以排到我的年度前二。生活压垮不了灵魂，其实这两个主角都算是幸运的人，能够在很早的时候就发现自己所爱并坚持下去，种种有些 nerd 的行为我也完全能够理解。《心灵奇旅》电影节奏轻松明快，在时而捧腹的观影过程中不时的冒出几句让人思考的鸡汤让观影体验十分舒适；而《海上钢琴师》的1900在舷梯上驻足了很久，看不到陆地的尽头。面对未知和不确定性，钢琴外的世界也不再以自己为中心。偌大的陆地，故事很多，但迷茫的是不知道那个故事和他有关。\n\n此外还有一堆现实纪录片《书记》《大同》《高三》《隐形亿万富翁》《怦然心动的人生整理魔法》等等，看完大同之后还专门去找了大同的学弟求证，确实如片中所说的真实，而《书记》就更真实了，那个书记在拍纪录片的时候不小心被录到行贿的内容，在他落马之后被放出，而《高三》这部纪录片让人唏嘘感叹，现如今片中学生的去向也很有趣。不过不可避免的是这群学生和《北京东路的日子》中学生的人生轨迹有太多太多的不一样，去年《北京东路的日子》的学生一起拍了个十年荣耀版，有兴趣的同学可以去看看。\n\n安利一些这段时间我看过觉得不错的动漫和漫画有《我的三体》《强风吹拂》《灵笼》《进击的巨人》《罗小黑战纪》《刺客伍六七》《四月是你的谎言》《咒术回战》《REFILE》《中国唱诗班》《某科学的超电磁炮》《一人之下》《镖人》，其中国漫占比十分大，虽然我入坑也不久，但是能感觉到国漫的水准已经很棒了，故事节奏和叙述手法都有自己比较独特的风格，让我印象深刻的有《一人之下》的道家底蕴，《镖人》的独特中国风，《我的三体》的像素风和社区自治，以及不断飞刀子的众多国漫真是让我又爱又恨！然后补的几部老番真的很戳，《强风吹拂》开跑箱根驿传的时候我和主角同样紧张，会为他们欢呼呐喊，沉浸感超强；《四月是你的谎言》成功骗出了我的眼泪和首次承包，音乐真真切切地会让我感到快乐，但故事又是这么让人忧伤；《进击的巨人》还是一如既往的高水准，但是据说漫画烂尾了就很气。",normalizedContent:"# 2020年终总结\n\n遗憾：未能成为一名创作者\n\n\n# 流水账的日常\n\n又到了写年终年中总结的时候了，一回想过去，就感到时间过的是真的快。2019年终总结 是在农历的腊月二十九写的，当时疫情已经爆发了，去年的年终写到 “今年的年关将至，社会不是很太平”，我没想到会持续到现在。今年的总结不像去年那样一月月的梳理了，今年大致分为四个阶段吧，宅在家--毕设和毕业季--暑假--正式研一。今年的自己感觉成长最大的是心态，心态相比于去年会柔和很多，不过还是一样容易受影响，最近已经到不敢去看那些消失的同学留下的遗书，每看一次我会失落很久，共情能力所带来的负面情绪会缠绕我很久，会让我不断追问自己生活的意义。感谢今年遇到的很多很多人，在年初以及前几天我也开了一个匿名提问箱，里面很多的问题都特别有意义，感谢这些提问的同学，让我在思考中梳理了自己的想法。下面开始以流水账的形式回忆以下魔幻的二零二零吧。\n\n2020年的寒假从一月份开始，一直放到了四月底（还在返校的火车上追了《四月是你的谎言》）。在家做毕设的日子是难捱的，我慢慢地做keep上的有氧无氧训练，一方面是想给家里人看我也是偶尔会锻炼锻炼的，一方面也是真的太闲了。还幻想着可以回学校考六级刷分，认真地在墨墨上背了100多天的单词，到学校就断掉了。刚刚去翻自己的tg channel，发现疫情在家的生活过的很平淡，除了偶尔到楼下的沿河绿道走走还有在家做做运动什么的就是待在电脑前，和在学校的状态差不多。不过很好玩的是学会了做饭，还顺手写了篇博客哈哈哈哈：如何在家里做一顿好的。在家的时间探索了很多新东西，有一个 app 叫“给未来写封信” 还蛮有意思的。\n\n和家人在一起的时间变多了，相处还算融洽，不过总是有看不惯父母行为的地方，比如外放抖音，比如特别晚还不睡觉看电视（我爸比我还能熬夜🤣）。也在家调节了父母的矛盾，很高兴的是自己有勇气去做这些事情了，原来我姐经常会说我很容易哭鼻子，其实我是在家人面前挺脆弱的，因为我觉得我还可以依赖他们，可现在不一样了。二是在家我们一家经常和几位高中同学的家里聚餐，也是那时候被叔叔们劝了些酒。意识到自己真的说话很笨，连一些基本的祝酒词都不会说。也有一次家里请客吃饭，二三十人，我爸不喝酒并且要开车，所以就让我代祝酒，我什么都没有准备，硬着头皮说了些连当祝酒词都很俗气的话，说下来自己干了小杯白酒，脸一阵红，无地自容。好在老爸后面也没说啥，不过我意识到自己好像已经不小了，也没有人会觉得自己是个学生了。在家还有次是父母特别敬重的一位爷爷回来了，据说就是这个爷爷给我和姐姐取名，我也蛮喜欢我的云字，很想感谢这位爷爷，帮老妈老爸陪爷爷喝了几杯。没想到我大意了，我本来喝酒就会上脸，几杯下肚后还敬了几位叔叔阿姨，忍不住去洗手间吐了一次，离席后在路边等车又直接吐到了路旁。总之在家关于喝酒这个话题出了很多次事故，让我意识到随着年岁的增长，一些事情正在慢慢被揭开，我需要积极的去面对的。\n\n接下来的5月和6月就被关在学校做毕设了，想起今年的上半年自己还在念大四，就觉得一阵魔幻和晕眩，原来几个月之前那群人都还在一起待了这么久呀。身处其间真的感知不到和室友在一起的幸福，那段时间的 tg channel 发的都是在实验室熬夜做毕设，写代码跑实验没什么好讲的，挺无聊的。不过有个晚上突然想玩植物大战僵尸，就硬玩了一个通宵，第二天照镜子感觉自己可以去客串僵尸了hhh。毕设渐渐的过去，我感受的毕业季到来的特别突兀，想到一起生活四年的同学要各奔东西了还是有些感伤的。以及难过的是几个月过去了，我还是没能瘦下来，去拍个好看的毕业照。还好有可爱的方方给俺买了一束花，拿到毕业典礼时超级受人喜欢，借给了好多同学拍照，然后我就成为了学院最后一个拨穗的同学，很不好意思的请许建秋老师穿上刚刚脱下的衣服给我拨了最后的穗，然后和以前很多的同学拍了照。一起打数模比赛的小凡队长拿到了宾大的cs offer，贺键老哥去了同济，我留守南航搬砖，大家都有光明的未来。和室友以及班上同学吃了散伙饭，告别了我的本科四年。\n\n\n\n\n\n\n\n暑假的日子\n\n十月去到上海和欣忆看了人生第一场音乐会，是彩虹合唱团的上海回归首场，\n\n三月参加了合唱团\n\n\n# 书影生活\n\n豆瓣上大概总结了下自己的书影生活，照旧的是书籍没看多少，重读了《龙族》，新看了《一句顶一万句》和《恶意》。看刘震云的文字和我看余华的书感觉十分相似，读着读着就有非常强的不真实感，总感觉书里的故事是和我周遭的现实是脱离的，但是回到现实往往又会邂逅书中的故事，这种魅力令人着迷。\n\n至于影视作品则是一大堆，影视剧有《绝命毒师》《觉醒年代》《去他妈的世界》，以及跟着b站up主看了一堆《疑犯追踪》《暗黑》等等剧。之前十分热衷追b站up主的解读，我真切的感受到自己的思维能力有一些下降，现在会有意识地去排斥这些二手三手的解读信息了，过于舒适的获取信息容易形成思维惯性，让我思考的能力变弱。\n\n电影部分《心灵奇旅》《海上钢琴师》可以排到我的年度前二。生活压垮不了灵魂，其实这两个主角都算是幸运的人，能够在很早的时候就发现自己所爱并坚持下去，种种有些 nerd 的行为我也完全能够理解。《心灵奇旅》电影节奏轻松明快，在时而捧腹的观影过程中不时的冒出几句让人思考的鸡汤让观影体验十分舒适；而《海上钢琴师》的1900在舷梯上驻足了很久，看不到陆地的尽头。面对未知和不确定性，钢琴外的世界也不再以自己为中心。偌大的陆地，故事很多，但迷茫的是不知道那个故事和他有关。\n\n此外还有一堆现实纪录片《书记》《大同》《高三》《隐形亿万富翁》《怦然心动的人生整理魔法》等等，看完大同之后还专门去找了大同的学弟求证，确实如片中所说的真实，而《书记》就更真实了，那个书记在拍纪录片的时候不小心被录到行贿的内容，在他落马之后被放出，而《高三》这部纪录片让人唏嘘感叹，现如今片中学生的去向也很有趣。不过不可避免的是这群学生和《北京东路的日子》中学生的人生轨迹有太多太多的不一样，去年《北京东路的日子》的学生一起拍了个十年荣耀版，有兴趣的同学可以去看看。\n\n安利一些这段时间我看过觉得不错的动漫和漫画有《我的三体》《强风吹拂》《灵笼》《进击的巨人》《罗小黑战纪》《刺客伍六七》《四月是你的谎言》《咒术回战》《refile》《中国唱诗班》《某科学的超电磁炮》《一人之下》《镖人》，其中国漫占比十分大，虽然我入坑也不久，但是能感觉到国漫的水准已经很棒了，故事节奏和叙述手法都有自己比较独特的风格，让我印象深刻的有《一人之下》的道家底蕴，《镖人》的独特中国风，《我的三体》的像素风和社区自治，以及不断飞刀子的众多国漫真是让我又爱又恨！然后补的几部老番真的很戳，《强风吹拂》开跑箱根驿传的时候我和主角同样紧张，会为他们欢呼呐喊，沉浸感超强；《四月是你的谎言》成功骗出了我的眼泪和首次承包，音乐真真切切地会让我感到快乐，但故事又是这么让人忧伤；《进击的巨人》还是一如既往的高水准，但是据说漫画烂尾了就很气。",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"六月的离别让人忧伤",frontmatter:{title:"六月的离别让人忧伤",date:"2021-06-03T22:49:07.000Z",permalink:"/pages/daccdc/",categories:["更多","心情杂货"],tags:[null]},regularPath:"/03.%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88/01.%E5%BF%83%E6%83%85%E6%9D%82%E8%B4%A7/05.%E5%85%AD%E6%9C%88%E7%9A%84%E7%A6%BB%E5%88%AB%E8%AE%A9%E4%BA%BA%E5%BF%A7%E4%BC%A4.html",relativePath:"03.生活杂谈/01.心情杂货/05.六月的离别让人忧伤.md",key:"v-61ae05f4",path:"/pages/daccdc/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/08/02, 21:04:52"},{title:"2021 年终总结 | 时光飞逝的一年",frontmatter:{title:"2021 年终总结 | 时光飞逝的一年",date:"2021-12-31T04:26:04.000Z",permalink:"/pages/aceb33/",categories:["生活杂谈","心情杂货"],tags:[null]},regularPath:"/03.%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88/01.%E5%BF%83%E6%83%85%E6%9D%82%E8%B4%A7/06.2021%20%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%20%7C%20%E6%97%B6%E5%85%89%E9%A3%9E%E9%80%9D%E7%9A%84%E4%B8%80%E5%B9%B4.html",relativePath:"03.生活杂谈/01.心情杂货/06.2021 年终总结 | 时光飞逝的一年.md",key:"v-20dcdbb3",path:"/pages/aceb33/",headers:[{level:3,title:"01 | 飞速逝去的一年",slug:"_01-飞速逝去的一年",normalizedTitle:"01 | 飞速逝去的一年",charIndex:2},{level:3,title:"02 | 以下是收集向，主要回顾自己的书影生活、听的歌曲以及在摘录些在生活中的只言片语",slug:"_02-以下是收集向-主要回顾自己的书影生活、听的歌曲以及在摘录些在生活中的只言片语",normalizedTitle:"02 | 以下是收集向，主要回顾自己的书影生活、听的歌曲以及在摘录些在生活中的只言片语",charIndex:4812}],headersStr:"01 | 飞速逝去的一年 02 | 以下是收集向，主要回顾自己的书影生活、听的歌曲以及在摘录些在生活中的只言片语",content:'# 01 | 飞速逝去的一年\n\n\n\n本来想要掺着零零散散没发出来的20年年终总结一起写的。看了看去年的流水账也没太大意义。回想下今年的日子，其实成片段的回忆镜头还是蛮少的，生活里一部分是歌唱，一部分是科研，剩下的就是快乐的摸鱼。先大致讲讲这一年值得记录的事情吧。\n\n2021 年我也正式的来到22岁，成为了南航的硕士生，经历了研一下学期和研二上学期两段学校时光。在年初归家的飞机落地后，我收到了论文录用的消息，一篇 CCF-B 的会议论文，所以年关过得还算快乐安稳。春节过后，我在川内的城市兜转了一圈，和高中的好朋友们相聚，也去见了高中班主任老唐，整个春节没有过多烦恼，过得还是很欢乐的。春节结束，我和姐姐带着父母去重庆做体检，希望这个习惯能一直保持下去，平安健康我现在觉得重要过一切。\n\n# 1.1 | 纯粹浪漫的合唱时光\n\n从山城归宁后，我记不清我的研一下学期做了些什么。我的室友都知道，我是个浴室歌手，喜欢在淋浴的时候歌唱。在今年三月的百团大战，我到微软学生俱乐部的摊位上和大四的学弟学妹聊了会天，然后趁旁边鹰之歌合唱团的学弟（现在想起来或许就是林总）不注意，偷偷扫了他们的招新群二维码。\n\n机缘巧合下，我或许成为了今年第一位面试的同学，面试唱的是陈鸿宇的《理想三旬》，中间有个换气点练习之后还是没卡好让我耿耿于怀。不过好在还是通过了，接下来我就作为合唱团男低声部的击剑人之一加入了排练，参与了不少演出。我真的非常非常感谢林总、郎总和雨菲姐带我唱歌，在组会定在周四之前我基本没有缺席过排练，在 105A 教室排练的时光是快乐的，大家都在非常纯粹的歌唱。\n\n\n\n\n\n到后来为了六月的音乐会排练，音乐会后的庆功宴上我们在小四川也一直歌唱。在暑假封校的时候每次循环到《醉鬼的敬酒曲》，我就回想起那天浪漫的大家，伴着音乐唱着喝着，互相敬酒。七月参加团中央举办的致敬最闪亮的星的活动，是几位院士在场的演出，也是首次进到录音棚录音。\n\n考试周结束，为了去上海保利大剧院参加比赛而新学了《玉门关》，词中描绘的故事让人动容，大家纷纷感叹“久仰了，玉门关”。那段时间我相当自律，为了第二天的排练状态，晚上都会早早的睡下，于我而言也是一段非常美好的记忆。近日，我们学的新曲子又变多了，我的谱夹一天天地变厚，让我想起第一次看到林总的谱夹。他每次都带着这么个沉甸甸的谱夹来排练，那是合唱团员的勋章。彩虹有首歌是《我歌唱的理由有很多》，字句戳心，用到合唱的大家身上毫不违和。总之，能加入鹰之歌是我超级幸运的事情，音乐真的是救世主，是一剂良药。\n\n# 1.2 | 不得不谈的科研学习\n\n接下来讲讲另外一部分，关于科研。总的来说，这一年我所感知到的同辈压力少了很多，但是如果用产出来衡量我这一年的科研学习生活的话，那无疑是糟糕的。\n\n竞赛方面，暑假因为南京疫情一直待在学校，接触了些视觉的比赛，零零散散地参考 OpenMMLab 系列的开源框架攒了一套自己的代码框架，用这套框架大大小小地去接触了些新任务，对我的视野有较大的提升吧，我了解了 Re-ID 是怎么做以及一些 trick，还有真实场景下的 few-shot 任务用什么方案更合理。比较难过的是，过去了这么久，我依然没找到合适的队友组队打比赛，也没能单枪匹马地拿到一些拿得出手的成绩，我知道竞赛圈确实就是成绩说话，后面会把竞赛的兴趣暂时搁置下，去多做做其他更加适合我的方向。\n\n论文方面呢，很遗憾的是今年没有一篇投稿，甚至没有攒出一些成体系的工作。上半年的话基本是在探索如何去更好地拓展我的方向，去年那篇论文的基础上，打算接下来专注于弱监督语义分割这个领域去做一些事情。但是调研结束的暑假和下半年我不知道时间为什么过得这么快，每天都很重复的实验室寝室的路线，没有给自己设限，给自己设定的会议 Deadline 一个都没有赶上，这半年的状态包括最近都很差。我的感觉是这段日子患上了很严重的科研洁癖和知行不一症，一边不愿意去做简陋的工作，觉得很多论文的想法不过是xxx或者缝合怪，以一种傲慢的姿态去批评他人的学术工作，另一方面我自己有很多的 idea 都没有去实现，没有真真切切地去将想法落地转换为成果。在近期我才意识到这个问题，作为研究者，其实应该去发现一篇文章的贡献，以这个导向去阅读论文，一直以挑错误的方式去对待他人的工作是种不好的习惯，我将其称之为盲目的学术优越感，仿佛批评他人的工作就能将自己抬高一般，我之后会警惕这种思想，并且尽可能快地将自己的idea 进行尝试和落地，有意识的去整理和总结。\n\n但是从另外的角度回想，我其实这一年收获了不少，我了解了更多的领域，拓宽了我的知识面。有意识地从度量学习/表示学习的角度去理解问题，感觉正在慢慢地形成个人较为稳定的思考范式，能够较好地去理解新工作的合理性；此外我也在尝试从不同的角度去解读工作，避免自己陷入单一的思维范式，想想作者为什么这么分析，其他的角度为什么不好，也有助于思考和写作能力的提升。\n\n# 1.3 | 平淡生活的些许光彩\n\n\n\n\n\n\n\n其实除了唱歌和科研之外还有大块的时间我是待在实验室的，宅在位置上这种状态其实大三在创新区大致就养成了，当时会和老王和赵神出去撸串，现在呢偶尔会出去走走，想想今年也去了一些地方，一月份回家的时候先到成都，和潇哥见了面，又去吃了《孤独的美食家》中心心念念很久的独食烤肉，三月份跟着袁伟伟老师去武汉逛了一圈，武大的樱花和人文气息给我留下了特别深的印象；五月份和欣忆去红山动物园逛了逛，也跟着衡帅去牛首山领略佛教圣地的恢弘；十月去杭州参加 VALSE，场场爆满的科研分享让我看到了学术界和产业界的活力，和刘师傅还有高中同桌兼组长小旻同学吃了饭，和老朋友吃饭总是心情愉悦的。十二月初，借着东部新区的活动回了次成都，这趟悠闲的旅途算是给我放了几天假。\n\n此外，还参加了一些活动，和南实的学弟学妹一起画风筝，拍加油视频，冬至前吃羊肉汤；在校研会大大小小地也办了一些活动，责任在肩不做事于我而言不太舒服；六月的毕业季和好朋友们也拍了照吃了饭，好好地道别；七月的疫情在学校做了不少志愿者的工作，和路哥他们在校内吃了顿火锅；十月和十二月作为学生代表参加了成都东部新区的推介会；年末参加了十佳歌手院推的选拔等等。\n\n\n\n\n\n\n\n然后梳理下今年在创作分享方面所达成的新成就吧，在知识分享方面，今年受邀参加了 Datawhale 关于"实际工作中数据和模型的价值"的Talk，分享了些关于 Model-centric 和 Data-centric 的一些观点，后续也参与了后续的一期论文分享的嘉宾。以及在微软开源学习社区做了一期自己研究方向“弱监督语义分割”的一些观点分享。在创作方面，整理了点竞赛的失败经验写成了博客，其实打竞赛的过程蛮暴力的，缺乏自己的思考，后当勉之。将 Model-centric 和 Data-centric 的一些观点整理好写成了一篇文章发到了知乎上，将自己参加十佳歌手院推的参赛视频上传到 Bilibili 上，开通了个人公众号并在生日和暑假分别写了两篇短文。也算是成为最最初级的创作者了吧，知乎的知识创作上，我要向宁鲲鹏师兄学习，如何在科研上拥有自己的观点，并批判性的去读文章的同时要发现文章的闪光点。Bilibili 的视频创作上，我要向衡帅和方方学习，真实欢乐，不拘一格。特别是衡帅的胶片相机和方方的老相机特有的复古感让我很沉浸其中。在个人公众号上，我也很喜欢方方的文字，细腻和贴近生活，又能狠下心来剖析自己和生活。好嘞，来年在创作上又有新的学习对象啦。\n\n# 1.4 | 用文字记录自我认知\n\n在这篇年终总结的尾声，我想用文字理解和分析下我自己。于我而言，研究生这段生活是为了更好地认知自己，是为了成为更完整的人。我今年对游戏的热情消退了些，我还记得我本科问过老王如何排遣无聊的时光，现在我大致有了答案了，生活充实起来其实就可以，不断地提醒自己戒掉那些不干净的多巴胺来源。游戏这个东西，耗掉了时间却不一定能置换来快乐。还会催生心里坏情绪的产生。而且现在游戏氛围越来越差，这种愤懑让人有非理智购物和暴食的冲动，以恶欲压制不满的情绪，终会让我觉得日子很糟糕。\n\n此外我也认识到人生总有个支线任务是得到认可，获得自己、个人和社会的认可，你把故事讲给自己和别人听是需要完善自己的逻辑的，把事情梳理清楚有助于形成自己的稳固的价值观。零散的想法时常会有，但是需要总结好才成体系，统一自己的逻辑有助于将事情简化，把欲望简化。我是喜欢观察和思考的，在有些时候我显得过于正经，但是我觉得改不掉了，我油滑不起来，我感受到父母有意识地在锻炼我酒桌上的讲话水平，但我确实学不来。\n\n还是和去年一样，我依然认为我是一个有浪漫主义色彩但又习惯在现实框架中观察和畅想的人，去年我的结论是：生存只是附庸，我一定要好好地生活，我觉得今年可能有小些变化，要加点前提，但内心依旧是认可这个想法的。我有时候做事其实感性理性的逻辑都有，但大部分情况下倾向于理性状态，大家都听周杰伦，我也听，但也并不是他的忠实粉丝。相比于周杰伦，我更喜欢陈奕迅。周杰伦的歌是美妙的，是鲜活的颜色，而陈奕迅的歌写的是一个个暗色调的故事，更残酷但更接近现实。\n\n关于感情方面，去年 Tape 上有人问我 “害怕进入一段亲密关系吗”，严格来讲，我是容易被距离感困扰的人，一旦距离远到离开我的日常生活，我是不善于维系感情的。我认为培养自己的完整人格以及认清自我是当下比较重要的事情，我不害怕进入一段亲密关系，我只是不刻意，但我其实满希望能拥有一段非异地的校园恋爱的，不过就现在来讲，应该是达不成了。琐事其实压不垮一个人，情绪可以，异地恋所带来的情绪上的影响在很多时刻都蛮关键的，我尝试了但我还是做不好，所以我应当是不愿意再去谈异地恋了，不过也不能说的绝对了，说不定能遇到个我愿意再重拾起热情的人。\n\n# 1.5 | 需要致谢的大家\n\n除了之前提到的人吧，我还非常感谢我的父母，在本科想要直接去就业的时候劝说我读个研究生。在高中也是这样，他们非常支持我去隔壁的市念高中，我极度不愿意的情况下还是去南实生活了三年，这三年对我的影响很大，甚至我觉得是我迄今为止对我影响最大的一段时光，在正确的时间教会了我很多事情。研究生这段经历我到现在来讲还是比较喜欢的。此外两个学长对我的职业生涯规划影响也蛮大的，史晨阳学长一直鼓励我，也给我分享一些自己最真实的想法，宁鲲鹏学长在科研上给予我的肯定和指导其实给了我不少信心，感谢两位学长一直以来的帮助。还有同组的师兄弟们，我们的关系相当融洽还是很幸运的，希望能一直这样下去。还要感谢能够打破边界感的朋友们，文清涛哥这些朋友真的很难得，于我而言边界感这种东西我不会主动去打破，还有朋友们推我一把才行。\n\n# 1.6 | 对未来的期许\n\n关于未来，总有些期许在，明年就应该找实习和正式工作了，这一年想必是比较难了，希望你坚持住，还要充满勇气，勇气是生活的赞歌。在这里给新的一年许一些小愿望吧\n\n * 今年顺便修了几个 typo 水了个 PR 成了mmdet 的 contributor，希望明年在开源方面要更进一步，不管是完善自己的框架还是参与到社区建设中去\n * 明晰时间规划和减轻拖延症，希望你明年末尽量不要再有今天这种时间逝去的无力感\n * 找到一份实习和正式工作，我对你的要求不太高你应该知道，但是希望你最终的去处自己满意就好啦\n * 身体健康也是第一位的，好好锻炼吧，尽量地瘦下去\n * 希望你能够中一篇顶会论文，可能会比较难\n * 在创作方面更进一步，记录生活，分享知识，用文字和画面去感受日常美好\n * 在空闲时候去些想去的地方，好好逛逛和记录这座城市\n\n新的一年好好加油吧，Muyun99！\n\n\n# 02 | 以下是收集向，主要回顾自己的书影生活、听的歌曲以及在摘录些在生活中的只言片语\n\n# 2.1 | 沉浸各类书影\n\n今年在豆瓣上大概总结了下自己的书影生活，回顾一下去年和今年的书影生活\n\n * 照旧的是书籍没看多少，去年重读了《龙族》，新看了《一句顶一万句》和《恶意》。看刘震云的文字和我看余华的书感觉十分相似，读着读着就有非常强的不真实感，总感觉书里的故事是和我周遭的现实是脱离的，但是回到现实往往又会邂逅书中的故事，这种魅力令人着迷。今年小说只看了本《球状闪电》，以及《睡眠革命》《浪潮之巅》之类并没有看完的效率类书籍，以及正在看的《明朝那些事儿》\n * 至于影视作品则是一大堆，比较推荐的影视剧有《绝命毒师》《觉醒年代》《去他妈的世界》《Sex Education》《硅谷》《365：逆转命运的一年》《人民的名义》《扫黑风暴》《禁忌少女》，以及跟着b站up主看了一堆《疑犯追踪》《暗黑》等等剧。之前十分热衷追b站up主的解读，但我真切的感受到自己的思维能力有一些下降，现在会有意识地去排斥这些二手三手的解读信息了，过于舒适的获取信息容易形成思维惯性，让我思考的能力变弱。\n * 电影部分《心灵奇旅》《海上钢琴师》可以排到我的去年年度前二。生活压垮不了灵魂，其实这两个主角都算是幸运的人，能够在很早的时候就发现自己所爱并坚持下去，种种有些 nerd 的行为我也完全能够理解。《心灵奇旅》电影节奏轻松明快，在时而捧腹的观影过程中不时的冒出几句让人思考的鸡汤让观影体验十分舒适；而《海上钢琴师》的1900在舷梯上驻足了很久，看不到陆地的尽头。面对未知和不确定性，钢琴外的世界也不再以自己为中心。偌大的陆地，故事很多，但迷茫的是不知道那个故事和他有关。今年重温了些经典老电影，《教父》《无间道》《暗战1》都给我留下特别深刻的印象，以及《深夜食堂》《柯达克罗姆胶卷》这类非常治愈人心的电影。\n * 去年还还有一堆现实纪录片《书记》《大同》《高三》《隐形亿万富翁》《怦然心动的人生整理魔法》等等，看完大同之后还专门去找了大同的学弟求证，确实如片中所说的真实，而《书记》就更真实了，那个书记在拍纪录片的时候不小心被录到行贿的内容，在他落马之后被放出，而《高三》这部纪录片让人唏嘘感叹，现如今片中学生的去向也很有趣。不过不可避免的是这群学生和《北京东路的日子》中学生的人生轨迹有太多太多的不一样，去年《北京东路的日子》的学生一起拍了个十年荣耀版，有兴趣的同学可以去看看。\n * 安利一些去年我看过觉得不错的动漫和漫画有《我的三体》《强风吹拂》《灵笼》《进击的巨人》《罗小黑战纪》《刺客伍六七》《四月是你的谎言》《咒术回战》《REFILE》《中国唱诗班》《某科学的超电磁炮》《一人之下》《镖人》，其中国漫占比十分大，虽然我入坑也不久，但是能感觉到国漫的水准已经很棒了，故事节奏和叙述手法都有自己比较独特的风格，让我印象深刻的有《一人之下》的道家底蕴，《镖人》的独特中国风，《我的三体》的像素风和社区自治，以及不断飞刀子的众多国漫真是让我又爱又恨！然后补的几部老番真的很戳，《强风吹拂》开跑箱根驿传的时候我和主角同样紧张，会为他们欢呼呐喊，沉浸感超强；《四月是你的谎言》成功骗出了我的眼泪和首次承包，音乐真真切切地会让我感到快乐，但故事又是这么让人忧伤；《进击的巨人》还是一如既往的高水准霸权作品。今年看过的《狐妖小红娘》《国王排名》《英雄联盟：双城之战》也都是好评。\n\n# 2.2 | 音乐就是救世主\n\n听的歌曲还是保留原来的风格，今年看了乐队的夏天的综艺，解锁了些乐队的歌，也很好听。\n\n * 李宗盛的《晚婚》，一定要听李宗盛版本的，我循环了好几天，很喜欢里面的歌词“我从来不想独身，却又预感晚婚，我在等，世上唯一契合灵魂”\n * 宋冬野的《郭源潮》，喜欢歌词中描写的意象，还有这句“层楼终究误少年，自由早晚乱余生”。\n * 陈奕迅的很多歌，听的最多的是《无条件》和《喜帖街》，以及年底新出的《孤勇者》\n * 毛不易的很多歌，最喜欢的是雄狮少年的主题曲《无名的人》，推荐大家关注一下作词者唐恬，也是《孤勇者》《体面》《追光者》的作词者，看完她的故事相信可以更好地共情\n * 上海彩虹室内合唱团的很多歌：《告别时刻》、《玉门关》、《外公写的一封信》、《醉鬼的敬酒曲》，《我歌唱的理由有很多》《道别是一件难事》。大家去实际听一下就能感受出来，告别时刻的前奏，外公的词，玉门关的恢弘，醉鬼的敬酒曲庆功宴的版本让我想起音乐会后的时光等等\n * 此外还有房东的猫、谢春花、新裤子乐队、棱镜乐队、薛之谦、李荣浩、梁博、莫文蔚、李宗盛、许嵩、赵雷、陈粒，前几年听的较多的花粥和宋冬野近几年听的较少了些。\n\n# 2.3 | 生活只言片语的摘录：\n\n日常会在 Telegram 上发些文字，有些关于科研，有些关于生活，回顾的时候觉得有些文字值得整理在年终总结里，也分享出来给大家看看，如果大家对其中某些观点有共鸣或者想讨论真的欢迎随时找我。不过这里的 Markdown 格式好难调...\n\n * 2021 年 01 月 03 日\n   \n   彩虹合唱团的很多成员都是兼职的，这我真的难以想象。“不过对这样一支由音乐爱好者组成的“兼职合唱团”来说，乐队的土壤与艺术的根源，依然来自音乐厅。”今年十月去上海听了他们 2020 第一场音乐会，也是我二十余年的第一次音乐会，很舒服的内容，内涵丰富而跳脱，严肃、深情、厚重多重的交织真的不易\n\n * 2021 年 04 月 01 日\n   \n   能够在四月的第一天就听到合唱团的钢琴曲真是太棒了。去年四月底在返校的列车上补完了四谎，没想到今年能够的幸运地去武大看樱花，也能够听到合唱团的钢琴。希望能够在樱花树下再听一曲合奏\n\n * 2021 年 04 月 12 日\n   \n   上交无可奉告论坛的告别视频，一个大作业衍生出来的论坛却真的见证了校园百态。匿名所带来的不可控性对社区是不利的，但其确实也是无法避免的。一代又一代的学生们想要做些事情，付出了巨大努力开创的“新时代”，很快就倒下了，甚至被人遗忘，但这种行动本身意义非凡。只是最后苦苦支撑着的人心中的不甘和无奈往往是无法随着时间完全散去的\n\n * 2021 年 04 月 15 日\n   \n   高中语文老师第一次的寒假作业就是让我们阅读《活着》，在寒假看完之后我在大学又陆续读了两三遍，还看了同名电影。后面又看了很多余华老师的书，《第七天》，《兄弟》，《许三观卖血记》等等。当时看的时候并无如此深刻的感触，毕竟在高中时代我们的签名和座右铭都是关于理想，关于星辰大海，向往着长大后的生活，哪里见过这般真实的人间。\n   \n   上大学了，纯粹的快乐越来越稀有，开始看到各种匪夷所思的事情，活久见以及不停地叹息愤怒，情绪混杂到一起，也默默地将年少时的签名抹去。这两天负面情绪比较多，我不敢说对所有事件的当事人共情，但我是容易陷进这种情绪中的。不管怎样，还是向前看，积极地去找寻生命中的光。有人喜欢低熵的生活，有人适应高熵的日常，人生百态，应当多些深层思考，多些时间认清自己\n\n * 2021 年 04 月 26 日\n   \n   "差异化才能生存，泯然众生是宇宙引力”。读起来是真的晦涩，但是读完解析后豁然开朗。动物为了对抗环境，所以在活着的时候努力维持自己的体温来与环境抗衡，直到死去才会被环境温度左右；历史中混乱是常态，和平是偶然，所以一代代政权为了对抗历史洪流的必然做出了极大的努力；宇宙里熵增是必然，所以我们现在处处听到推崇低熵生活的声音，来避免被宇宙同化。宇宙是否是拟人态的不重要，但它给予我们阻力让我们进化和成长的过程很有意思，与天斗真是其乐无穷呵。持续不断地付出努力避免被同化是我们进化的唯一途径，被宇宙同化，任引力摆布，终是会把人拉扯成奇形怪状，不是人的模样\n\n * 2021 年 05 月 05 日\n   \n   很喜欢这句话诶：“只希望我们都要好好睡觉、该充电的时候好好充电、明白自己的一天是如何构成的，然后可以踏实地入睡～ Be water，my friends！一直在追求生活的绵柔感”\n\n * 2021 年 07 月 15 日\n   \n   原来余震会间隔这么久的，10多年前的汶川地震印象还是十分深刻。2点多我正走在去学校的路上，突然旁边的房子开始摇晃，像被撕扯开一样，旁边的大人叫嚷着说房子要塌了，我在马路中央本能地蹲下（记得当时并没有什么害怕的情绪）。\n   \n   我们当地的震感比较强烈但是没有持续太久，我起身继续去学校。学校通知放假了，同学们欢呼雀跃。我和一个小伙伴就去隔壁中学打乒乓球了，打完回家被爸妈好生数落了一阵，说到处找我都没找到。接着大家就都不住楼房了，我记得我和姐姐在老爸的大货车里睡觉，租了碟片看电视剧，是与霍去病有关的战争题材的古装剧，老爸的货车停在广场里，旁边都是大家的帐篷。\n   \n   就这样睡了几天之后，我们回了老家，晚上就把床搬到大姑家的院坝里歇息，中途通知复课了好 几次，去了好几次又通知继续放假，就这样大段无聊的时光过去，终于复课了，小学的教学楼成了危房，我们转移到了操场的棚子里上课，暑天的太阳格外毒辣，数学老师都中暑了，我记得语文课讲了文成公主进藏，松赞干布之类的课文。\n   \n   以上是我个人的汶川地震的经历，平淡乏味。但无数的人在那个时间倒下，再也不能继续前行。到现在 B 站依然还会给我推这些视频：播报汶川地震的主持人的专业素养抑不住泪流和哽咽；凭着对家人的强烈信念坚持了70多小时的老朽被救出后仍然不治身亡；\n   \n   能够普普通通过完一生本就极度幸运了，如果有机会做一些事情当然更好。这次余震很多人的电视和手机上都收到了地震预警，有效地减少了人员伤亡，致敬背后的科研人员！是真正的将论文落到实处，写在祖国大地上\n\n * 2021 年 08 月 08 日\n   \n   分享文章：关于高考的杂思（https://zhuanlan.zhihu.com/p/378250408）"等到长大了开始直面真实世界，社会就露出了凶残的獠牙，虽然目标仍然明确，但再也没有按部就班的学习资源，学习任务变成了半监督、自监督和强化学习，得要自己给自己找事做，自我评判，主动探索，自我总结，找高手请教帮忙，等等。\n   \n   既然最终的目标并非有监督学习，那么小时候的教育还是要集中在有监督学习呢？我想这是因为它的可靠性和标准化。就像我们在做任何一件新任务的时候，一开始永远是拿现成数据做一做模仿学习看看效果的。我们几乎可以确认，它不会产生特别好的结果，也不会是最后结稿方案的主力，但却是最有把握，风险也最小的方案。\n\n * 2021 年 10 月 03 日\n   \n   分享文章：中国应该重视本科教育质量，而不是研究经费和论文数量（https://mp.weixin.qq.com/s/vgOvXdU6l6OgP9HH_rDVyw）本科时候喜欢在五四评优论坛上看大家分享自己的故事，喜欢这种不同际遇所带来的多元化；大四时候给学弟学妹分享的时候也说了好好思考自己喜不喜欢科研，很多人是不适合读研究生的；《硅谷》电视剧里也说，课堂会泯灭人的创造力。\n   \n   回首我的大学生涯，我认为是较为丰富多元的经历帮助我塑造了我的人格，潜移默化地影响着我，我很感谢这段经历。而对于本科教育，我感兴趣的课程我都课下私底学过，不感兴趣的就真的没有好好学，翘课是经常的。导致我最近比较偏激的是，经常怂恿一些学弟学妹不要去上课，研究生就多听听国外的课，本科生就去MOOC上找国内的好老师的课。\n   \n   不过冷静下来想，就算是在合唱团这种纯粹歌唱的地方，大家也避免不了谈论绩点和卷，谈论保研，绩点是保研的敲门砖，但我觉得没必要去用课堂那么多时间去完成。\n   \n   我上述的那种较为偏激的做法有他的合理性，但是我也是会只看我自己感兴趣的课程。所以针对还不知道对什么感兴趣的同学，通识类课程和人文类课程是很重要的，不过可惜的是，包括我在内的大部分同学都会把这些当成水课。大三下学期幡然醒悟该去找实习了才想起之前好像上过一门职业规划的课，研究生看论文和刷题越发理解线代、概率论和研究生阶段修的组合数学的重要性。\n   \n   所以课程存在是有他的合理性在的，问题出在哪里呢，我个人认为是对学生和教师的评价体系出了问题。教师评优评奖看论文和项目，学生评优评奖绩点占了80%甚至更多，这就导致压力很大的情况下大家只能往评价体系的这把尺子方面靠。一些老师不看重教学，祖传PPT也不想着如何改进；一些学生为了保研而功利性地努力，导致上研究生才发现自己不适合做科研，悻悻地多读了几年书，完成研究生学业。课堂为教师和学生带来的都是普适的约束力，但其如何改善，任重而道远\n\n * 2021 年 10 月 09 日\n   \n   前段时间有个很厉害的老师问我读不读他的博士，我当时心无旁骛，直接拒绝说要去工业界挣钱。后知后觉，过了这么久想想感觉好像错过了一个好机会，可能是此生仅有的读博的机会了。工作后努努力看能不能再弥补这个遗憾\n   \n   在电话里给我妈提到这个事，我感觉我帅呆了，我还以为她也会一如既往支持我的决定。没想到她这次劝我好好考虑考虑，让我不用担心赚钱的事情，后面给我姐说的时候也再次提到了这个问题，父母还是很希望我能继续深造下去的。哎这算是个遗憾吧，但我应当不会太后悔，在学校吊儿郎当地搞学术，不如去工业界待几年见识见识。上次组会师兄他们汇报讲了讲实习做的事情我觉得解决真实业务下的问题也蛮有意义的，而且我想挣挣钱。说实话我的兴趣太广泛了，而且没要生活费之后没地方搞钱了，我自己也拿不到竞赛的奖金，发了奖学金啥的就去买点好玩的玩意自己玩\n   \n   钱在我这倒不重要，只是拥有能让我实现一些小愿望的资金对我来讲很重要。我深知未来的人生会为了固定的大部头去努力，所以我更珍惜在青年时代给我们带来幸福的物件，这有助于塑造我们成为一个完全的人\n\n * 2021 年 10 月 20 日\n   \n   分享文章：为什么你应该（从现在开始就）写博客（http://mindhacks.cn/2009/02/15/why-you-should-start-blogging-now/）。其实就是对知识有意识的整理和输出，我一直认为：人生总有个支线任务是得到认可，获得自己、个人和社会的认可。将故事讲给自己和别人听是需要完善自己的逻辑的，把事情梳理清楚有助于形成自己的稳固的价值观。零散的想法时常会有，但是需要总结好才成体系，统一自己的逻辑有助于将事情简化，把欲望简化\n\n * 2021 年 11 月 20 日\n   \n   分享文章：愿君多修葺，此物最相思：从我的故事谈「数字遗产」保护（https://sspai.com/post/69901）：作者谈了一个小众的话题：数字遗产的保护。众所周知，在互联网上存在着另外一个自己，存在着一份现实世界的投影，无数的例子从正面侧面勾勒出一个人的模样，甚至还能够发现其随着时序而变化。那数字遗产中总会有些内容当事人不希望被某些人继承，不希望死去的人对活着的人造成更沉重的打击，文章以伴侣的角度讨论了这件事情。\n   \n   但基于这个我想到最近在思考的另一件事情，在年轻人数字遗产无比丰富的同时。我们的父辈和爷爷辈并没在数字世界上留下太多东西。他们不写博客，不太发朋友圈。我认为我们家已经是很传统了，但还是只有每年春节的全家福才有较为正式隆重的影像记录，但这也更像是一种任务。我最近在担心一件事情，我担心我的爷爷去世后找不到一张好的照片，我担心他去世后他的故事就这样淹没在岁月里，我爸知道他的故事多，但我知道的寥寥无几，我只知道他年青时是个纤夫。\n   \n   还有我父亲的故事，囿于学业，我和身为货车驾驶员的父亲的时间几乎是错开的，他习惯晚上出车，因为路上车少速度容易提上去。我大学有次回家的火车坐到隔壁市里，他刚好到那里装货，我费劲地上了货车（货车还挺高的，许久不上了不太熟练），坐在车上去到当地的水泥厂，轰隆轰隆的装货噪音。我现在在想如果买一个海螺给我爸，他听到的会不会是装货的呼啸声，而不是海的声音。今年给他买无线耳机的时候他用不习惯，说可能是装货时留下的耳疾。自那以后到现在四五年了，我再未去过水泥厂，但他还是在出车，我担心这些故事就这样散到岁月中收不回来了。 有些数字遗产对我们是累赘，但对我的爷爷和我的父亲这样的时代洪流中的小人物来讲，却是无比珍贵\n\n * 2021 年 12 月 14 日\n   \n   分享文章 https://fars.ee/qSdy：喜欢"利维亚的杰洛特"同学的陈述：\n   \n   1、避免标签化的烙印，让人回归人的价值，重建人与人之间的联系\n   \n   2、身为一介书生，但浪漫至死不渝\n   \n   我想，总有些人前赴后继地想做些什么，但总是前赴后继地颓颓倒下，沦为一场生动有代价的社会实验。但这意义重大，挣破框架和尺度，为普众探寻什么路走的通，什么路就算走不通也要硬走，这是身为这样的人，必经的路',normalizedContent:'# 01 | 飞速逝去的一年\n\n\n\n本来想要掺着零零散散没发出来的20年年终总结一起写的。看了看去年的流水账也没太大意义。回想下今年的日子，其实成片段的回忆镜头还是蛮少的，生活里一部分是歌唱，一部分是科研，剩下的就是快乐的摸鱼。先大致讲讲这一年值得记录的事情吧。\n\n2021 年我也正式的来到22岁，成为了南航的硕士生，经历了研一下学期和研二上学期两段学校时光。在年初归家的飞机落地后，我收到了论文录用的消息，一篇 ccf-b 的会议论文，所以年关过得还算快乐安稳。春节过后，我在川内的城市兜转了一圈，和高中的好朋友们相聚，也去见了高中班主任老唐，整个春节没有过多烦恼，过得还是很欢乐的。春节结束，我和姐姐带着父母去重庆做体检，希望这个习惯能一直保持下去，平安健康我现在觉得重要过一切。\n\n# 1.1 | 纯粹浪漫的合唱时光\n\n从山城归宁后，我记不清我的研一下学期做了些什么。我的室友都知道，我是个浴室歌手，喜欢在淋浴的时候歌唱。在今年三月的百团大战，我到微软学生俱乐部的摊位上和大四的学弟学妹聊了会天，然后趁旁边鹰之歌合唱团的学弟（现在想起来或许就是林总）不注意，偷偷扫了他们的招新群二维码。\n\n机缘巧合下，我或许成为了今年第一位面试的同学，面试唱的是陈鸿宇的《理想三旬》，中间有个换气点练习之后还是没卡好让我耿耿于怀。不过好在还是通过了，接下来我就作为合唱团男低声部的击剑人之一加入了排练，参与了不少演出。我真的非常非常感谢林总、郎总和雨菲姐带我唱歌，在组会定在周四之前我基本没有缺席过排练，在 105a 教室排练的时光是快乐的，大家都在非常纯粹的歌唱。\n\n\n\n\n\n到后来为了六月的音乐会排练，音乐会后的庆功宴上我们在小四川也一直歌唱。在暑假封校的时候每次循环到《醉鬼的敬酒曲》，我就回想起那天浪漫的大家，伴着音乐唱着喝着，互相敬酒。七月参加团中央举办的致敬最闪亮的星的活动，是几位院士在场的演出，也是首次进到录音棚录音。\n\n考试周结束，为了去上海保利大剧院参加比赛而新学了《玉门关》，词中描绘的故事让人动容，大家纷纷感叹“久仰了，玉门关”。那段时间我相当自律，为了第二天的排练状态，晚上都会早早的睡下，于我而言也是一段非常美好的记忆。近日，我们学的新曲子又变多了，我的谱夹一天天地变厚，让我想起第一次看到林总的谱夹。他每次都带着这么个沉甸甸的谱夹来排练，那是合唱团员的勋章。彩虹有首歌是《我歌唱的理由有很多》，字句戳心，用到合唱的大家身上毫不违和。总之，能加入鹰之歌是我超级幸运的事情，音乐真的是救世主，是一剂良药。\n\n# 1.2 | 不得不谈的科研学习\n\n接下来讲讲另外一部分，关于科研。总的来说，这一年我所感知到的同辈压力少了很多，但是如果用产出来衡量我这一年的科研学习生活的话，那无疑是糟糕的。\n\n竞赛方面，暑假因为南京疫情一直待在学校，接触了些视觉的比赛，零零散散地参考 openmmlab 系列的开源框架攒了一套自己的代码框架，用这套框架大大小小地去接触了些新任务，对我的视野有较大的提升吧，我了解了 re-id 是怎么做以及一些 trick，还有真实场景下的 few-shot 任务用什么方案更合理。比较难过的是，过去了这么久，我依然没找到合适的队友组队打比赛，也没能单枪匹马地拿到一些拿得出手的成绩，我知道竞赛圈确实就是成绩说话，后面会把竞赛的兴趣暂时搁置下，去多做做其他更加适合我的方向。\n\n论文方面呢，很遗憾的是今年没有一篇投稿，甚至没有攒出一些成体系的工作。上半年的话基本是在探索如何去更好地拓展我的方向，去年那篇论文的基础上，打算接下来专注于弱监督语义分割这个领域去做一些事情。但是调研结束的暑假和下半年我不知道时间为什么过得这么快，每天都很重复的实验室寝室的路线，没有给自己设限，给自己设定的会议 deadline 一个都没有赶上，这半年的状态包括最近都很差。我的感觉是这段日子患上了很严重的科研洁癖和知行不一症，一边不愿意去做简陋的工作，觉得很多论文的想法不过是xxx或者缝合怪，以一种傲慢的姿态去批评他人的学术工作，另一方面我自己有很多的 idea 都没有去实现，没有真真切切地去将想法落地转换为成果。在近期我才意识到这个问题，作为研究者，其实应该去发现一篇文章的贡献，以这个导向去阅读论文，一直以挑错误的方式去对待他人的工作是种不好的习惯，我将其称之为盲目的学术优越感，仿佛批评他人的工作就能将自己抬高一般，我之后会警惕这种思想，并且尽可能快地将自己的idea 进行尝试和落地，有意识的去整理和总结。\n\n但是从另外的角度回想，我其实这一年收获了不少，我了解了更多的领域，拓宽了我的知识面。有意识地从度量学习/表示学习的角度去理解问题，感觉正在慢慢地形成个人较为稳定的思考范式，能够较好地去理解新工作的合理性；此外我也在尝试从不同的角度去解读工作，避免自己陷入单一的思维范式，想想作者为什么这么分析，其他的角度为什么不好，也有助于思考和写作能力的提升。\n\n# 1.3 | 平淡生活的些许光彩\n\n\n\n\n\n\n\n其实除了唱歌和科研之外还有大块的时间我是待在实验室的，宅在位置上这种状态其实大三在创新区大致就养成了，当时会和老王和赵神出去撸串，现在呢偶尔会出去走走，想想今年也去了一些地方，一月份回家的时候先到成都，和潇哥见了面，又去吃了《孤独的美食家》中心心念念很久的独食烤肉，三月份跟着袁伟伟老师去武汉逛了一圈，武大的樱花和人文气息给我留下了特别深的印象；五月份和欣忆去红山动物园逛了逛，也跟着衡帅去牛首山领略佛教圣地的恢弘；十月去杭州参加 valse，场场爆满的科研分享让我看到了学术界和产业界的活力，和刘师傅还有高中同桌兼组长小旻同学吃了饭，和老朋友吃饭总是心情愉悦的。十二月初，借着东部新区的活动回了次成都，这趟悠闲的旅途算是给我放了几天假。\n\n此外，还参加了一些活动，和南实的学弟学妹一起画风筝，拍加油视频，冬至前吃羊肉汤；在校研会大大小小地也办了一些活动，责任在肩不做事于我而言不太舒服；六月的毕业季和好朋友们也拍了照吃了饭，好好地道别；七月的疫情在学校做了不少志愿者的工作，和路哥他们在校内吃了顿火锅；十月和十二月作为学生代表参加了成都东部新区的推介会；年末参加了十佳歌手院推的选拔等等。\n\n\n\n\n\n\n\n然后梳理下今年在创作分享方面所达成的新成就吧，在知识分享方面，今年受邀参加了 datawhale 关于"实际工作中数据和模型的价值"的talk，分享了些关于 model-centric 和 data-centric 的一些观点，后续也参与了后续的一期论文分享的嘉宾。以及在微软开源学习社区做了一期自己研究方向“弱监督语义分割”的一些观点分享。在创作方面，整理了点竞赛的失败经验写成了博客，其实打竞赛的过程蛮暴力的，缺乏自己的思考，后当勉之。将 model-centric 和 data-centric 的一些观点整理好写成了一篇文章发到了知乎上，将自己参加十佳歌手院推的参赛视频上传到 bilibili 上，开通了个人公众号并在生日和暑假分别写了两篇短文。也算是成为最最初级的创作者了吧，知乎的知识创作上，我要向宁鲲鹏师兄学习，如何在科研上拥有自己的观点，并批判性的去读文章的同时要发现文章的闪光点。bilibili 的视频创作上，我要向衡帅和方方学习，真实欢乐，不拘一格。特别是衡帅的胶片相机和方方的老相机特有的复古感让我很沉浸其中。在个人公众号上，我也很喜欢方方的文字，细腻和贴近生活，又能狠下心来剖析自己和生活。好嘞，来年在创作上又有新的学习对象啦。\n\n# 1.4 | 用文字记录自我认知\n\n在这篇年终总结的尾声，我想用文字理解和分析下我自己。于我而言，研究生这段生活是为了更好地认知自己，是为了成为更完整的人。我今年对游戏的热情消退了些，我还记得我本科问过老王如何排遣无聊的时光，现在我大致有了答案了，生活充实起来其实就可以，不断地提醒自己戒掉那些不干净的多巴胺来源。游戏这个东西，耗掉了时间却不一定能置换来快乐。还会催生心里坏情绪的产生。而且现在游戏氛围越来越差，这种愤懑让人有非理智购物和暴食的冲动，以恶欲压制不满的情绪，终会让我觉得日子很糟糕。\n\n此外我也认识到人生总有个支线任务是得到认可，获得自己、个人和社会的认可，你把故事讲给自己和别人听是需要完善自己的逻辑的，把事情梳理清楚有助于形成自己的稳固的价值观。零散的想法时常会有，但是需要总结好才成体系，统一自己的逻辑有助于将事情简化，把欲望简化。我是喜欢观察和思考的，在有些时候我显得过于正经，但是我觉得改不掉了，我油滑不起来，我感受到父母有意识地在锻炼我酒桌上的讲话水平，但我确实学不来。\n\n还是和去年一样，我依然认为我是一个有浪漫主义色彩但又习惯在现实框架中观察和畅想的人，去年我的结论是：生存只是附庸，我一定要好好地生活，我觉得今年可能有小些变化，要加点前提，但内心依旧是认可这个想法的。我有时候做事其实感性理性的逻辑都有，但大部分情况下倾向于理性状态，大家都听周杰伦，我也听，但也并不是他的忠实粉丝。相比于周杰伦，我更喜欢陈奕迅。周杰伦的歌是美妙的，是鲜活的颜色，而陈奕迅的歌写的是一个个暗色调的故事，更残酷但更接近现实。\n\n关于感情方面，去年 tape 上有人问我 “害怕进入一段亲密关系吗”，严格来讲，我是容易被距离感困扰的人，一旦距离远到离开我的日常生活，我是不善于维系感情的。我认为培养自己的完整人格以及认清自我是当下比较重要的事情，我不害怕进入一段亲密关系，我只是不刻意，但我其实满希望能拥有一段非异地的校园恋爱的，不过就现在来讲，应该是达不成了。琐事其实压不垮一个人，情绪可以，异地恋所带来的情绪上的影响在很多时刻都蛮关键的，我尝试了但我还是做不好，所以我应当是不愿意再去谈异地恋了，不过也不能说的绝对了，说不定能遇到个我愿意再重拾起热情的人。\n\n# 1.5 | 需要致谢的大家\n\n除了之前提到的人吧，我还非常感谢我的父母，在本科想要直接去就业的时候劝说我读个研究生。在高中也是这样，他们非常支持我去隔壁的市念高中，我极度不愿意的情况下还是去南实生活了三年，这三年对我的影响很大，甚至我觉得是我迄今为止对我影响最大的一段时光，在正确的时间教会了我很多事情。研究生这段经历我到现在来讲还是比较喜欢的。此外两个学长对我的职业生涯规划影响也蛮大的，史晨阳学长一直鼓励我，也给我分享一些自己最真实的想法，宁鲲鹏学长在科研上给予我的肯定和指导其实给了我不少信心，感谢两位学长一直以来的帮助。还有同组的师兄弟们，我们的关系相当融洽还是很幸运的，希望能一直这样下去。还要感谢能够打破边界感的朋友们，文清涛哥这些朋友真的很难得，于我而言边界感这种东西我不会主动去打破，还有朋友们推我一把才行。\n\n# 1.6 | 对未来的期许\n\n关于未来，总有些期许在，明年就应该找实习和正式工作了，这一年想必是比较难了，希望你坚持住，还要充满勇气，勇气是生活的赞歌。在这里给新的一年许一些小愿望吧\n\n * 今年顺便修了几个 typo 水了个 pr 成了mmdet 的 contributor，希望明年在开源方面要更进一步，不管是完善自己的框架还是参与到社区建设中去\n * 明晰时间规划和减轻拖延症，希望你明年末尽量不要再有今天这种时间逝去的无力感\n * 找到一份实习和正式工作，我对你的要求不太高你应该知道，但是希望你最终的去处自己满意就好啦\n * 身体健康也是第一位的，好好锻炼吧，尽量地瘦下去\n * 希望你能够中一篇顶会论文，可能会比较难\n * 在创作方面更进一步，记录生活，分享知识，用文字和画面去感受日常美好\n * 在空闲时候去些想去的地方，好好逛逛和记录这座城市\n\n新的一年好好加油吧，muyun99！\n\n\n# 02 | 以下是收集向，主要回顾自己的书影生活、听的歌曲以及在摘录些在生活中的只言片语\n\n# 2.1 | 沉浸各类书影\n\n今年在豆瓣上大概总结了下自己的书影生活，回顾一下去年和今年的书影生活\n\n * 照旧的是书籍没看多少，去年重读了《龙族》，新看了《一句顶一万句》和《恶意》。看刘震云的文字和我看余华的书感觉十分相似，读着读着就有非常强的不真实感，总感觉书里的故事是和我周遭的现实是脱离的，但是回到现实往往又会邂逅书中的故事，这种魅力令人着迷。今年小说只看了本《球状闪电》，以及《睡眠革命》《浪潮之巅》之类并没有看完的效率类书籍，以及正在看的《明朝那些事儿》\n * 至于影视作品则是一大堆，比较推荐的影视剧有《绝命毒师》《觉醒年代》《去他妈的世界》《sex education》《硅谷》《365：逆转命运的一年》《人民的名义》《扫黑风暴》《禁忌少女》，以及跟着b站up主看了一堆《疑犯追踪》《暗黑》等等剧。之前十分热衷追b站up主的解读，但我真切的感受到自己的思维能力有一些下降，现在会有意识地去排斥这些二手三手的解读信息了，过于舒适的获取信息容易形成思维惯性，让我思考的能力变弱。\n * 电影部分《心灵奇旅》《海上钢琴师》可以排到我的去年年度前二。生活压垮不了灵魂，其实这两个主角都算是幸运的人，能够在很早的时候就发现自己所爱并坚持下去，种种有些 nerd 的行为我也完全能够理解。《心灵奇旅》电影节奏轻松明快，在时而捧腹的观影过程中不时的冒出几句让人思考的鸡汤让观影体验十分舒适；而《海上钢琴师》的1900在舷梯上驻足了很久，看不到陆地的尽头。面对未知和不确定性，钢琴外的世界也不再以自己为中心。偌大的陆地，故事很多，但迷茫的是不知道那个故事和他有关。今年重温了些经典老电影，《教父》《无间道》《暗战1》都给我留下特别深刻的印象，以及《深夜食堂》《柯达克罗姆胶卷》这类非常治愈人心的电影。\n * 去年还还有一堆现实纪录片《书记》《大同》《高三》《隐形亿万富翁》《怦然心动的人生整理魔法》等等，看完大同之后还专门去找了大同的学弟求证，确实如片中所说的真实，而《书记》就更真实了，那个书记在拍纪录片的时候不小心被录到行贿的内容，在他落马之后被放出，而《高三》这部纪录片让人唏嘘感叹，现如今片中学生的去向也很有趣。不过不可避免的是这群学生和《北京东路的日子》中学生的人生轨迹有太多太多的不一样，去年《北京东路的日子》的学生一起拍了个十年荣耀版，有兴趣的同学可以去看看。\n * 安利一些去年我看过觉得不错的动漫和漫画有《我的三体》《强风吹拂》《灵笼》《进击的巨人》《罗小黑战纪》《刺客伍六七》《四月是你的谎言》《咒术回战》《refile》《中国唱诗班》《某科学的超电磁炮》《一人之下》《镖人》，其中国漫占比十分大，虽然我入坑也不久，但是能感觉到国漫的水准已经很棒了，故事节奏和叙述手法都有自己比较独特的风格，让我印象深刻的有《一人之下》的道家底蕴，《镖人》的独特中国风，《我的三体》的像素风和社区自治，以及不断飞刀子的众多国漫真是让我又爱又恨！然后补的几部老番真的很戳，《强风吹拂》开跑箱根驿传的时候我和主角同样紧张，会为他们欢呼呐喊，沉浸感超强；《四月是你的谎言》成功骗出了我的眼泪和首次承包，音乐真真切切地会让我感到快乐，但故事又是这么让人忧伤；《进击的巨人》还是一如既往的高水准霸权作品。今年看过的《狐妖小红娘》《国王排名》《英雄联盟：双城之战》也都是好评。\n\n# 2.2 | 音乐就是救世主\n\n听的歌曲还是保留原来的风格，今年看了乐队的夏天的综艺，解锁了些乐队的歌，也很好听。\n\n * 李宗盛的《晚婚》，一定要听李宗盛版本的，我循环了好几天，很喜欢里面的歌词“我从来不想独身，却又预感晚婚，我在等，世上唯一契合灵魂”\n * 宋冬野的《郭源潮》，喜欢歌词中描写的意象，还有这句“层楼终究误少年，自由早晚乱余生”。\n * 陈奕迅的很多歌，听的最多的是《无条件》和《喜帖街》，以及年底新出的《孤勇者》\n * 毛不易的很多歌，最喜欢的是雄狮少年的主题曲《无名的人》，推荐大家关注一下作词者唐恬，也是《孤勇者》《体面》《追光者》的作词者，看完她的故事相信可以更好地共情\n * 上海彩虹室内合唱团的很多歌：《告别时刻》、《玉门关》、《外公写的一封信》、《醉鬼的敬酒曲》，《我歌唱的理由有很多》《道别是一件难事》。大家去实际听一下就能感受出来，告别时刻的前奏，外公的词，玉门关的恢弘，醉鬼的敬酒曲庆功宴的版本让我想起音乐会后的时光等等\n * 此外还有房东的猫、谢春花、新裤子乐队、棱镜乐队、薛之谦、李荣浩、梁博、莫文蔚、李宗盛、许嵩、赵雷、陈粒，前几年听的较多的花粥和宋冬野近几年听的较少了些。\n\n# 2.3 | 生活只言片语的摘录：\n\n日常会在 telegram 上发些文字，有些关于科研，有些关于生活，回顾的时候觉得有些文字值得整理在年终总结里，也分享出来给大家看看，如果大家对其中某些观点有共鸣或者想讨论真的欢迎随时找我。不过这里的 markdown 格式好难调...\n\n * 2021 年 01 月 03 日\n   \n   彩虹合唱团的很多成员都是兼职的，这我真的难以想象。“不过对这样一支由音乐爱好者组成的“兼职合唱团”来说，乐队的土壤与艺术的根源，依然来自音乐厅。”今年十月去上海听了他们 2020 第一场音乐会，也是我二十余年的第一次音乐会，很舒服的内容，内涵丰富而跳脱，严肃、深情、厚重多重的交织真的不易\n\n * 2021 年 04 月 01 日\n   \n   能够在四月的第一天就听到合唱团的钢琴曲真是太棒了。去年四月底在返校的列车上补完了四谎，没想到今年能够的幸运地去武大看樱花，也能够听到合唱团的钢琴。希望能够在樱花树下再听一曲合奏\n\n * 2021 年 04 月 12 日\n   \n   上交无可奉告论坛的告别视频，一个大作业衍生出来的论坛却真的见证了校园百态。匿名所带来的不可控性对社区是不利的，但其确实也是无法避免的。一代又一代的学生们想要做些事情，付出了巨大努力开创的“新时代”，很快就倒下了，甚至被人遗忘，但这种行动本身意义非凡。只是最后苦苦支撑着的人心中的不甘和无奈往往是无法随着时间完全散去的\n\n * 2021 年 04 月 15 日\n   \n   高中语文老师第一次的寒假作业就是让我们阅读《活着》，在寒假看完之后我在大学又陆续读了两三遍，还看了同名电影。后面又看了很多余华老师的书，《第七天》，《兄弟》，《许三观卖血记》等等。当时看的时候并无如此深刻的感触，毕竟在高中时代我们的签名和座右铭都是关于理想，关于星辰大海，向往着长大后的生活，哪里见过这般真实的人间。\n   \n   上大学了，纯粹的快乐越来越稀有，开始看到各种匪夷所思的事情，活久见以及不停地叹息愤怒，情绪混杂到一起，也默默地将年少时的签名抹去。这两天负面情绪比较多，我不敢说对所有事件的当事人共情，但我是容易陷进这种情绪中的。不管怎样，还是向前看，积极地去找寻生命中的光。有人喜欢低熵的生活，有人适应高熵的日常，人生百态，应当多些深层思考，多些时间认清自己\n\n * 2021 年 04 月 26 日\n   \n   "差异化才能生存，泯然众生是宇宙引力”。读起来是真的晦涩，但是读完解析后豁然开朗。动物为了对抗环境，所以在活着的时候努力维持自己的体温来与环境抗衡，直到死去才会被环境温度左右；历史中混乱是常态，和平是偶然，所以一代代政权为了对抗历史洪流的必然做出了极大的努力；宇宙里熵增是必然，所以我们现在处处听到推崇低熵生活的声音，来避免被宇宙同化。宇宙是否是拟人态的不重要，但它给予我们阻力让我们进化和成长的过程很有意思，与天斗真是其乐无穷呵。持续不断地付出努力避免被同化是我们进化的唯一途径，被宇宙同化，任引力摆布，终是会把人拉扯成奇形怪状，不是人的模样\n\n * 2021 年 05 月 05 日\n   \n   很喜欢这句话诶：“只希望我们都要好好睡觉、该充电的时候好好充电、明白自己的一天是如何构成的，然后可以踏实地入睡～ be water，my friends！一直在追求生活的绵柔感”\n\n * 2021 年 07 月 15 日\n   \n   原来余震会间隔这么久的，10多年前的汶川地震印象还是十分深刻。2点多我正走在去学校的路上，突然旁边的房子开始摇晃，像被撕扯开一样，旁边的大人叫嚷着说房子要塌了，我在马路中央本能地蹲下（记得当时并没有什么害怕的情绪）。\n   \n   我们当地的震感比较强烈但是没有持续太久，我起身继续去学校。学校通知放假了，同学们欢呼雀跃。我和一个小伙伴就去隔壁中学打乒乓球了，打完回家被爸妈好生数落了一阵，说到处找我都没找到。接着大家就都不住楼房了，我记得我和姐姐在老爸的大货车里睡觉，租了碟片看电视剧，是与霍去病有关的战争题材的古装剧，老爸的货车停在广场里，旁边都是大家的帐篷。\n   \n   就这样睡了几天之后，我们回了老家，晚上就把床搬到大姑家的院坝里歇息，中途通知复课了好 几次，去了好几次又通知继续放假，就这样大段无聊的时光过去，终于复课了，小学的教学楼成了危房，我们转移到了操场的棚子里上课，暑天的太阳格外毒辣，数学老师都中暑了，我记得语文课讲了文成公主进藏，松赞干布之类的课文。\n   \n   以上是我个人的汶川地震的经历，平淡乏味。但无数的人在那个时间倒下，再也不能继续前行。到现在 b 站依然还会给我推这些视频：播报汶川地震的主持人的专业素养抑不住泪流和哽咽；凭着对家人的强烈信念坚持了70多小时的老朽被救出后仍然不治身亡；\n   \n   能够普普通通过完一生本就极度幸运了，如果有机会做一些事情当然更好。这次余震很多人的电视和手机上都收到了地震预警，有效地减少了人员伤亡，致敬背后的科研人员！是真正的将论文落到实处，写在祖国大地上\n\n * 2021 年 08 月 08 日\n   \n   分享文章：关于高考的杂思（https://zhuanlan.zhihu.com/p/378250408）"等到长大了开始直面真实世界，社会就露出了凶残的獠牙，虽然目标仍然明确，但再也没有按部就班的学习资源，学习任务变成了半监督、自监督和强化学习，得要自己给自己找事做，自我评判，主动探索，自我总结，找高手请教帮忙，等等。\n   \n   既然最终的目标并非有监督学习，那么小时候的教育还是要集中在有监督学习呢？我想这是因为它的可靠性和标准化。就像我们在做任何一件新任务的时候，一开始永远是拿现成数据做一做模仿学习看看效果的。我们几乎可以确认，它不会产生特别好的结果，也不会是最后结稿方案的主力，但却是最有把握，风险也最小的方案。\n\n * 2021 年 10 月 03 日\n   \n   分享文章：中国应该重视本科教育质量，而不是研究经费和论文数量（https://mp.weixin.qq.com/s/vgovxdu6l6ogp9hh_rdvyw）本科时候喜欢在五四评优论坛上看大家分享自己的故事，喜欢这种不同际遇所带来的多元化；大四时候给学弟学妹分享的时候也说了好好思考自己喜不喜欢科研，很多人是不适合读研究生的；《硅谷》电视剧里也说，课堂会泯灭人的创造力。\n   \n   回首我的大学生涯，我认为是较为丰富多元的经历帮助我塑造了我的人格，潜移默化地影响着我，我很感谢这段经历。而对于本科教育，我感兴趣的课程我都课下私底学过，不感兴趣的就真的没有好好学，翘课是经常的。导致我最近比较偏激的是，经常怂恿一些学弟学妹不要去上课，研究生就多听听国外的课，本科生就去mooc上找国内的好老师的课。\n   \n   不过冷静下来想，就算是在合唱团这种纯粹歌唱的地方，大家也避免不了谈论绩点和卷，谈论保研，绩点是保研的敲门砖，但我觉得没必要去用课堂那么多时间去完成。\n   \n   我上述的那种较为偏激的做法有他的合理性，但是我也是会只看我自己感兴趣的课程。所以针对还不知道对什么感兴趣的同学，通识类课程和人文类课程是很重要的，不过可惜的是，包括我在内的大部分同学都会把这些当成水课。大三下学期幡然醒悟该去找实习了才想起之前好像上过一门职业规划的课，研究生看论文和刷题越发理解线代、概率论和研究生阶段修的组合数学的重要性。\n   \n   所以课程存在是有他的合理性在的，问题出在哪里呢，我个人认为是对学生和教师的评价体系出了问题。教师评优评奖看论文和项目，学生评优评奖绩点占了80%甚至更多，这就导致压力很大的情况下大家只能往评价体系的这把尺子方面靠。一些老师不看重教学，祖传ppt也不想着如何改进；一些学生为了保研而功利性地努力，导致上研究生才发现自己不适合做科研，悻悻地多读了几年书，完成研究生学业。课堂为教师和学生带来的都是普适的约束力，但其如何改善，任重而道远\n\n * 2021 年 10 月 09 日\n   \n   前段时间有个很厉害的老师问我读不读他的博士，我当时心无旁骛，直接拒绝说要去工业界挣钱。后知后觉，过了这么久想想感觉好像错过了一个好机会，可能是此生仅有的读博的机会了。工作后努努力看能不能再弥补这个遗憾\n   \n   在电话里给我妈提到这个事，我感觉我帅呆了，我还以为她也会一如既往支持我的决定。没想到她这次劝我好好考虑考虑，让我不用担心赚钱的事情，后面给我姐说的时候也再次提到了这个问题，父母还是很希望我能继续深造下去的。哎这算是个遗憾吧，但我应当不会太后悔，在学校吊儿郎当地搞学术，不如去工业界待几年见识见识。上次组会师兄他们汇报讲了讲实习做的事情我觉得解决真实业务下的问题也蛮有意义的，而且我想挣挣钱。说实话我的兴趣太广泛了，而且没要生活费之后没地方搞钱了，我自己也拿不到竞赛的奖金，发了奖学金啥的就去买点好玩的玩意自己玩\n   \n   钱在我这倒不重要，只是拥有能让我实现一些小愿望的资金对我来讲很重要。我深知未来的人生会为了固定的大部头去努力，所以我更珍惜在青年时代给我们带来幸福的物件，这有助于塑造我们成为一个完全的人\n\n * 2021 年 10 月 20 日\n   \n   分享文章：为什么你应该（从现在开始就）写博客（http://mindhacks.cn/2009/02/15/why-you-should-start-blogging-now/）。其实就是对知识有意识的整理和输出，我一直认为：人生总有个支线任务是得到认可，获得自己、个人和社会的认可。将故事讲给自己和别人听是需要完善自己的逻辑的，把事情梳理清楚有助于形成自己的稳固的价值观。零散的想法时常会有，但是需要总结好才成体系，统一自己的逻辑有助于将事情简化，把欲望简化\n\n * 2021 年 11 月 20 日\n   \n   分享文章：愿君多修葺，此物最相思：从我的故事谈「数字遗产」保护（https://sspai.com/post/69901）：作者谈了一个小众的话题：数字遗产的保护。众所周知，在互联网上存在着另外一个自己，存在着一份现实世界的投影，无数的例子从正面侧面勾勒出一个人的模样，甚至还能够发现其随着时序而变化。那数字遗产中总会有些内容当事人不希望被某些人继承，不希望死去的人对活着的人造成更沉重的打击，文章以伴侣的角度讨论了这件事情。\n   \n   但基于这个我想到最近在思考的另一件事情，在年轻人数字遗产无比丰富的同时。我们的父辈和爷爷辈并没在数字世界上留下太多东西。他们不写博客，不太发朋友圈。我认为我们家已经是很传统了，但还是只有每年春节的全家福才有较为正式隆重的影像记录，但这也更像是一种任务。我最近在担心一件事情，我担心我的爷爷去世后找不到一张好的照片，我担心他去世后他的故事就这样淹没在岁月里，我爸知道他的故事多，但我知道的寥寥无几，我只知道他年青时是个纤夫。\n   \n   还有我父亲的故事，囿于学业，我和身为货车驾驶员的父亲的时间几乎是错开的，他习惯晚上出车，因为路上车少速度容易提上去。我大学有次回家的火车坐到隔壁市里，他刚好到那里装货，我费劲地上了货车（货车还挺高的，许久不上了不太熟练），坐在车上去到当地的水泥厂，轰隆轰隆的装货噪音。我现在在想如果买一个海螺给我爸，他听到的会不会是装货的呼啸声，而不是海的声音。今年给他买无线耳机的时候他用不习惯，说可能是装货时留下的耳疾。自那以后到现在四五年了，我再未去过水泥厂，但他还是在出车，我担心这些故事就这样散到岁月中收不回来了。 有些数字遗产对我们是累赘，但对我的爷爷和我的父亲这样的时代洪流中的小人物来讲，却是无比珍贵\n\n * 2021 年 12 月 14 日\n   \n   分享文章 https://fars.ee/qsdy：喜欢"利维亚的杰洛特"同学的陈述：\n   \n   1、避免标签化的烙印，让人回归人的价值，重建人与人之间的联系\n   \n   2、身为一介书生，但浪漫至死不渝\n   \n   我想，总有些人前赴后继地想做些什么，但总是前赴后继地颓颓倒下，沦为一场生动有代价的社会实验。但这意义重大，挣破框架和尺度，为普众探寻什么路走的通，什么路就算走不通也要硬走，这是身为这样的人，必经的路',charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"年少可以听听李宗盛，只是容易上头",frontmatter:{title:"年少可以听听李宗盛，只是容易上头",date:"2022-01-07T16:32:19.000Z",permalink:"/pages/de6315/",categories:["生活杂谈","心情杂货"],tags:[null]},regularPath:"/03.%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88/01.%E5%BF%83%E6%83%85%E6%9D%82%E8%B4%A7/07.%E5%B9%B4%E5%B0%91%E5%8F%AF%E4%BB%A5%E5%90%AC%E5%90%AC%E6%9D%8E%E5%AE%97%E7%9B%9B%EF%BC%8C%E5%8F%AA%E6%98%AF%E5%AE%B9%E6%98%93%E4%B8%8A%E5%A4%B4.html",relativePath:"03.生活杂谈/01.心情杂货/07.年少可以听听李宗盛，只是容易上头.md",key:"v-44df5588",path:"/pages/de6315/",headersStr:null,content:"晚上总会找些歌听，巧的是又找到了李宗盛，找到了《新写的旧歌》。我点进去之前就做好了 Emo 的准备，不过我没想到会这么彻底。\n\n歌词娓娓道来，但把自己狠狠地剖析，真实和容易令人共情。李宗盛唱腔里独有的叹息着念歌词，是对自己的对话，像个突然恍惚过来的孩子似的。但醒过神来，已经到了容易落泪的年岁了。\n\n这是封充满遗憾的信，在外风光的孩子回家会对父亲有意无意地炫耀自己的成就，但是遇到些严肃的感情释放却还是怯生生。过时不候，还没来得及和解却失去机会。\n\n厘清父子关系这个命题并非易事。我和我爸很不像，小时候他晚上开车，一般凌晨才回家，我偶尔能被开门和开灯的声响中醒来，白天我去上学时他还在呼呼大睡，用尽力气补够觉好迎接下趟出车，这种错开的生物钟习惯了也还好。还有我和他的性格蛮不同的，他脾气很爆，声音很大，但他反应很快，做事情很周到。我做事说话总是温吞不利索，有时词穷了也只顾着笑。\n\n我以为这样的风格是生来就有的，后来他有次给我埋怨说耳机声音有点小，说经常去装货，机器轰鸣，可能对听力有影响。我这才恍惚地意识这是到他说话很吵，电视声音放很大的原因。我又想到小时候我上货车很费劲，当我能轻松地上去的时候，却又换成更大更高的车了。\n\n我坐我爸的货车不晕车，坐其他车一会儿就晕，我爷爷也是，我小时候很理性地认为这是隔代遗传，还是有次听着我表弟夸我爸车技好才意识到些不同的缘由。很多时候我都是后知后觉，并不能探清内里的逻辑，站在我的角度去叙事，现在看来是存在偏见和误读，有些文字从我的嘴里蹦出来可能更让父母失望，我也会在事后懊恼。\n\n我不知道怎么来衔接这首歌和我的故事，回忆着回忆着就忘记主题了。很多个晚上，总是有意无意地看到李宗盛三个字就会陷进去，前段时间听到他的《晚婚》，循环了很久。大家都在讲，年少不听李宗盛，但我总想从李宗盛的歌里听到些年少时就应该去做的事情。所以我在有些事情上变得成熟，要归功于李宗盛的音乐，归功于歌里绘述的人生故事。\n\n得有四五年没坐过老爸的货车了，这个支撑起我们家的老伙计，待我春节归家，得去探望探望他了。",normalizedContent:"晚上总会找些歌听，巧的是又找到了李宗盛，找到了《新写的旧歌》。我点进去之前就做好了 emo 的准备，不过我没想到会这么彻底。\n\n歌词娓娓道来，但把自己狠狠地剖析，真实和容易令人共情。李宗盛唱腔里独有的叹息着念歌词，是对自己的对话，像个突然恍惚过来的孩子似的。但醒过神来，已经到了容易落泪的年岁了。\n\n这是封充满遗憾的信，在外风光的孩子回家会对父亲有意无意地炫耀自己的成就，但是遇到些严肃的感情释放却还是怯生生。过时不候，还没来得及和解却失去机会。\n\n厘清父子关系这个命题并非易事。我和我爸很不像，小时候他晚上开车，一般凌晨才回家，我偶尔能被开门和开灯的声响中醒来，白天我去上学时他还在呼呼大睡，用尽力气补够觉好迎接下趟出车，这种错开的生物钟习惯了也还好。还有我和他的性格蛮不同的，他脾气很爆，声音很大，但他反应很快，做事情很周到。我做事说话总是温吞不利索，有时词穷了也只顾着笑。\n\n我以为这样的风格是生来就有的，后来他有次给我埋怨说耳机声音有点小，说经常去装货，机器轰鸣，可能对听力有影响。我这才恍惚地意识这是到他说话很吵，电视声音放很大的原因。我又想到小时候我上货车很费劲，当我能轻松地上去的时候，却又换成更大更高的车了。\n\n我坐我爸的货车不晕车，坐其他车一会儿就晕，我爷爷也是，我小时候很理性地认为这是隔代遗传，还是有次听着我表弟夸我爸车技好才意识到些不同的缘由。很多时候我都是后知后觉，并不能探清内里的逻辑，站在我的角度去叙事，现在看来是存在偏见和误读，有些文字从我的嘴里蹦出来可能更让父母失望，我也会在事后懊恼。\n\n我不知道怎么来衔接这首歌和我的故事，回忆着回忆着就忘记主题了。很多个晚上，总是有意无意地看到李宗盛三个字就会陷进去，前段时间听到他的《晚婚》，循环了很久。大家都在讲，年少不听李宗盛，但我总想从李宗盛的歌里听到些年少时就应该去做的事情。所以我在有些事情上变得成熟，要归功于李宗盛的音乐，归功于歌里绘述的人生故事。\n\n得有四五年没坐过老爸的货车了，这个支撑起我们家的老伙计，待我春节归家，得去探望探望他了。",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"Data-centric vs Model-centric 的个人拙见",frontmatter:{title:"Data-centric vs Model-centric 的个人拙见",date:"2022-03-08T19:28:53.000Z",permalink:"/pages/4341a5/",categories:["生活杂谈","学术杂谈"],tags:[null]},regularPath:"/03.%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88/02.%E5%AD%A6%E6%9C%AF%E6%9D%82%E8%B0%88/00.Data-centric%20vs%20Model-centric%20%E7%9A%84%E4%B8%AA%E4%BA%BA%E6%8B%99%E8%A7%81.html",relativePath:"03.生活杂谈/02.学术杂谈/00.Data-centric vs Model-centric 的个人拙见.md",key:"v-644ecf6a",path:"/pages/4341a5/",headers:[{level:2,title:"Data-centric vs Model-centric 的个人拙见",slug:"data-centric-vs-model-centric-的个人拙见",normalizedTitle:"data-centric vs model-centric 的个人拙见",charIndex:2}],headersStr:"Data-centric vs Model-centric 的个人拙见",content:'# Data-centric vs Model-centric 的个人拙见\n\n\n\n该篇文章大概记录了我在 Datawhale 的“实际工作中数据和模型的价值"的相关分享。题图是我最近很喜欢的彩虹合唱团的歌《醉鬼的敬酒曲》，分享给大家。\n\n数据和模型是深度学习两条支柱，都是应该被关注的。但目前业界对数据的关注程度不够，所以今天我想分享有关 Data-centric 的一些个人理解，包括三个不成熟的观点。\n\n# 1、样本生而不等\n\n深度学习时代数据为王，但是数据其实生而不等。如今有各种注意力机制，这些注意力建模了特征图上的不同坐标、不同通道以及特征间的重要性，并学出对应的权重，我将这类注意力想要统称为特征注意力。我认为数据注意力也是同样重要的，使用数据训练模型的过程中，如果平等地去处理每个样本，是非常粗粒度的处理方式，而数据本身存在非常多的问题，介绍几个学界里定义的任务：\n\n * 主动学习是一种降低标注代价的算法，它建模了哪些数据是更加值得去标注的，相同的样本数量，利用主动学习算法标注的样本子集训出的模型性能更好。\n\n * 标签带噪学习适用于标签有错误的情况，常用的做法是给予样本不同的权重，分类任务里面我们找到了错误标签，它的权重就为0，但在检测分割里面，我们要更细粒度地考虑这个权重的获取\n\n * 自步学习的理念是，不同的训练阶段要喂给模型不同的数据。例如和人类学习一样，在刚开始训练时我们应当学简单的数据，渐渐加大学习样本的难度，让学习过程变得更加平滑一些。\n\n上述三个学术问题其实都可以看作是获得一个样本的权重来表示其重要性。所以我们除了用扩展样本维度的手段来近似现实之外（从时序上扩展到视频数据，从空间上扩展到 3D 数据），我们至少还应该建模每个样本的权重来体现其生而不等的特性。\n\n# 2、Data-centric 的进步会倒逼 Model-centric 的进步\n\nTransformer 其实是将图像数据建模为类似于NLP序列输入的方式取得了一定的成功，让我们看到了CV 和 NLP 两个大领域大一统的可能性。但是往本质上去想，他其实是对图像的输入做了另一个方面的解构。图像输入其实是有人的倾向性在里面的，这种输入形式源自于我们很久之前对颜色通道的定义，然后就用这种形式存储在了计算机中。那么这个定义是否有利于机器学习的应用呢，我们是否可以信息量更丰富的方式呢？\n\n除了对样本做别样的解构，自然的，可以想到对标签做独特的解构。首先思考一个问题，人为设定的标签的设置是好的吗？我认为这其实带有人对目标任务的直觉和偏好在里面。个人有一个不太准确的观点，数据驱动任务（虽然有时候是先有任务再去收集数据），我们收集来的数据可以帮助我们学习不同的模型。但是我们收集数据的过程目前还是原始且符合直觉的，例如类别标注驱动图像分类，目标框标注驱动目标检测，我们是否可以用类别标注驱动目标检测（弱监督），是否可以用颜色标注来驱动表示学习（自监督），我认为对于数据的理解会推动模型的进步。\n\n另外一方面，我们对于目前的样本输入和标签体系应该提出质疑，应该努力提升它们的信息量。知识蒸馏为什么能够成功呢，我觉得软标签给出了类别分布，从信息论的角度软标签含有更为丰富的信息量。但是正常人去标注一个类别是给不出精确的标签分布的，我们一般不会说一张图像有百分之多少的可能性像一只猫，另外百分之多少的可能性像一只老虎，对于人类来讲估计一个可以量化的标签分布是比较难的，但是即使有噪声，我们是否可以尝试类似的任务呢。所以我认为我们需要对标签体系也需要进行适当的质疑，我们对分类任务是否只需要一个硬标签，对于检测任务我们是不是只用一个框，用几个中心点或者显著性能不能做检测任务呢，我们使用怎样的标签能够使得我们获得更大的信息量。\n\n这里有一个不太恰当的比喻：大部分的鲸鱼的歌声频率大约在17~18赫兹，这个频率太低了，人的耳朵是无法听见的，但我们就可以说这部分人类没有能力接收到的歌声没有信息量吗？我们如今的样本和标签都是非常直觉的处理方式，我们可以尝试一些不符合视觉的解读方式来，得益于深度学习模型拥有足够大的模型容量，辅以多样化的设计，我觉得有很大的几率可以开辟一个新的潮流。\n\n# 3、Data-Centric 相关竞赛\n\n业界应该关注 Data-centric 的竞赛，将关注点置于数据的相关操作。我们可以固定模型以及超参数，让选手对数据做对应的调整，最后选手上交的策略可以分为三部分（欢迎补充）：\n\n * 样本的增强策略：例如对低光照亮度增强，样本去噪，图像增广\n\n * 标签的增强策略：例如从硬标签到软标签，纠正错误标签\n\n * 对每个样本给定独有的动态变化的学习权重：例如在学习过程中其学习权重可以自适应的变化\n\n最后再举个例子，以自动驾驶为代表的真实场景拥有大量的长尾数据，学术界倾向于用算法去解决这样的问题，定义类似 Open-set 这样的学术设置，期望对于没见过的类别也可以做到比较好。而工业界倾向于加数据，例如用大量的车大量的众包标注去搜集数据集，去做高精地图以及完善数据搜集、提高数据质量的自动化流程等等 ROI 高的工作。我认为两个方向都没错，大家都在致力于去解决问题。模型和数据永远是深度学习的两条腿，现如今模型容量已经足够大，大到对于某些任务，模型参数大到有能力记住所有的输入参数。所以我们应当要提升数据的信息量，更好地近似和逼近现实。并且数据理解是有很大复用性的，这种领域知识可以迁移，\n\n此外，今后对于规模数据的存储及共享的解决方案，例如面向AI的云原生数据库（如Milvus）以及联邦学习等会带来很好的创业和发展机会，这是 MLOps 的基础设施，也是深度学习落地非常大的助力。\n\n以上就是我的一些不太成熟的想法，我认为 Data-centric 相关的工作是非常重要的，欢迎有兴趣的同学一起讨论，如有纰漏请大家指正，提前感谢各位的不吝赐教。\n\n# 4、参考资料\n\n * 样本生而不等——聊聊那些对训练数据加权的方法\n * MLOps: From Model-centric to Data-centric AI\n * 斯坦福2021秋：实用机器学习\n * 关于主动学习、标签分布学习、偏标记学习：可以关注东南大学耿新老师以及南航黄圣君老师的相关工作',normalizedContent:'# data-centric vs model-centric 的个人拙见\n\n\n\n该篇文章大概记录了我在 datawhale 的“实际工作中数据和模型的价值"的相关分享。题图是我最近很喜欢的彩虹合唱团的歌《醉鬼的敬酒曲》，分享给大家。\n\n数据和模型是深度学习两条支柱，都是应该被关注的。但目前业界对数据的关注程度不够，所以今天我想分享有关 data-centric 的一些个人理解，包括三个不成熟的观点。\n\n# 1、样本生而不等\n\n深度学习时代数据为王，但是数据其实生而不等。如今有各种注意力机制，这些注意力建模了特征图上的不同坐标、不同通道以及特征间的重要性，并学出对应的权重，我将这类注意力想要统称为特征注意力。我认为数据注意力也是同样重要的，使用数据训练模型的过程中，如果平等地去处理每个样本，是非常粗粒度的处理方式，而数据本身存在非常多的问题，介绍几个学界里定义的任务：\n\n * 主动学习是一种降低标注代价的算法，它建模了哪些数据是更加值得去标注的，相同的样本数量，利用主动学习算法标注的样本子集训出的模型性能更好。\n\n * 标签带噪学习适用于标签有错误的情况，常用的做法是给予样本不同的权重，分类任务里面我们找到了错误标签，它的权重就为0，但在检测分割里面，我们要更细粒度地考虑这个权重的获取\n\n * 自步学习的理念是，不同的训练阶段要喂给模型不同的数据。例如和人类学习一样，在刚开始训练时我们应当学简单的数据，渐渐加大学习样本的难度，让学习过程变得更加平滑一些。\n\n上述三个学术问题其实都可以看作是获得一个样本的权重来表示其重要性。所以我们除了用扩展样本维度的手段来近似现实之外（从时序上扩展到视频数据，从空间上扩展到 3d 数据），我们至少还应该建模每个样本的权重来体现其生而不等的特性。\n\n# 2、data-centric 的进步会倒逼 model-centric 的进步\n\ntransformer 其实是将图像数据建模为类似于nlp序列输入的方式取得了一定的成功，让我们看到了cv 和 nlp 两个大领域大一统的可能性。但是往本质上去想，他其实是对图像的输入做了另一个方面的解构。图像输入其实是有人的倾向性在里面的，这种输入形式源自于我们很久之前对颜色通道的定义，然后就用这种形式存储在了计算机中。那么这个定义是否有利于机器学习的应用呢，我们是否可以信息量更丰富的方式呢？\n\n除了对样本做别样的解构，自然的，可以想到对标签做独特的解构。首先思考一个问题，人为设定的标签的设置是好的吗？我认为这其实带有人对目标任务的直觉和偏好在里面。个人有一个不太准确的观点，数据驱动任务（虽然有时候是先有任务再去收集数据），我们收集来的数据可以帮助我们学习不同的模型。但是我们收集数据的过程目前还是原始且符合直觉的，例如类别标注驱动图像分类，目标框标注驱动目标检测，我们是否可以用类别标注驱动目标检测（弱监督），是否可以用颜色标注来驱动表示学习（自监督），我认为对于数据的理解会推动模型的进步。\n\n另外一方面，我们对于目前的样本输入和标签体系应该提出质疑，应该努力提升它们的信息量。知识蒸馏为什么能够成功呢，我觉得软标签给出了类别分布，从信息论的角度软标签含有更为丰富的信息量。但是正常人去标注一个类别是给不出精确的标签分布的，我们一般不会说一张图像有百分之多少的可能性像一只猫，另外百分之多少的可能性像一只老虎，对于人类来讲估计一个可以量化的标签分布是比较难的，但是即使有噪声，我们是否可以尝试类似的任务呢。所以我认为我们需要对标签体系也需要进行适当的质疑，我们对分类任务是否只需要一个硬标签，对于检测任务我们是不是只用一个框，用几个中心点或者显著性能不能做检测任务呢，我们使用怎样的标签能够使得我们获得更大的信息量。\n\n这里有一个不太恰当的比喻：大部分的鲸鱼的歌声频率大约在17~18赫兹，这个频率太低了，人的耳朵是无法听见的，但我们就可以说这部分人类没有能力接收到的歌声没有信息量吗？我们如今的样本和标签都是非常直觉的处理方式，我们可以尝试一些不符合视觉的解读方式来，得益于深度学习模型拥有足够大的模型容量，辅以多样化的设计，我觉得有很大的几率可以开辟一个新的潮流。\n\n# 3、data-centric 相关竞赛\n\n业界应该关注 data-centric 的竞赛，将关注点置于数据的相关操作。我们可以固定模型以及超参数，让选手对数据做对应的调整，最后选手上交的策略可以分为三部分（欢迎补充）：\n\n * 样本的增强策略：例如对低光照亮度增强，样本去噪，图像增广\n\n * 标签的增强策略：例如从硬标签到软标签，纠正错误标签\n\n * 对每个样本给定独有的动态变化的学习权重：例如在学习过程中其学习权重可以自适应的变化\n\n最后再举个例子，以自动驾驶为代表的真实场景拥有大量的长尾数据，学术界倾向于用算法去解决这样的问题，定义类似 open-set 这样的学术设置，期望对于没见过的类别也可以做到比较好。而工业界倾向于加数据，例如用大量的车大量的众包标注去搜集数据集，去做高精地图以及完善数据搜集、提高数据质量的自动化流程等等 roi 高的工作。我认为两个方向都没错，大家都在致力于去解决问题。模型和数据永远是深度学习的两条腿，现如今模型容量已经足够大，大到对于某些任务，模型参数大到有能力记住所有的输入参数。所以我们应当要提升数据的信息量，更好地近似和逼近现实。并且数据理解是有很大复用性的，这种领域知识可以迁移，\n\n此外，今后对于规模数据的存储及共享的解决方案，例如面向ai的云原生数据库（如milvus）以及联邦学习等会带来很好的创业和发展机会，这是 mlops 的基础设施，也是深度学习落地非常大的助力。\n\n以上就是我的一些不太成熟的想法，我认为 data-centric 相关的工作是非常重要的，欢迎有兴趣的同学一起讨论，如有纰漏请大家指正，提前感谢各位的不吝赐教。\n\n# 4、参考资料\n\n * 样本生而不等——聊聊那些对训练数据加权的方法\n * mlops: from model-centric to data-centric ai\n * 斯坦福2021秋：实用机器学习\n * 关于主动学习、标签分布学习、偏标记学习：可以关注东南大学耿新老师以及南航黄圣君老师的相关工作',charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"使用单机多卡分布式训练",frontmatter:{title:"使用单机多卡分布式训练",date:"2022-05-11T16:24:18.000Z",permalink:"/pages/eb4db7/",categories:["学习笔记","PyTorch Tricks"],tags:[null]},regularPath:"/02.%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/26.PyTorch%20Tricks/01.%E4%BD%BF%E7%94%A8%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83.html",relativePath:"02.学习笔记/26.PyTorch Tricks/01.使用单机多卡分布式训练.md",key:"v-34007dfb",path:"/pages/eb4db7/",headersStr:null,content:"1、将模型中所有的普通 BN 转为 SyscBN\n\n\n\n\n1\n\n\n2、调用 PyTorch 的 DDP",normalizedContent:"1、将模型中所有的普通 bn 转为 syscbn\n\n\n\n\n1\n\n\n2、调用 pytorch 的 ddp",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"重参数化宇宙的起源",frontmatter:{title:"重参数化宇宙的起源",date:"2022-03-16T10:17:16.000Z",permalink:"/pages/06b1ed/",categories:["生活杂谈","学术杂谈"],tags:[null]},regularPath:"/03.%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88/02.%E5%AD%A6%E6%9C%AF%E6%9D%82%E8%B0%88/01.%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96%E5%AE%87%E5%AE%99%E7%9A%84%E8%B5%B7%E6%BA%90.html",relativePath:"03.生活杂谈/02.学术杂谈/01.重参数化宇宙的起源.md",key:"v-903271d4",path:"/pages/06b1ed/",headersStr:null,content:"深度可分离卷积\n\nConv 和 BN 的结合\n\n“结构重参数化”这个词的本意就是：用一个结构的一组参数转换为另一组参数，并用转换得到的参数来参数化（parameterize）另一个结构。只要参数的转换是等价的，这两个结构的替换就是等价的。\n\nACNet (ICCV-2019)：Reparam(KxK) = KxK-BN + 1xK-BN + Kx1-BN。这一记法表示用三个平行分支（KxK，1xK，Kx1）的加和来替换一个KxK卷积。注意三个分支各跟一个BN，三个分支分别过BN之后再相加。这样做可以提升卷积网络的性能。\n\nRepVGG (CVPR-2021)：Reparam(3x3) = 3x3-BN + 1x1-BN + BN。对每个3x3卷积，在训练时给它构造并行的恒等和1x1卷积分支，并各自过BN后相加。\n\nDiverse Branch Block (DBB) (CVPR-2021) ：Reparam(KxK) = KxK-BN + 1x1-BN + 1x1-BN-AVG-BN + 1x1-BN-KxK-BN。\n\nResRep (ICCV 2021) : Reparam(KxK) = KxK-BN-1x1。\n\n为什么会work\n\nACNet\n\nRepLKNet (CVPR 2022):\n\nRMNet\n\nhttps://zhuanlan.zhihu.com/p/453479354",normalizedContent:"深度可分离卷积\n\nconv 和 bn 的结合\n\n“结构重参数化”这个词的本意就是：用一个结构的一组参数转换为另一组参数，并用转换得到的参数来参数化（parameterize）另一个结构。只要参数的转换是等价的，这两个结构的替换就是等价的。\n\nacnet (iccv-2019)：reparam(kxk) = kxk-bn + 1xk-bn + kx1-bn。这一记法表示用三个平行分支（kxk，1xk，kx1）的加和来替换一个kxk卷积。注意三个分支各跟一个bn，三个分支分别过bn之后再相加。这样做可以提升卷积网络的性能。\n\nrepvgg (cvpr-2021)：reparam(3x3) = 3x3-bn + 1x1-bn + bn。对每个3x3卷积，在训练时给它构造并行的恒等和1x1卷积分支，并各自过bn后相加。\n\ndiverse branch block (dbb) (cvpr-2021) ：reparam(kxk) = kxk-bn + 1x1-bn + 1x1-bn-avg-bn + 1x1-bn-kxk-bn。\n\nresrep (iccv 2021) : reparam(kxk) = kxk-bn-1x1。\n\n为什么会work\n\nacnet\n\nreplknet (cvpr 2022):\n\nrmnet\n\nhttps://zhuanlan.zhihu.com/p/453479354",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"动态卷积",frontmatter:{title:"动态卷积",date:"2022-03-16T11:05:59.000Z",permalink:"/pages/ba21cc/",categories:["生活杂谈","学术杂谈"],tags:[null]},regularPath:"/03.%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88/02.%E5%AD%A6%E6%9C%AF%E6%9D%82%E8%B0%88/02.%E5%8A%A8%E6%80%81%E5%8D%B7%E7%A7%AF.html",relativePath:"03.生活杂谈/02.学术杂谈/02.动态卷积.md",key:"v-176eac29",path:"/pages/ba21cc/",headersStr:null,content:"# 普通卷积\n\n以 3x3 的卷积为例，padding = 1 代表输出输出尺寸不变\n\n输入：N C1 W H 的 tensor\n\n输出：N C2 W H 的 tensor\n\n卷积核的参数：C1∗C2∗W∗HC_1 * C_2 * W * HC1 ∗C2 ∗W∗H\n\n对于输出的 C2C_2C2 个通道，都需要一个 C1∗W∗HC_1 * W *HC1 ∗W∗H 的卷积核去卷积得到特征图\n\n * 卷积核的通道数量 = 输入通道数量 C1C_1C1\n\n * 卷积核的数量 = 输出通道个数 C2C_2C2\n\n * 卷积核的尺寸 = 特征图的尺寸\n\n# 深度可分离卷积（来自 MobileNet）\n\n将标准卷积分解为逐深度卷积 Depthwise convolution 和逐点卷积 Pointwise convolution\n\n输入：N∗C1∗W∗HN * C_1 * W * HN∗C1 ∗W∗H 的 tensor\n\n输出：N∗C2∗W∗HN * C_2 * W * HN∗C2 ∗W∗H 的 tensor\n\n卷积核的参数：C1∗C2∗W∗HC_1 * C_2 * W * HC1 ∗C2 ∗W∗H\n\n逐深度卷积 Depthwise convolution\n\n * 每个通道进行卷积操作，就得到了和输入特征图通道数一致的输出特征图\n\n * 该步卷积核的参数是：C1∗1∗W∗HC_1 * 1 * W * HC1 ∗1∗W∗H\n\n逐点卷积 Pointwise convolution：\n\n * 使用 1x1 的卷积对上一步得到的特征图进行调整\n\n * 该步卷积核的参数是：C1∗1∗W∗HC_1 * 1 * W * HC1 ∗1∗W∗H\n\n# 空间可分离卷积（来自 ACNet）\n\n# 动态卷积\n\n下采样的方式\n\n上采样的方式\n\n网格效应\n\n#",normalizedContent:"# 普通卷积\n\n以 3x3 的卷积为例，padding = 1 代表输出输出尺寸不变\n\n输入：n c1 w h 的 tensor\n\n输出：n c2 w h 的 tensor\n\n卷积核的参数：c1∗c2∗w∗hc_1 * c_2 * w * hc1 ∗c2 ∗w∗h\n\n对于输出的 c2c_2c2 个通道，都需要一个 c1∗w∗hc_1 * w *hc1 ∗w∗h 的卷积核去卷积得到特征图\n\n * 卷积核的通道数量 = 输入通道数量 c1c_1c1\n\n * 卷积核的数量 = 输出通道个数 c2c_2c2\n\n * 卷积核的尺寸 = 特征图的尺寸\n\n# 深度可分离卷积（来自 mobilenet）\n\n将标准卷积分解为逐深度卷积 depthwise convolution 和逐点卷积 pointwise convolution\n\n输入：n∗c1∗w∗hn * c_1 * w * hn∗c1 ∗w∗h 的 tensor\n\n输出：n∗c2∗w∗hn * c_2 * w * hn∗c2 ∗w∗h 的 tensor\n\n卷积核的参数：c1∗c2∗w∗hc_1 * c_2 * w * hc1 ∗c2 ∗w∗h\n\n逐深度卷积 depthwise convolution\n\n * 每个通道进行卷积操作，就得到了和输入特征图通道数一致的输出特征图\n\n * 该步卷积核的参数是：c1∗1∗w∗hc_1 * 1 * w * hc1 ∗1∗w∗h\n\n逐点卷积 pointwise convolution：\n\n * 使用 1x1 的卷积对上一步得到的特征图进行调整\n\n * 该步卷积核的参数是：c1∗1∗w∗hc_1 * 1 * w * hc1 ∗1∗w∗h\n\n# 空间可分离卷积（来自 acnet）\n\n# 动态卷积\n\n下采样的方式\n\n上采样的方式\n\n网格效应\n\n#",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"从 Tesla AI Day 看自动驾驶的进展",frontmatter:{title:"从 Tesla AI Day 看自动驾驶的进展",date:"2022-03-31T21:50:41.000Z",permalink:"/pages/4de976/",categories:["生活杂谈","学术杂谈"],tags:[null]},regularPath:"/03.%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88/02.%E5%AD%A6%E6%9C%AF%E6%9D%82%E8%B0%88/03.%E4%BB%8E%20Tesla%20AI%20Day%20%E7%9C%8B%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E7%9A%84%E8%BF%9B%E5%B1%95.html",relativePath:"03.生活杂谈/02.学术杂谈/03.从 Tesla AI Day 看自动驾驶的进展.md",key:"v-3a012f16",path:"/pages/4de976/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2023/03/25, 19:58:09"},{title:"Vuepress deploy时的若干问题",frontmatter:{title:"Vuepress deploy时的若干问题",date:"2021-05-31T21:15:18.000Z",permalink:"/pages/3379fb/",categories:["技术文章","常见 bug 修复"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/01.%E5%B8%B8%E8%A7%81%20bug%20%E4%BF%AE%E5%A4%8D/00.Vuepress%20deploy%E6%97%B6%E7%9A%84%E8%8B%A5%E5%B9%B2%E9%97%AE%E9%A2%98.html",relativePath:"04.wiki搬运/01.常见 bug 修复/00.Vuepress deploy时的若干问题.md",key:"v-4582675f",path:"/pages/3379fb/",headersStr:null,content:"1、在 LaTeX 公式中使用中文括号\n\nLaTeX-incompatible input and strict mode is set to 'warn': Unrecognized Unicode character \"−\" (8722) [unknownSymbol]\nLaTeX-incompatible input and strict mode is set to 'warn': Unicode text character \"（\" used in math mode [unicodeTextInMathMode]\nLaTeX-incompatible input and strict mode is set to 'warn': Unicode text character \"）\" used in math mode [unicodeTextInMathMode]\n\n\n1\n2\n3\n\n\n",normalizedContent:"1、在 latex 公式中使用中文括号\n\nlatex-incompatible input and strict mode is set to 'warn': unrecognized unicode character \"−\" (8722) [unknownsymbol]\nlatex-incompatible input and strict mode is set to 'warn': unicode text character \"（\" used in math mode [unicodetextinmathmode]\nlatex-incompatible input and strict mode is set to 'warn': unicode text character \"）\" used in math mode [unicodetextinmathmode]\n\n\n1\n2\n3\n\n\n",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"npm项目启动报错",frontmatter:{title:"npm项目启动报错",date:"2021-03-09T21:41:58.000Z",permalink:"/pages/b5ac46/",categories:["技术文章","常见 bug 修复"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/01.%E5%B8%B8%E8%A7%81%20bug%20%E4%BF%AE%E5%A4%8D/01.npm%E9%A1%B9%E7%9B%AE%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99.html",relativePath:"04.wiki搬运/01.常见 bug 修复/01.npm项目启动报错.md",key:"v-53ea97c3",path:"/pages/b5ac46/",headersStr:null,content:"参考链接：https://blog.csdn.net/feinifi/article/details/103777406\n\n如题所示，最近在使用npm start启动react项目的时候，经常会报这个错误\n\nError: ENOSPC: System limit for number of file watchers reached\n\n\n1\n\n\n出错原因大致意思是文件监控数量超过了系统限制。其实就是打开的文件过多导致的，不管是什么文件，只要有进程在，就是一个file watchers，临时解决办法就是关掉几个进程，再运行npm start，就好了，但是等到系统开启的进程一多起来，再次运行又有可能出现同样的错误，为了永久解决这个问题，必须修改系统参数。\n\n系统默认的参数可以在/proc/sys/fs/inotify/max_user_watches变量中看到，默认是8192。\n\n修改/etc/sysctl.conf文件，在末尾增加一行记录：\n\nfs.inotify.max_user_watches=524288\n\n\n1\n\n\n最后通过 sudo sysctl -p 启用该配置。\n\nsudo sysctl -p\n\n\n1\n\n\n下面是具体的命令\n\ncat /proc/sys/fs/inotify/max_user_watches # 8192\nsudo vim /etc/sysctl.conf # 在最后一行添加 fs.inotify.max_user_watches=524288\n\nsudo sysctl -p\ncat /proc/sys/fs/inotify/max_user_watches # 524288\n\n\n1\n2\n3\n4\n5\n",normalizedContent:"参考链接：https://blog.csdn.net/feinifi/article/details/103777406\n\n如题所示，最近在使用npm start启动react项目的时候，经常会报这个错误\n\nerror: enospc: system limit for number of file watchers reached\n\n\n1\n\n\n出错原因大致意思是文件监控数量超过了系统限制。其实就是打开的文件过多导致的，不管是什么文件，只要有进程在，就是一个file watchers，临时解决办法就是关掉几个进程，再运行npm start，就好了，但是等到系统开启的进程一多起来，再次运行又有可能出现同样的错误，为了永久解决这个问题，必须修改系统参数。\n\n系统默认的参数可以在/proc/sys/fs/inotify/max_user_watches变量中看到，默认是8192。\n\n修改/etc/sysctl.conf文件，在末尾增加一行记录：\n\nfs.inotify.max_user_watches=524288\n\n\n1\n\n\n最后通过 sudo sysctl -p 启用该配置。\n\nsudo sysctl -p\n\n\n1\n\n\n下面是具体的命令\n\ncat /proc/sys/fs/inotify/max_user_watches # 8192\nsudo vim /etc/sysctl.conf # 在最后一行添加 fs.inotify.max_user_watches=524288\n\nsudo sysctl -p\ncat /proc/sys/fs/inotify/max_user_watches # 524288\n\n\n1\n2\n3\n4\n5\n",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"Anaconda下载及配置",frontmatter:{title:"Anaconda下载及配置",date:"2021-03-09T21:12:27.000Z",permalink:"/pages/df0682/",categories:["技术文章","环境配置"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/02.%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/01.Anaconda%E4%B8%8B%E8%BD%BD%E5%8F%8A%E9%85%8D%E7%BD%AE.html",relativePath:"04.wiki搬运/02.环境配置/01.Anaconda下载及配置.md",key:"v-08d4a690",path:"/pages/df0682/",headersStr:null,content:"# 1. 下载 Anaconda\n\nhttps://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/\n\nbash\n\n# 2. 更新Anaconda的源\n\n# 换为清华源\n\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/\nconda config --set show_channel_urls yes   \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 查看源\n\nconda config --show\nvim ~/.condarc\nconda config --show-sources\n\n\n1\n2\n3\n\n\n# 换回原来的源\n\nconda config --remove-key channels\n\n\n1\n\n\n# 3. conda创建虚拟环境\n\n 1. 创建虚拟环境\n    \n    conda create --name PyTorch python=3.7\n    conda create --name TensorFlow python=3.7\n    \n    \n    1\n    2\n    \n\n 2. 删除虚拟环境\n    \n    conda remove -n open-mmlab --all\n    \n    \n    1\n    \n\n# 4. 进入/退出conda创建的环境\n\n 1. 进入conda创建的环境\n    \n    conda activate PyTorch\n    \n    \n    1\n    \n\n 2. 退出conda创建的环境\n    \n    conda activate PyTorch\n    \n    \n    1\n    \n\n# 5. 安装相应的包\n\n 1. 安装PyTorch\n    \n    conda install pytorch torchvision cudatoolkit=10.0 \n    \n    \n    1\n    \n\n 2. 安装TensorFlow\n    \n    conda install tensorflow-gpu=1.13.0\n    \n    \n    1\n    \n\n 3. 安装keras\n    \n    conda install keras\n    \n    \n    1\n    ",normalizedContent:"# 1. 下载 anaconda\n\nhttps://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/\n\nbash\n\n# 2. 更新anaconda的源\n\n# 换为清华源\n\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/\nconda config --set show_channel_urls yes   \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 查看源\n\nconda config --show\nvim ~/.condarc\nconda config --show-sources\n\n\n1\n2\n3\n\n\n# 换回原来的源\n\nconda config --remove-key channels\n\n\n1\n\n\n# 3. conda创建虚拟环境\n\n 1. 创建虚拟环境\n    \n    conda create --name pytorch python=3.7\n    conda create --name tensorflow python=3.7\n    \n    \n    1\n    2\n    \n\n 2. 删除虚拟环境\n    \n    conda remove -n open-mmlab --all\n    \n    \n    1\n    \n\n# 4. 进入/退出conda创建的环境\n\n 1. 进入conda创建的环境\n    \n    conda activate pytorch\n    \n    \n    1\n    \n\n 2. 退出conda创建的环境\n    \n    conda activate pytorch\n    \n    \n    1\n    \n\n# 5. 安装相应的包\n\n 1. 安装pytorch\n    \n    conda install pytorch torchvision cudatoolkit=10.0 \n    \n    \n    1\n    \n\n 2. 安装tensorflow\n    \n    conda install tensorflow-gpu=1.13.0\n    \n    \n    1\n    \n\n 3. 安装keras\n    \n    conda install keras\n    \n    \n    1\n    ",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"连接远程服务器显示Host key verification failed",frontmatter:{title:"连接远程服务器显示Host key verification failed",date:"2021-05-25T23:47:31.000Z",permalink:"/pages/63e1ef/",categories:["技术文章","常见 bug 修复"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/01.%E5%B8%B8%E8%A7%81%20bug%20%E4%BF%AE%E5%A4%8D/02.%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%98%BE%E7%A4%BAHost%20key%20verification%20failed.html",relativePath:"04.wiki搬运/01.常见 bug 修复/02.连接远程服务器显示Host key verification failed.md",key:"v-38d61985",path:"/pages/63e1ef/",headers:[{level:3,title:"连接远程服务器显示Host key verification failed",slug:"连接远程服务器显示host-key-verification-failed",normalizedTitle:"连接远程服务器显示host key verification failed",charIndex:2}],headersStr:"连接远程服务器显示Host key verification failed",content:"# 连接远程服务器显示Host key verification failed\n\n问题显示：\n\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\nThe fingerprint for the ECDSA key sent by the remote host is\nSHA256:***.\nPlease contact your system administrator.\nAdd correct host key in /home/muyun99/.ssh/known_hosts to get rid of this message.\nOffending ECDSA key in /home/muyun99/.ssh/known_hosts:16\n  remove with:\n  ssh-keygen -f ***\nECDSA host key for has changed and you have requested strict checking.\nHost key verification failed.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n老样子，按照提示来即可\n\nssh-keygen -f ***\n\n\n1\n",normalizedContent:"# 连接远程服务器显示host key verification failed\n\n问题显示：\n\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    warning: remote host identification has changed!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nit is possible that someone is doing something nasty!\nsomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nit is also possible that a host key has just been changed.\nthe fingerprint for the ecdsa key sent by the remote host is\nsha256:***.\nplease contact your system administrator.\nadd correct host key in /home/muyun99/.ssh/known_hosts to get rid of this message.\noffending ecdsa key in /home/muyun99/.ssh/known_hosts:16\n  remove with:\n  ssh-keygen -f ***\necdsa host key for has changed and you have requested strict checking.\nhost key verification failed.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n老样子，按照提示来即可\n\nssh-keygen -f ***\n\n\n1\n",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"GPU功率不一致",frontmatter:{title:"GPU功率不一致",date:"2021-07-16T01:59:28.000Z",permalink:"/pages/5628db/",categories:["技术文章","环境配置"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/02.%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/00.GPU%E5%8A%9F%E7%8E%87%E4%B8%8D%E4%B8%80%E8%87%B4.html",relativePath:"04.wiki搬运/02.环境配置/00.GPU功率不一致.md",key:"v-170f256a",path:"/pages/5628db/",headersStr:null,content:"实验ID        备注   \n原有设置             \nGPU显卡调换顺序        \n                 \n\n实验二：\n\n\n\n\n\n实验三：将卡插到上面的插槽\n\n\n\n实验四：将卡插到中间的插槽\n\n\n\n实验五：将两张卡插到第二和第三个插槽\n\n\n\n",normalizedContent:"实验id        备注   \n原有设置             \ngpu显卡调换顺序        \n                 \n\n实验二：\n\n\n\n\n\n实验三：将卡插到上面的插槽\n\n\n\n实验四：将卡插到中间的插槽\n\n\n\n实验五：将两张卡插到第二和第三个插槽\n\n\n\n",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"从零开始配PyTorch GPU环境",frontmatter:{title:"从零开始配PyTorch GPU环境",date:"2021-05-07T16:46:53.000Z",permalink:"/pages/b6a8b0/",categories:["更多","环境搭建"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/02.%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/02.%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E9%85%8DPyTorch%20GPU%E7%8E%AF%E5%A2%83.html",relativePath:"04.wiki搬运/02.环境配置/02.从零开始配PyTorch GPU环境.md",key:"v-b4b99e96",path:"/pages/b6a8b0/",headers:[{level:2,title:"从零开始配PyTorch GPU环境",slug:"从零开始配pytorch-gpu环境",normalizedTitle:"从零开始配pytorch gpu环境",charIndex:2}],headersStr:"从零开始配PyTorch GPU环境",content:"# 从零开始配PyTorch GPU环境\n\n1、下载 Anaconda\n\n * https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/\n\n2、Conda 换源\n\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/\nconda config --set show_channel_urls yes\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 查看镜像源\nconda config --show-sources\n\n\n1\n2\n\n\n# 创建虚拟环境\nconda create --name PyTorch python=3.7\n# 进入虚拟环境\nconda activate PyTorch\n\n\n1\n2\n3\n4\n\n\n3、安装 对应版本的Visual Studio\n\n * 查看对应版本CUDA的文档：https://developer.nvidia.com/cuda-toolkit-archive\n\n * 以 11.0 为例：https://docs.nvidia.com/cuda/archive/11.0/cuda-installation-guide-microsoft-windows/index.html\n\n * \n\n * 下载 Visual Studio 2019 即可：https://visualstudio.microsoft.com/zh-hans/\n\n * 把使用C++的桌面开发勾选上，就能把cl.exe安装好。整个安装好了以后，把包含cl.exe的文件目录添加到PATH环境变量里，环境变量怎么添加这个就自行百度，具体的我添加的cl.exe的路径是：\n   \n   * C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\Hostx64\\x64\n     \n     \n     1\n     \n\n4、更新 GPU 驱动\n\n * 桌面右键点击 NVIDIA 控制面板 -> 点击控制面板的左下角系统信息 -> 组件中可以看到支持的 CUDA 版本\n * \n\n4、安装CUDA 以及 CUDNN\n\n * 下载 CUDA 以及 CUDNN\n\n * 工具及库     版本                                             下载链接\n   Python   3.6.8                                          https://www.python.org/downloads/\n   CUDA     cuda_10.0.130_win10_network                    https://developer.nvidia.com/cuda-toolkit-archive\n   cuDNN    cuDNN v7.5.1 (April 22, 2019), for CUDA 10.0   https://developer.nvidia.com/rdp/cudnn-download\n\n * 安装",normalizedContent:"# 从零开始配pytorch gpu环境\n\n1、下载 anaconda\n\n * https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/\n\n2、conda 换源\n\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/\nconda config --set show_channel_urls yes\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 查看镜像源\nconda config --show-sources\n\n\n1\n2\n\n\n# 创建虚拟环境\nconda create --name pytorch python=3.7\n# 进入虚拟环境\nconda activate pytorch\n\n\n1\n2\n3\n4\n\n\n3、安装 对应版本的visual studio\n\n * 查看对应版本cuda的文档：https://developer.nvidia.com/cuda-toolkit-archive\n\n * 以 11.0 为例：https://docs.nvidia.com/cuda/archive/11.0/cuda-installation-guide-microsoft-windows/index.html\n\n * \n\n * 下载 visual studio 2019 即可：https://visualstudio.microsoft.com/zh-hans/\n\n * 把使用c++的桌面开发勾选上，就能把cl.exe安装好。整个安装好了以后，把包含cl.exe的文件目录添加到path环境变量里，环境变量怎么添加这个就自行百度，具体的我添加的cl.exe的路径是：\n   \n   * c:\\program files (x86)\\microsoft visual studio\\2019\\community\\vc\\tools\\msvc\\14.27.29110\\bin\\hostx64\\x64\n     \n     \n     1\n     \n\n4、更新 gpu 驱动\n\n * 桌面右键点击 nvidia 控制面板 -> 点击控制面板的左下角系统信息 -> 组件中可以看到支持的 cuda 版本\n * \n\n4、安装cuda 以及 cudnn\n\n * 下载 cuda 以及 cudnn\n\n * 工具及库     版本                                             下载链接\n   python   3.6.8                                          https://www.python.org/downloads/\n   cuda     cuda_10.0.130_win10_network                    https://developer.nvidia.com/cuda-toolkit-archive\n   cudnn    cudnn v7.5.1 (april 22, 2019), for cuda 10.0   https://developer.nvidia.com/rdp/cudnn-download\n\n * 安装",charsets:{cjk:!0},lastUpdated:"2021/08/08, 21:56:44"},{title:"ubuntu 18-04 搭建 go 语言开发环境",frontmatter:{title:"ubuntu 18-04 搭建 go 语言开发环境",date:"2021-08-02T21:50:20.000Z",permalink:"/pages/cf9a89/",categories:["wiki搬运","环境配置"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/02.%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/03.ubuntu%2018-04%20%E6%90%AD%E5%BB%BA%20go%20%E8%AF%AD%E8%A8%80%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83.html",relativePath:"04.wiki搬运/02.环境配置/03.ubuntu 18-04 搭建 go 语言开发环境.md",key:"v-22b71997",path:"/pages/cf9a89/",headers:[{level:2,title:"Ubuntu 18.04 搭建 go 语言开发环境.md",slug:"ubuntu-18-04-搭建-go-语言开发环境-md",normalizedTitle:"ubuntu 18.04 搭建 go 语言开发环境.md",charIndex:2}],headersStr:"Ubuntu 18.04 搭建 go 语言开发环境.md",content:"# Ubuntu 18.04 搭建 go 语言开发环境.md\n\nGo 官方下载页面：https://golang.org/dl/\n\n选择稳定版的 go 安装包，文件名如：*.linux-amd64.tar.gz\n\n此处我们选择 1.16.6 版本：下载链接\n\n解压 Go 压缩包\n\ntar -xzf go1.16.6.linux-amd64.tar.gz\nsudo mv go /usr/local\n\n\n1\n2\n\n\n调整环境变量\n\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\nexport PATH=$GOPATH/bin:$GOROOT/bin:$PATH\n\nsource ~/.profile\n\n\n1\n2\n3\n4\n5\n\n\n验证 Go 安装过程\n\ngo version\n\n\n1\n\n\n参考资料\n\n * https://www.itcoder.tech/posts/how-to-install-go-on-ubuntu-20-04",normalizedContent:"# ubuntu 18.04 搭建 go 语言开发环境.md\n\ngo 官方下载页面：https://golang.org/dl/\n\n选择稳定版的 go 安装包，文件名如：*.linux-amd64.tar.gz\n\n此处我们选择 1.16.6 版本：下载链接\n\n解压 go 压缩包\n\ntar -xzf go1.16.6.linux-amd64.tar.gz\nsudo mv go /usr/local\n\n\n1\n2\n\n\n调整环境变量\n\nexport goroot=/usr/local/go\nexport gopath=$home/go\nexport path=$gopath/bin:$goroot/bin:$path\n\nsource ~/.profile\n\n\n1\n2\n3\n4\n5\n\n\n验证 go 安装过程\n\ngo version\n\n\n1\n\n\n参考资料\n\n * https://www.itcoder.tech/posts/how-to-install-go-on-ubuntu-20-04",charsets:{cjk:!0},lastUpdated:"2021/08/08, 21:56:44"},{title:"GPU速度太慢问题排查",frontmatter:{title:"GPU速度太慢问题排查",date:"2021-09-07T14:07:49.000Z",permalink:"/pages/894290/",categories:["wiki搬运","环境配置"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/02.%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/04.GPU%E9%80%9F%E5%BA%A6%E5%A4%AA%E6%85%A2%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5.html",relativePath:"04.wiki搬运/02.环境配置/04.GPU速度太慢问题排查.md",key:"v-58bcda0a",path:"/pages/894290/",headers:[{level:2,title:"多卡 GPU 速度太慢问题排查",slug:"多卡-gpu-速度太慢问题排查",normalizedTitle:"多卡 gpu 速度太慢问题排查",charIndex:2}],headersStr:"多卡 GPU 速度太慢问题排查",content:"# 多卡 GPU 速度太慢问题排查\n\n# 1、根据参考资料 [1] 查看 GPU 之间的通信方式\n\nnvidia-smi topo --matrix\n\n\n1\n\n\n\tGPU0\tGPU1\tCPU Affinity\tNUMA Affinity\nGPU0\t X \tNODE\t0-15,32-47\t0\nGPU1\tNODE\t X \t0-15,32-47\t0\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n在双卡 RTX TITAN 的工作站上看到 GPU0 和 GPU1 是通过 NODE 方式通信的\n\nGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\nGPU0\t X \tPIX\tSYS\tSYS\t0-19\t\tN/A\nGPU1\tPIX\t X \tSYS\tSYS\t0-19\t\tN/A\nGPU2\tSYS\tSYS\t X \tPIX\t0-19\t\tN/A\nGPU3\tSYS\tSYS\tPIX\t X \t0-19\t\tN/A\n \nLegend:\n \n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks \n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n在四卡 RTX 2080Ti 的服务器上是上述输出，\n\n * GPU0 与 GPU1 是 PIX 通信方式，\n * GPU2 与 GPU3 是 PIX 通信方式\n * GPU0 与 GPU2、GPU3 是 SYS 通信方式\n * GPU1 与 GPU2、GPU3 是 SYS 通信方式\n\n# 2、测试通信带宽\n\n使用 CUDA Samples 提供的 P2P 带宽测试脚本\n\n * https://github.com/NVIDIA/cuda-samples/tree/master/Samples/p2pBandwidthLatencyTest\n\n测试方法：\n\n * 在 https://github.com/NVIDIA/cuda-samples/releases 中找到对应 CUDA 版本的 CUDA Samples\n * nvcc -V 查看 CUDA 版本\n * 例如我本地是CUDA-10.2 ，然后 Download 以及 unzip，随后按照下面的命令编译并运行\n\n> cd cuda-samples-10.2/Samples/p2pBandwidthLatencyTest\n> make\n> ./p2pBandwidthLatencyTest\n\n\n1\n2\n3\n\n\nBidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n   D\\D     0      1 \n     0 556.92   8.65 \n     1   8.58 457.95\n\n\n1\n2\n3\n4\n\n\nBidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n   DD     0      1      2      3 \n     0 526.19   8.27  10.05  16.41 \n     1   8.37 527.06  10.12  10.53 \n     2  10.47   9.67 526.39   5.80 \n     3  12.30   8.93   8.20 436.79\n\n\n1\n2\n3\n4\n5\n6\n\n\nGPU 之间的通信带宽属实有点低，尝试通过安装 nvlink 来解决\n\n------------2021/09/19更新------------\n\n更换显卡插槽后再度测试一下\n\n> nvidia-smi topo --matrix\n\tGPU0\tGPU1\tCPU Affinity\tNUMA Affinity\nGPU0\t X \tSYS\t0-15,32-47\t0\nGPU1\tSYS\t X \t16-31,48-63\t1\n\n\n1\n2\n3\n4\n\n\n> ./p2pBandwidthLatencyTest\nBidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n   D\\D     0      1 \n     0 554.55  11.42 \n     1  11.30 558.10\n\n\n1\n2\n3\n4\n5\n\n\n发现更换插槽后使用了不同组的 cpu ，通信速度有所加快\n\n# 3、 查看 nvlink 的状态\n\nnvidia-smi nvlink --status\n\n\n1\n\n\nGPU 0: TITAN RTX (UUID: GPU-486a9b0f-d80e-668e-fa93-cf5988109248)\n\t Link 0: <inactive>\n\t Link 1: <inactive>\nGPU 1: TITAN RTX (UUID: GPU-726723cc-04b1-d7a2-7638-0b73154449bc)\n\t Link 0: <inactive>\n\t Link 1: <inactive>\n\n\n1\n2\n3\n4\n5\n6\n\n\n处于 inactive 状态\n\n# 4、安装 NCCL\n\nkvstore\n\n4.1 下载 NCCL\n\n * https://developer.nvidia.com/nccl\n\n4.2 安装 NCCL\n\nsudo dpkg -i nccl-local-repo-ubuntu1804-2.10.3-cuda10.2_1.0-1_amd64.deb\nsudo apt update\nsudo apt install libnccl2 libnccl-dev\n\n\n1\n2\n3\n\n\n# 4.3 查看 NCCL 安装情况\n\n> dpkg -l|grep nccl\n\nii  libnccl-dev                                2.10.3-1+cuda10.2                                   amd64        NVIDIA Collective Communication Library (NCCL) Development Files\nii  libnccl2                                   2.10.3-1+cuda10.2                                   amd64        NVIDIA Collective Communication Library (NCCL) Runtime\nii  libvncclient1:amd64                        0.9.11+dfsg-1ubuntu1.4                              amd64        API to write one's own VNC server - client library\nii  nccl-local-repo-ubuntu1804-2.10.3-cuda10.2 1.0-1                                               amd64        nccl-local repository configuration files\nii  nccl-repo-ubuntu1804-2.7.8-ga-cuda10.2     1-1                                                 amd64        nccl repository configuration files\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n接着测试通信带宽并无变化，看来不是 NCCL 的原因\n\nBidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n   D\\D     0      1 \n     0 555.65   8.89 \n     1   8.74 453.78\n\n\n1\n2\n3\n4\n\n\n# 5、可能的问题\n\n由于客观原因未排查出具体原因，列出可能的两点原因，希望对你有所帮助\n\n * 温度太高：据参考资料[2]，温度在 85-90 度会到显卡的温度墙，会降频运行，解决方案可能是改装成水冷或者刀片式服务器，控制显卡温度\n * 缺少 nvlink：根据 nvidia-smi nvlink --status 得知，解决方案是购买显卡桥接器看是否能加大 GPU 间通信带宽\n\n# 6、根据参考资料[3] 进行排查\n\n------------2021/09/19更新------------\n\n6.1 查看 nvlink 安装情况\n\n> nvidia-smi topo -p2p n\n\n \tGPU0\tGPU1\t\n GPU0\tX\tNS\t\n GPU1\tNS\tX\t\n\nLegend:\n\n  X    = Self\n  OK   = Status Ok\n  CNS  = Chipset not supported\n  GNS  = GPU not supported\n  TNS  = Topology not supported\n  NS   = Not supported\n  U    = Unknown\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n显示结果为 Not supported\n\n6.2 查看 P2P 通信情况\n\n> cd ~/NVIDIA_CUDA-10.2_Samples/0_Simple/simpleP2P\n> make\n> ./simpleP2P\n\n[./simpleP2P] - Starting...\nChecking for multiple GPUs...\nCUDA-capable device count: 2\n\nChecking GPU(s) for support of peer to peer memory access...\n> Peer access from TITAN RTX (GPU0) -> TITAN RTX (GPU1) : No\n> Peer access from TITAN RTX (GPU1) -> TITAN RTX (GPU0) : No\nTwo or more GPUs with Peer-to-Peer access capability are required for ./simpleP2P.\nPeer to Peer access is not available amongst GPUs in the system, waiving test.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n结果显示在不使用桥接器的情况下，双卡无法实现 P2P 通信\n\n6.3 开启 TCC 计算模式\n\nnvidia-smi -i 0 -dm TCC\n\n\n1\n\n\n# 6、参考资料\n\n * [1] https://www.microway.com/hpc-tech-tips/nvidia-smi_control-your-gpus/\n\n * [2] https://bizon-tech.com/blog/bizon-z9000-8gpu-deeplearning-server-rtx2080ti-titan-benchamarks-review#:~:text=Idle%20temperature%3A%2040C%20Max%20load%20noise%20level%20%28deep,%28deep%20learning%29%3A%206X%20times%20lower%20%28150%20vs.%20900%29\n\n * [3] https://www.cnblogs.com/devilmaycry812839668/p/13264080.html",normalizedContent:"# 多卡 gpu 速度太慢问题排查\n\n# 1、根据参考资料 [1] 查看 gpu 之间的通信方式\n\nnvidia-smi topo --matrix\n\n\n1\n\n\n\tgpu0\tgpu1\tcpu affinity\tnuma affinity\ngpu0\t x \tnode\t0-15,32-47\t0\ngpu1\tnode\t x \t0-15,32-47\t0\n\nlegend:\n\n  x    = self\n  sys  = connection traversing pcie as well as the smp interconnect between numa nodes (e.g., qpi/upi)\n  node = connection traversing pcie as well as the interconnect between pcie host bridges within a numa node\n  phb  = connection traversing pcie as well as a pcie host bridge (typically the cpu)\n  pxb  = connection traversing multiple pcie bridges (without traversing the pcie host bridge)\n  pix  = connection traversing at most a single pcie bridge\n  nv#  = connection traversing a bonded set of # nvlinks\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n在双卡 rtx titan 的工作站上看到 gpu0 和 gpu1 是通过 node 方式通信的\n\ngpu0\tgpu1\tgpu2\tgpu3\tcpu affinity\tnuma affinity\ngpu0\t x \tpix\tsys\tsys\t0-19\t\tn/a\ngpu1\tpix\t x \tsys\tsys\t0-19\t\tn/a\ngpu2\tsys\tsys\t x \tpix\t0-19\t\tn/a\ngpu3\tsys\tsys\tpix\t x \t0-19\t\tn/a\n \nlegend:\n \n  x    = self\n  sys  = connection traversing pcie as well as the smp interconnect between numa nodes (e.g., qpi/upi)\n  node = connection traversing pcie as well as the interconnect between pcie host bridges within a numa node\n  phb  = connection traversing pcie as well as a pcie host bridge (typically the cpu)\n  pxb  = connection traversing multiple pcie bridges (without traversing the pcie host bridge)\n  pix  = connection traversing at most a single pcie bridge\n  nv#  = connection traversing a bonded set of # nvlinks \n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\n在四卡 rtx 2080ti 的服务器上是上述输出，\n\n * gpu0 与 gpu1 是 pix 通信方式，\n * gpu2 与 gpu3 是 pix 通信方式\n * gpu0 与 gpu2、gpu3 是 sys 通信方式\n * gpu1 与 gpu2、gpu3 是 sys 通信方式\n\n# 2、测试通信带宽\n\n使用 cuda samples 提供的 p2p 带宽测试脚本\n\n * https://github.com/nvidia/cuda-samples/tree/master/samples/p2pbandwidthlatencytest\n\n测试方法：\n\n * 在 https://github.com/nvidia/cuda-samples/releases 中找到对应 cuda 版本的 cuda samples\n * nvcc -v 查看 cuda 版本\n * 例如我本地是cuda-10.2 ，然后 download 以及 unzip，随后按照下面的命令编译并运行\n\n> cd cuda-samples-10.2/samples/p2pbandwidthlatencytest\n> make\n> ./p2pbandwidthlatencytest\n\n\n1\n2\n3\n\n\nbidirectional p2p=enabled bandwidth matrix (gb/s)\n   d\\d     0      1 \n     0 556.92   8.65 \n     1   8.58 457.95\n\n\n1\n2\n3\n4\n\n\nbidirectional p2p=enabled bandwidth matrix (gb/s)\n   dd     0      1      2      3 \n     0 526.19   8.27  10.05  16.41 \n     1   8.37 527.06  10.12  10.53 \n     2  10.47   9.67 526.39   5.80 \n     3  12.30   8.93   8.20 436.79\n\n\n1\n2\n3\n4\n5\n6\n\n\ngpu 之间的通信带宽属实有点低，尝试通过安装 nvlink 来解决\n\n------------2021/09/19更新------------\n\n更换显卡插槽后再度测试一下\n\n> nvidia-smi topo --matrix\n\tgpu0\tgpu1\tcpu affinity\tnuma affinity\ngpu0\t x \tsys\t0-15,32-47\t0\ngpu1\tsys\t x \t16-31,48-63\t1\n\n\n1\n2\n3\n4\n\n\n> ./p2pbandwidthlatencytest\nbidirectional p2p=enabled bandwidth matrix (gb/s)\n   d\\d     0      1 \n     0 554.55  11.42 \n     1  11.30 558.10\n\n\n1\n2\n3\n4\n5\n\n\n发现更换插槽后使用了不同组的 cpu ，通信速度有所加快\n\n# 3、 查看 nvlink 的状态\n\nnvidia-smi nvlink --status\n\n\n1\n\n\ngpu 0: titan rtx (uuid: gpu-486a9b0f-d80e-668e-fa93-cf5988109248)\n\t link 0: <inactive>\n\t link 1: <inactive>\ngpu 1: titan rtx (uuid: gpu-726723cc-04b1-d7a2-7638-0b73154449bc)\n\t link 0: <inactive>\n\t link 1: <inactive>\n\n\n1\n2\n3\n4\n5\n6\n\n\n处于 inactive 状态\n\n# 4、安装 nccl\n\nkvstore\n\n4.1 下载 nccl\n\n * https://developer.nvidia.com/nccl\n\n4.2 安装 nccl\n\nsudo dpkg -i nccl-local-repo-ubuntu1804-2.10.3-cuda10.2_1.0-1_amd64.deb\nsudo apt update\nsudo apt install libnccl2 libnccl-dev\n\n\n1\n2\n3\n\n\n# 4.3 查看 nccl 安装情况\n\n> dpkg -l|grep nccl\n\nii  libnccl-dev                                2.10.3-1+cuda10.2                                   amd64        nvidia collective communication library (nccl) development files\nii  libnccl2                                   2.10.3-1+cuda10.2                                   amd64        nvidia collective communication library (nccl) runtime\nii  libvncclient1:amd64                        0.9.11+dfsg-1ubuntu1.4                              amd64        api to write one's own vnc server - client library\nii  nccl-local-repo-ubuntu1804-2.10.3-cuda10.2 1.0-1                                               amd64        nccl-local repository configuration files\nii  nccl-repo-ubuntu1804-2.7.8-ga-cuda10.2     1-1                                                 amd64        nccl repository configuration files\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n接着测试通信带宽并无变化，看来不是 nccl 的原因\n\nbidirectional p2p=enabled bandwidth matrix (gb/s)\n   d\\d     0      1 \n     0 555.65   8.89 \n     1   8.74 453.78\n\n\n1\n2\n3\n4\n\n\n# 5、可能的问题\n\n由于客观原因未排查出具体原因，列出可能的两点原因，希望对你有所帮助\n\n * 温度太高：据参考资料[2]，温度在 85-90 度会到显卡的温度墙，会降频运行，解决方案可能是改装成水冷或者刀片式服务器，控制显卡温度\n * 缺少 nvlink：根据 nvidia-smi nvlink --status 得知，解决方案是购买显卡桥接器看是否能加大 gpu 间通信带宽\n\n# 6、根据参考资料[3] 进行排查\n\n------------2021/09/19更新------------\n\n6.1 查看 nvlink 安装情况\n\n> nvidia-smi topo -p2p n\n\n \tgpu0\tgpu1\t\n gpu0\tx\tns\t\n gpu1\tns\tx\t\n\nlegend:\n\n  x    = self\n  ok   = status ok\n  cns  = chipset not supported\n  gns  = gpu not supported\n  tns  = topology not supported\n  ns   = not supported\n  u    = unknown\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n显示结果为 not supported\n\n6.2 查看 p2p 通信情况\n\n> cd ~/nvidia_cuda-10.2_samples/0_simple/simplep2p\n> make\n> ./simplep2p\n\n[./simplep2p] - starting...\nchecking for multiple gpus...\ncuda-capable device count: 2\n\nchecking gpu(s) for support of peer to peer memory access...\n> peer access from titan rtx (gpu0) -> titan rtx (gpu1) : no\n> peer access from titan rtx (gpu1) -> titan rtx (gpu0) : no\ntwo or more gpus with peer-to-peer access capability are required for ./simplep2p.\npeer to peer access is not available amongst gpus in the system, waiving test.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n结果显示在不使用桥接器的情况下，双卡无法实现 p2p 通信\n\n6.3 开启 tcc 计算模式\n\nnvidia-smi -i 0 -dm tcc\n\n\n1\n\n\n# 6、参考资料\n\n * [1] https://www.microway.com/hpc-tech-tips/nvidia-smi_control-your-gpus/\n\n * [2] https://bizon-tech.com/blog/bizon-z9000-8gpu-deeplearning-server-rtx2080ti-titan-benchamarks-review#:~:text=idle%20temperature%3a%2040c%20max%20load%20noise%20level%20%28deep,%28deep%20learning%29%3a%206x%20times%20lower%20%28150%20vs.%20900%29\n\n * [3] https://www.cnblogs.com/devilmaycry812839668/p/13264080.html",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"Ubuntu系统安装",frontmatter:{title:"Ubuntu系统安装",date:"2021-09-15T13:48:09.000Z",permalink:"/pages/0b52e1/",categories:["wiki搬运","环境配置"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/02.%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/05.Ubuntu%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85.html",relativePath:"04.wiki搬运/02.环境配置/05.Ubuntu系统安装.md",key:"v-4d3dd9d1",path:"/pages/0b52e1/",headersStr:null,content:"在 TUNA 上下载 iso 文件\n\n * https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/\n\n按照下面所述的方式制作 U 盘镜像，建议使用 windows 制作镜像，速度会快很多\n\n * https://ubuntu.com/tutorials/create-a-usb-stick-on-windows#10-installation-complete\n\n * https://ubuntu.com/tutorials/create-a-usb-stick-on-ubuntu#10-installation-complete",normalizedContent:"在 tuna 上下载 iso 文件\n\n * https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/\n\n按照下面所述的方式制作 u 盘镜像，建议使用 windows 制作镜像，速度会快很多\n\n * https://ubuntu.com/tutorials/create-a-usb-stick-on-windows#10-installation-complete\n\n * https://ubuntu.com/tutorials/create-a-usb-stick-on-ubuntu#10-installation-complete",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"服务器重装系统",frontmatter:{title:"服务器重装系统",date:"2021-12-17T11:44:04.000Z",permalink:"/pages/0d3088/",categories:["wiki搬运","环境配置"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/02.%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/06.%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%87%8D%E8%A3%85%E7%B3%BB%E7%BB%9F.html",relativePath:"04.wiki搬运/02.环境配置/06.服务器重装系统.md",key:"v-e762e6a2",path:"/pages/0d3088/",headers:[{level:3,title:"制作启动盘",slug:"制作启动盘",normalizedTitle:"制作启动盘",charIndex:2},{level:3,title:"换源",slug:"换源",normalizedTitle:"换源",charIndex:49},{level:3,title:"软件安装",slug:"软件安装",normalizedTitle:"软件安装",charIndex:99}],headersStr:"制作启动盘 换源 软件安装",content:"# 制作启动盘\n\nrufus制作Ubuntu18.04启动盘(Linux/Ubuntu)\n\n\n# 换源\n\n【Linux教程】Ubuntu Linux 更换源教程_菜鸟的后花园-CSDN博客\n\n\n# 软件安装\n\n 1. 中文輸入法\n\n搜狗输入法 for linux\n\n 1. Anaconda\n\nIndex of /anaconda/archive/ | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror\n\n 1. Jetbrains Toolbox\n\nJetBrains Toolbox App: Manage Your Tools with Ease\n\n 1. 驱动安装（建议安装440版本的）\n\nUbuntu 18.04 安装 NVIDIA 显卡驱动\n\n 1. 远程桌面\n\n * 安装openssh\n\nfailed to start ssh.service. unit ssh.service not found (ubuntu 18.04)_xiato_yu的专栏-CSDN博客\n\nUbuntu 18.04 上使用xrdp远程桌面连接_qq_25556149的博客-CSDN博客",normalizedContent:"# 制作启动盘\n\nrufus制作ubuntu18.04启动盘(linux/ubuntu)\n\n\n# 换源\n\n【linux教程】ubuntu linux 更换源教程_菜鸟的后花园-csdn博客\n\n\n# 软件安装\n\n 1. 中文輸入法\n\n搜狗输入法 for linux\n\n 1. anaconda\n\nindex of /anaconda/archive/ | 清华大学开源软件镜像站 | tsinghua open source mirror\n\n 1. jetbrains toolbox\n\njetbrains toolbox app: manage your tools with ease\n\n 1. 驱动安装（建议安装440版本的）\n\nubuntu 18.04 安装 nvidia 显卡驱动\n\n 1. 远程桌面\n\n * 安装openssh\n\nfailed to start ssh.service. unit ssh.service not found (ubuntu 18.04)_xiato_yu的专栏-csdn博客\n\nubuntu 18.04 上使用xrdp远程桌面连接_qq_25556149的博客-csdn博客",charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"Clash 配置",frontmatter:{title:"Clash 配置",date:"2021-12-24T14:47:15.000Z",permalink:"/pages/715b93/",categories:["wiki搬运","环境配置"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/02.%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/07.Clash%20%E9%85%8D%E7%BD%AE.html",relativePath:"04.wiki搬运/02.环境配置/07.Clash 配置.md",key:"v-62fa926d",path:"/pages/715b93/",headersStr:null,content:"https://merlinblog.xyz/wiki/cfw.html",normalizedContent:"https://merlinblog.xyz/wiki/cfw.html",charsets:{},lastUpdated:"2023/03/25, 19:58:09"},{title:"pandas 库常用用法",frontmatter:{title:"pandas 库常用用法",date:"2021-08-11T16:28:34.000Z",permalink:"/pages/95cffd/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/00.pandas%20%E5%BA%93%E5%B8%B8%E7%94%A8%E7%94%A8%E6%B3%95.html",relativePath:"04.wiki搬运/03.常用库的常见用法/00.pandas 库常用用法.md",key:"v-4392326d",path:"/pages/95cffd/",headers:[{level:2,title:"Pandas 库常见用法",slug:"pandas-库常见用法",normalizedTitle:"pandas 库常见用法",charIndex:2}],headersStr:"Pandas 库常见用法",content:"# Pandas 库常见用法\n\n# pd.concat\n\n使用场景示例：提取 test_data dataframe 的 filename 列，从另一个列表中构造另外一列，再 concat 到一起\n\nimport pandas as pd\ntest_data['image_raw'] = test_data['filename']\ntest_data['label_raw'] = pd.Series(pred_ensemble_test_list)\ntest_pseudo_df = pd.concat([test_data['image_raw'], test_data['label_raw']], axis=1)\ntest_pseudo_df.to_csv(cfg.test_submission_path[:-4] + '_pseudo.csv', index=False)\n\n\n1\n2\n3\n4\n5\n\n\n# df.sort_values\n\n使用场景示例：对 ‘filename‘ 该列排序\n\nimport pandas as pd\nraw_test_df = pd.read_csv(cfg.path_raw_test_csv)\nraw_test_df.sort_values('filename', inplace=True)\n\n\n1\n2\n3\n\n\n# lambda + apply\n\n使用场景示例：对某一列数据要做统一的操作，例如字符串的信息提取\n\nimport pandas as pd\ntest_data = pd.read_csv(cfg.path_raw_test_csv)\ntest_data['name'] = test_data['filename'].apply(lambda x: x.split('/')[-1])\n\n\n1\n2\n3\n\n\n# value_counts\n\n使用场景示例：我想要查看某个 csv 文件中 label 这一列的分布情况\n\nimport pandas as pd\nraw_train_df = pd.read_csv(cfg.path_raw_train_csv)\nprint(raw_train_df['label'].value_counts())\n\n\n1\n2\n3\n\n\n# df.column\n\n使用场景示例：修改某个 df 的列名\n\nimport pandas as pd\nraw_train_df = pd.read_csv(cfg.path_raw_train_csv)\nraw_train_df.columns = ['filename', 'label']\n\n\n1\n2\n3\n\n\n# df.index.value\n\n使用场景示例：想要输出某一列的值，只知道这一列的索引\n\nimport pandas as pd\nraw_train_df = pd.read_csv(cfg.path_raw_train_csv)\nprint(raw_train_df.index.value)\nprint(raw_train_df.loc['xxx'][0])\n\n\n1\n2\n3\n4\n\n\n# pd.DataFrame.from_dict\n\n使用场景示例：想要将一个字典转成 df\n\nimport pandas df\ndf_clean_4000 = pd.DataFrame.from_dict(dict_clean_4000, orient='index')\ndf_noise_500 = pd.DataFrame.from_dict(dict_clean_4000_noise_500, orient='index')\ndf_clean_valid = pd.DataFrame.from_dict(dict_clean_valid, orient='index')\ndf_clean_test = pd.DataFrame.from_dict(dict_clean_test, orient='index')\n\ndf_clean_4000.to_csv(csv_path_clean_4000)\ndf_noise_500.to_csv(csv_path_clean_4000_noise_500_weight)\ndf_clean_valid.to_csv(csv_path_clean_valid)\ndf_clean_test.to_csv(csv_path_clean_test)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# df.iloc\n\n使用场景示例：获取 df 的元素\n\nimport pandas as pd\n\n# 已知坐标x,y\ndf = pd.read_csv(cfg.path_csv)\nitem = df.iloc[x, y]\nitem = df.iat[x, y]\n\n#取data的第一行\ndf.iloc[0, :]  \n\n#取data的第一列\ndf.iloc[:, 0]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# df.drop\n\n使用场景示例：删除 ‘B’ 与 'C' 列\n\nimport pandas as pd\ndf = pd.read_csv(cfg.path_csv)\ndf.drop(columns=['B', 'C'])\n\n\n1\n2\n3\n\n\n# df.unique()\n\n使用场景示例：想得到一列的唯一值\n\n>>> pd.Series([2, 1, 3, 3], name='A').unique()\narray([2, 1, 3])\n\n\n1\n2\n\n\n# df.describe()",normalizedContent:"# pandas 库常见用法\n\n# pd.concat\n\n使用场景示例：提取 test_data dataframe 的 filename 列，从另一个列表中构造另外一列，再 concat 到一起\n\nimport pandas as pd\ntest_data['image_raw'] = test_data['filename']\ntest_data['label_raw'] = pd.series(pred_ensemble_test_list)\ntest_pseudo_df = pd.concat([test_data['image_raw'], test_data['label_raw']], axis=1)\ntest_pseudo_df.to_csv(cfg.test_submission_path[:-4] + '_pseudo.csv', index=false)\n\n\n1\n2\n3\n4\n5\n\n\n# df.sort_values\n\n使用场景示例：对 ‘filename‘ 该列排序\n\nimport pandas as pd\nraw_test_df = pd.read_csv(cfg.path_raw_test_csv)\nraw_test_df.sort_values('filename', inplace=true)\n\n\n1\n2\n3\n\n\n# lambda + apply\n\n使用场景示例：对某一列数据要做统一的操作，例如字符串的信息提取\n\nimport pandas as pd\ntest_data = pd.read_csv(cfg.path_raw_test_csv)\ntest_data['name'] = test_data['filename'].apply(lambda x: x.split('/')[-1])\n\n\n1\n2\n3\n\n\n# value_counts\n\n使用场景示例：我想要查看某个 csv 文件中 label 这一列的分布情况\n\nimport pandas as pd\nraw_train_df = pd.read_csv(cfg.path_raw_train_csv)\nprint(raw_train_df['label'].value_counts())\n\n\n1\n2\n3\n\n\n# df.column\n\n使用场景示例：修改某个 df 的列名\n\nimport pandas as pd\nraw_train_df = pd.read_csv(cfg.path_raw_train_csv)\nraw_train_df.columns = ['filename', 'label']\n\n\n1\n2\n3\n\n\n# df.index.value\n\n使用场景示例：想要输出某一列的值，只知道这一列的索引\n\nimport pandas as pd\nraw_train_df = pd.read_csv(cfg.path_raw_train_csv)\nprint(raw_train_df.index.value)\nprint(raw_train_df.loc['xxx'][0])\n\n\n1\n2\n3\n4\n\n\n# pd.dataframe.from_dict\n\n使用场景示例：想要将一个字典转成 df\n\nimport pandas df\ndf_clean_4000 = pd.dataframe.from_dict(dict_clean_4000, orient='index')\ndf_noise_500 = pd.dataframe.from_dict(dict_clean_4000_noise_500, orient='index')\ndf_clean_valid = pd.dataframe.from_dict(dict_clean_valid, orient='index')\ndf_clean_test = pd.dataframe.from_dict(dict_clean_test, orient='index')\n\ndf_clean_4000.to_csv(csv_path_clean_4000)\ndf_noise_500.to_csv(csv_path_clean_4000_noise_500_weight)\ndf_clean_valid.to_csv(csv_path_clean_valid)\ndf_clean_test.to_csv(csv_path_clean_test)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n# df.iloc\n\n使用场景示例：获取 df 的元素\n\nimport pandas as pd\n\n# 已知坐标x,y\ndf = pd.read_csv(cfg.path_csv)\nitem = df.iloc[x, y]\nitem = df.iat[x, y]\n\n#取data的第一行\ndf.iloc[0, :]  \n\n#取data的第一列\ndf.iloc[:, 0]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# df.drop\n\n使用场景示例：删除 ‘b’ 与 'c' 列\n\nimport pandas as pd\ndf = pd.read_csv(cfg.path_csv)\ndf.drop(columns=['b', 'c'])\n\n\n1\n2\n3\n\n\n# df.unique()\n\n使用场景示例：想得到一列的唯一值\n\n>>> pd.series([2, 1, 3, 3], name='a').unique()\narray([2, 1, 3])\n\n\n1\n2\n\n\n# df.describe()",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"sklearn 库常用用法",frontmatter:{title:"sklearn 库常用用法",date:"2021-08-11T16:35:16.000Z",permalink:"/pages/a5960e/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/02.sklearn%20%E5%BA%93%E5%B8%B8%E7%94%A8%E7%94%A8%E6%B3%95.html",relativePath:"04.wiki搬运/03.常用库的常见用法/02.sklearn 库常用用法.md",key:"v-7953ded5",path:"/pages/a5960e/",headers:[{level:2,title:"sklearn 库常用用法",slug:"sklearn-库常用用法",normalizedTitle:"sklearn 库常用用法",charIndex:2}],headersStr:"sklearn 库常用用法",content:"# sklearn 库常用用法\n\n# train_test_split 方法\n\n使用场景示例：我想要对 trainval 文件分成 train 文件 和 valid 文件\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nraw_train_df = pd.read_csv(cfg.path_raw_train_csv)\nraw_train_df.columns = ['filename', 'label']\n\ntrain_data, valid_data = train_test_split(\n            raw_train_df, shuffle=True, test_size=cfg.size_valid, random_state=cfg.seed_random)\n        train_data.to_csv(os.path.join(cfg.path_save_trainval_csv, f'train.csv'), index=False)\n        valid_data.to_csv(os.path.join(cfg.path_save_trainval_csv, f'valid.csv'), index=False)\n        print(f'train:{train_data.shape[0]}, valid:{valid_data.shape[0]}')\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# StratifiedKFold 用法\n\n使用示例：我想要将一个 train 文件和 valid 文件分成多个 fold\n\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\n\nraw_train_df = pd.read_csv(cfg.path_raw_train_csv)\nraw_train_df.columns = ['filename', 'label']\n\nskf = StratifiedKFold(\n    n_splits=cfg.num_KFold,\n    random_state=cfg.seed_random,\n    shuffle=True\n)\nfor fold_idx, (train_idx, val_idx) in enumerate(skf.split(x, y)):\n    fold_train = raw_train_df.iloc[train_idx]\n    fold_valid = raw_train_df.iloc[val_idx]\n    fold_train.to_csv(os.path.join(cfg.path_save_trainval_csv, f'train_fold{fold_idx}.csv'), index=False)\n    fold_valid.to_csv(os.path.join(cfg.path_save_trainval_csv, f'valid_fold{fold_idx}.csv'), index=False)\n    print(f'train_fold{fold_idx}: {fold_train.shape[0]}, valid_fold{fold_idx}: {fold_valid.shape[0]}')\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n",normalizedContent:"# sklearn 库常用用法\n\n# train_test_split 方法\n\n使用场景示例：我想要对 trainval 文件分成 train 文件 和 valid 文件\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, stratifiedkfold\n\nraw_train_df = pd.read_csv(cfg.path_raw_train_csv)\nraw_train_df.columns = ['filename', 'label']\n\ntrain_data, valid_data = train_test_split(\n            raw_train_df, shuffle=true, test_size=cfg.size_valid, random_state=cfg.seed_random)\n        train_data.to_csv(os.path.join(cfg.path_save_trainval_csv, f'train.csv'), index=false)\n        valid_data.to_csv(os.path.join(cfg.path_save_trainval_csv, f'valid.csv'), index=false)\n        print(f'train:{train_data.shape[0]}, valid:{valid_data.shape[0]}')\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n# stratifiedkfold 用法\n\n使用示例：我想要将一个 train 文件和 valid 文件分成多个 fold\n\nimport pandas as pd\nfrom sklearn.model_selection import stratifiedkfold\n\nraw_train_df = pd.read_csv(cfg.path_raw_train_csv)\nraw_train_df.columns = ['filename', 'label']\n\nskf = stratifiedkfold(\n    n_splits=cfg.num_kfold,\n    random_state=cfg.seed_random,\n    shuffle=true\n)\nfor fold_idx, (train_idx, val_idx) in enumerate(skf.split(x, y)):\n    fold_train = raw_train_df.iloc[train_idx]\n    fold_valid = raw_train_df.iloc[val_idx]\n    fold_train.to_csv(os.path.join(cfg.path_save_trainval_csv, f'train_fold{fold_idx}.csv'), index=false)\n    fold_valid.to_csv(os.path.join(cfg.path_save_trainval_csv, f'valid_fold{fold_idx}.csv'), index=false)\n    print(f'train_fold{fold_idx}: {fold_train.shape[0]}, valid_fold{fold_idx}: {fold_valid.shape[0]}')\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"glob 库常用用法",frontmatter:{title:"glob 库常用用法",date:"2021-08-11T16:34:13.000Z",permalink:"/pages/52270e/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/01.glob%20%E5%BA%93%E5%B8%B8%E7%94%A8%E7%94%A8%E6%B3%95.html",relativePath:"04.wiki搬运/03.常用库的常见用法/01.glob 库常用用法.md",key:"v-5457056d",path:"/pages/52270e/",headers:[{level:2,title:"glob 库常用用法",slug:"glob-库常用用法",normalizedTitle:"glob 库常用用法",charIndex:2}],headersStr:"glob 库常用用法",content:"# glob 库常用用法\n\n# glob 函数\n\n用法示例：我想遍历某个数据集下的所有 png 图像\n\nfrom glob import glob\ntrain_imgs = glob(os.path.join(cfg.path_train_img, '*/*.png'))\ntest_imgs = glob(os.path.join(cfg.path_test_img, '*.png'))\n\n\n1\n2\n3\n",normalizedContent:"# glob 库常用用法\n\n# glob 函数\n\n用法示例：我想遍历某个数据集下的所有 png 图像\n\nfrom glob import glob\ntrain_imgs = glob(os.path.join(cfg.path_train_img, '*/*.png'))\ntest_imgs = glob(os.path.join(cfg.path_test_img, '*.png'))\n\n\n1\n2\n3\n",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"图像处理库的常用用法",frontmatter:{title:"图像处理库的常用用法",date:"2021-08-11T16:45:33.000Z",permalink:"/pages/997b21/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/04.%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%BA%93%E7%9A%84%E5%B8%B8%E7%94%A8%E7%94%A8%E6%B3%95.html",relativePath:"04.wiki搬运/03.常用库的常见用法/04.图像处理库的常用用法.md",key:"v-59f183b7",path:"/pages/997b21/",headersStr:null,content:"# 图像的相关操作\n\n1、numpy图像可视化\n\nplt.imshow(gray,cmap='gray')\nplt.axis('off')\nplt.show()\n\n\n1\n2\n3\n\n\n2、numpy图像\n\nimport matplotlib\nim_array = pred.squeeze().numpy()\nmatplotlib.image.imsave('/home/muyun99/Pictures/1173_student.png', im_array)\n\nimport cv2\nim_array = pred.squeeze().numpy()\ncv2.imwrite('/home/muyun99/Pictures/1173_student.png', im_array)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n3、突出两幅图像不一样的像素点\n\n\n\n\n1\n\n\n4、读取图像\n\nimport cv2\nfrom PIL import Image\nimg = Image.open(train_df['filename'].iloc[index]).convert('RGB')\nimg = np.array(img)\n\n\n1\n2\n3\n4\n\n\n5、将两张图像组合到一起\n\n\n\n\n1\n",normalizedContent:"# 图像的相关操作\n\n1、numpy图像可视化\n\nplt.imshow(gray,cmap='gray')\nplt.axis('off')\nplt.show()\n\n\n1\n2\n3\n\n\n2、numpy图像\n\nimport matplotlib\nim_array = pred.squeeze().numpy()\nmatplotlib.image.imsave('/home/muyun99/pictures/1173_student.png', im_array)\n\nimport cv2\nim_array = pred.squeeze().numpy()\ncv2.imwrite('/home/muyun99/pictures/1173_student.png', im_array)\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n3、突出两幅图像不一样的像素点\n\n\n\n\n1\n\n\n4、读取图像\n\nimport cv2\nfrom pil import image\nimg = image.open(train_df['filename'].iloc[index]).convert('rgb')\nimg = np.array(img)\n\n\n1\n2\n3\n4\n\n\n5、将两张图像组合到一起\n\n\n\n\n1\n",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"matplotlib 库常用用法--散点图绘制",frontmatter:{title:"matplotlib 库常用用法--散点图绘制",date:"2021-08-11T16:44:58.000Z",permalink:"/pages/42f579/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/03.matplotlib%20%E5%BA%93%E5%B8%B8%E7%94%A8%E7%94%A8%E6%B3%95--%E6%95%A3%E7%82%B9%E5%9B%BE%E7%BB%98%E5%88%B6.html",relativePath:"04.wiki搬运/03.常用库的常见用法/03.matplotlib 库常用用法--散点图绘制.md",key:"v-732238a5",path:"/pages/42f579/",headers:[{level:3,title:"散点图绘制",slug:"散点图绘制",normalizedTitle:"散点图绘制",charIndex:2},{level:3,title:"拼图",slug:"拼图",normalizedTitle:"拼图",charIndex:913}],headersStr:"散点图绘制 拼图",content:"# 散点图绘制\n\n# 1、代码示例\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n\nN = 50\nx = np.random.rand(N)\ny = np.random.rand(N)\ncolors = np.random.rand(N)\narea = (30 * np.random.rand(N))**2  # 0 to 15 point radii\n\nplt.scatter(x, y, s=area, c=colors, alpha=0.5)\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n# 2、自写代码\n\n ax.scatter(noisy_sample_pd1, noisy_sample_pd2, c='lightcoral', marker='+', alpha=0.6, label=\"Noisy Sample\")  \n ax.scatter(clean_sample_pd1, clean_sample_pd2, c='darkcyan',  marker='x', alpha=0.6, label=\"Clean Sample\")\n\n# concat two image\ndef mix_img(path1, path2, save_path):\n    img1 = cv2.imread(path1)\n    img2 = cv2.imread(path2)\n    image_concat = np.concatenate([img1, img2], axis=1)\n    cv2.imwrite(save_path, img=image_concat)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n#\n\n# 3、参数解析\n\n * c 参数如下图（参考链接）\n   \n   \n\n * m 参数如下图（参考链接）\n   \n   \n\n\n# 拼图\n\nimage_concat1 = np.concatenate([img, mask], axis=1)\nimage_concat2 = np.concatenate([img, color_mask], axis=1)\nmmcv.imwrite(image_concat1, 'test1.png')\nmmcv.imwrite(image_concat2, 'test2.png')\n\nplt.subplot(1, 3, 1)\nplt.imshow(img)\nplt.title(\"img\", fontsize=8)\n\nplt.subplot(1, 3, 2)\nplt.imshow(mask, cmap ='gray')\nplt.title(f\"img_noise_erode_\", fontsize=8)\n\nplt.subplot(1, 3, 3)\nplt.imshow(color_mask)\nplt.title(f\"img_noise_dilate_\", fontsize=8)\n\nplt.savefig('test.png')\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n# 参考资料\n\n * https://matplotlib.org/stable/api/index.html\n\n * https://matplotlib.org/2.0.2/examples/color/named_colors.html\n\n * https://matplotlib.org/stable/api/markers_api.html#module-matplotlib.markers\n\n * https://matplotlib.org/stable/gallery/shapes_and_collections/scatter.html#sphx-glr-gallery-shapes-and-collections-scatter-py\n\n * https://matplotlib.org/stable/tutorials/introductory/sample_plots.html#sphx-glr-tutorials-introductory-sample-plots-py\n\n参考资料\n\n * https://flashgene.com/archives/163100.html",normalizedContent:"# 散点图绘制\n\n# 1、代码示例\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# fixing random state for reproducibility\nnp.random.seed(19680801)\n\n\nn = 50\nx = np.random.rand(n)\ny = np.random.rand(n)\ncolors = np.random.rand(n)\narea = (30 * np.random.rand(n))**2  # 0 to 15 point radii\n\nplt.scatter(x, y, s=area, c=colors, alpha=0.5)\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n# 2、自写代码\n\n ax.scatter(noisy_sample_pd1, noisy_sample_pd2, c='lightcoral', marker='+', alpha=0.6, label=\"noisy sample\")  \n ax.scatter(clean_sample_pd1, clean_sample_pd2, c='darkcyan',  marker='x', alpha=0.6, label=\"clean sample\")\n\n# concat two image\ndef mix_img(path1, path2, save_path):\n    img1 = cv2.imread(path1)\n    img2 = cv2.imread(path2)\n    image_concat = np.concatenate([img1, img2], axis=1)\n    cv2.imwrite(save_path, img=image_concat)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n#\n\n# 3、参数解析\n\n * c 参数如下图（参考链接）\n   \n   \n\n * m 参数如下图（参考链接）\n   \n   \n\n\n# 拼图\n\nimage_concat1 = np.concatenate([img, mask], axis=1)\nimage_concat2 = np.concatenate([img, color_mask], axis=1)\nmmcv.imwrite(image_concat1, 'test1.png')\nmmcv.imwrite(image_concat2, 'test2.png')\n\nplt.subplot(1, 3, 1)\nplt.imshow(img)\nplt.title(\"img\", fontsize=8)\n\nplt.subplot(1, 3, 2)\nplt.imshow(mask, cmap ='gray')\nplt.title(f\"img_noise_erode_\", fontsize=8)\n\nplt.subplot(1, 3, 3)\nplt.imshow(color_mask)\nplt.title(f\"img_noise_dilate_\", fontsize=8)\n\nplt.savefig('test.png')\nplt.show()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n# 参考资料\n\n * https://matplotlib.org/stable/api/index.html\n\n * https://matplotlib.org/2.0.2/examples/color/named_colors.html\n\n * https://matplotlib.org/stable/api/markers_api.html#module-matplotlib.markers\n\n * https://matplotlib.org/stable/gallery/shapes_and_collections/scatter.html#sphx-glr-gallery-shapes-and-collections-scatter-py\n\n * https://matplotlib.org/stable/tutorials/introductory/sample_plots.html#sphx-glr-tutorials-introductory-sample-plots-py\n\n参考资料\n\n * https://flashgene.com/archives/163100.html",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"ubuntu 系统常用命令",frontmatter:{title:"ubuntu 系统常用命令",date:"2021-08-11T16:48:00.000Z",permalink:"/pages/e7ed74/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/05.ubuntu%20%E7%B3%BB%E7%BB%9F%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html",relativePath:"04.wiki搬运/03.常用库的常见用法/05.ubuntu 系统常用命令.md",key:"v-1c2d5bbc",path:"/pages/e7ed74/",headers:[{level:2,title:"Ubuntu 系统常用命令",slug:"ubuntu-系统常用命令",normalizedTitle:"ubuntu 系统常用命令",charIndex:2}],headersStr:"Ubuntu 系统常用命令",content:'# Ubuntu 系统常用命令\n\n1、查看端口占用\n\nps aux | grep python\n\nps -ef\n\n\n1\n2\n3\n\n\n2、查看系统版本\n\n> uname -a\nLinux muyun99-7920 5.4.0-80-generic #90~18.04.1-Ubuntu SMP Tue Jul 13 19:40:02 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\n\n> cat /etc/os-release\n\nNAME="Ubuntu"\nVERSION="18.04.5 LTS (Bionic Beaver)"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME="Ubuntu 18.04.5 LTS"\nVERSION_ID="18.04"\nHOME_URL="https://www.ubuntu.com/"\nSUPPORT_URL="https://help.ubuntu.com/"\nBUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"\nPRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"\nVERSION_CODENAME=bionic\nUBUNTU_CODENAME=bionic\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 参考资料\n\n * https://www.cxyxiaowu.com/17747.html',normalizedContent:'# ubuntu 系统常用命令\n\n1、查看端口占用\n\nps aux | grep python\n\nps -ef\n\n\n1\n2\n3\n\n\n2、查看系统版本\n\n> uname -a\nlinux muyun99-7920 5.4.0-80-generic #90~18.04.1-ubuntu smp tue jul 13 19:40:02 utc 2021 x86_64 x86_64 x86_64 gnu/linux\n\n> cat /etc/os-release\n\nname="ubuntu"\nversion="18.04.5 lts (bionic beaver)"\nid=ubuntu\nid_like=debian\npretty_name="ubuntu 18.04.5 lts"\nversion_id="18.04"\nhome_url="https://www.ubuntu.com/"\nsupport_url="https://help.ubuntu.com/"\nbug_report_url="https://bugs.launchpad.net/ubuntu/"\nprivacy_policy_url="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"\nversion_codename=bionic\nubuntu_codename=bionic\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n# 参考资料\n\n * https://www.cxyxiaowu.com/17747.html',charsets:{cjk:!0},lastUpdated:"2021/09/05, 19:05:19"},{title:"numpy 库常用用法",frontmatter:{title:"numpy 库常用用法",date:"2021-08-14T19:48:51.000Z",permalink:"/pages/37da51/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/06.numpy%20%E5%BA%93%E5%B8%B8%E7%94%A8%E7%94%A8%E6%B3%95.html",relativePath:"04.wiki搬运/03.常用库的常见用法/06.numpy 库常用用法.md",key:"v-d9a68072",path:"/pages/37da51/",headersStr:null,content:"转换 array 的shape\n\nmask.shape为(512, 1024)，转换为(512, 1024， 1)\nnp.expand_dims(mask,axis=2)\n\n\n1\n2\n",normalizedContent:"转换 array 的shape\n\nmask.shape为(512, 1024)，转换为(512, 1024， 1)\nnp.expand_dims(mask,axis=2)\n\n\n1\n2\n",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"matplotlib 库常用用法",frontmatter:{title:"matplotlib 库常用用法",date:"2021-08-15T15:24:01.000Z",permalink:"/pages/20455a/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/07.matplotlib%20%E5%BA%93%E5%B8%B8%E7%94%A8%E7%94%A8%E6%B3%95.html",relativePath:"04.wiki搬运/03.常用库的常见用法/07.matplotlib 库常用用法.md",key:"v-08f68526",path:"/pages/20455a/",headers:[{level:2,title:"matplotlib Cheatsheet",slug:"matplotlib-cheatsheet",normalizedTitle:"matplotlib cheatsheet",charIndex:2}],headersStr:"matplotlib Cheatsheet",content:"# matplotlib Cheatsheet\n\n\n\n\n\n\n\n# 参考资料\n\n * https://github.com/matplotlib/cheatsheets",normalizedContent:"# matplotlib cheatsheet\n\n\n\n\n\n\n\n# 参考资料\n\n * https://github.com/matplotlib/cheatsheets",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"Docker 常用用法",frontmatter:{title:"Docker 常用用法",date:"2021-09-02T22:33:21.000Z",permalink:"/pages/cdf916/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/09.Docker%20%E5%B8%B8%E7%94%A8%E7%94%A8%E6%B3%95.html",relativePath:"04.wiki搬运/03.常用库的常见用法/09.Docker 常用用法.md",key:"v-fcbfac08",path:"/pages/cdf916/",headersStr:null,content:"docker\n\ndocker-compose\n\n通过 yaml 来管理 docker 的命令\n\nk8s\n\n管理一系列的 container\n\n# 安装以下软件：\n\n * nvidia-deivce：nvidia-smi\n\n * cuda：nvcc -V\n\n * cudnn:\n   \n   * whereis cudnn\n   * cat /usr/include/cudnn.h|grep CUDNN_MAJOR -A 2\n\n * nvidia-docker: nvidia-docker version\n\n * docker: docker images\n\ncuda 类型有三种\n\n * base：基于CUDA，包含最精简的依赖，用于部署预编译的CUDA应用，需要手工安装所需的其他依赖\n * runtime：基于base，添加了 CUDA toolkit 共享库，没有 nvcc 编译器？\n * devel：基于 runtime，添加了编译工具链，调试工具，头文件，静态库。用于从源码编译 CUDA 应用\n\ndocker ps -a\n\n把容器的端口映射一下\n\n# 01、安装 Docker\n\n\n# 安装 Docker\nsudo apt install docker\n\n# 查看 Docker 版本\ndocker version\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 02、安装 nvidia-docker\n\n\n\n\n1\n\n\n# 03、Docker 容器与本机之间的文件传输\n\ndocker cp 本地文件路径 ID全称:容器路径\n\ndocker cp ID全称:容器文件路径 本地路径\n\n\n1\n2\n3\n",normalizedContent:"docker\n\ndocker-compose\n\n通过 yaml 来管理 docker 的命令\n\nk8s\n\n管理一系列的 container\n\n# 安装以下软件：\n\n * nvidia-deivce：nvidia-smi\n\n * cuda：nvcc -v\n\n * cudnn:\n   \n   * whereis cudnn\n   * cat /usr/include/cudnn.h|grep cudnn_major -a 2\n\n * nvidia-docker: nvidia-docker version\n\n * docker: docker images\n\ncuda 类型有三种\n\n * base：基于cuda，包含最精简的依赖，用于部署预编译的cuda应用，需要手工安装所需的其他依赖\n * runtime：基于base，添加了 cuda toolkit 共享库，没有 nvcc 编译器？\n * devel：基于 runtime，添加了编译工具链，调试工具，头文件，静态库。用于从源码编译 cuda 应用\n\ndocker ps -a\n\n把容器的端口映射一下\n\n# 01、安装 docker\n\n\n# 安装 docker\nsudo apt install docker\n\n# 查看 docker 版本\ndocker version\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 02、安装 nvidia-docker\n\n\n\n\n1\n\n\n# 03、docker 容器与本机之间的文件传输\n\ndocker cp 本地文件路径 id全称:容器路径\n\ndocker cp id全称:容器文件路径 本地路径\n\n\n1\n2\n3\n",charsets:{cjk:!0},lastUpdated:"2021/09/05, 19:05:19"},{title:"tmux 常用用法",frontmatter:{title:"tmux 常用用法",date:"2021-09-02T20:05:30.000Z",permalink:"/pages/25adce/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/08.tmux%20%E5%B8%B8%E7%94%A8%E7%94%A8%E6%B3%95.html",relativePath:"04.wiki搬运/03.常用库的常见用法/08.tmux 常用用法.md",key:"v-82aa7594",path:"/pages/25adce/",headers:[{level:2,title:"tmux 常用用法",slug:"tmux-常用用法",normalizedTitle:"tmux 常用用法",charIndex:2},{level:3,title:"修改tmux 配置文件，设置自己的洗好",slug:"修改tmux-配置文件-设置自己的洗好",normalizedTitle:"修改tmux 配置文件，设置自己的洗好",charIndex:239}],headersStr:"tmux 常用用法 修改tmux 配置文件，设置自己的洗好",content:'# tmux 常用用法\n\n# 创建tmux\n\n# [name] 是所创建窗口的名字\ntmux new -s [name]\n\n\n1\n2\n\n\n# 进入已创建的tmux\n\n# [name] 是所创建窗口的名字\ntmux attach -t [name]\n\n\n1\n2\n\n\n# 临时退出 tmux\n\nctrl + b + d\n\n\n1\n\n\n# 杀死 tmux 所有窗口\n\n# [name] 是所创建窗口的名字\ntmux kill-session - t [name]\n\n\n1\n2\n\n\n\n# 修改tmux 配置文件，设置自己的洗好\n\n# 1、可以在 ~/.tmux.conf 配置文件中修改；\n\nvim ~/.tmux.conf\n\n\n1\n\n\n# 2、修改前缀命令\n\nunbind C-b\nset -g prefix C-a\n\n\n1\n2\n\n\n# 3、用 ctrl + 方向键切换窗口\n\nbind -n M-Left select-pane -L\nbind -n M-Right select-pane -R\nbind -n M-Up select-pane -U\nbind -n M-Down select-pane -D\n\n\n1\n2\n3\n4\n\n\n# 4、修改完毕使配置生效\n\ntmux source-file .tmux.conf\n\n\n1\n\n\n# tmux 分屏\n\n左右分屏： tmux split-pane -h  对应快捷键： Ctrl+b    %\n\n上下分屏：tmux spilt-pane -v     对应快捷键： Ctrl+b    "\n\n关闭分屏： 快捷键 Ctrl + b    x\n\n选择分屏：alt + 方向键\n\n\n1\n2\n3\n4\n5\n6\n7\n',normalizedContent:'# tmux 常用用法\n\n# 创建tmux\n\n# [name] 是所创建窗口的名字\ntmux new -s [name]\n\n\n1\n2\n\n\n# 进入已创建的tmux\n\n# [name] 是所创建窗口的名字\ntmux attach -t [name]\n\n\n1\n2\n\n\n# 临时退出 tmux\n\nctrl + b + d\n\n\n1\n\n\n# 杀死 tmux 所有窗口\n\n# [name] 是所创建窗口的名字\ntmux kill-session - t [name]\n\n\n1\n2\n\n\n\n# 修改tmux 配置文件，设置自己的洗好\n\n# 1、可以在 ~/.tmux.conf 配置文件中修改；\n\nvim ~/.tmux.conf\n\n\n1\n\n\n# 2、修改前缀命令\n\nunbind c-b\nset -g prefix c-a\n\n\n1\n2\n\n\n# 3、用 ctrl + 方向键切换窗口\n\nbind -n m-left select-pane -l\nbind -n m-right select-pane -r\nbind -n m-up select-pane -u\nbind -n m-down select-pane -d\n\n\n1\n2\n3\n4\n\n\n# 4、修改完毕使配置生效\n\ntmux source-file .tmux.conf\n\n\n1\n\n\n# tmux 分屏\n\n左右分屏： tmux split-pane -h  对应快捷键： ctrl+b    %\n\n上下分屏：tmux spilt-pane -v     对应快捷键： ctrl+b    "\n\n关闭分屏： 快捷键 ctrl + b    x\n\n选择分屏：alt + 方向键\n\n\n1\n2\n3\n4\n5\n6\n7\n',charsets:{cjk:!0},lastUpdated:"2021/09/05, 19:05:19"},{title:"zsh 相关用法",frontmatter:{title:"zsh 相关用法",date:"2021-09-02T22:51:43.000Z",permalink:"/pages/018ff0/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/10.zsh%20%E7%9B%B8%E5%85%B3%E7%94%A8%E6%B3%95.html",relativePath:"04.wiki搬运/03.常用库的常见用法/10.zsh 相关用法.md",key:"v-f453b4b0",path:"/pages/018ff0/",headersStr:null,content:"# 1、bash 切换到 zsh 环境变量丢失问题\n\n * 参考链接：https://www.jianshu.com/p/a3e0bb16675e",normalizedContent:"# 1、bash 切换到 zsh 环境变量丢失问题\n\n * 参考链接：https://www.jianshu.com/p/a3e0bb16675e",charsets:{cjk:!0},lastUpdated:"2021/09/05, 19:05:19"},{title:"常见的专有名词",frontmatter:{title:"常见的专有名词",date:"2021-09-02T22:57:05.000Z",permalink:"/pages/b96e43/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/11.%E5%B8%B8%E8%A7%81%E7%9A%84%E4%B8%93%E6%9C%89%E5%90%8D%E8%AF%8D.html",relativePath:"04.wiki搬运/03.常用库的常见用法/11.常见的专有名词.md",key:"v-4be9171a",path:"/pages/b96e43/",headersStr:null,content:"# 单体架构\n\n * 将所有功能打包在一个容器中运行的设计风格\n\n * 一个实例中集成了一个系统的所有功能\n\n * 通过负载均衡软件/设备实现多实例调用\n\n * 优点：易开发、易调试、易部署\n\n * 缺点：可靠性差、不易协同、升级困难\n\n# 面向服务的架构（SOA）\n\n * 是一种分布式服务架构\n * 将应用程序的不同服务进行拆分、通过这些服务之间定义明确的借口和协议联系起来\n * 进而实现跨服务/系统交互的能力\n * 优点：松耦合、独立性、可重用\n * 挑战：随着大型互联网公司对大规模弹性部署和敏捷开发的需求，SOA 逐渐难以应付\n\n# 微服务（Microservices）\n\n * 一种软件架构风格\n * 专注于单一责任与功能的服务为基础，服务之间互相协调配合\n * 每个服务运行在其独立的进程中，服务之间采用轻量级的通信机制互相沟通，能够被独立部署\n * 优点：\n   * 服务松耦合、独立开发、独立部署\n   * 服务可以用不同语言开发\n * 相较于 SOA 的优势\n   * 复用率更高\n   * 快速响应\n   * 弹性扩展\n   * 支持异构\n * 微服务要解决的问题\n   * 服务划分\n     * 业务边界清晰\n     * 最小化地变更\n     * 最大化地复用\n   * 服务注册与调用\n   * 延迟队列\n   * 服务熔断处理\n   * 缓存设计\n   * 分布式事务实现\n * 典型代表\n   * Spring Cloud\n     * 基于 Spring Boot 实现的服务治理框架\n     * Spring Boot 专注于快速、方便集成的单个个体\n     * Spring Cloud 关注全局服务治理\n   * Dubbo\n     * 阿里开源的高性能、轻量级分布式RPC框架\n     * 面向接口的远程方法调用\n     * 智能容错和负载均衡\n     * 服务自动注册和发现\n * 困难\n   * 业务架构复杂\n   * 服务拆分粒度难以把握\n   * 部署维护困难\n\n# 敏捷开发",normalizedContent:"# 单体架构\n\n * 将所有功能打包在一个容器中运行的设计风格\n\n * 一个实例中集成了一个系统的所有功能\n\n * 通过负载均衡软件/设备实现多实例调用\n\n * 优点：易开发、易调试、易部署\n\n * 缺点：可靠性差、不易协同、升级困难\n\n# 面向服务的架构（soa）\n\n * 是一种分布式服务架构\n * 将应用程序的不同服务进行拆分、通过这些服务之间定义明确的借口和协议联系起来\n * 进而实现跨服务/系统交互的能力\n * 优点：松耦合、独立性、可重用\n * 挑战：随着大型互联网公司对大规模弹性部署和敏捷开发的需求，soa 逐渐难以应付\n\n# 微服务（microservices）\n\n * 一种软件架构风格\n * 专注于单一责任与功能的服务为基础，服务之间互相协调配合\n * 每个服务运行在其独立的进程中，服务之间采用轻量级的通信机制互相沟通，能够被独立部署\n * 优点：\n   * 服务松耦合、独立开发、独立部署\n   * 服务可以用不同语言开发\n * 相较于 soa 的优势\n   * 复用率更高\n   * 快速响应\n   * 弹性扩展\n   * 支持异构\n * 微服务要解决的问题\n   * 服务划分\n     * 业务边界清晰\n     * 最小化地变更\n     * 最大化地复用\n   * 服务注册与调用\n   * 延迟队列\n   * 服务熔断处理\n   * 缓存设计\n   * 分布式事务实现\n * 典型代表\n   * spring cloud\n     * 基于 spring boot 实现的服务治理框架\n     * spring boot 专注于快速、方便集成的单个个体\n     * spring cloud 关注全局服务治理\n   * dubbo\n     * 阿里开源的高性能、轻量级分布式rpc框架\n     * 面向接口的远程方法调用\n     * 智能容错和负载均衡\n     * 服务自动注册和发现\n * 困难\n   * 业务架构复杂\n   * 服务拆分粒度难以把握\n   * 部署维护困难\n\n# 敏捷开发",charsets:{cjk:!0},lastUpdated:"2021/09/05, 19:05:19"},{title:"sharelatex 部署",frontmatter:{title:"sharelatex 部署",date:"2021-09-02T23:52:27.000Z",permalink:"/pages/023d1e/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/12.sharelatex%20%E9%83%A8%E7%BD%B2.html",relativePath:"04.wiki搬运/03.常用库的常见用法/12.sharelatex 部署.md",key:"v-0cefb3e1",path:"/pages/023d1e/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/09/05, 19:05:19"},{title:"cpp STL 常用用法",frontmatter:{title:"cpp STL 常用用法",date:"2021-09-04T20:30:51.000Z",permalink:"/pages/8b291c/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/14.cpp%20STL%20%E5%B8%B8%E7%94%A8%E7%94%A8%E6%B3%95.html",relativePath:"04.wiki搬运/03.常用库的常见用法/14.cpp STL 常用用法.md",key:"v-203c8ca1",path:"/pages/8b291c/",headers:[{level:3,title:"CPP STL 常用用法",slug:"cpp-stl-常用用法",normalizedTitle:"cpp stl 常用用法",charIndex:2},{level:3,title:"01、vector",slug:"_01、vector",normalizedTitle:"01、vector",charIndex:19}],headersStr:"CPP STL 常用用法 01、vector",content:"# CPP STL 常用用法\n\n\n# 01、vector\n\n# vector 定义\n\n1、Vector<类型>标识符\n2、Vector<类型>标识符(最大容量)\n3、Vector<类型>标识符(最大容量,初始所有值)\n4、Int i[5]={1,2,3,4,5}\n\tVector<类型>vi(I,i+2);//得到i索引值为3以后的值\n5、Vector< vector< int> >v; 二维向量//这里最外的<>要有空格。否则在比较旧的编译器下无法通过\n\n\n1\n2\n3\n4\n5\n6\n\n\n# vector 函数\n\n1.push_back 在数组的最后添加一个数据\n2.pop_back 去掉数组的最后一个数据\n3.at 得到编号位置的数据\n4.begin 得到数组头的指针\n5.end 得到数组的最后一个单元+1的指针\n6．front 得到数组头的引用\n7.back 得到数组的最后一个单元的引用\n8.max_size 得到vector最大可以是多大\n9.capacity 当前vector分配的大小\n10.size 当前使用数据的大小\n11.resize 改变当前使用数据的大小，如果它比当前使用的大，者填充默认值\n12.reserve 改变当前vecotr所分配空间的大小\n13.erase 删除指针指向的数据项\n14.clear 清空当前的vector\n15.rbegin 将vector反转后的开始指针返回(其实就是原来的end-1)\n16.rend 将vector反转构的结束指针返回(其实就是原来的begin-1)\n17.empty 判断vector是否为空\n18.swap 与另一个vector交换数据\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n",normalizedContent:"# cpp stl 常用用法\n\n\n# 01、vector\n\n# vector 定义\n\n1、vector<类型>标识符\n2、vector<类型>标识符(最大容量)\n3、vector<类型>标识符(最大容量,初始所有值)\n4、int i[5]={1,2,3,4,5}\n\tvector<类型>vi(i,i+2);//得到i索引值为3以后的值\n5、vector< vector< int> >v; 二维向量//这里最外的<>要有空格。否则在比较旧的编译器下无法通过\n\n\n1\n2\n3\n4\n5\n6\n\n\n# vector 函数\n\n1.push_back 在数组的最后添加一个数据\n2.pop_back 去掉数组的最后一个数据\n3.at 得到编号位置的数据\n4.begin 得到数组头的指针\n5.end 得到数组的最后一个单元+1的指针\n6．front 得到数组头的引用\n7.back 得到数组的最后一个单元的引用\n8.max_size 得到vector最大可以是多大\n9.capacity 当前vector分配的大小\n10.size 当前使用数据的大小\n11.resize 改变当前使用数据的大小，如果它比当前使用的大，者填充默认值\n12.reserve 改变当前vecotr所分配空间的大小\n13.erase 删除指针指向的数据项\n14.clear 清空当前的vector\n15.rbegin 将vector反转后的开始指针返回(其实就是原来的end-1)\n16.rend 将vector反转构的结束指针返回(其实就是原来的begin-1)\n17.empty 判断vector是否为空\n18.swap 与另一个vector交换数据\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n",charsets:{cjk:!0},lastUpdated:"2021/09/05, 19:05:19"},{title:"Tensorboard 常用用法",frontmatter:{title:"Tensorboard 常用用法",date:"2021-10-03T00:22:50.000Z",permalink:"/pages/29671f/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/15.Tensorboard%20%E5%B8%B8%E7%94%A8%E7%94%A8%E6%B3%95.html",relativePath:"04.wiki搬运/03.常用库的常见用法/15.Tensorboard 常用用法.md",key:"v-b5744878",path:"/pages/29671f/",headersStr:null,content:"1、使用场景示例：有一个需求是提取出 tensorboard 中保存的性能值，然后自己画图\n\n * 参考资料：https://blog.csdn.net/nima1994/article/details/82844988#commentBox\n\n教师网络：\n\nFor Supervisely Fine and Supervisely Manual Coarse，PointRend、DeepLabv3+、OCRNet、HRNet are selected as teacher Network\n\nFor Cityscapes Fine and Cityscapes Official Coarse，我们使用PointRend、DeepLabv3+、OCRNet、HRNet作为Teacher Network\n\n其中DeepLabv3+ 使用 ResNet101 作为Backbone\n\n学生网络：\n\nFor Supervisely Fine and Supervisely Manual Coarse，U-Net was selected as the student network. For Cityscapes Fine and Cityscapes Official Coarse，DeepLabv3+ with ResNet18 Backbone was selected as the student network.\n\n教师网络权重推理\n\nFor Supervisely Fine and Supervisely Manual Coarse, we leverage the original weight file of the trained models from the {HRNet \\cite{WangSCJDZLMTWLX19}}, {OCRNet \\cite{YuanCW19} }, {PointRend\\cite{kirillov2020pointrend}} and, {DeepLabv3+\\cite{chen2018encoder} } trained on the Cityscapes fine dataset as our teacher network. The results of the above methods are reproduced by the publicly available models provided with the recommended test settings.\n\nIn order to initially verify the cross-domain performance of these teacher models, the weights pretrained on Cityscapes Fine are used to initialize the teacher network, and directly use the supervisely dataset for inferring. Portrait results are retained to generate teacher weights.\n\nAs shown in Tab.~\\ref{t_param_flops}, the four methods (including a student network Deeplabv3+) achieve 60-90% mIoU with 33-79 BF score on the test dataset of Supervisely Fine . The best performance on both mIoU and BF score belongs to PointRend which is the newest semantic segmentation model among the four methods.\n\nFor Cityscapes Fine and Cityscapes Official Coarse, the teacher networks pretrained on on ADE20k dataset. It is worth mentioning that Cityscapes and ADE20k have only 13 crossover categories that were retained to generate teacher weights, and these categories include “road, sidewalk, building, wall, fence, pole, traffic light, sky, person, car, truck, bus, bicycle”.\n\nIn the following experiment, we hope to verify whether our proposed method is effective for the knowledge distillation of teacher models with different model capacities and different segmentation performances through various comparisons.\n\nFor Supervisely Fine and Supervisely Manual Coarse, the teacher Network trained on on Cityscapes dataset.\n\n我们使用的教师模型都是在 Cityscapes 上预训练权重推理伪标签with only Portrait result\n\n我们使用的教师模型都是在 ADE20k 上预训练权重推理伪标签with only Portrait result，保留交叉的结果，包括13个类别，\n\n训练学生网络\n\nFor Supervisely Fine and Supervisely Manual Coarse, the learning rate starts at 0.1 and changes to 0.5 times the original rate every 25 epochs. We set the number of training epochs to 100 and batch size to 32 for all trials. , we set the optimizer to Ranger\\cite{yong2020gradient}with a weight decay 5e−4. The data augmentation is only normalization.\n\nFor Cityscapes Fine and Cityscapes Official Coarse, the student network is trained for 80000 iterations with 2 batchsize for each GPU. The learning rate policy is poly, from 0.01 to 0.0001. The optimizer is SGD with a weight decay 0.0001 and momentum 0.9. The data augmentation contains a series of randomly cropping to (512, 1024), randomly flipping, photometric distortion and normalization.\n\nAll experiments are performed on four GTX2080Ti GPUs with mixed precision training. Besides, the λ is initialized to 0 during the 10% training epochs/iterations, linearly increases λ to 0.5 during the rest of the training phase.",normalizedContent:"1、使用场景示例：有一个需求是提取出 tensorboard 中保存的性能值，然后自己画图\n\n * 参考资料：https://blog.csdn.net/nima1994/article/details/82844988#commentbox\n\n教师网络：\n\nfor supervisely fine and supervisely manual coarse，pointrend、deeplabv3+、ocrnet、hrnet are selected as teacher network\n\nfor cityscapes fine and cityscapes official coarse，我们使用pointrend、deeplabv3+、ocrnet、hrnet作为teacher network\n\n其中deeplabv3+ 使用 resnet101 作为backbone\n\n学生网络：\n\nfor supervisely fine and supervisely manual coarse，u-net was selected as the student network. for cityscapes fine and cityscapes official coarse，deeplabv3+ with resnet18 backbone was selected as the student network.\n\n教师网络权重推理\n\nfor supervisely fine and supervisely manual coarse, we leverage the original weight file of the trained models from the {hrnet \\cite{wangscjdzlmtwlx19}}, {ocrnet \\cite{yuancw19} }, {pointrend\\cite{kirillov2020pointrend}} and, {deeplabv3+\\cite{chen2018encoder} } trained on the cityscapes fine dataset as our teacher network. the results of the above methods are reproduced by the publicly available models provided with the recommended test settings.\n\nin order to initially verify the cross-domain performance of these teacher models, the weights pretrained on cityscapes fine are used to initialize the teacher network, and directly use the supervisely dataset for inferring. portrait results are retained to generate teacher weights.\n\nas shown in tab.~\\ref{t_param_flops}, the four methods (including a student network deeplabv3+) achieve 60-90% miou with 33-79 bf score on the test dataset of supervisely fine . the best performance on both miou and bf score belongs to pointrend which is the newest semantic segmentation model among the four methods.\n\nfor cityscapes fine and cityscapes official coarse, the teacher networks pretrained on on ade20k dataset. it is worth mentioning that cityscapes and ade20k have only 13 crossover categories that were retained to generate teacher weights, and these categories include “road, sidewalk, building, wall, fence, pole, traffic light, sky, person, car, truck, bus, bicycle”.\n\nin the following experiment, we hope to verify whether our proposed method is effective for the knowledge distillation of teacher models with different model capacities and different segmentation performances through various comparisons.\n\nfor supervisely fine and supervisely manual coarse, the teacher network trained on on cityscapes dataset.\n\n我们使用的教师模型都是在 cityscapes 上预训练权重推理伪标签with only portrait result\n\n我们使用的教师模型都是在 ade20k 上预训练权重推理伪标签with only portrait result，保留交叉的结果，包括13个类别，\n\n训练学生网络\n\nfor supervisely fine and supervisely manual coarse, the learning rate starts at 0.1 and changes to 0.5 times the original rate every 25 epochs. we set the number of training epochs to 100 and batch size to 32 for all trials. , we set the optimizer to ranger\\cite{yong2020gradient}with a weight decay 5e−4. the data augmentation is only normalization.\n\nfor cityscapes fine and cityscapes official coarse, the student network is trained for 80000 iterations with 2 batchsize for each gpu. the learning rate policy is poly, from 0.01 to 0.0001. the optimizer is sgd with a weight decay 0.0001 and momentum 0.9. the data augmentation contains a series of randomly cropping to (512, 1024), randomly flipping, photometric distortion and normalization.\n\nall experiments are performed on four gtx2080ti gpus with mixed precision training. besides, the λ is initialized to 0 during the 10% training epochs/iterations, linearly increases λ to 0.5 during the rest of the training phase.",charsets:{cjk:!0},lastUpdated:"2021/10/12, 20:54:32"},{title:"typecho 部署",frontmatter:{title:"typecho 部署",date:"2021-09-02T23:52:43.000Z",permalink:"/pages/dfec85/",categories:["wiki搬运","常用库的常见用法"],tags:[null]},regularPath:"/04.wiki%E6%90%AC%E8%BF%90/03.%E5%B8%B8%E7%94%A8%E5%BA%93%E7%9A%84%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/13.typecho%20%E9%83%A8%E7%BD%B2.html",relativePath:"04.wiki搬运/03.常用库的常见用法/13.typecho 部署.md",key:"v-4401a9cd",path:"/pages/dfec85/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/09/05, 19:05:19"},{title:"面试问题集锦",frontmatter:{title:"面试问题集锦",date:"2019-12-25T14:27:01.000Z",permalink:"/pages/aea6571b7a8bae86",categories:["更多","面试"],tags:[null],author:{name:"xugaoyi",link:"https://github.com/xugaoyi"}},regularPath:"/05.%E8%B5%84%E6%BA%90%E6%94%B6%E8%97%8F/01.%E9%9D%A2%E8%AF%95%E8%B5%84%E6%96%99/00.%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E9%9B%86%E9%94%A6.html",relativePath:"05.资源收藏/01.面试资料/00.面试问题集锦.md",key:"v-7e1bdab1",path:"/pages/aea6571b7a8bae86/",headers:[{level:2,title:"请做一下自我介绍",slug:"请做一下自我介绍",normalizedTitle:"请做一下自我介绍",charIndex:13},{level:2,title:"你最大的优点是什么？",slug:"你最大的优点是什么",normalizedTitle:"你最大的优点是什么？",charIndex:166},{level:2,title:"说说你最大的缺点？",slug:"说说你最大的缺点",normalizedTitle:"说说你最大的缺点？",charIndex:252},{level:2,title:"说说你对加班的看法？",slug:"说说你对加班的看法",normalizedTitle:"说说你对加班的看法？",charIndex:378},{level:2,title:"说说你对薪资的要求？",slug:"说说你对薪资的要求",normalizedTitle:"说说你对薪资的要求？",charIndex:513},{level:2,title:"在五年内，你的职业规划？",slug:"在五年内-你的职业规划",normalizedTitle:"在五年内，你的职业规划？",charIndex:903},{level:2,title:"你朋友对你的评价?",slug:"你朋友对你的评价",normalizedTitle:"你朋友对你的评价?",charIndex:1081},{level:2,title:"你还有什么问题要问吗?",slug:"你还有什么问题要问吗",normalizedTitle:"你还有什么问题要问吗?",charIndex:1241},{level:2,title:"录用后发现不适合这个职位，怎么办?",slug:"录用后发现不适合这个职位-怎么办",normalizedTitle:"录用后发现不适合这个职位，怎么办?",charIndex:1424},{level:2,title:"工作时跟领导意见不同怎么办?",slug:"工作时跟领导意见不同怎么办",normalizedTitle:"工作时跟领导意见不同怎么办?",charIndex:1601},{level:2,title:"工作出现失误并造成损失，你会怎么做?",slug:"工作出现失误并造成损失-你会怎么做",normalizedTitle:"工作出现失误并造成损失，你会怎么做?",charIndex:1779},{level:2,title:"谈谈你对跳槽的看法?",slug:"谈谈你对跳槽的看法",normalizedTitle:"谈谈你对跳槽的看法?",charIndex:2028},{level:2,title:"和同事、上司难以相处，你怎么办?",slug:"和同事、上司难以相处-你怎么办",normalizedTitle:"和同事、上司难以相处，你怎么办?",charIndex:2098},{level:2,title:"上级领导抢了你的功劳怎么办?",slug:"上级领导抢了你的功劳怎么办",normalizedTitle:"上级领导抢了你的功劳怎么办?",charIndex:2311},{level:2,title:"同事孤立你，你怎么办?",slug:"同事孤立你-你怎么办",normalizedTitle:"同事孤立你，你怎么办?",charIndex:2485},{level:2,title:"你最近是否参加了培训课程?",slug:"你最近是否参加了培训课程",normalizedTitle:"你最近是否参加了培训课程?",charIndex:2582},{level:2,title:"你对于我们公司了解多少?",slug:"你对于我们公司了解多少",normalizedTitle:"你对于我们公司了解多少?",charIndex:2636},{level:2,title:"你最擅长的技术方向是什么?",slug:"你最擅长的技术方向是什么",normalizedTitle:"你最擅长的技术方向是什么?",charIndex:2727},{level:2,title:"请说出你选择这份工作的动机?",slug:"请说出你选择这份工作的动机",normalizedTitle:"请说出你选择这份工作的动机?",charIndex:2784},{level:2,title:"你能为我们公司带来什么呢?",slug:"你能为我们公司带来什么呢",normalizedTitle:"你能为我们公司带来什么呢?",charIndex:2912},{level:2,title:"最能概括你自己的三个词?",slug:"最能概括你自己的三个词",normalizedTitle:"最能概括你自己的三个词?",charIndex:3159},{level:2,title:"作为被面试者给我打一下分?",slug:"作为被面试者给我打一下分",normalizedTitle:"作为被面试者给我打一下分?",charIndex:3225},{level:2,title:"你怎么理解你应聘的职位?",slug:"你怎么理解你应聘的职位",normalizedTitle:"你怎么理解你应聘的职位?",charIndex:3302},{level:2,title:"喜欢这份工作的哪一点?",slug:"喜欢这份工作的哪一点",normalizedTitle:"喜欢这份工作的哪一点?",charIndex:3346},{level:2,title:"为什么要离职?",slug:"为什么要离职",normalizedTitle:"为什么要离职?",charIndex:3497},{level:2,title:"说说你对行业、技术发展趋势的看法?",slug:"说说你对行业、技术发展趋势的看法",normalizedTitle:"说说你对行业、技术发展趋势的看法?",charIndex:3739},{level:2,title:"对工作的期望与目标何在?",slug:"对工作的期望与目标何在",normalizedTitle:"对工作的期望与目标何在?",charIndex:3923},{level:2,title:"谈谈你的家庭?",slug:"谈谈你的家庭",normalizedTitle:"谈谈你的家庭?",charIndex:4183},{level:2,title:"你认为自己申请这个职位还欠缺什么?",slug:"你认为自己申请这个职位还欠缺什么",normalizedTitle:"你认为自己申请这个职位还欠缺什么?",charIndex:4413},{level:2,title:"你欣赏哪种性格的人?",slug:"你欣赏哪种性格的人",normalizedTitle:"你欣赏哪种性格的人?",charIndex:4580},{level:2,title:"你通常如何处理别人的批评?",slug:"你通常如何处理别人的批评",normalizedTitle:"你通常如何处理别人的批评?",charIndex:4633},{level:2,title:"怎样对待自己的失败?",slug:"怎样对待自己的失败",normalizedTitle:"怎样对待自己的失败?",charIndex:4705},{level:2,title:"什么会让你有成就感?",slug:"什么会让你有成就感",normalizedTitle:"什么会让你有成就感?",charIndex:4761},{level:2,title:"眼下你生活中最重要的是什么?",slug:"眼下你生活中最重要的是什么",normalizedTitle:"眼下你生活中最重要的是什么?",charIndex:4805},{level:2,title:"你为什么愿意到我们公司来工作?",slug:"你为什么愿意到我们公司来工作",normalizedTitle:"你为什么愿意到我们公司来工作?",charIndex:4867},{level:2,title:"你和别人发生过争执吗?",slug:"你和别人发生过争执吗",normalizedTitle:"你和别人发生过争执吗?",charIndex:5043},{level:2,title:"你做过的哪件事最令自己感到骄傲?",slug:"你做过的哪件事最令自己感到骄傲",normalizedTitle:"你做过的哪件事最令自己感到骄傲?",charIndex:5234},{level:2,title:"对这项工作，你有哪些可预见的困难?",slug:"对这项工作-你有哪些可预见的困难",normalizedTitle:"对这项工作，你有哪些可预见的困难?",charIndex:5366},{level:2,title:"录用后你将怎样开展工作?",slug:"录用后你将怎样开展工作",normalizedTitle:"录用后你将怎样开展工作?",charIndex:5512},{level:2,title:"你希望与什么样的上级共事?",slug:"你希望与什么样的上级共事",normalizedTitle:"你希望与什么样的上级共事?",charIndex:5657},{level:2,title:"你工作经验欠缺，如何能胜任这项工作?",slug:"你工作经验欠缺-如何能胜任这项工作",normalizedTitle:"你工作经验欠缺，如何能胜任这项工作?",charIndex:5813},{level:2,title:"你会怎样获得同事的帮助?",slug:"你会怎样获得同事的帮助",normalizedTitle:"你会怎样获得同事的帮助?",charIndex:6093},{level:2,title:"如果你没被录用，你怎么打算?",slug:"如果你没被录用-你怎么打算",normalizedTitle:"如果你没被录用，你怎么打算?",charIndex:6229},{level:2,title:"最令你沮丧的事情?",slug:"最令你沮丧的事情",normalizedTitle:"最令你沮丧的事情?",charIndex:6671},{level:2,title:"想过创业吗?",slug:"想过创业吗",normalizedTitle:"想过创业吗?",charIndex:6868},{level:2,title:"为什么我们要在众多的面试者中选择你?",slug:"为什么我们要在众多的面试者中选择你",normalizedTitle:"为什么我们要在众多的面试者中选择你?",charIndex:6941},{level:2,title:"除了本公司外，还应聘了哪些公司?",slug:"除了本公司外-还应聘了哪些公司",normalizedTitle:"除了本公司外，还应聘了哪些公司?",charIndex:7126},{level:2,title:"你并非毕业于名牌院校?",slug:"你并非毕业于名牌院校",normalizedTitle:"你并非毕业于名牌院校?",charIndex:7255},{level:2,title:"怎样看待学历和能力?",slug:"怎样看待学历和能力",normalizedTitle:"怎样看待学历和能力?",charIndex:7379},{level:2,title:"谈谈如何适应办公室工作的新环境?",slug:"谈谈如何适应办公室工作的新环境",normalizedTitle:"谈谈如何适应办公室工作的新环境?",charIndex:7605},{level:2,title:"谈谈对这个职务的期许?",slug:"谈谈对这个职务的期许",normalizedTitle:"谈谈对这个职务的期许?",charIndex:7742},{level:2,title:"何时可以到职?",slug:"何时可以到职",normalizedTitle:"何时可以到职?",charIndex:7882}],headersStr:"请做一下自我介绍 你最大的优点是什么？ 说说你最大的缺点？ 说说你对加班的看法？ 说说你对薪资的要求？ 在五年内，你的职业规划？ 你朋友对你的评价? 你还有什么问题要问吗? 录用后发现不适合这个职位，怎么办? 工作时跟领导意见不同怎么办? 工作出现失误并造成损失，你会怎么做? 谈谈你对跳槽的看法? 和同事、上司难以相处，你怎么办? 上级领导抢了你的功劳怎么办? 同事孤立你，你怎么办? 你最近是否参加了培训课程? 你对于我们公司了解多少? 你最擅长的技术方向是什么? 请说出你选择这份工作的动机? 你能为我们公司带来什么呢? 最能概括你自己的三个词? 作为被面试者给我打一下分? 你怎么理解你应聘的职位? 喜欢这份工作的哪一点? 为什么要离职? 说说你对行业、技术发展趋势的看法? 对工作的期望与目标何在? 谈谈你的家庭? 你认为自己申请这个职位还欠缺什么? 你欣赏哪种性格的人? 你通常如何处理别人的批评? 怎样对待自己的失败? 什么会让你有成就感? 眼下你生活中最重要的是什么? 你为什么愿意到我们公司来工作? 你和别人发生过争执吗? 你做过的哪件事最令自己感到骄傲? 对这项工作，你有哪些可预见的困难? 录用后你将怎样开展工作? 你希望与什么样的上级共事? 你工作经验欠缺，如何能胜任这项工作? 你会怎样获得同事的帮助? 如果你没被录用，你怎么打算? 最令你沮丧的事情? 想过创业吗? 为什么我们要在众多的面试者中选择你? 除了本公司外，还应聘了哪些公司? 你并非毕业于名牌院校? 怎样看待学历和能力? 谈谈如何适应办公室工作的新环境? 谈谈对这个职务的期许? 何时可以到职?",content:"# 面试问题集锦\n\n\n# 请做一下自我介绍\n\n回答提示： 一般人回答这个问题过于平常，只说姓名、年龄、工作经验，这些在简历上都有。其实，企业最希望知道的是求职者能否胜任工作，包括：最强的技能、最深入研究的知识领域、个性中最积极的部分、做过的最成功的事，主要的成就等，要突出积极的个性和做事的能力，说的合情合理企业才会相信。\n\n\n# 你最大的优点是什么？\n\n回答提示： 沉着冷静、条理清楚、立场坚定、乐于助人等，加上例子如：我在XX经过一到两年的培训及项目实战，加上实习工作，我想我适合这份工作。\n\n\n# 说说你最大的缺点？\n\n回答提示： 这个问题企业问的概率很大，通常不希望听到直接回答的缺点是什么等，如果求职者说自己小心眼、非常懒、工作效率低，企业肯定不会录用你。要从自己的优点说起，中间加一些小缺点，最后再把问答转回到优点上，突出优点的部分。\n\n\n# 说说你对加班的看法？\n\n回答提示： 实际上好多公司问这个问题，并不证明一定要加班，只是想测试你是否愿意为公司奉献。\n\n回答样本： 如果是工作需要我会义不容辞加班，我现在单身，没有任何家庭负担，可以全身心的投入工作。但同时，我也会提高工作效率，减少不必要的加班。\n\n\n# 说说你对薪资的要求？\n\n回答提示： 如果你对薪资的要求太低，那显然贬低自己的能力；如果你对薪资的要求太高，那又会显得你分量过重，公司受用不起。一些雇主通常都事先对求聘的职位定下开支预算，因而他们第一次提出的价钱往往是他们所能给予的最高价钱，他们问你只不过想证实一下这笔钱是否足以引起你对该工作的兴趣。\n\n回答样本：\n\n①我对工资没有硬性要求，我相信贵公司会友善合理。我注重的是找到工作机会，所以只要条件公平，我则不会计较太多。\n\n②我受过系统的软件编程的训练，不需要进行大量的培训，而且我本人也对编程特别感兴趣。因此，我希望公司能根据我的情况和市场标准的水平，给我合理的薪水。\n\n③如果你必须自己说出具体数目，请不要说一个宽泛的范围，那样你将只能得到最低限度的数字。最好给出一个具体的数字，这样表明你已经对当今的人才市场做了调查，知道像自己这样学历的雇员有什么样的价值。\n\n\n# 在五年内，你的职业规划？\n\n回答提示： 这是每一个应聘者都不希望被问到的问题，但是几乎每个人都会被问到，比较多的答案是“管理者”。当然，说出其他一些你感兴趣的职位也是可以的。要知道，考官总是喜欢有进取心的应聘者，此时如果说“不知道”，或许就会使你丧失一个好机会。最普通的回答应该是“我准备在技术领域有所作为”或“我希望能按照公司的管理思路发展”。\n\n\n# 你朋友对你的评价?\n\n回答提示： 想从侧面了解一下你的性格及与人相处的问题。\n\n回答样本：\n\n①我朋友都说我是一个可以信赖的人。因为，我一旦答应别人的事情，就一定会做到。如果我做不到，我就不会轻易许诺。\n\n②我觉的我是一个比较随和的人，与不同的人都可以友好相处。在我与人相处时，我总是能站在别人的角度考虑问题。\n\n\n# 你还有什么问题要问吗?\n\n回答提示： 企业的这个问题看上去可有可无，其实很关键，企业不喜欢说“没问题”的人，因为其很注重员工的个性和创新能力。企业不喜欢求职者问个人福利之类的问题，如果有人这样问：贵公司对新入公司的员工有没有什么培训项目，我可以参加吗？或者说贵公司的晋升机制是什么样的？企业将很欢迎，因为体现出你对学习的热情和对公司的忠诚度以及你的上进心。\n\n\n# 录用后发现不适合这个职位，怎么办?\n\n回答提示： 工作一段时间发现工作不适合，有两种情况：①如果你确实热爱这个职业，那就要不断学习，虚心向领导和同事学习业务知识和处事经验，了解这个职业的精神内涵和职业要求，力争减少差距；②你觉得这个职业可有可无，那还是趁早换个职业，去发现适合你的，你热爱的职业，那样你的发展前途也会大点，对单位和个人都有好处。\n\n\n# 工作时跟领导意见不同怎么办?\n\n回答样本： ①原则上我会尊重和服从领导的工作安排，同时私底下找机会以请教的口吻，婉转地表达自己的想法，看看领导是否能改变想法。②如果领导没有采纳我的建议，我也同样会按领导的要求认真地去完成这项工作。③还有一种情况，假如领导要求的方式违背原则，我会坚决提出反对意见，如领导仍固执己见，我会毫不犹豫地再向上级领导反映。\n\n\n# 工作出现失误并造成损失，你会怎么做?\n\n回答样本： ①我本意是为公司努力工作，如果造成经济损失，我认为首要的问题是想方设法去弥补或挽回经济损失。如果我无能力负责，希望单位帮助解决。\n\n②分清责任，各负其责，如果是我的责任，我甘愿受罚；如果是一个我负责的团队中别人的失误，也不能幸灾乐祸，作为一个团队，需要互相提携共同成工作，安慰同事并且帮助同事查找原因总结经验。\n\n③个人的一生不可能不犯错误，重要的是能从自己的或者是别人的错误中吸取经验教训，要检讨自己的工作方法、分析问题的深度和力度。\n\n\n# 谈谈你对跳槽的看法?\n\n回答样本： ①正常的“跳槽”能够促进人才合理流动，应该支持。②频繁的跳槽对单位和个人双方都不利，应该反对。\n\n\n# 和同事、上司难以相处，你怎么办?\n\n回答样本： ①我会服从领导的指挥，配合同事的工作。②我会从自身找原因，仔细分析是不是自己工作做得不好。还要看看是不是为人处世方面做得不好，如果是这样的话我会努力改正。③如果我找不到原因，我会找机会跟他们沟通，请他们指出我的不足，及时改正。④作为优秀的员工，应该时刻以大局为重，即使在一段时间内，领导和同事对我不理解，我也会做好本职工作，虚心向他们学习，我相信，他们会看见我的努力。\n\n\n# 上级领导抢了你的功劳怎么办?\n\n回答样本： 首先我不会找那位上级领导说明这事，我会主动找我的主管领导来沟通，因为沟通是解决人际关系的最好办法，但结果会有两种：①我的主管领导认识到自己的错误，我想我会视具体情况决定是否原谅他。②他更加变本加厉的来威胁我，那我会毫不犹豫地找我的上级领导反映此事，因为他这样做会造成负面影响，对今后的工作不利。\n\n\n# 同事孤立你，你怎么办?\n\n回答样本： ①检讨一下自己是不是对工作的热心度超过同事间交往的热心了，加强同事间的交往及共同的兴趣爱好。②工作中，切勿伤害别人的自尊心。③不在领导前拨弄是非。\n\n\n# 你最近是否参加了培训课程?\n\n回答提示： 自费参加，就是XX的培训课程（可以多谈谈自己学的技术）。\n\n\n# 你对于我们公司了解多少?\n\n回答提示： 在去公司面试前上网查一下该公司主营业务。可类似回答：贵公司有意改变策略，加强与国外大厂的OEM合作，自有品牌的部分则透过海外经销商。\n\n\n# 你最擅长的技术方向是什么?\n\n回答提示： 说和你要应聘的职位相关的技术，表现一下自己的热诚没有什么坏处。\n\n\n# 请说出你选择这份工作的动机?\n\n回答提示： 这是想知道面试者对这份工作的热忱及理解度，并筛选因一时兴起而来应试的人，如果是无经验者，可以强调“就算职种不同，也希望有机会发挥之前的经验”。\n\n回答样本： 因为我很热爱这个工作，很想从事这方面的工作。\n\n\n# 你能为我们公司带来什么呢?\n\n回答提示： ①假如你可以的话，试着告诉他们你可以减低他们的费用“我已经接受过XX近两年专业的培训，立刻就可以上岗工作”。②企业很想知道未来的员工能为企业做什么，求职者应再次重复自己的优势，然后表示：“就我的能力，我可以做一个优秀的员工在组织中发挥能力，给组织带来高效率和更多的收益”。企业喜欢求职者就申请的职位表明自己的能力，比如申请营销之类的职位，可以说：“我可以开发大量的新客户，同时，对老客户做更全面周到的服务，开发老客户的新需求和消费。” 等等。\n\n\n# 最能概括你自己的三个词?\n\n回答样本： 我经常用的三个词是：适应能力强，有责任心和做事有始终，并结合具体例子向主考官解释。\n\n\n# 作为被面试者给我打一下分?\n\n回答提示： 试着列出四个优点和一个非常非常非常小的缺点（可以抱怨一下设施，没有明确责任人的缺点是不会有人介意的）。\n\n\n# 你怎么理解你应聘的职位?\n\n回答提示： 可以把岗位职责和任务及工作态度阐述下。\n\n\n# 喜欢这份工作的哪一点?\n\n回答提示： 在回答面试官这个问题时不能太直接就把自己心理的话说出来，尤其是薪资方面的问题，不过一些无伤大雅的回答是不错的考虑，如交通方便，工作性质及内容颇能符合自己的兴趣等等都是不错的答案，不过如果这时自己能仔细思考出这份工作的与众不同之处，相信在面试上会大大加分。\n\n\n# 为什么要离职?\n\n回答提示： ①回答这个问题时一定要小心，就算在前一个工作受到再大的委屈，对公司有多少的怨言都千万不要表现出来，尤其要避免对公司本身主管的批评，避免面试官的负面情绪及印象。建议此时最好的回答方式是将问题归咎在自己身上，例如觉得工作没有学习发展的空间，自己想在面试工作的相关产业中多加学习，或是前一份工作与自己的生涯规划不合等等，回答的答案最好是积极正面的。②我希望能获得一份更好的工作，如果机会来临，我会抓住。我觉得目前的工作，已经达到顶峰，即没有升迁机会。\n\n\n# 说说你对行业、技术发展趋势的看法?\n\n回答提示： 企业对这个问题很感兴趣，只有有备而来的求职者能够过关。求职者可以直接在网上查找对你所申请的行业部门的信息，只有深入了解才能产生独特的见解。企业认为最聪明的求职者是对所面试的公司预先了解很多，包括公司各个部门，发展情况，在面试回答问题的时候可以提到所了解的情况，企业欢迎进入企业的人是“知己”，而不是“盲人”。\n\n\n# 对工作的期望与目标何在?\n\n回答提示： 这是面试者用来评断求职者是否对自己有一定程度的期望、对这份工作是否了解的问题。 对于工作有确实学习目标的人通常学习较快，对于新工作自然较容易进入状况，这时建议你，最好针对工作的性质找出一个确实的答案，如业务员的工作可以这样回答：“我的目标是能成为一个超级业务员，将公司的产品广泛的推销出去，达到最好的业绩成效；为达到这个目标，我一定会努力学习，而我相信以我认真负责的态度，一定可以达到这个目标。” 其他类的工作也可以比照这个方式回答，只要在目标方面稍微修改一下就可以了。\n\n\n# 谈谈你的家庭?\n\n回答提示： 企业面试时询问家庭问题不是非要知道求职者家庭的情况，而是要了解家庭背景对求职者的塑造和影响。企业希望听到的重点也在于家庭对求职者的积极影响。\n\n回答样本： 我很爱我的家庭，我们家一向很和睦，虽然我的父亲和母亲都是普通人，但是从小，我就看到我父亲起早贪黑，每天工作特别勤劳，他的行动无形中培养了我认真负责的态度和勤劳的精神。我母亲为人善良，对人热情，特别乐于助人，所以在单位人缘很好，她的一言一行也一直在教导我做人的道理。\n\n\n# 你认为自己申请这个职位还欠缺什么?\n\n回答提示： 企业喜欢问求职者弱点，但精明的求职者一般不直接回答。\n\n回答样本： 继续重复自己的优势，然后说：“对于这个职位和我的能力来说，我相信自己是可以胜任的，只是缺乏经验，这个问题可以进入公司以后以最短的时间来解决，我的学习能力很强，我相信可以很快融入公司的企业文化，进入工作状态。\n\n\n# 你欣赏哪种性格的人?\n\n回答提示： 诚实、不死板而且容易相处的人、有实际行动的人，加上具体例子。\n\n\n# 你通常如何处理别人的批评?\n\n回答提示： ①沈默是金，不必说什么，否则情况更糟，不过我会接受建设性的批评。②我会等大家冷静下来再讨论。\n\n\n# 怎样对待自己的失败?\n\n回答提示： 大意：我们生来都不是十全十美的，我相信我有第二个机会改正我的错误。\n\n\n# 什么会让你有成就感?\n\n回答提示： 为贵公司竭力效劳，尽我所能，完成一个项目。\n\n\n# 眼下你生活中最重要的是什么?\n\n回答提示： 对我来说，能在这个领域找到工作是最重要的，能在贵公司任职对我说最重要。\n\n\n# 你为什么愿意到我们公司来工作?\n\n回答提示： 对于这个问题，你要格外小心，如果你已经对该单位作了研究，你可以回答一些详细的原因。\n\n回答样本： ①公司本身高技术开发环境很吸引我我同公司出生在同样的时代，我希望能够进入一家与我共同成长的公司。②你们公司一直都稳定发展，在近几年来在市场上有竞争力。我认为贵公司能够给我提供一个与众不同的发展道路。\n\n\n# 你和别人发生过争执吗?\n\n回答提示： 这是面试中最险恶的问题，其实是考官布下的一个陷阱，成功解决矛盾是一个协作团体中成员所必备的能力。假如你工作在一个服务行业，这个问题简直成了最重要的个环节。你是否能获得这份工作，将取决于这个问题的回答。考官希望看到你是成熟且乐于奉献的。他们通过这个问题了解你的成熟度和处世能力。在没有外界干涉的情况下，通过妥协的方式来解决才是正确答案。\n\n\n# 你做过的哪件事最令自己感到骄傲?\n\n回答提示： 这是考官给你的一个机会，让你展示自己把握命运的能力。这会体现你潜在的领导能力以及你被提升的可能性。假如你应聘于一个服务性质的单位，你很可能会被邀请去餐。记住：你的前途取决于你的知识、你的社交能力和综合表现。\n\n\n# 对这项工作，你有哪些可预见的困难?\n\n回答提示： ①不宜直接说出具体的困难，否则可能令对方怀疑应聘者不行。②可以尝试迂回战术，说出应聘者对困难所持有的态度工作中出现一些困难是正常的，也是难免的，但是只要有坚忍不拔的毅力、良好的合作精神以及事前周密而充分的准备，任何困难都是可以克服。\n\n\n# 录用后你将怎样开展工作?\n\n回答提示： ①如果应聘者对于应聘的职位缺乏足够的了解，最好不要直接说出自己开展工作的具体办法。②可以尝试采用迂回战术来回答，如“首先听取领导的指示和要求，然后就有关情况进行了解和熟悉，接下来制定一份近期的工作计划并报领导批准，最后根据计划开展工作。”。\n\n\n# 你希望与什么样的上级共事?\n\n回答提示： ①通过应聘者对上级的“希望”可以判断出应聘者对自我要求的意识，这既上一个陷阱，又是一次机会。②最好回避对上级具体的希望，多谈对自己的要求。如“做为刚步入社会的新人，我应该多要求自己尽快熟悉环境、适应环境，而不应该对环境提出什么要求，只要能发挥我的专长就可以了。\n\n\n# 你工作经验欠缺，如何能胜任这项工作?\n\n回答提示： ①如果招聘单位对应届毕业生的应聘者提出这个问题，说明招聘公司并不真正在乎经验，关键看应聘者怎样回答。②对这个问题的回答最好要体现出应聘者的诚恳、机智、果敢及敬业。\n\n回答样本： 作为应届毕业生，在工作经验方面的确会有所欠缺，因此在读书期间我一直利用各种机会在这个行业里做兼职。我也发现，实际工作远比书本知识丰富、复杂。但我有较强的责任心、适应能力和学习能力，而且比较勤奋，所以在兼职中均能圆满完成各项工作，从中获取的经验也令我受益非浅。请贵公司放心，学校所学及兼职的工作经验使我一定能胜任这个职位。\n\n\n# 你会怎样获得同事的帮助?\n\n回答提示： 每个公司都在不断变化发展的过程中，你当然希望你的员工也是这样。你希望得到那些希望并欢迎变化的人，因为这些明白，为了公司的发展，变化是公司日常生活中重要组成部分。这样的员工往往很容易适应公司的变化，并会对变化做出积极的响应。\n\n\n# 如果你没被录用，你怎么打算?\n\n回答样本： 现在的社会是一个竞争的社会，从这次面试中也可看出这一点，有竞争就必然有优劣，有成功必定就会有失败。往往成功的背后有许多的困难和挫折，如果这次失败了也仅仅是一次而已，只有经过经验经历的积累才能塑造出一个完全的成功者。我会从以下几个方面来正确看待这次失败：①要敢于面对，面对这次失败不气馁，接受已经失去了这次机会就不会回头这个现实，从心理意志和现出对这次失败的抵抗力。要有自信，相信自己经历了这次之后经过努力一定能行，能够超越自我。②善于反思，对于这次面试经验要认真总结，思考剖析，能够从自身的角度找差距正确对自己，实事求是地评价自己，辩证的看待自己的长短得失，做一个明白人。③走出阴影，克服这一次失败带给自己的心理压力，时刻牢记自己弱点，防患于未然，加强学习，提高自身素质。④认真工作，回到原单位岗位上后，要实实在在、踏踏实实地工作，三十六行行行出状元，争取在本岗位上做出一定的成绩。⑤再接再厉，以后如果有机会我仍然后再次参加竞争。\n\n\n# 最令你沮丧的事情?\n\n回答样本： 曾经接触过一个客户，原本就有耳闻他以挑剔出名，所以事前的准备功夫做得十分充分，也投入了相当多的时间与精力，最后客户虽然并没有照单全收，但是接受的程度已经出乎我们意料之外了。原以为从此可以合作愉快，却得知客户最后因为预算关系选择了另一家代理商，之前的努力因而付诸流水。尽管如此，我还是从这次的经验学到很多，如对该产业的了解，整个team的默契也更好了。\n\n\n# 想过创业吗?\n\n回答提示： 这个问题可以显示你的冲劲，但如果你的回答是“有”的话，千万小心，下一个问题可能就是：那么为什么你不这样做呢?\n\n\n# 为什么我们要在众多的面试者中选择你?\n\n回答提示： 别过度吹嘘自己的能力，或信口开河地乱开支票，例如一定会为该公司带来多少钱的业务等，这样很容易给人一种爱说大话、不切实际的感觉。\n\n回答样本： 根据我对贵公司的了解，以及我在这份工作上所累积的专业、经验及人脉，相信正是贵公司所找寻的人才。而我在工作态度、EQ上，也有圆融、成熟的一面，和主管、同事都能合作愉快。\n\n\n# 除了本公司外，还应聘了哪些公司?\n\n回答提示： 这是相当多公司会问的问题，其用意是要概略知道应征者的求职志向，所以这并非绝对是负面答案，就算不便说出公司名称，也应回答“销售同种产品的公司”，如果应聘的其他公司是不同业界，容易让人产生无法信任的感觉。\n\n\n# 你并非毕业于名牌院校?\n\n回答样本： 是否毕业于名牌院校不重要，重要的是有能力完成您交给我的工作，我接受了XX的职业培训，掌握的技能完全可以胜任贵公司现在工作，而且我比一些名牌院校的应届毕业生的动手能力还要强，我想我更适合贵公司这个职位。\n\n\n# 怎样看待学历和能力?\n\n回答样本： 学历我想只要是大学专科的学历，就表明觉得我具备了根本的学习能力。剩下的，你是学士也好，还是博士也好，对于这一点的讨论，不是看你学了多少知识，而是看你在这个领域上发挥了什么，也就是所说的能力问题。一个人工作能力的高低直接决定其职场命运，而学历的高低只是进入一个企业的敲门砖，如果公司把学历卡在博士上我就无法进入贵公司，当然这不一定只是我个人的损失，如果一个专科生都能完成的工作，您又何必非要招聘一位博士生呢?\n\n\n# 谈谈如何适应办公室工作的新环境?\n\n回答样本： ①办公室里每个人都有各自的岗位与职责，不得擅离岗位。②根据领导指示和工作安排，制定工作计划，提前预备，并按计划完成。③多请示并及时汇报，遇到不明白的要虚心请教。④抓间隙时间，多学习，努力提高自己的政治素质和业务水平。\n\n\n# 谈谈对这个职务的期许?\n\n回答提示： 回答前不妨先询问公司对这项职务的责任认定及归属，因为每一家公司的状况不尽相同，以免说了一堆理想抱负却发现牛头不对马嘴。\n\n回答样本： 希望能借此发挥我的所学及专长，同时也吸收贵公司在这方面的经验，就公司、我个人而言，缔造双赢的局面。\n\n\n# 何时可以到职?\n\n回答提示： 大多数企业会关心就职时间，最好是回答：“如果被录用的话，到职日可以按公司规定上班”，但是如果还未辞去上一个工作、上班时间又太近，似乎有些强人所难，因为交接至少要一个月的时间，应进一步说明原因，录取公司应该会通融的。",normalizedContent:"# 面试问题集锦\n\n\n# 请做一下自我介绍\n\n回答提示： 一般人回答这个问题过于平常，只说姓名、年龄、工作经验，这些在简历上都有。其实，企业最希望知道的是求职者能否胜任工作，包括：最强的技能、最深入研究的知识领域、个性中最积极的部分、做过的最成功的事，主要的成就等，要突出积极的个性和做事的能力，说的合情合理企业才会相信。\n\n\n# 你最大的优点是什么？\n\n回答提示： 沉着冷静、条理清楚、立场坚定、乐于助人等，加上例子如：我在xx经过一到两年的培训及项目实战，加上实习工作，我想我适合这份工作。\n\n\n# 说说你最大的缺点？\n\n回答提示： 这个问题企业问的概率很大，通常不希望听到直接回答的缺点是什么等，如果求职者说自己小心眼、非常懒、工作效率低，企业肯定不会录用你。要从自己的优点说起，中间加一些小缺点，最后再把问答转回到优点上，突出优点的部分。\n\n\n# 说说你对加班的看法？\n\n回答提示： 实际上好多公司问这个问题，并不证明一定要加班，只是想测试你是否愿意为公司奉献。\n\n回答样本： 如果是工作需要我会义不容辞加班，我现在单身，没有任何家庭负担，可以全身心的投入工作。但同时，我也会提高工作效率，减少不必要的加班。\n\n\n# 说说你对薪资的要求？\n\n回答提示： 如果你对薪资的要求太低，那显然贬低自己的能力；如果你对薪资的要求太高，那又会显得你分量过重，公司受用不起。一些雇主通常都事先对求聘的职位定下开支预算，因而他们第一次提出的价钱往往是他们所能给予的最高价钱，他们问你只不过想证实一下这笔钱是否足以引起你对该工作的兴趣。\n\n回答样本：\n\n①我对工资没有硬性要求，我相信贵公司会友善合理。我注重的是找到工作机会，所以只要条件公平，我则不会计较太多。\n\n②我受过系统的软件编程的训练，不需要进行大量的培训，而且我本人也对编程特别感兴趣。因此，我希望公司能根据我的情况和市场标准的水平，给我合理的薪水。\n\n③如果你必须自己说出具体数目，请不要说一个宽泛的范围，那样你将只能得到最低限度的数字。最好给出一个具体的数字，这样表明你已经对当今的人才市场做了调查，知道像自己这样学历的雇员有什么样的价值。\n\n\n# 在五年内，你的职业规划？\n\n回答提示： 这是每一个应聘者都不希望被问到的问题，但是几乎每个人都会被问到，比较多的答案是“管理者”。当然，说出其他一些你感兴趣的职位也是可以的。要知道，考官总是喜欢有进取心的应聘者，此时如果说“不知道”，或许就会使你丧失一个好机会。最普通的回答应该是“我准备在技术领域有所作为”或“我希望能按照公司的管理思路发展”。\n\n\n# 你朋友对你的评价?\n\n回答提示： 想从侧面了解一下你的性格及与人相处的问题。\n\n回答样本：\n\n①我朋友都说我是一个可以信赖的人。因为，我一旦答应别人的事情，就一定会做到。如果我做不到，我就不会轻易许诺。\n\n②我觉的我是一个比较随和的人，与不同的人都可以友好相处。在我与人相处时，我总是能站在别人的角度考虑问题。\n\n\n# 你还有什么问题要问吗?\n\n回答提示： 企业的这个问题看上去可有可无，其实很关键，企业不喜欢说“没问题”的人，因为其很注重员工的个性和创新能力。企业不喜欢求职者问个人福利之类的问题，如果有人这样问：贵公司对新入公司的员工有没有什么培训项目，我可以参加吗？或者说贵公司的晋升机制是什么样的？企业将很欢迎，因为体现出你对学习的热情和对公司的忠诚度以及你的上进心。\n\n\n# 录用后发现不适合这个职位，怎么办?\n\n回答提示： 工作一段时间发现工作不适合，有两种情况：①如果你确实热爱这个职业，那就要不断学习，虚心向领导和同事学习业务知识和处事经验，了解这个职业的精神内涵和职业要求，力争减少差距；②你觉得这个职业可有可无，那还是趁早换个职业，去发现适合你的，你热爱的职业，那样你的发展前途也会大点，对单位和个人都有好处。\n\n\n# 工作时跟领导意见不同怎么办?\n\n回答样本： ①原则上我会尊重和服从领导的工作安排，同时私底下找机会以请教的口吻，婉转地表达自己的想法，看看领导是否能改变想法。②如果领导没有采纳我的建议，我也同样会按领导的要求认真地去完成这项工作。③还有一种情况，假如领导要求的方式违背原则，我会坚决提出反对意见，如领导仍固执己见，我会毫不犹豫地再向上级领导反映。\n\n\n# 工作出现失误并造成损失，你会怎么做?\n\n回答样本： ①我本意是为公司努力工作，如果造成经济损失，我认为首要的问题是想方设法去弥补或挽回经济损失。如果我无能力负责，希望单位帮助解决。\n\n②分清责任，各负其责，如果是我的责任，我甘愿受罚；如果是一个我负责的团队中别人的失误，也不能幸灾乐祸，作为一个团队，需要互相提携共同成工作，安慰同事并且帮助同事查找原因总结经验。\n\n③个人的一生不可能不犯错误，重要的是能从自己的或者是别人的错误中吸取经验教训，要检讨自己的工作方法、分析问题的深度和力度。\n\n\n# 谈谈你对跳槽的看法?\n\n回答样本： ①正常的“跳槽”能够促进人才合理流动，应该支持。②频繁的跳槽对单位和个人双方都不利，应该反对。\n\n\n# 和同事、上司难以相处，你怎么办?\n\n回答样本： ①我会服从领导的指挥，配合同事的工作。②我会从自身找原因，仔细分析是不是自己工作做得不好。还要看看是不是为人处世方面做得不好，如果是这样的话我会努力改正。③如果我找不到原因，我会找机会跟他们沟通，请他们指出我的不足，及时改正。④作为优秀的员工，应该时刻以大局为重，即使在一段时间内，领导和同事对我不理解，我也会做好本职工作，虚心向他们学习，我相信，他们会看见我的努力。\n\n\n# 上级领导抢了你的功劳怎么办?\n\n回答样本： 首先我不会找那位上级领导说明这事，我会主动找我的主管领导来沟通，因为沟通是解决人际关系的最好办法，但结果会有两种：①我的主管领导认识到自己的错误，我想我会视具体情况决定是否原谅他。②他更加变本加厉的来威胁我，那我会毫不犹豫地找我的上级领导反映此事，因为他这样做会造成负面影响，对今后的工作不利。\n\n\n# 同事孤立你，你怎么办?\n\n回答样本： ①检讨一下自己是不是对工作的热心度超过同事间交往的热心了，加强同事间的交往及共同的兴趣爱好。②工作中，切勿伤害别人的自尊心。③不在领导前拨弄是非。\n\n\n# 你最近是否参加了培训课程?\n\n回答提示： 自费参加，就是xx的培训课程（可以多谈谈自己学的技术）。\n\n\n# 你对于我们公司了解多少?\n\n回答提示： 在去公司面试前上网查一下该公司主营业务。可类似回答：贵公司有意改变策略，加强与国外大厂的oem合作，自有品牌的部分则透过海外经销商。\n\n\n# 你最擅长的技术方向是什么?\n\n回答提示： 说和你要应聘的职位相关的技术，表现一下自己的热诚没有什么坏处。\n\n\n# 请说出你选择这份工作的动机?\n\n回答提示： 这是想知道面试者对这份工作的热忱及理解度，并筛选因一时兴起而来应试的人，如果是无经验者，可以强调“就算职种不同，也希望有机会发挥之前的经验”。\n\n回答样本： 因为我很热爱这个工作，很想从事这方面的工作。\n\n\n# 你能为我们公司带来什么呢?\n\n回答提示： ①假如你可以的话，试着告诉他们你可以减低他们的费用“我已经接受过xx近两年专业的培训，立刻就可以上岗工作”。②企业很想知道未来的员工能为企业做什么，求职者应再次重复自己的优势，然后表示：“就我的能力，我可以做一个优秀的员工在组织中发挥能力，给组织带来高效率和更多的收益”。企业喜欢求职者就申请的职位表明自己的能力，比如申请营销之类的职位，可以说：“我可以开发大量的新客户，同时，对老客户做更全面周到的服务，开发老客户的新需求和消费。” 等等。\n\n\n# 最能概括你自己的三个词?\n\n回答样本： 我经常用的三个词是：适应能力强，有责任心和做事有始终，并结合具体例子向主考官解释。\n\n\n# 作为被面试者给我打一下分?\n\n回答提示： 试着列出四个优点和一个非常非常非常小的缺点（可以抱怨一下设施，没有明确责任人的缺点是不会有人介意的）。\n\n\n# 你怎么理解你应聘的职位?\n\n回答提示： 可以把岗位职责和任务及工作态度阐述下。\n\n\n# 喜欢这份工作的哪一点?\n\n回答提示： 在回答面试官这个问题时不能太直接就把自己心理的话说出来，尤其是薪资方面的问题，不过一些无伤大雅的回答是不错的考虑，如交通方便，工作性质及内容颇能符合自己的兴趣等等都是不错的答案，不过如果这时自己能仔细思考出这份工作的与众不同之处，相信在面试上会大大加分。\n\n\n# 为什么要离职?\n\n回答提示： ①回答这个问题时一定要小心，就算在前一个工作受到再大的委屈，对公司有多少的怨言都千万不要表现出来，尤其要避免对公司本身主管的批评，避免面试官的负面情绪及印象。建议此时最好的回答方式是将问题归咎在自己身上，例如觉得工作没有学习发展的空间，自己想在面试工作的相关产业中多加学习，或是前一份工作与自己的生涯规划不合等等，回答的答案最好是积极正面的。②我希望能获得一份更好的工作，如果机会来临，我会抓住。我觉得目前的工作，已经达到顶峰，即没有升迁机会。\n\n\n# 说说你对行业、技术发展趋势的看法?\n\n回答提示： 企业对这个问题很感兴趣，只有有备而来的求职者能够过关。求职者可以直接在网上查找对你所申请的行业部门的信息，只有深入了解才能产生独特的见解。企业认为最聪明的求职者是对所面试的公司预先了解很多，包括公司各个部门，发展情况，在面试回答问题的时候可以提到所了解的情况，企业欢迎进入企业的人是“知己”，而不是“盲人”。\n\n\n# 对工作的期望与目标何在?\n\n回答提示： 这是面试者用来评断求职者是否对自己有一定程度的期望、对这份工作是否了解的问题。 对于工作有确实学习目标的人通常学习较快，对于新工作自然较容易进入状况，这时建议你，最好针对工作的性质找出一个确实的答案，如业务员的工作可以这样回答：“我的目标是能成为一个超级业务员，将公司的产品广泛的推销出去，达到最好的业绩成效；为达到这个目标，我一定会努力学习，而我相信以我认真负责的态度，一定可以达到这个目标。” 其他类的工作也可以比照这个方式回答，只要在目标方面稍微修改一下就可以了。\n\n\n# 谈谈你的家庭?\n\n回答提示： 企业面试时询问家庭问题不是非要知道求职者家庭的情况，而是要了解家庭背景对求职者的塑造和影响。企业希望听到的重点也在于家庭对求职者的积极影响。\n\n回答样本： 我很爱我的家庭，我们家一向很和睦，虽然我的父亲和母亲都是普通人，但是从小，我就看到我父亲起早贪黑，每天工作特别勤劳，他的行动无形中培养了我认真负责的态度和勤劳的精神。我母亲为人善良，对人热情，特别乐于助人，所以在单位人缘很好，她的一言一行也一直在教导我做人的道理。\n\n\n# 你认为自己申请这个职位还欠缺什么?\n\n回答提示： 企业喜欢问求职者弱点，但精明的求职者一般不直接回答。\n\n回答样本： 继续重复自己的优势，然后说：“对于这个职位和我的能力来说，我相信自己是可以胜任的，只是缺乏经验，这个问题可以进入公司以后以最短的时间来解决，我的学习能力很强，我相信可以很快融入公司的企业文化，进入工作状态。\n\n\n# 你欣赏哪种性格的人?\n\n回答提示： 诚实、不死板而且容易相处的人、有实际行动的人，加上具体例子。\n\n\n# 你通常如何处理别人的批评?\n\n回答提示： ①沈默是金，不必说什么，否则情况更糟，不过我会接受建设性的批评。②我会等大家冷静下来再讨论。\n\n\n# 怎样对待自己的失败?\n\n回答提示： 大意：我们生来都不是十全十美的，我相信我有第二个机会改正我的错误。\n\n\n# 什么会让你有成就感?\n\n回答提示： 为贵公司竭力效劳，尽我所能，完成一个项目。\n\n\n# 眼下你生活中最重要的是什么?\n\n回答提示： 对我来说，能在这个领域找到工作是最重要的，能在贵公司任职对我说最重要。\n\n\n# 你为什么愿意到我们公司来工作?\n\n回答提示： 对于这个问题，你要格外小心，如果你已经对该单位作了研究，你可以回答一些详细的原因。\n\n回答样本： ①公司本身高技术开发环境很吸引我我同公司出生在同样的时代，我希望能够进入一家与我共同成长的公司。②你们公司一直都稳定发展，在近几年来在市场上有竞争力。我认为贵公司能够给我提供一个与众不同的发展道路。\n\n\n# 你和别人发生过争执吗?\n\n回答提示： 这是面试中最险恶的问题，其实是考官布下的一个陷阱，成功解决矛盾是一个协作团体中成员所必备的能力。假如你工作在一个服务行业，这个问题简直成了最重要的个环节。你是否能获得这份工作，将取决于这个问题的回答。考官希望看到你是成熟且乐于奉献的。他们通过这个问题了解你的成熟度和处世能力。在没有外界干涉的情况下，通过妥协的方式来解决才是正确答案。\n\n\n# 你做过的哪件事最令自己感到骄傲?\n\n回答提示： 这是考官给你的一个机会，让你展示自己把握命运的能力。这会体现你潜在的领导能力以及你被提升的可能性。假如你应聘于一个服务性质的单位，你很可能会被邀请去餐。记住：你的前途取决于你的知识、你的社交能力和综合表现。\n\n\n# 对这项工作，你有哪些可预见的困难?\n\n回答提示： ①不宜直接说出具体的困难，否则可能令对方怀疑应聘者不行。②可以尝试迂回战术，说出应聘者对困难所持有的态度工作中出现一些困难是正常的，也是难免的，但是只要有坚忍不拔的毅力、良好的合作精神以及事前周密而充分的准备，任何困难都是可以克服。\n\n\n# 录用后你将怎样开展工作?\n\n回答提示： ①如果应聘者对于应聘的职位缺乏足够的了解，最好不要直接说出自己开展工作的具体办法。②可以尝试采用迂回战术来回答，如“首先听取领导的指示和要求，然后就有关情况进行了解和熟悉，接下来制定一份近期的工作计划并报领导批准，最后根据计划开展工作。”。\n\n\n# 你希望与什么样的上级共事?\n\n回答提示： ①通过应聘者对上级的“希望”可以判断出应聘者对自我要求的意识，这既上一个陷阱，又是一次机会。②最好回避对上级具体的希望，多谈对自己的要求。如“做为刚步入社会的新人，我应该多要求自己尽快熟悉环境、适应环境，而不应该对环境提出什么要求，只要能发挥我的专长就可以了。\n\n\n# 你工作经验欠缺，如何能胜任这项工作?\n\n回答提示： ①如果招聘单位对应届毕业生的应聘者提出这个问题，说明招聘公司并不真正在乎经验，关键看应聘者怎样回答。②对这个问题的回答最好要体现出应聘者的诚恳、机智、果敢及敬业。\n\n回答样本： 作为应届毕业生，在工作经验方面的确会有所欠缺，因此在读书期间我一直利用各种机会在这个行业里做兼职。我也发现，实际工作远比书本知识丰富、复杂。但我有较强的责任心、适应能力和学习能力，而且比较勤奋，所以在兼职中均能圆满完成各项工作，从中获取的经验也令我受益非浅。请贵公司放心，学校所学及兼职的工作经验使我一定能胜任这个职位。\n\n\n# 你会怎样获得同事的帮助?\n\n回答提示： 每个公司都在不断变化发展的过程中，你当然希望你的员工也是这样。你希望得到那些希望并欢迎变化的人，因为这些明白，为了公司的发展，变化是公司日常生活中重要组成部分。这样的员工往往很容易适应公司的变化，并会对变化做出积极的响应。\n\n\n# 如果你没被录用，你怎么打算?\n\n回答样本： 现在的社会是一个竞争的社会，从这次面试中也可看出这一点，有竞争就必然有优劣，有成功必定就会有失败。往往成功的背后有许多的困难和挫折，如果这次失败了也仅仅是一次而已，只有经过经验经历的积累才能塑造出一个完全的成功者。我会从以下几个方面来正确看待这次失败：①要敢于面对，面对这次失败不气馁，接受已经失去了这次机会就不会回头这个现实，从心理意志和现出对这次失败的抵抗力。要有自信，相信自己经历了这次之后经过努力一定能行，能够超越自我。②善于反思，对于这次面试经验要认真总结，思考剖析，能够从自身的角度找差距正确对自己，实事求是地评价自己，辩证的看待自己的长短得失，做一个明白人。③走出阴影，克服这一次失败带给自己的心理压力，时刻牢记自己弱点，防患于未然，加强学习，提高自身素质。④认真工作，回到原单位岗位上后，要实实在在、踏踏实实地工作，三十六行行行出状元，争取在本岗位上做出一定的成绩。⑤再接再厉，以后如果有机会我仍然后再次参加竞争。\n\n\n# 最令你沮丧的事情?\n\n回答样本： 曾经接触过一个客户，原本就有耳闻他以挑剔出名，所以事前的准备功夫做得十分充分，也投入了相当多的时间与精力，最后客户虽然并没有照单全收，但是接受的程度已经出乎我们意料之外了。原以为从此可以合作愉快，却得知客户最后因为预算关系选择了另一家代理商，之前的努力因而付诸流水。尽管如此，我还是从这次的经验学到很多，如对该产业的了解，整个team的默契也更好了。\n\n\n# 想过创业吗?\n\n回答提示： 这个问题可以显示你的冲劲，但如果你的回答是“有”的话，千万小心，下一个问题可能就是：那么为什么你不这样做呢?\n\n\n# 为什么我们要在众多的面试者中选择你?\n\n回答提示： 别过度吹嘘自己的能力，或信口开河地乱开支票，例如一定会为该公司带来多少钱的业务等，这样很容易给人一种爱说大话、不切实际的感觉。\n\n回答样本： 根据我对贵公司的了解，以及我在这份工作上所累积的专业、经验及人脉，相信正是贵公司所找寻的人才。而我在工作态度、eq上，也有圆融、成熟的一面，和主管、同事都能合作愉快。\n\n\n# 除了本公司外，还应聘了哪些公司?\n\n回答提示： 这是相当多公司会问的问题，其用意是要概略知道应征者的求职志向，所以这并非绝对是负面答案，就算不便说出公司名称，也应回答“销售同种产品的公司”，如果应聘的其他公司是不同业界，容易让人产生无法信任的感觉。\n\n\n# 你并非毕业于名牌院校?\n\n回答样本： 是否毕业于名牌院校不重要，重要的是有能力完成您交给我的工作，我接受了xx的职业培训，掌握的技能完全可以胜任贵公司现在工作，而且我比一些名牌院校的应届毕业生的动手能力还要强，我想我更适合贵公司这个职位。\n\n\n# 怎样看待学历和能力?\n\n回答样本： 学历我想只要是大学专科的学历，就表明觉得我具备了根本的学习能力。剩下的，你是学士也好，还是博士也好，对于这一点的讨论，不是看你学了多少知识，而是看你在这个领域上发挥了什么，也就是所说的能力问题。一个人工作能力的高低直接决定其职场命运，而学历的高低只是进入一个企业的敲门砖，如果公司把学历卡在博士上我就无法进入贵公司，当然这不一定只是我个人的损失，如果一个专科生都能完成的工作，您又何必非要招聘一位博士生呢?\n\n\n# 谈谈如何适应办公室工作的新环境?\n\n回答样本： ①办公室里每个人都有各自的岗位与职责，不得擅离岗位。②根据领导指示和工作安排，制定工作计划，提前预备，并按计划完成。③多请示并及时汇报，遇到不明白的要虚心请教。④抓间隙时间，多学习，努力提高自己的政治素质和业务水平。\n\n\n# 谈谈对这个职务的期许?\n\n回答提示： 回答前不妨先询问公司对这项职务的责任认定及归属，因为每一家公司的状况不尽相同，以免说了一堆理想抱负却发现牛头不对马嘴。\n\n回答样本： 希望能借此发挥我的所学及专长，同时也吸收贵公司在这方面的经验，就公司、我个人而言，缔造双赢的局面。\n\n\n# 何时可以到职?\n\n回答提示： 大多数企业会关心就职时间，最好是回答：“如果被录用的话，到职日可以按公司规定上班”，但是如果还未辞去上一个工作、上班时间又太近，似乎有些强人所难，因为交接至少要一个月的时间，应进一步说明原因，录取公司应该会通融的。",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"面试可能会用到的知识点",frontmatter:{title:"面试可能会用到的知识点",date:"2021-06-27T16:29:33.000Z",permalink:"/pages/4f4178/",categories:["计算机视觉","资料搜集"],tags:[null]},regularPath:"/05.%E8%B5%84%E6%BA%90%E6%94%B6%E8%97%8F/01.%E9%9D%A2%E8%AF%95%E8%B5%84%E6%96%99/01.%E9%9D%A2%E8%AF%95%E5%8F%AF%E8%83%BD%E4%BC%9A%E7%94%A8%E5%88%B0%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9.html",relativePath:"05.资源收藏/01.面试资料/01.面试可能会用到的知识点.md",key:"v-038a1890",path:"/pages/4f4178/",headers:[{level:3,title:"CNN基础",slug:"cnn基础",normalizedTitle:"cnn基础",charIndex:2},{level:3,title:"图神经网络",slug:"图神经网络",normalizedTitle:"图神经网络",charIndex:214}],headersStr:"CNN基础 图神经网络",content:"# CNN基础\n\n如何设置 Batch size?\n\n * CNN基础——如何设置BatchSize\n\n * 怎么选取训练神经网络时的Batch size?\n\n * 训练神经网络时如何确定batch size？\n\n如何并行训练\n\n * [原创][深度][PyTorch] DDP系列第三篇：实战与技巧\n\n什么是过拟合和欠拟合\n\n * 欠拟合与过拟合技术总结\n\n特征图尺寸和感受野计算\n\n * 特征图尺寸和感受野计算详解\n\n\n# 图神经网络\n\n * 图神经网络从入门到入门",normalizedContent:"# cnn基础\n\n如何设置 batch size?\n\n * cnn基础——如何设置batchsize\n\n * 怎么选取训练神经网络时的batch size?\n\n * 训练神经网络时如何确定batch size？\n\n如何并行训练\n\n * [原创][深度][pytorch] ddp系列第三篇：实战与技巧\n\n什么是过拟合和欠拟合\n\n * 欠拟合与过拟合技术总结\n\n特征图尺寸和感受野计算\n\n * 特征图尺寸和感受野计算详解\n\n\n# 图神经网络\n\n * 图神经网络从入门到入门",charsets:{cjk:!0},lastUpdated:"2021/10/03, 15:21:19"},{title:"Supermemo 面试知识点卡片-20210808",frontmatter:{title:"Supermemo 面试知识点卡片-20210808",date:"2021-08-08T22:28:55.000Z",permalink:"/pages/85131e/",categories:["资源收藏","面试资料"],tags:[null]},regularPath:"/05.%E8%B5%84%E6%BA%90%E6%94%B6%E8%97%8F/01.%E9%9D%A2%E8%AF%95%E8%B5%84%E6%96%99/02.Supermemo%20%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9%E5%8D%A1%E7%89%87-20210808.html",relativePath:"05.资源收藏/01.面试资料/02.Supermemo 面试知识点卡片-20210808.md",key:"v-60771a10",path:"/pages/85131e/",headers:[{level:2,title:"Supermemo 面试知识点卡片-20210808",slug:"supermemo-面试知识点卡片-20210808",normalizedTitle:"supermemo 面试知识点卡片-20210808",charIndex:2}],headersStr:"Supermemo 面试知识点卡片-20210808",content:"# Supermemo 面试知识点卡片-20210808\n\n# 01、感受野计算公式\n\n# 问题：\n\n# 回答：\n\n# 参考资料\n\n# 02、特征图尺寸计算公式\n\n# 问题：\n\n# 回答：\n\n# 参考资料\n\n# 03、常见的评价指标：\n\n# 问题：请解释 xx 指标是什么含义\n\n图像分类：Accuracy、Precision、Recall、F-score、P-R曲线、ROC、AUC\n\n图像分割：pixel accuracy、mPA(mean Pixel Accuracy)、mIoU\n\n目标检测：mAP、P-R曲线、Precision、Recall、TP、FP、CN、IoU、NMS\n\n模型效率衡量：FLOPs\n\n# 回答：\n\n# 参考资料\n\n * https://mp.weixin.qq.com/s/pQhynK-JKtFrUpDil15WQA\n\n# 04、面试最后一问：你有什么想要问我的？\n\n# 问题：你有什么想要问我的？\n\n# 回答：\n\n# 参考资料\n\n * https://github.com/yifeikong/reverse-interview-zh\n\n# 05、ResNet 到底解决了什么问题？\n\n# 问题：ResNet 到底解决了什么问题？\n\n# 回答：\n\n# 参考资料\n\n# 06、ResNet 手推以及相关变形\n\n# 问题：ResNet 手推以及相关变形\n\n# 回答：\n\n# 参考资料\n\n * https://mp.weixin.qq.com/s/rUCBAL7jagkRtMHFiK3W7Q\n\n# 07、常用的图像增强方式有哪些？\n\n# 问题：常用的图像增强方式有哪些？\n\n# 回答：\n\n我认为图像增强是一个比较更泛的话题，我想聊聊图像数据的质量增强。主要包括图像增广技术和解决数据漂移的问题。\n\n首先通用的图像增强和增广应该算是一个名词，常见的图像增广方式是\n\n * \n\n数据漂移我觉得是如何解决数据对于业务的不适配性\n\n * 是否存在类别不平衡的现象？如何解决？ 一般来讲，为了提升泛化性能，我们会对类别少的样本做过采样，这部分内容我希望是离线去做的，更加省事，大家的数据版本也能保证一致性。\n\n * 是否存在噪声标注的情况？\n   \n   利用多个指标去选择可能的噪声样本，使用伪标签或者软标签的方式重新处理标签，或者调整该样本的损失权重，使其对损失的影响较小\n\n * 是否存在类别之间难易程度不均衡的问题？ 保证在训练的过程中难易程度是均衡的\n\n# 参考资料\n\n# 08、简述以下 CAM 是如何计算的？\n\n# 问题：简述以下 CAM 是如何计算的？\n\n# 回答：\n\n# 参考资料\n\n * https://github.com/zhoubolei/CAM\n\n# 09、除了RGB，还有哪些颜色通道？\n\n# 问题：除了RGB，还有哪些颜色通道？\n\n# 回答：\n\n# 参考资料\n\n * https://mp.weixin.qq.com/s/iDd02l-D1oHwzSwtdXemjw\n\n# 10、简要介绍一下 DenseNet\n\n# 问题：简要介绍一下 DenseNet\n\n# 回答：\n\n# 参考资料\n\n * https://mp.weixin.qq.com/s/9QrzvciTr15mSUKLd1hQbQ\n\n# 其他问题\n\n * 什么是知识蒸馏？\n\n *  Why we use AdaptiveAvgPool2d? What is the difference between the AvgPool2d and AdaptiveAvgPool2d\n   \n   \n   1\n   ",normalizedContent:"# supermemo 面试知识点卡片-20210808\n\n# 01、感受野计算公式\n\n# 问题：\n\n# 回答：\n\n# 参考资料\n\n# 02、特征图尺寸计算公式\n\n# 问题：\n\n# 回答：\n\n# 参考资料\n\n# 03、常见的评价指标：\n\n# 问题：请解释 xx 指标是什么含义\n\n图像分类：accuracy、precision、recall、f-score、p-r曲线、roc、auc\n\n图像分割：pixel accuracy、mpa(mean pixel accuracy)、miou\n\n目标检测：map、p-r曲线、precision、recall、tp、fp、cn、iou、nms\n\n模型效率衡量：flops\n\n# 回答：\n\n# 参考资料\n\n * https://mp.weixin.qq.com/s/pqhynk-jktfrupdil15wqa\n\n# 04、面试最后一问：你有什么想要问我的？\n\n# 问题：你有什么想要问我的？\n\n# 回答：\n\n# 参考资料\n\n * https://github.com/yifeikong/reverse-interview-zh\n\n# 05、resnet 到底解决了什么问题？\n\n# 问题：resnet 到底解决了什么问题？\n\n# 回答：\n\n# 参考资料\n\n# 06、resnet 手推以及相关变形\n\n# 问题：resnet 手推以及相关变形\n\n# 回答：\n\n# 参考资料\n\n * https://mp.weixin.qq.com/s/rucbal7jagkrtmhfik3w7q\n\n# 07、常用的图像增强方式有哪些？\n\n# 问题：常用的图像增强方式有哪些？\n\n# 回答：\n\n我认为图像增强是一个比较更泛的话题，我想聊聊图像数据的质量增强。主要包括图像增广技术和解决数据漂移的问题。\n\n首先通用的图像增强和增广应该算是一个名词，常见的图像增广方式是\n\n * \n\n数据漂移我觉得是如何解决数据对于业务的不适配性\n\n * 是否存在类别不平衡的现象？如何解决？ 一般来讲，为了提升泛化性能，我们会对类别少的样本做过采样，这部分内容我希望是离线去做的，更加省事，大家的数据版本也能保证一致性。\n\n * 是否存在噪声标注的情况？\n   \n   利用多个指标去选择可能的噪声样本，使用伪标签或者软标签的方式重新处理标签，或者调整该样本的损失权重，使其对损失的影响较小\n\n * 是否存在类别之间难易程度不均衡的问题？ 保证在训练的过程中难易程度是均衡的\n\n# 参考资料\n\n# 08、简述以下 cam 是如何计算的？\n\n# 问题：简述以下 cam 是如何计算的？\n\n# 回答：\n\n# 参考资料\n\n * https://github.com/zhoubolei/cam\n\n# 09、除了rgb，还有哪些颜色通道？\n\n# 问题：除了rgb，还有哪些颜色通道？\n\n# 回答：\n\n# 参考资料\n\n * https://mp.weixin.qq.com/s/idd02l-d1ohwzswtdxemjw\n\n# 10、简要介绍一下 densenet\n\n# 问题：简要介绍一下 densenet\n\n# 回答：\n\n# 参考资料\n\n * https://mp.weixin.qq.com/s/9qrzvcitr15msukld1hqbq\n\n# 其他问题\n\n * 什么是知识蒸馏？\n\n *  why we use adaptiveavgpool2d? what is the difference between the avgpool2d and adaptiveavgpool2d\n   \n   \n   1\n   ",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"关于",frontmatter:{title:"关于",date:"2019-12-25T14:27:01.000Z",permalink:"/about",sidebar:!1,article:!1},regularPath:"/06.%E5%85%B3%E4%BA%8E/01.%E5%85%B3%E4%BA%8E.html",relativePath:"06.关于/01.关于.md",key:"v-1acf53d4",path:"/about/",headers:[{level:3,title:"个人简介",slug:"个人简介",normalizedTitle:"个人简介",charIndex:2},{level:3,title:"研究兴趣",slug:"研究兴趣",normalizedTitle:"研究兴趣",charIndex:211},{level:3,title:"学术工作",slug:"学术工作",normalizedTitle:"学术工作",charIndex:380},{level:3,title:"竞赛及获奖",slug:"竞赛及获奖",normalizedTitle:"竞赛及获奖",charIndex:751}],headersStr:"个人简介 研究兴趣 学术工作 竞赛及获奖",content:'# 个人简介\n\n * 姓名：杜云（Yun Du）\n\n * 年级：2020级硕士\n\n * 专业：计算机科学与技术\n\n * 学院：南京航空航天大学计算机科学与技术学院\n\n * 导师：梁栋 副教授\n\n * 邮箱：muyun@nuaa.edu.cn\n\n在攻读硕士学位前，我于 2020 年南京航空航天大学物联网工程专业获得了我的本科学位证书，并保研至南航。\n\n在 2023 年，我将加入 地平线 担任感知算法工程师一职。\n\n\n# 研究兴趣\n\n我的研究兴趣包括：\n\n * Label Noise\n * Weakly-supervised Semantic Segmentation\n * Knowledge Distillation for Semantic Segmentation\n * Anything about Autonomous Driving.\n\n\n# 学术工作\n\n * (PRICAI 2022) Yun Du, Dong Liang, Rong Quan, Songlin Du, and Yaping Yan. "More Than Accuracy: An Empirical Study of Consistency Between Performance and Interpretability."\n\n * (ICASSP 2021) Liang, Dong, Yun Du, Han Sun, Liyan Zhang, Ningzhong Liu, and Mingqiang Wei. "Nlkd: using coarse annotations for semantic segmentation based on knowledge distillation."\n\n\n# 竞赛及获奖\n\n * 2022 年 The 1st Learning and Mining with Noisy Labels Challenge at IJCAI Runner-up\n * 2021 年 中国魅力校园合唱节大学组二等奖\n * 2019 年 美国大学生数学建模竞赛 Meritorious Winner\n * 2019 年 微软亚洲研究院 优秀俱乐部领袖',normalizedContent:'# 个人简介\n\n * 姓名：杜云（yun du）\n\n * 年级：2020级硕士\n\n * 专业：计算机科学与技术\n\n * 学院：南京航空航天大学计算机科学与技术学院\n\n * 导师：梁栋 副教授\n\n * 邮箱：muyun@nuaa.edu.cn\n\n在攻读硕士学位前，我于 2020 年南京航空航天大学物联网工程专业获得了我的本科学位证书，并保研至南航。\n\n在 2023 年，我将加入 地平线 担任感知算法工程师一职。\n\n\n# 研究兴趣\n\n我的研究兴趣包括：\n\n * label noise\n * weakly-supervised semantic segmentation\n * knowledge distillation for semantic segmentation\n * anything about autonomous driving.\n\n\n# 学术工作\n\n * (pricai 2022) yun du, dong liang, rong quan, songlin du, and yaping yan. "more than accuracy: an empirical study of consistency between performance and interpretability."\n\n * (icassp 2021) liang, dong, yun du, han sun, liyan zhang, ningzhong liu, and mingqiang wei. "nlkd: using coarse annotations for semantic segmentation based on knowledge distillation."\n\n\n# 竞赛及获奖\n\n * 2022 年 the 1st learning and mining with noisy labels challenge at ijcai runner-up\n * 2021 年 中国魅力校园合唱节大学组二等奖\n * 2019 年 美国大学生数学建模竞赛 meritorious winner\n * 2019 年 微软亚洲研究院 优秀俱乐部领袖',charsets:{cjk:!0},lastUpdated:"2023/03/25, 19:58:09"},{title:"LeetCode 面试算法题卡片-2021-0808",frontmatter:{title:"LeetCode 面试算法题卡片-2021-0808",date:"2021-08-08T22:36:06.000Z",permalink:"/pages/d22623/",categories:["资源收藏","面试资料"],tags:[null]},regularPath:"/05.%E8%B5%84%E6%BA%90%E6%94%B6%E8%97%8F/01.%E9%9D%A2%E8%AF%95%E8%B5%84%E6%96%99/03.LeetCode%20%E9%9D%A2%E8%AF%95%E7%AE%97%E6%B3%95%E9%A2%98%E5%8D%A1%E7%89%87-2021-0808.html",relativePath:"05.资源收藏/01.面试资料/03.LeetCode 面试算法题卡片-2021-0808.md",key:"v-02f11e84",path:"/pages/d22623/",headers:[{level:3,title:"2021/08/08",slug:"_2021-08-08",normalizedTitle:"2021/08/08",charIndex:2},{level:3,title:"2021/08/09",slug:"_2021-08-09",normalizedTitle:"2021/08/09",charIndex:2225}],headersStr:"2021/08/08 2021/08/09",content:"# 2021/08/08\n\n# 1、1137. 第 N 个泰波那契数（自底向上）\n\n波那契序列 Tn 定义如下： \nT0 = 0, T1 = 1, T2 = 1, 且在 n >= 0 的条件下 Tn+3 = Tn + Tn+1 + Tn+2\n给你整数 n，请返回第 n 个泰波那契数 Tn 的值。\n\n\n1\n2\n3\n\n\n# 递归做法（超时）\nclass Solution:\n    def tribonacci(self, n: int) -> int:\n        if n == 0:\n            return 0\n        elif n == 1:\n            return 1\n        elif n == 2:\n            return 1\n        else:\n            return self.tribonacci(n-3) + self.tribonacci(n-2) + self.tribonacci(n-1)\n        \n# 自底向上\nclass Solution:\n    def tribonacci(self, n: int) -> int:\n        a, b, c = 0, 1, 1\n        for i in range(n):\n            a, b, c = b, c, a+b+c\n        return a\n# 打表\nclass Solution:\n    def tribonacci(self, n: int) -> int:\n        Ts = [0, 1, 1, 2, 4, 7, 13, 24, 44, 81, 149, 274, 504, 927, 1705, 3136, 5768, 10609, 19513, 35890, 66012, 121415, 223317, 410744, 755476, 1389537, 2555757, 4700770, 8646064, 15902591, 29249425, 53798080, 98950096, 181997601, 334745777, 615693474, 1132436852, 2082876103]\n        return Ts[n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n# 2、剑指 Offer 09. 用两个栈实现队列\n\n定义栈的数据结构，请在该类型中实现一个能够得到栈的最小元素的 min 函数在该栈中，调用 min、push 及 pop 的时间复杂度都是 O(1)。\n\n\n1\n\n\nclass MinStack:\n    def __init__(self):\n        self.A, self.B = [], []\n    def push(self, x: int) -> None:\n        self.A.append(x)\n        if not self.B or self.B[-1] >= x:\n            self.B.append(x)\n    def pop(self) -> None:\n        if self.A.pop() == self.B[-1]:\n            self.B.pop()\n    def top(self) -> int:\n        return self.A[-1]\n    def min(self) -> int:\n        return self.B[-1]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 3、剑指 Offer 30. 包含min函数的栈\n\n用两个栈实现一个队列。队列的声明如下，请实现它的两个函数 appendTail 和 deleteHead ，分别完成在队列尾部插入整数和在队列头部删除整数的功能。(若队列中没有元素，deleteHead 操作返回 -1 )\n\n\n1\n\n\nclass CQueue:\n\n    def __init__(self):\n        self.stack_A = []\n        self.stack_B = []\n    def appendTail(self, value: int) -> None:\n        self.stack_A.append(value)\n    def deleteHead(self) -> int:\n        if self.stack_B:\n            return self.stack_B.pop()\n        if not self.stack_A:\n            return -1;\n        while(self.stack_A):\n            self.stack_B.append(self.stack_A.pop())\n        return self.stack_B.pop()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 2021/08/09\n\n# 1、剑指 Offer 06. 从尾到头打印链表\n\n输入一个链表的头节点，从尾到头反过来返回每个节点的值（用数组返回）。\n\n\n1\n\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = None\nclass Solution:\n    def reversePrint(self, head: ListNode) -> List[int]:\n        stack = []\n        while(head != None):\n            stack.append(head.val)\n            head = head.next\n        return stack[::-1]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 2、剑指 Offer 24. 反转链表\n\n定义一个函数，输入一个链表的头节点，反转该链表并输出反转后链表的头节点。\n\n\n1\n\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = None\n\nclass Solution:\n    def reverseList(self, head: ListNode) -> ListNode:\n        point1 = None\n        point2 = head\n        while point2:\n            cur = point2\n            point2 = point2.next\n            cur.next = point1\n            point1 = cur\n        return point1\n            \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 3、剑指 Offer 35. 复杂链表的复制\n\n请实现 copyRandomList 函数，复制一个复杂链表。在复杂链表中，每个节点除了有一个 next 指针指向下一个节点，还有一个 random 指针指向链表中的任意节点或者 null。\n\n\n1\n\n\n\n\n\n1\n\n\n# 4、313. 超级丑数（动态规划）\n\n超级丑数 是一个正整数，并满足其所有质因数都出现在质数数组 primes 中。\n给你一个整数 n 和一个整数数组 primes ，返回第 n 个 超级丑数 。\n题目数据保证第 n 个 超级丑数 在 32-bit 带符号整数范围内。\n\n\n1\n2\n3\n\n\nclass Solution:\n    def nthSuperUglyNumber(self, n: int, primes: List[int]) -> int:\n        dp = [0] * (n+1)\n        dp[1] = 1\n        len_primes = len(primes)\n\n        # pointers记录质数应该与哪个丑数做乘积\n        pointers = [1] * len_primes \n        \n        for i in range(2, n+1):\n            list1 = []\n            for j in range(len_primes):\n                list1.append(dp[pointers[j]] * primes[j])\n            \n            min_num = min(list1)\n            dp[i] = min_num\n            for j in range(len_primes):\n                if dp[pointers[j]] * primes[j] == min_num:\n                    pointers[j] += 1\n        return dp[n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 参考资料\n\n * 常见面试算法题突击手册：https://github.com/yifeikong/interview",normalizedContent:"# 2021/08/08\n\n# 1、1137. 第 n 个泰波那契数（自底向上）\n\n波那契序列 tn 定义如下： \nt0 = 0, t1 = 1, t2 = 1, 且在 n >= 0 的条件下 tn+3 = tn + tn+1 + tn+2\n给你整数 n，请返回第 n 个泰波那契数 tn 的值。\n\n\n1\n2\n3\n\n\n# 递归做法（超时）\nclass solution:\n    def tribonacci(self, n: int) -> int:\n        if n == 0:\n            return 0\n        elif n == 1:\n            return 1\n        elif n == 2:\n            return 1\n        else:\n            return self.tribonacci(n-3) + self.tribonacci(n-2) + self.tribonacci(n-1)\n        \n# 自底向上\nclass solution:\n    def tribonacci(self, n: int) -> int:\n        a, b, c = 0, 1, 1\n        for i in range(n):\n            a, b, c = b, c, a+b+c\n        return a\n# 打表\nclass solution:\n    def tribonacci(self, n: int) -> int:\n        ts = [0, 1, 1, 2, 4, 7, 13, 24, 44, 81, 149, 274, 504, 927, 1705, 3136, 5768, 10609, 19513, 35890, 66012, 121415, 223317, 410744, 755476, 1389537, 2555757, 4700770, 8646064, 15902591, 29249425, 53798080, 98950096, 181997601, 334745777, 615693474, 1132436852, 2082876103]\n        return ts[n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n# 2、剑指 offer 09. 用两个栈实现队列\n\n定义栈的数据结构，请在该类型中实现一个能够得到栈的最小元素的 min 函数在该栈中，调用 min、push 及 pop 的时间复杂度都是 o(1)。\n\n\n1\n\n\nclass minstack:\n    def __init__(self):\n        self.a, self.b = [], []\n    def push(self, x: int) -> none:\n        self.a.append(x)\n        if not self.b or self.b[-1] >= x:\n            self.b.append(x)\n    def pop(self) -> none:\n        if self.a.pop() == self.b[-1]:\n            self.b.pop()\n    def top(self) -> int:\n        return self.a[-1]\n    def min(self) -> int:\n        return self.b[-1]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n# 3、剑指 offer 30. 包含min函数的栈\n\n用两个栈实现一个队列。队列的声明如下，请实现它的两个函数 appendtail 和 deletehead ，分别完成在队列尾部插入整数和在队列头部删除整数的功能。(若队列中没有元素，deletehead 操作返回 -1 )\n\n\n1\n\n\nclass cqueue:\n\n    def __init__(self):\n        self.stack_a = []\n        self.stack_b = []\n    def appendtail(self, value: int) -> none:\n        self.stack_a.append(value)\n    def deletehead(self) -> int:\n        if self.stack_b:\n            return self.stack_b.pop()\n        if not self.stack_a:\n            return -1;\n        while(self.stack_a):\n            self.stack_b.append(self.stack_a.pop())\n        return self.stack_b.pop()\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n# 2021/08/09\n\n# 1、剑指 offer 06. 从尾到头打印链表\n\n输入一个链表的头节点，从尾到头反过来返回每个节点的值（用数组返回）。\n\n\n1\n\n\n# definition for singly-linked list.\n# class listnode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = none\nclass solution:\n    def reverseprint(self, head: listnode) -> list[int]:\n        stack = []\n        while(head != none):\n            stack.append(head.val)\n            head = head.next\n        return stack[::-1]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n# 2、剑指 offer 24. 反转链表\n\n定义一个函数，输入一个链表的头节点，反转该链表并输出反转后链表的头节点。\n\n\n1\n\n\n# definition for singly-linked list.\n# class listnode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = none\n\nclass solution:\n    def reverselist(self, head: listnode) -> listnode:\n        point1 = none\n        point2 = head\n        while point2:\n            cur = point2\n            point2 = point2.next\n            cur.next = point1\n            point1 = cur\n        return point1\n            \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# 3、剑指 offer 35. 复杂链表的复制\n\n请实现 copyrandomlist 函数，复制一个复杂链表。在复杂链表中，每个节点除了有一个 next 指针指向下一个节点，还有一个 random 指针指向链表中的任意节点或者 null。\n\n\n1\n\n\n\n\n\n1\n\n\n# 4、313. 超级丑数（动态规划）\n\n超级丑数 是一个正整数，并满足其所有质因数都出现在质数数组 primes 中。\n给你一个整数 n 和一个整数数组 primes ，返回第 n 个 超级丑数 。\n题目数据保证第 n 个 超级丑数 在 32-bit 带符号整数范围内。\n\n\n1\n2\n3\n\n\nclass solution:\n    def nthsuperuglynumber(self, n: int, primes: list[int]) -> int:\n        dp = [0] * (n+1)\n        dp[1] = 1\n        len_primes = len(primes)\n\n        # pointers记录质数应该与哪个丑数做乘积\n        pointers = [1] * len_primes \n        \n        for i in range(2, n+1):\n            list1 = []\n            for j in range(len_primes):\n                list1.append(dp[pointers[j]] * primes[j])\n            \n            min_num = min(list1)\n            dp[i] = min_num\n            for j in range(len_primes):\n                if dp[pointers[j]] * primes[j] == min_num:\n                    pointers[j] += 1\n        return dp[n]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n# 参考资料\n\n * 常见面试算法题突击手册：https://github.com/yifeikong/interview",charsets:{cjk:!0},lastUpdated:"2021/08/17, 18:07:06"},{title:"资料搜集",frontmatter:{title:"资料搜集",date:"2021-05-07T16:46:58.000Z",permalink:"/pages/32cc8f/",categories:["计算机视觉","资料搜集"],tags:[null]},regularPath:"/05.%E8%B5%84%E6%BA%90%E6%94%B6%E8%97%8F/02.%E8%B5%84%E6%96%99%E6%90%9C%E9%9B%86/00.%E8%B5%84%E6%96%99%E6%90%9C%E9%9B%86.html",relativePath:"05.资源收藏/02.资料搜集/00.资料搜集.md",key:"v-04b09795",path:"/pages/32cc8f/",headers:[{level:3,title:"代码方向可以借鉴的仓库",slug:"代码方向可以借鉴的仓库",normalizedTitle:"代码方向可以借鉴的仓库",charIndex:2},{level:3,title:"图像标注",slug:"图像标注",normalizedTitle:"图像标注",charIndex:1143},{level:3,title:"向量检索",slug:"向量检索",normalizedTitle:"向量检索",charIndex:1420},{level:3,title:"自监督学习",slug:"自监督学习",normalizedTitle:"自监督学习",charIndex:1569},{level:3,title:"图像处理的库",slug:"图像处理的库",normalizedTitle:"图像处理的库",charIndex:1677},{level:3,title:"图像分类",slug:"图像分类",normalizedTitle:"图像分类",charIndex:1777},{level:3,title:"图像分割",slug:"图像分割",normalizedTitle:"图像分割",charIndex:3049},{level:3,title:"目标检测",slug:"目标检测",normalizedTitle:"目标检测",charIndex:4513},{level:3,title:"图像检索（Image Retrieval）",slug:"图像检索-image-retrieval",normalizedTitle:"图像检索（image retrieval）",charIndex:4932},{level:3,title:"度量学习（Metric Learning）",slug:"度量学习-metric-learning",normalizedTitle:"度量学习（metric learning）",charIndex:5056},{level:3,title:"图像增强（Image Augmentation）",slug:"图像增强-image-augmentation",normalizedTitle:"图像增强（image augmentation）",charIndex:5412},{level:3,title:"单目标跟踪（Single Object Tracking）",slug:"单目标跟踪-single-object-tracking",normalizedTitle:"单目标跟踪（single object tracking）",charIndex:5790},{level:3,title:"多目标跟踪（Multi Object Tracking）",slug:"多目标跟踪-multi-object-tracking",normalizedTitle:"多目标跟踪（multi object tracking）",charIndex:5958},{level:3,title:"人脸识别",slug:"人脸识别",normalizedTitle:"人脸识别",charIndex:6095},{level:3,title:"人像抠图",slug:"人像抠图",normalizedTitle:"人像抠图",charIndex:6370},{level:3,title:"行人计数",slug:"行人计数",normalizedTitle:"行人计数",charIndex:6540},{level:3,title:"AI部署",slug:"ai部署",normalizedTitle:"ai部署",charIndex:6618},{level:3,title:"可解释性",slug:"可解释性",normalizedTitle:"可解释性",charIndex:6811},{level:3,title:"刷算法题",slug:"刷算法题",normalizedTitle:"刷算法题",charIndex:6858},{level:3,title:"模板匹配",slug:"模板匹配",normalizedTitle:"模板匹配",charIndex:6914},{level:3,title:"utils",slug:"utils",normalizedTitle:"utils",charIndex:313},{level:3,title:"官方仓库",slug:"官方仓库",normalizedTitle:"官方仓库",charIndex:8052},{level:3,title:"可以参考的文档",slug:"可以参考的文档",normalizedTitle:"可以参考的文档",charIndex:8200},{level:3,title:"竞赛代码库",slug:"竞赛代码库",normalizedTitle:"竞赛代码库",charIndex:9508},{level:3,title:"音频处理的库",slug:"音频处理的库",normalizedTitle:"音频处理的库",charIndex:9625},{level:3,title:"可视化的库",slug:"可视化的库",normalizedTitle:"可视化的库",charIndex:9679},{level:3,title:"做简历",slug:"做简历",normalizedTitle:"做简历",charIndex:9802},{level:3,title:"公众号文章排版",slug:"公众号文章排版",normalizedTitle:"公众号文章排版",charIndex:9889},{level:3,title:"知识蒸馏",slug:"知识蒸馏",normalizedTitle:"知识蒸馏",charIndex:9946},{level:3,title:"值得阅读的文章",slug:"值得阅读的文章",normalizedTitle:"值得阅读的文章",charIndex:10108},{level:3,title:"值得观看的分享视频",slug:"值得观看的分享视频",normalizedTitle:"值得观看的分享视频",charIndex:10304}],headersStr:"代码方向可以借鉴的仓库 图像标注 向量检索 自监督学习 图像处理的库 图像分类 图像分割 目标检测 图像检索（Image Retrieval） 度量学习（Metric Learning） 图像增强（Image Augmentation） 单目标跟踪（Single Object Tracking） 多目标跟踪（Multi Object Tracking） 人脸识别 人像抠图 行人计数 AI部署 可解释性 刷算法题 模板匹配 utils 官方仓库 可以参考的文档 竞赛代码库 音频处理的库 可视化的库 做简历 公众号文章排版 知识蒸馏 值得阅读的文章 值得观看的分享视频",content:"# 代码方向可以借鉴的仓库\n\n * https://github.com/bobo0810/PytorchNetHub\n * https://github.com/Media-Smart/volkscv（带分析和指标）\n * https://mp.weixin.qq.com/s/dXc5fYftceJ8r7MwyAOGOw（CVChain牛逼牛逼）\n * https://github.com/sailist/thexp（自己写的实验框架，可以参考）\n * https://github.com/misads/cv_template（一个自己写的实验框架）\n * https://github.com/eladhoffer/utils.pytorch（一些基于Pytorch的有用的代码片段）\n * https://github.com/CoinCheung/pytorch-loss（各类损失）\n * https://github.com/Megvii-BaseDetection/cvpods（旷视的code base）\n * https://github.com/donnyyou/torchcv\n * https://github.com/PyTorchLightning/metrics（各类metrics）\n * https://bbs.huaweicloud.com/blogs/267402?utm_source=51cto&utm_medium=bbs-ex&utm_campaign=ei&utm_content=content（分类任务评价指标）\n * https://github.com/zhanghang1989/PyTorch-Encoding\n * https://github.com/lxztju/pytorch_classification\n * https://github.com/xuyxu/Ensemble-Pytorch（PyTorch Ensemble的库）\n * https://github.com/xiaoyufenfei/Efficient-Segmentation-Networks\n * https://github.com/CVHuber/Convolution\n * https://github.com/ming71/toolbox\n * https://github.com/BloodAxe/pytorch-toolbelt（一个非常好的竞赛代码template）\n * https://github.com/Alibaba-MIIL/ImageNet21K（ImageNet 21K 的预训练模型库）\n\n\n# 图像标注\n\n * https://github.com/lzx1413/labelImgPlus\n * https://github.com/lanbing510/ImageLabel\n * https://github.com/wkentaro/labelme\n * https://github.com/hzylmf/od-annotation\n * https://github.com/rachelcao277/LabelImage\n * https://github.com/abreheret/PixelAnnotationTool\n\n\n# 向量检索\n\n * https://github.com/facebookresearch/faiss\n * https://github.com/milvus-io/milvus/\n * https://www.qbitai.com/2021/03/22165.html（Proxima）\n\n\n# 自监督学习\n\n * https://github.com/KeremTurgutlu/self_supervised\n * https://github.com/open-mmlab/OpenSelfSup\n\n\n# 图像处理的库\n\n * https://github.com/libvips/libvips（速度很快并且不占内存）\n * https://github.com/open-mmlab/mmcv\n\n\n# 图像分类\n\n * https://github.com/weiaicunzai/Bag_of_Tricks_for_Image_Classification_with_Convolutional_Neural_Networks\n\n * https://github.com/weiaicunzai/awesome-image-classification\n\n * https://github.com/rwightman/pytorch-image-models/（牛逼）\n\n * https://github.com/PistonY/torch-toolbox\n\n * https://github.com/spytensor/pytorch_img_classification_for_competition\n\n * https://github.com/Cadene/pretrained-models.pytorch（牛逼）\n\n * https://github.com/weiaicunzai/awesome-image-classification\n\n * https://github.com/PaddlePaddle/PaddleClas#Model_zoo_overview\n\n * https://github.com/facebookresearch/deit\n\n * https://github.com/DingXiaoH/RepVGG\n\n * https://github.com/XingangPan/IBN-Net\n\n * https://github.com/yifan123/IC-Conv\n\n * https://github.com/d-li14/involution （RedNet）\n\n * https://github.com/huawei-noah/CV-Backbones\n\n * https://github.com/facebookresearch/pycls\n\n * Attention机制\n   \n   * https://github.com/Andrew-Qibin/CoordAttention\n\n * https://github.com/CyberZHG/torch-multi-head-attention\n\n * Transformer\n   \n   * https://github.com/microsoft/Swin-Transformer\n   * https://github.com/zihangJiang/TokenLabeling\n   * https://github.com/JDAI-CV/CoTNet\n\n * 轻量化模型\n   \n   * https://github.com/HRNet/Lite-HRNet\n\n * 细粒度识别\n   \n   * https://github.com/FouriYe/CNL-ICIP2020\n * \n\n\n# 图像分割\n\n * https://github.com/JunMa11/SegLoss\n * https://github.com/linbo0518/BLSeg\n * https://github.com/qubvel/segmentation_models.pytorch\n * https://github.com/donnyyou/torchcv\n * https://github.com/open-mmlab/mmsegmentation\n * https://github.com/NVIDIA/semantic-segmentation\n * https://github.com/Media-Smart/vedaseg\n * https://github.com/cvlab-yonsei/SFNet\n * https://github.com/Tramac/awesome-semantic-segmentation-pytorch\n * https://github.com/Gsunshine/Enjoy-Hamburger\n * https://github.com/LikeLy-Journey/SegmenTron\n * https://github.com/CSAILVision/semantic-segmentation-pytorch（MIT ADE20K dataset）\n * https://github.com/sacmehta/ESPNet\n * https://github.com/xiaoyufenfei/Efficient-Segmentation-Networks\n * matting 方向\n   * https://github.com/PeterL1n/BackgroundMattingV2\n * 实例分割方向\n   * https://github.com/Epiphqny/VisTR\n   * https://github.com/conradry/max-deeplab\n * 全景分割\n   * https://github.com/chensnathan/SpatialFlow\n   * https://github.com/w-hc/pcv\n * 自动驾驶\n   * https://github.com/voldemortX/pytorch-auto-drive/blob/master/docs/MODEL_ZOO.md（包括semantic segmentation和lane detection）\n * 视频分割\n   * https://hkchengrex.github.io/MiVOS/\n * 无监督语义分割\n   * https://github.com/wvangansbeke/Unsupervised-Semantic-Segmentation\n * 表面缺陷检测\n   * https://github.com/mengcius/Surface-Defect-Detection\n * 半监督语义分割\n   * https://github.com/WilhelmT/ClassMix\n   * https://github.com/charlesCXK/TorchSemiSeg\n   * https://github.com/dvlab-research/Context-Aware-Consistency\n\n\n# 目标检测\n\n * https://github.com/hoya012/deep_learning_object_detection\n * \n * https://github.com/hukaixuan19970627/YOLOv5_DOTA_OBB\n * https://github.com/rafaelpadilla/Object-Detection-Metrics\n * 实时目标检测\n   * https://github.com/Robert-JunWang/PeleeNet\n   * https://github.com/RangiLyu/nanodet\n     * https://blog.csdn.net/walletiger/article/details/110733604\n * 遥感图像检测\n   * https://github.com/SJTU-Thinklab-Det/DOTA-DOAI\n\n\n# 图像检索（Image Retrieval）\n\n * https://github.com/gniknoil/Perfect500K-Beauty-and-Personal-Care-Products-Retrieval-Challenge\n\n\n# 度量学习（Metric Learning）\n\n * https://github.com/KevinMusgrave/pytorch-metric-learning\n * https://github.com/PyTorchLightning/metrics\n * https://github.com/enochkan/torch-metrics\n * https://github.com/bnu-wangxun/Deep_Metric\n * https://www.aiuai.cn/aifarm1707.html\n * https://www.aiuai.cn/aifarm1697.html\n * 度量学习的攻击方式\n   * https://github.com/layumi/U_turn\n\n\n# 图像增强（Image Augmentation）\n\n * https://github.com/albumentations-team/albumentations\n * https://github.com/kornia/kornia\n * https://github.com/YU-Zhiyang/opencv_transforms_torchvision\n * https://github.com/aleju/imgaug\n * https://github.com/qq995431104/Copy-Paste-for-Semantic-Segmentation\n * https://github.com/snu-mllab/Co-Mixup\n * https://github.com/clovaai/CutMix-PyTorch\n\n\n# 单目标跟踪（Single Object Tracking）\n\n * https://github.com/STVIR/pysot\n * https://github.com/visionml/pytracking\n * https://github.com/zengwb-lx/Yolov5-Deepsort-Fastreid\n\n\n# 多目标跟踪（Multi Object Tracking）\n\n * https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch\n * https://github.com/PeizeSun/TransTrack\n\n\n# 人脸识别\n\n * https://github.com/cavalleria/cavaface.pytorch\n * https://github.com/ZhaoJ9014/face.evoLVe.PyTorch\n * https://github.com/JDAI-CV/FaceX-Zoo\n * https://github.com/Tencent/TFace\n * https://github.com/deepinsight/insightface\n * https://github.com/IrvingMeng/MagFace\n\n\n# 人像抠图\n\n * https://github.com/lizhengwei1992/Semantic_Human_Matting\n * https://github.com/Shirhe-Lyh/deep_image_matting_pytorch\n * https://github.com/xuebinqin/U-2-Net\n\n\n# 行人计数\n\n * https://github.com/zengwb-lx/yolov5-deepsort-pedestrian-counting\n\n\n# AI部署\n\n * https://github.com/Tencent/ncnn\n * https://github.com/alibaba/MNN\n * https://github.com/openvinotoolkit/openvino\n * 模型剪枝\n   * https://github.com/Sharpiless/Pytorch-Auto-Slim-Tools\n\n\n# 可解释性\n\n * https://github.com/pytorch/captum\n\n\n# 刷算法题\n\n * https://github.com/chefyuan/algorithm-base\n\n\n# 模板匹配\n\n * https://github.com/multi-template-matching/MultiTemplateMatching-Python\n\n\n# utils\n\n * pytorch sort\n   \n   * https://github.com/teddykoker/torchsort\n\n * 优化器\n   \n   * https://github.com/jettify/pytorch-optimizer\n   * https://github.com/lessw2020/Best-Deep-Learning-Optimizers\n\n * AutoML\n   \n   * https://github.com/microsoft/nni\n\n * 配置框架\n   \n   * https://github.com/facebookresearch/hydra\n\n * 实验记录\n   \n   * https://github.com/NVIDIA/runx\n   * https://github.com/tqdm/tqdm\n   * https://github.com/tensorflow/tensorboard\n   * https://github.com/sailist/thexp\n\n * 训练加速\n   \n   * https://github.com/NVIDIA/DALI\n\n * Summary\n   \n   * https://github.com/sksq96/pytorch-summary\n\n * 各类Trick\n   \n   * https://github.com/davidtvs/pytorch-lr-finder\n   * https://github.com/lartpang/PyTorchTricks\n   * https://github.com/adobe/antialiased-cnns/（抗锯齿化）\n\n * 可视化\n   \n   * https://github.com/jacobgil/pytorch-grad-cam\n   * https://github.com/yiskw713/SmoothGradCAMplusplus\n   * https://github.com/utkuozbulak/pytorch-cnn-visualizations\n   * https://github.com/yizt/Grad-CAM.pytorch\n\n * 统计参数量\n   \n   * https://github.com/Lyken17/pytorch-OpCounter\n\n\n# 官方仓库\n\n * https://github.com/HRNet\n\n * https://github.com/Res2Net/\n\n * https://github.com/pytorch/fairseq/tree/master/fairseq/optim/lr_scheduler\n\n\n# 可以参考的文档\n\n图像分类竞赛入门文档\n\n * https://www.kaggle.com/abhinand05/vision-transformer-vit-tutorial-baseline\n\n分布式训练\n\n * https://tramac.github.io/2019/03/06/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83-PyTorch/\n * https://www.zybuluo.com/huanghaian/note/1711283\n * https://github.com/tczhangzhi/pytorch-distributed\n * https://github.com/horovod/horovod\n * https://github.com/horovod/horovod/blob/master/docs/pytorch.rst\n * https://zhuanlan.zhihu.com/p/294698838\n * https://zhuanlan.zhihu.com/p/276122469\n * https://github.com/jia-zhuang/pytorch-multi-gpu-training\n * https://github.com/rentainhe/pytorch-distributed-training\n * https://github.com/huggingface/accelerate\n\n新手入门的文档\n\n * https://bbs.cvmart.net/topics/134\n\n项目配置框架\n\n * https://zhuanlan.zhihu.com/p/106103204\n\n训练加速\n\n * 使用pytorch时，训练集数据太多达到上千万张，Dataloader加载很慢怎么办? - 人民艺术家的回答 - 知乎\n * https://github.com/lartpang/PyTorchTricks\n * https://zhuanlan.zhihu.com/p/30383580\n * https://flashgene.com/archives/150604.html\n * https://github.com/lartpang/PyTorchTricks\n * https://www.yuque.com/lart\n * https://github.com/Lyken17/Efficient-PyTorch#data-loader\n * https://bbs.cvmart.net/topics/1977\n * pytorch dataloader数据加载占用了大部分时间，各位大佬都是怎么解决的？ - Yuwei的回答 - 知乎\n * https://zhuanlan.zhihu.com/p/80299784\n * https://zhuanlan.zhihu.com/p/77633542\n * 当代研究生应当掌握的并行训练方法（单机多卡）\n\n\n# 竞赛代码库\n\n * https://github.com/cizhenshi/TianchiGuangdong2019_2th\n * https://github.com/ShuaiBai623/AIC2021-T5-CLV\n\n\n# 音频处理的库\n\n * https://github.com/KinWaiCheuk/nnAudio\n\n\n# 可视化的库\n\n * https://github.com/DesertsX/dataviz-collections\n * https://github.com/utkuozbulak/pytorch-cnn-visualizations\n\n\n# 做简历\n\n * https://sspai.com/post/66053\n\n * https://github.com/CyC2018/Markdown-Resum\n\n\n# 公众号文章排版\n\n * https://github.com/lyricat/wechat-format\n\n\n# 知识蒸馏\n\n * https://github.com/zhouzaida/channel-distillation\n * https://github.com/Sharpiless/yolov5-knowledge-distillation\n * https://arxiv.org/abs/1805.06361\n\n\n# 值得阅读的文章\n\n * CNN：我不是你想的那样\n\n * Fast benchmark: Pillow vs OpenCV\n\n * CV 中的特征金字塔\n\n * CV 中的注意力机制\n\n * 尽览卷积神经网络\n\n * 频域中的 CNN\n\n * 手工调参 Tricks\n\n * 综述 | 一文看尽神经网络中不同种类的卷积层\n\n * CNN网络的设计论：NAS vs Handcraft\n\n\n# 值得观看的分享视频\n\n * 【极市】马智恒-深度学习在工业检测中的应用\n\n * 【极市】Matt&黄伟林 WebVision冠军技术分享 (课程学习的论文，看完了)\n\n * https://github.com/Flowingsun007/DeepLearningTutorial",normalizedContent:"# 代码方向可以借鉴的仓库\n\n * https://github.com/bobo0810/pytorchnethub\n * https://github.com/media-smart/volkscv（带分析和指标）\n * https://mp.weixin.qq.com/s/dxc5fyftcej8r7mwyaogow（cvchain牛逼牛逼）\n * https://github.com/sailist/thexp（自己写的实验框架，可以参考）\n * https://github.com/misads/cv_template（一个自己写的实验框架）\n * https://github.com/eladhoffer/utils.pytorch（一些基于pytorch的有用的代码片段）\n * https://github.com/coincheung/pytorch-loss（各类损失）\n * https://github.com/megvii-basedetection/cvpods（旷视的code base）\n * https://github.com/donnyyou/torchcv\n * https://github.com/pytorchlightning/metrics（各类metrics）\n * https://bbs.huaweicloud.com/blogs/267402?utm_source=51cto&utm_medium=bbs-ex&utm_campaign=ei&utm_content=content（分类任务评价指标）\n * https://github.com/zhanghang1989/pytorch-encoding\n * https://github.com/lxztju/pytorch_classification\n * https://github.com/xuyxu/ensemble-pytorch（pytorch ensemble的库）\n * https://github.com/xiaoyufenfei/efficient-segmentation-networks\n * https://github.com/cvhuber/convolution\n * https://github.com/ming71/toolbox\n * https://github.com/bloodaxe/pytorch-toolbelt（一个非常好的竞赛代码template）\n * https://github.com/alibaba-miil/imagenet21k（imagenet 21k 的预训练模型库）\n\n\n# 图像标注\n\n * https://github.com/lzx1413/labelimgplus\n * https://github.com/lanbing510/imagelabel\n * https://github.com/wkentaro/labelme\n * https://github.com/hzylmf/od-annotation\n * https://github.com/rachelcao277/labelimage\n * https://github.com/abreheret/pixelannotationtool\n\n\n# 向量检索\n\n * https://github.com/facebookresearch/faiss\n * https://github.com/milvus-io/milvus/\n * https://www.qbitai.com/2021/03/22165.html（proxima）\n\n\n# 自监督学习\n\n * https://github.com/keremturgutlu/self_supervised\n * https://github.com/open-mmlab/openselfsup\n\n\n# 图像处理的库\n\n * https://github.com/libvips/libvips（速度很快并且不占内存）\n * https://github.com/open-mmlab/mmcv\n\n\n# 图像分类\n\n * https://github.com/weiaicunzai/bag_of_tricks_for_image_classification_with_convolutional_neural_networks\n\n * https://github.com/weiaicunzai/awesome-image-classification\n\n * https://github.com/rwightman/pytorch-image-models/（牛逼）\n\n * https://github.com/pistony/torch-toolbox\n\n * https://github.com/spytensor/pytorch_img_classification_for_competition\n\n * https://github.com/cadene/pretrained-models.pytorch（牛逼）\n\n * https://github.com/weiaicunzai/awesome-image-classification\n\n * https://github.com/paddlepaddle/paddleclas#model_zoo_overview\n\n * https://github.com/facebookresearch/deit\n\n * https://github.com/dingxiaoh/repvgg\n\n * https://github.com/xingangpan/ibn-net\n\n * https://github.com/yifan123/ic-conv\n\n * https://github.com/d-li14/involution （rednet）\n\n * https://github.com/huawei-noah/cv-backbones\n\n * https://github.com/facebookresearch/pycls\n\n * attention机制\n   \n   * https://github.com/andrew-qibin/coordattention\n\n * https://github.com/cyberzhg/torch-multi-head-attention\n\n * transformer\n   \n   * https://github.com/microsoft/swin-transformer\n   * https://github.com/zihangjiang/tokenlabeling\n   * https://github.com/jdai-cv/cotnet\n\n * 轻量化模型\n   \n   * https://github.com/hrnet/lite-hrnet\n\n * 细粒度识别\n   \n   * https://github.com/fouriye/cnl-icip2020\n * \n\n\n# 图像分割\n\n * https://github.com/junma11/segloss\n * https://github.com/linbo0518/blseg\n * https://github.com/qubvel/segmentation_models.pytorch\n * https://github.com/donnyyou/torchcv\n * https://github.com/open-mmlab/mmsegmentation\n * https://github.com/nvidia/semantic-segmentation\n * https://github.com/media-smart/vedaseg\n * https://github.com/cvlab-yonsei/sfnet\n * https://github.com/tramac/awesome-semantic-segmentation-pytorch\n * https://github.com/gsunshine/enjoy-hamburger\n * https://github.com/likely-journey/segmentron\n * https://github.com/csailvision/semantic-segmentation-pytorch（mit ade20k dataset）\n * https://github.com/sacmehta/espnet\n * https://github.com/xiaoyufenfei/efficient-segmentation-networks\n * matting 方向\n   * https://github.com/peterl1n/backgroundmattingv2\n * 实例分割方向\n   * https://github.com/epiphqny/vistr\n   * https://github.com/conradry/max-deeplab\n * 全景分割\n   * https://github.com/chensnathan/spatialflow\n   * https://github.com/w-hc/pcv\n * 自动驾驶\n   * https://github.com/voldemortx/pytorch-auto-drive/blob/master/docs/model_zoo.md（包括semantic segmentation和lane detection）\n * 视频分割\n   * https://hkchengrex.github.io/mivos/\n * 无监督语义分割\n   * https://github.com/wvangansbeke/unsupervised-semantic-segmentation\n * 表面缺陷检测\n   * https://github.com/mengcius/surface-defect-detection\n * 半监督语义分割\n   * https://github.com/wilhelmt/classmix\n   * https://github.com/charlescxk/torchsemiseg\n   * https://github.com/dvlab-research/context-aware-consistency\n\n\n# 目标检测\n\n * https://github.com/hoya012/deep_learning_object_detection\n * \n * https://github.com/hukaixuan19970627/yolov5_dota_obb\n * https://github.com/rafaelpadilla/object-detection-metrics\n * 实时目标检测\n   * https://github.com/robert-junwang/peleenet\n   * https://github.com/rangilyu/nanodet\n     * https://blog.csdn.net/walletiger/article/details/110733604\n * 遥感图像检测\n   * https://github.com/sjtu-thinklab-det/dota-doai\n\n\n# 图像检索（image retrieval）\n\n * https://github.com/gniknoil/perfect500k-beauty-and-personal-care-products-retrieval-challenge\n\n\n# 度量学习（metric learning）\n\n * https://github.com/kevinmusgrave/pytorch-metric-learning\n * https://github.com/pytorchlightning/metrics\n * https://github.com/enochkan/torch-metrics\n * https://github.com/bnu-wangxun/deep_metric\n * https://www.aiuai.cn/aifarm1707.html\n * https://www.aiuai.cn/aifarm1697.html\n * 度量学习的攻击方式\n   * https://github.com/layumi/u_turn\n\n\n# 图像增强（image augmentation）\n\n * https://github.com/albumentations-team/albumentations\n * https://github.com/kornia/kornia\n * https://github.com/yu-zhiyang/opencv_transforms_torchvision\n * https://github.com/aleju/imgaug\n * https://github.com/qq995431104/copy-paste-for-semantic-segmentation\n * https://github.com/snu-mllab/co-mixup\n * https://github.com/clovaai/cutmix-pytorch\n\n\n# 单目标跟踪（single object tracking）\n\n * https://github.com/stvir/pysot\n * https://github.com/visionml/pytracking\n * https://github.com/zengwb-lx/yolov5-deepsort-fastreid\n\n\n# 多目标跟踪（multi object tracking）\n\n * https://github.com/mikel-brostrom/yolov5_deepsort_pytorch\n * https://github.com/peizesun/transtrack\n\n\n# 人脸识别\n\n * https://github.com/cavalleria/cavaface.pytorch\n * https://github.com/zhaoj9014/face.evolve.pytorch\n * https://github.com/jdai-cv/facex-zoo\n * https://github.com/tencent/tface\n * https://github.com/deepinsight/insightface\n * https://github.com/irvingmeng/magface\n\n\n# 人像抠图\n\n * https://github.com/lizhengwei1992/semantic_human_matting\n * https://github.com/shirhe-lyh/deep_image_matting_pytorch\n * https://github.com/xuebinqin/u-2-net\n\n\n# 行人计数\n\n * https://github.com/zengwb-lx/yolov5-deepsort-pedestrian-counting\n\n\n# ai部署\n\n * https://github.com/tencent/ncnn\n * https://github.com/alibaba/mnn\n * https://github.com/openvinotoolkit/openvino\n * 模型剪枝\n   * https://github.com/sharpiless/pytorch-auto-slim-tools\n\n\n# 可解释性\n\n * https://github.com/pytorch/captum\n\n\n# 刷算法题\n\n * https://github.com/chefyuan/algorithm-base\n\n\n# 模板匹配\n\n * https://github.com/multi-template-matching/multitemplatematching-python\n\n\n# utils\n\n * pytorch sort\n   \n   * https://github.com/teddykoker/torchsort\n\n * 优化器\n   \n   * https://github.com/jettify/pytorch-optimizer\n   * https://github.com/lessw2020/best-deep-learning-optimizers\n\n * automl\n   \n   * https://github.com/microsoft/nni\n\n * 配置框架\n   \n   * https://github.com/facebookresearch/hydra\n\n * 实验记录\n   \n   * https://github.com/nvidia/runx\n   * https://github.com/tqdm/tqdm\n   * https://github.com/tensorflow/tensorboard\n   * https://github.com/sailist/thexp\n\n * 训练加速\n   \n   * https://github.com/nvidia/dali\n\n * summary\n   \n   * https://github.com/sksq96/pytorch-summary\n\n * 各类trick\n   \n   * https://github.com/davidtvs/pytorch-lr-finder\n   * https://github.com/lartpang/pytorchtricks\n   * https://github.com/adobe/antialiased-cnns/（抗锯齿化）\n\n * 可视化\n   \n   * https://github.com/jacobgil/pytorch-grad-cam\n   * https://github.com/yiskw713/smoothgradcamplusplus\n   * https://github.com/utkuozbulak/pytorch-cnn-visualizations\n   * https://github.com/yizt/grad-cam.pytorch\n\n * 统计参数量\n   \n   * https://github.com/lyken17/pytorch-opcounter\n\n\n# 官方仓库\n\n * https://github.com/hrnet\n\n * https://github.com/res2net/\n\n * https://github.com/pytorch/fairseq/tree/master/fairseq/optim/lr_scheduler\n\n\n# 可以参考的文档\n\n图像分类竞赛入门文档\n\n * https://www.kaggle.com/abhinand05/vision-transformer-vit-tutorial-baseline\n\n分布式训练\n\n * https://tramac.github.io/2019/03/06/%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83-pytorch/\n * https://www.zybuluo.com/huanghaian/note/1711283\n * https://github.com/tczhangzhi/pytorch-distributed\n * https://github.com/horovod/horovod\n * https://github.com/horovod/horovod/blob/master/docs/pytorch.rst\n * https://zhuanlan.zhihu.com/p/294698838\n * https://zhuanlan.zhihu.com/p/276122469\n * https://github.com/jia-zhuang/pytorch-multi-gpu-training\n * https://github.com/rentainhe/pytorch-distributed-training\n * https://github.com/huggingface/accelerate\n\n新手入门的文档\n\n * https://bbs.cvmart.net/topics/134\n\n项目配置框架\n\n * https://zhuanlan.zhihu.com/p/106103204\n\n训练加速\n\n * 使用pytorch时，训练集数据太多达到上千万张，dataloader加载很慢怎么办? - 人民艺术家的回答 - 知乎\n * https://github.com/lartpang/pytorchtricks\n * https://zhuanlan.zhihu.com/p/30383580\n * https://flashgene.com/archives/150604.html\n * https://github.com/lartpang/pytorchtricks\n * https://www.yuque.com/lart\n * https://github.com/lyken17/efficient-pytorch#data-loader\n * https://bbs.cvmart.net/topics/1977\n * pytorch dataloader数据加载占用了大部分时间，各位大佬都是怎么解决的？ - yuwei的回答 - 知乎\n * https://zhuanlan.zhihu.com/p/80299784\n * https://zhuanlan.zhihu.com/p/77633542\n * 当代研究生应当掌握的并行训练方法（单机多卡）\n\n\n# 竞赛代码库\n\n * https://github.com/cizhenshi/tianchiguangdong2019_2th\n * https://github.com/shuaibai623/aic2021-t5-clv\n\n\n# 音频处理的库\n\n * https://github.com/kinwaicheuk/nnaudio\n\n\n# 可视化的库\n\n * https://github.com/desertsx/dataviz-collections\n * https://github.com/utkuozbulak/pytorch-cnn-visualizations\n\n\n# 做简历\n\n * https://sspai.com/post/66053\n\n * https://github.com/cyc2018/markdown-resum\n\n\n# 公众号文章排版\n\n * https://github.com/lyricat/wechat-format\n\n\n# 知识蒸馏\n\n * https://github.com/zhouzaida/channel-distillation\n * https://github.com/sharpiless/yolov5-knowledge-distillation\n * https://arxiv.org/abs/1805.06361\n\n\n# 值得阅读的文章\n\n * cnn：我不是你想的那样\n\n * fast benchmark: pillow vs opencv\n\n * cv 中的特征金字塔\n\n * cv 中的注意力机制\n\n * 尽览卷积神经网络\n\n * 频域中的 cnn\n\n * 手工调参 tricks\n\n * 综述 | 一文看尽神经网络中不同种类的卷积层\n\n * cnn网络的设计论：nas vs handcraft\n\n\n# 值得观看的分享视频\n\n * 【极市】马智恒-深度学习在工业检测中的应用\n\n * 【极市】matt&黄伟林 webvision冠军技术分享 (课程学习的论文，看完了)\n\n * https://github.com/flowingsun007/deeplearningtutorial",charsets:{cjk:!0},lastUpdated:"2021/09/26, 00:09:41"},{title:"标签",frontmatter:{tagsPage:!0,title:"标签",permalink:"/tags/",article:!1},regularPath:"/@pages/tagsPage.html",relativePath:"@pages/tagsPage.md",key:"v-3e931a1c",path:"/tags/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/03/08, 15:40:10"},{title:"Home",frontmatter:{home:!0,heroText:"Muyun's wiki",tagline:"日拱一卒，保持好奇",features:[{title:"学术搬砖",details:"论文阅读笔记",link:"/research/",imgUrl:"/img/research.png"},{title:"学习笔记",details:"对于某个系列的课程或者读书笔记",link:"/notes/",imgUrl:"/img/notes.png"},{title:"生活杂谈",details:"生活的一些杂谈",link:"/life/",imgUrl:"/img/life.png"}]},regularPath:"/",relativePath:"index.md",key:"v-171a9f6a",path:"/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/08/02, 21:04:52"},{title:"分类",frontmatter:{categoriesPage:!0,title:"分类",permalink:"/categories/",article:!1},regularPath:"/@pages/categoriesPage.html",relativePath:"@pages/categoriesPage.md",key:"v-78788d52",path:"/categories/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/03/08, 15:40:10"},{title:"友情链接",frontmatter:{title:"友情链接",date:"2019-12-25T14:27:01.000Z",permalink:"/friends",article:!1,sidebar:!1},regularPath:"/07.%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5/01.%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5.html",relativePath:"07.友情链接/01.友情链接.md",key:"v-4d695430",path:"/friends/",headers:[{level:3,title:"友链申请",slug:"友链申请",normalizedTitle:"友链申请",charIndex:2}],headersStr:"友链申请",content:"# 友链申请\n\n与我 联系",normalizedContent:"# 友链申请\n\n与我 联系",charsets:{cjk:!0},lastUpdated:"2021/08/02, 21:04:52"},{title:"归档",frontmatter:{archivesPage:!0,title:"归档",permalink:"/archives/",article:!1},regularPath:"/@pages/archivesPage.html",relativePath:"@pages/archivesPage.md",key:"v-20ca5f1c",path:"/archives/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2021/03/08, 15:40:10"}],themeConfig:{nav:[{text:"首页",link:"/"},{text:"学术搬砖",link:"/research/",items:[]},{text:"学习笔记",link:"/notes/",items:[]},{text:"生活杂谈",link:"/life/",items:[]},{text:"wiki搬运",link:"/wiki/",items:[]},{text:"资源收藏",link:"/resources/",items:[]},{text:"关于",link:"/about/"},{text:"索引",link:"/archives/",items:[{text:"分类",link:"/categories/"},{text:"标签",link:"/tags/"},{text:"归档",link:"/archives/"}]}],sidebarDepth:2,logo:"/img/EB-logo.png",repo:"xugaoyi/vuepress-theme-vdoing",searchMaxSuggestions:10,lastUpdated:"上次更新",docsDir:"docs",editLinks:!1,editLinkText:"编辑",sidebar:{"/00.目录页/":[["01.学术搬砖.md","学术搬砖","/research"],["02.学习笔记.md","学习笔记","/notes"],["03.生活杂谈.md","生活杂谈","/life"],["04.wiki搬运.md","wiki搬运","/wiki"],["05.资源收藏.md","资源收藏","/resources"]],catalogue:{"学术搬砖":"/research","学习笔记":"/notes","生活杂谈":"/life","wiki搬运":"/wiki","资源收藏":"/resources"},"/01.学术搬砖/":[{title:"论文摘抄",collapsable:!0,children:[["01.论文摘抄/00.论文中值得摘抄的句子.md","论文中值得摘抄的句子","/pages/3fe31b/"],["01.论文摘抄/01.论文常用表达.md","论文常用表达","/pages/7e3623/"],["01.论文摘抄/02.LaTeX常用公式.md","LaTeX常用表达","/pages/551fae/"],["01.论文摘抄/03.论文阅读笔记范文.md","论文阅读笔记范文","/pages/7e460e/"],["01.论文摘抄/04.可能会用到的表达.md","可能会用到的表达","/pages/a2c095/"],["01.论文摘抄/05.撰写论文工具.md","撰写论文工具","/pages/268893/"]]},{title:"论文阅读-图像分类",collapsable:!0,children:[["02.论文阅读-图像分类/00.Query2Label A Simple Transformer Way to Multi-Label Classification.md","Query2Label A Simple Transformer Way to Multi-Label Classification","/pages/80034b/"],["02.论文阅读-图像分类/01.Contextual Transformer Networks for Visual Recognition.md","Contextual Transformer Networks for Visual Recognition","/pages/2203ea/"],["02.论文阅读-图像分类/02.General Multi-label Image Classification with Transformers.md","General Multi-label Image Classification with Transformers","/pages/499d0c/"],["02.论文阅读-图像分类/03.RepVGG.md","RepVGG","/pages/a34c56/"]]},{title:"论文阅读-语义分割",collapsable:!0,children:[["03.论文阅读-语义分割/00.(DeepLabv1) Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs.md","(DeepLabv1) Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs","/pages/8d4552/"]]},{title:"论文阅读-知识蒸馏",collapsable:!0,children:[["04.论文阅读-知识蒸馏/00.Awesome-Knowledge-distillation.md","Awesome-Knowledge-distillation","/pages/885a91/"]]},{title:"论文阅读-Transformer",collapsable:!0,children:[["05.论文阅读-Transformer/00.Awesome-Visual-Transformer.md","Transformer系列代码","/pages/b34b2b/"],["05.论文阅读-Transformer/01.An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.md","An Image is Worth 16x16 Words Transformers for Image Recognition at Scale","/pages/a96d59/"],["05.论文阅读-Transformer/02.Do Vision Transformers See Like Convolutional Neural Networks.md","Do Vision Transformers See Like Convolutional Neural Networks","/pages/7cfb60/"]]},{title:"论文阅读-图卷积网络",collapsable:!0,children:[["06.论文阅读-图卷积网络/00.Awesome-Graph-Neural-Network.md","Awesome-Graph-Neural-Network","/pages/2a87f2/"]]},{title:"论文阅读-弱监督图像分割",collapsable:!0,children:[["07.论文阅读-弱监督图像分割/00.Awesome weakly supervised semantic segmentation.md","Awesome weakly supervised semantic segmentation","/pages/6d34a8/"],["07.论文阅读-弱监督图像分割/01.Self-supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation.md","Self-supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation","/pages/abd9df/"],["07.论文阅读-弱监督图像分割/02.Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks.md","Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks","/pages/a03587/"],["07.论文阅读-弱监督图像分割/03.Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation.md","Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation","/pages/7232a7/"],["07.论文阅读-弱监督图像分割/04.Weakly-Supervised Semantic Segmentation via Sub-category Exploration.md","Weakly-Supervised Semantic Segmentation via Sub-category Exploration","/pages/60761b/"],["07.论文阅读-弱监督图像分割/05.Learning Pixel level Semantic Affinity with Image level Supervision for Weakly Supervised Semantic Segmentation.md","AffinityNet Learning Pixel level Semantic Affinity with Image level Supervision for Weakly Supervised Semantic Segmentation","/pages/35ccd6/"],["07.论文阅读-弱监督图像分割/06.Grad-CAM Visual Explanations from Deep Networks via Gradient-based Localization.md","Grad-CAM Visual Explanations from Deep Networks via Gradient-based Localization","/pages/cf4b85/"],["07.论文阅读-弱监督图像分割/07.Grad-CAM++ Improved Visual Explanations for Deep Convolutional Networks.md","Grad-CAM++ Improved Visual Explanations for Deep Convolutional Networks","/pages/fa177d/"],["07.论文阅读-弱监督图像分割/08.Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation.md","Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation","/pages/1e1493/"],["07.论文阅读-弱监督图像分割/09.Embedded Discriminative Attention Mechanism for Weakly Supervised Semantic Segmentation.md","Embedded Discriminative Attention Mechanism for Weakly Supervised Semantic Segmentation","/pages/6947ae/"],["07.论文阅读-弱监督图像分割/10.Group-Wise Semantic Mining for Weakly Supervised Semantic Segmentation.md","Group-Wise Semantic Mining for Weakly Supervised Semantic Segmentation","/pages/dd295d/"],["07.论文阅读-弱监督图像分割/11.Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation.md","Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation","/pages/27c6f8/"],["07.论文阅读-弱监督图像分割/12.NoPeopleAllowed The Three-Step Approach to Weakly Supervised SemanticSegmentation.md","NoPeopleAllowed The Three-Step Approach to Weakly Supervised SemanticSegmentation","/pages/611606/"],["07.论文阅读-弱监督图像分割/13.Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations.md","Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations","/pages/72359e/"],["07.论文阅读-弱监督图像分割/14.Learning Deep Features for Discriminative Localization.md","Learning Deep Features for Discriminative Localization","/pages/632c01/"],["07.论文阅读-弱监督图像分割/15.Convolutional Random Walk Networks for Semantic Image Segmentation.md","Convolutional Random Walk Networks for Semantic Image Segmentation","/pages/25f8e7/"],["07.论文阅读-弱监督图像分割/16.Learning random-walk label propagation for weakly-supervised semantic segmentation.md","Learning random-walk label propagation for weakly-supervised semantic segmentation","/pages/fc1a12/"],["07.论文阅读-弱监督图像分割/17.Puzzle-CAM Improved localization via matching partial and full features.md","Puzzle-CAM Improved localization via matching partial and full features","/pages/25dbf3/"],["07.论文阅读-弱监督图像分割/18.Learning Visual Words for Weakly-Supervised Semantic Segmentation.md","Learning Visual Words for Weakly-Supervised Semantic Segmentation","/pages/62e38a/"],["07.论文阅读-弱监督图像分割/19.区域擦除 | Object Region Mining with Adversarial Erasing A Simple Classification to Semantic Segmentation Approach.md","区域擦除 | Object Region Mining with Adversarial Erasing A Simple Classification to Semantic Segmentation Approach","/pages/9bc70f/"],["07.论文阅读-弱监督图像分割/20.CAM 扩散 | Tell Me Where to Look Guided Attention Inference Network.md","CAM 扩散 | Tell Me Where to Look Guided Attention Inference Network","/pages/c691d0/"],["07.论文阅读-弱监督图像分割/21.Self-Erasing Network for Integral Object Attention.md","Self-Erasing Network for Integral Object Attention","/pages/742623/"],["07.论文阅读-弱监督图像分割/22.Transformer CAM|Transformer Interpretability Beyond Attention Visualization.md","Transformer CAM|Transformer Interpretability Beyond Attention Visualization","/pages/e9bd5f/"],["07.论文阅读-弱监督图像分割/23.GETAM Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation.md","GETAM Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation","/pages/54a25d/"],["07.论文阅读-弱监督图像分割/24.Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation.md","Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation","/pages/fe1b38/"]]},{title:"论文阅读-半监督图像分割",collapsable:!0,children:[["08.论文阅读-半监督图像分割/00.Learning from Pixel-Level Label Noise A NewPerspective for Semi-Supervised SemanticSegmentation.md","Learning from Pixel-Level Label Noise A NewPerspective for Semi-Supervised SemanticSegmentation","/pages/c672e6/"],["08.论文阅读-半监督图像分割/01.Semi-supervised Semantic Segmentation via Strong-weak Dual-branch Network.md","Semi-supervised Semantic Segmentation via Strong-weak Dual-branch Network","/pages/b3d215/"],["08.论文阅读-半监督图像分割/02.Semi-Supervised Learning by exploiting unlabeled data correlations in a dual-branch network.md","Semi-Supervised Learning by exploiting unlabeled data correlations in a dual-branch network","/pages/8109a2/"],["08.论文阅读-半监督图像分割/03.DMT Dynamic Mutual Training for Semi-Supervised Learning.md","DMT Dynamic Mutual Training for Semi-Supervised Learning","/pages/26f3ac/"],["08.论文阅读-半监督图像分割/04.Semi-supervised semantic segmentation needs strong, varied perturbations.md","Semi-supervised semantic segmentation needs strong, varied perturbations","/pages/c40a2c/"],["08.论文阅读-半监督图像分割/05.ClassMix Segmentation-Based Data Augmentation for Semi-Supervised Learning.md","ClassMix Segmentation-Based Data Augmentation for Semi-Supervised Learning","/pages/247800/"],["08.论文阅读-半监督图像分割/06.Social-STGCNN A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction.md","Social-STGCNN A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction","/pages/e8a5ee/"],["08.论文阅读-半监督图像分割/07.Semi-supevised Semantic Segmentation with High- and Low-level Consistency.md","Semi-supevised Semantic Segmentation with High- and Low-level Consistency","/pages/3f8f22/"],["08.论文阅读-半监督图像分割/08.Self-Tuning for Data-Efficient Deep Learning.md","Self-Tuning for Data-Efficient Deep Learning","/pages/a86584/"],["08.论文阅读-半监督图像分割/09.FixMatch Simplifying Semi-Supervised Learning with Consistency and Confidence.md","FixMatch Simplifying Semi-Supervised Learning with Consistency and Confidence","/pages/29ae03/"],["08.论文阅读-半监督图像分割/10.Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation A Baseline Investigation.md","Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation A Baseline Investigation","/pages/fc0825/"],["08.论文阅读-半监督图像分割/11.Mean teachers are better role models Weight-averaged consistency targets improve semi-supervised deep learning results.md","Mean teachers are better role models Weight-averaged consistency targets improve semi-supervised deep learning results","/pages/8246b6/"]]},{title:"论文阅读-带噪学习",collapsable:!0,children:[]},{title:"论文阅读-小样本学习",collapsable:!0,children:[["10.论文阅读-小样本学习/00.SPICE Semantic Pseudo-labeling for Image Clustering.md","SPICE Semantic Pseudo-labeling for Image Clustering","/pages/a5a6bb/"],["10.论文阅读-小样本学习/01.Improving Unsupervised Image Clustering With Robust Learning.md","Improving Unsupervised Image Clustering With Robust Learning","/pages/bdd933/"],["10.论文阅读-小样本学习/02.SCAN Learning to Classify Images without Labels.md","SCAN Learning to Classify Images without Labels","/pages/0f51c6/"],["10.论文阅读-小样本学习/03.Sill-Net Feature Augmentation with Separated Illumination Representation.md","Sill-Net Feature Augmentation with Separated Illumination Representation","/pages/83a0c4/"]]},{title:"论文阅读-自监督学习",collapsable:!0,children:[["11.论文阅读-自监督学习/00.(MoCov1) Momentum Contrast for Unsupervised Visual Representation Learning.md","(MoCov1) Momentum Contrast for Unsupervised Visual Representation Learning","/pages/1e8d33/"],["11.论文阅读-自监督学习/01.(SimCLRv1) A Simple Framework for Contrastive Learning of Visual Representations.md","(SimCLRv1) A Simple Framework for Contrastive Learning of Visual Representations","/pages/464aed/"],["11.论文阅读-自监督学习/02.(SimCLRv2) Big Self-Supervised Models are Strong Semi-Supervised Learners.md","(SimCLRv2) Big Self-Supervised Models are Strong Semi-Supervised Learners","/pages/1fa222/"],["11.论文阅读-自监督学习/03.(InstDis) Unsupervised Feature Learning via Non-Parametric Instance Discrimination.md","(InstDis) Unsupervised Feature Learning via Non-Parametric Instance Discrimination","/pages/bb05d4/"],["11.论文阅读-自监督学习/04.(CPC) Representation Learning with Contrastive Predictive Coding.md","(CPC) Representation Learning with Contrastive Predictive Coding","/pages/2b22cb/"],["11.论文阅读-自监督学习/05.(CMC) Contrastive Multiview Coding, also contains implementations for MoCo and InstDis.md","(CMC) Contrastive Multiview Coding, also contains implementations for MoCo and InstDis","/pages/f6097d/"],["11.论文阅读-自监督学习/06.(HPT) Self-Supervised Pretraining Improves Self-Supervised Pretraining.md","(HPT) Self-Supervised Pretraining Improves Self-Supervised Pretraining","/pages/682176/"],["11.论文阅读-自监督学习/07.(SimSiam) SimSiam Exploring Simple Siamese Representation Learning.md","(SimSiam) SimSiam Exploring Simple Siamese Representation Learning","/pages/f3d018/"],["11.论文阅读-自监督学习/13.自监督系列代码.md","自监督系列代码","/pages/09e28c/"]]},{title:"语义分割中的知识蒸馏",collapsable:!0,children:[["12.语义分割中的知识蒸馏/00.Structured Knowledge Distillation for Semantic Segmentation.md","Structured Knowledge Distillation for Semantic Segmentation","/pages/eb7136/"]]},{title:"学术文章搜集",collapsable:!0,children:[["13.学术文章搜集/00.Awesome-Academic-Articals.md","Awesome-weakly-supervised-semantic-segmentation","/pages/865735/"]]},{title:"论文阅读-其他文章",collapsable:!0,children:[["14.论文阅读-其他文章/01.Decompose to Adapt Domain Disentanglement Faster-RCNN for Cross-domain Object Detection.md","Decompose to Adapt Domain Disentanglement Faster-RCNN for Cross-domain Object Detection","/pages/5f322e/"]]}],"/02.学习笔记/":[{title:"代码实践-目标检测",collapsable:!0,children:[["01.代码实践-目标检测/00.基于深度学习的目标检测技术.md","基于深度学习的目标检测技术","/pages/f542b4/"],["01.代码实践-目标检测/01.mmdetection voc.md","mmdetection voc","/pages/0d3c60/"],["01.代码实践-目标检测/02.mmdetection COCO格式数据集.md","mmdetection COCO格式数据集","/pages/1708c0/"],["01.代码实践-目标检测/03.常用数据集简介.md","常用数据集简介","/pages/0bdb48/"],["01.代码实践-目标检测/04.cowfits竞赛记录.md","cowfits竞赛记录","/pages/c06123/"]]},{title:"代码实践-图像分割",collapsable:!0,children:[["02.代码实践-图像分割/00.基于深度学习的图像分割技术.md","基于深度学习的图像分割技术","/pages/14bcdb/"],["02.代码实践-图像分割/01.领域自适应.md","领域自适应","/pages/7302ec/"],["02.代码实践-图像分割/02.如何计算一个模型的FPS,Params,GFLOPs.md","如何计算一个模型的FPS,Params,GFLOPs","/pages/4e1e41/"],["02.代码实践-图像分割/03.常见数据集的相关知识.md","常见数据集的相关知识","/pages/679017/"],["02.代码实践-图像分割/04.如何加载数据集.md","如何加载数据集","/pages/80ffb0/"],["02.代码实践-图像分割/05.半监督与弱监督图像分割.md","半监督与弱监督图像分割","/pages/b8e080/"],["02.代码实践-图像分割/06.PASCAL VOC 2012 调色板 color map 赋值.md","PASCAL VOC 2012调色板 color map生成源代码分析","/pages/15a0e4/"],["02.代码实践-图像分割/07.语义分割数据集灰度分割图转彩色分割图代码.md","语义分割数据集灰度分割图转彩色分割图代码","/pages/f64db3/"],["02.代码实践-图像分割/08.复现PSA.md","复现PSA","/pages/5e185e/"],["02.代码实践-图像分割/09.转换cityscapes 到对应的类别.md","转换cityscapes 到对应的类别","/pages/6c2aa7/"],["02.代码实践-图像分割/10.上采样函数.md","上采样函数","/pages/ce4f65/"],["02.代码实践-图像分割/11.DeepLab系列代码.md","DeepLab系列代码","/pages/181ce5/"],["02.代码实践-图像分割/12.mIoU的计算.md","mIoU的计算","/pages/f55018/"],["02.代码实践-图像分割/13.Multi-label 分类中如何计算 mAP.md","Multi-label 分类中如何计算 mAP","/pages/a0a28d/"]]},{title:"代码实践-自监督学习",collapsable:!0,children:[["03.代码实践-自监督学习/00.自监督学习的一些文章.md","自监督学习的一些文章","/pages/aad696/"],["03.代码实践-自监督学习/01.名词解释.md","名词解释","/pages/a3f895/"],["03.代码实践-自监督学习/02.组会思路.md","组会思路","/pages/e182f4/"]]},{title:"竞赛笔记-视觉竞赛",collapsable:!0,children:[["04.竞赛笔记-视觉竞赛/00.当我们在谈论图像竞赛EDA时在谈论些什么.md","当我们在谈论图像竞赛EDA时在谈论些什么","/pages/86aedf/"],["04.竞赛笔记-视觉竞赛/01.kaggle-Classify Leaves 竞赛方案学习.md","kaggle-Classify Leaves 竞赛方案学习","/pages/e518f8/"],["04.竞赛笔记-视觉竞赛/02.图像检索orReID 竞赛的 Tricks.md","图像检索orReID 竞赛的 Tricks","/pages/7f2968/"],["04.竞赛笔记-视觉竞赛/03.kaggle-CowBoy Outfits Detection 竞赛方案学习.md","kaggle-CowBoy Outfits Detection 竞赛方案学习","/pages/9f35c1/"],["04.竞赛笔记-视觉竞赛/04.kaggle 图像分割竞赛学习.md","kaggle 图像分割竞赛学习","/pages/31d8c4/"],["04.竞赛笔记-视觉竞赛/05.few-shot learning 竞赛学习-1.md","few-shot learning 竞赛学习","/pages/e4a923/"],["04.竞赛笔记-视觉竞赛/06.few-shot learning 竞赛学习-2.md","few-shot learning 竞赛学习-2","/pages/8432b6/"],["04.竞赛笔记-视觉竞赛/07.遥感图像建筑物变化检测竞赛学习-1.md","遥感图像建筑物变化检测竞赛学习-1","/pages/59507a/"]]},{title:"框架解析-mmlab系列",collapsable:!0,children:[["05.框架解析-mmlab系列/00.MMClassifiction 框架学习导言.md","MMClassifiction 框架学习导言","/pages/fbe79c/"],["05.框架解析-mmlab系列/01.mmcls 是如何能够通过config 就搭建好一个模型的？.md","mmcls 是如何能够通过config 就搭建好一个模型的？","/pages/a070a4/"],["05.框架解析-mmlab系列/02.为自己的 inicls 框架加上 fp16 训练.md","为自己的 inicls 框架加上 fp16 训练","/pages/e677b8/"],["05.框架解析-mmlab系列/03.为自己的 inicls 框架集成 Horovod.md","为自己的 inicls 框架集成 Horovod","/pages/5a9505/"],["05.框架解析-mmlab系列/04.为自己的 inicls 框架集成 DALI.md","为自己的 inicls 框架集成 DALI","/pages/37ee3e/"],["05.框架解析-mmlab系列/05.mmsegmentation框架解析（上）.md","mmsegmentation框架解析（上）","/pages/3fb5c1/"],["05.框架解析-mmlab系列/06.mmsegmentation框架解析（中）.md","mmsegmentation框架解析（中）","/pages/2c9bb8/"],["05.框架解析-mmlab系列/07.mmsegmentation框架解析（下）.md","mmsegmentation框架解析（下）","/pages/759a29/"],["05.框架解析-mmlab系列/08.mmcv 使用（上）--Fileio&Image.md","mmcv 使用","/pages/944bac/"],["05.框架解析-mmlab系列/09.mmcv使用（中）--Config.md","mmcv使用（中）--Config","/pages/125b38/"],["05.框架解析-mmlab系列/10.什么是 Register.md","什么是 Register","/pages/16f307/"],["05.框架解析-mmlab系列/11.什么是 ABCMeta.md","什么是 ABCMeta","/pages/1a8345/"],["05.框架解析-mmlab系列/12.mmseg数据集.md","mmseg数据集","/pages/6b655a/"],["05.框架解析-mmlab系列/13.mmseg 推理单张图像并保存.md","mmseg 推理单张图像并保存","/pages/1789a5/"],["05.框架解析-mmlab系列/14.计算loss和计算metric.md","计算loss和计算metric","/pages/9b8ee5/"]]},{title:"讲座记录-有意思的文章集合",collapsable:!0,children:[["06.讲座记录-有意思的文章集合/00.尚未阅读的各类文章.md","尚未阅读的各类文章","/pages/234602/"],["06.讲座记录-有意思的文章集合/01.VALSE Paper 探索简单孪生网络表征学习.md","一些需要注意的点","/pages/1b9e2a/"],["06.讲座记录-有意思的文章集合/02.TeachBeat 不确定性学习在视觉识别中的应用.md","不确定性学习在视觉识别中的应用","/pages/49a01a/"],["06.讲座记录-有意思的文章集合/03.VALSE Webinar 21-19 弱监督视觉学习：定位、分割和其他.md","VALSE Webinar 21-19 弱监督视觉学习：定位、分割和其他","/pages/ada345/"],["06.讲座记录-有意思的文章集合/04.VALSE Tutorial A Tutorial of Transformers.md","A Tutorial of Transformers","/pages/cdcabd/"],["06.讲座记录-有意思的文章集合/05.TeachBeat 物体检测中的训练样本采样.md","样本生而不等","/pages/669e4a/"],["06.讲座记录-有意思的文章集合/06.VALSE Webinar 20-02 元学习与小样本学习.md","VALSE Webinar 20-02 元学习与小样本学习","/pages/7f9724/"]]},{title:"体会感悟-产品沉思录观后有感",collapsable:!0,children:[["07.体会感悟-产品沉思录观后有感/00.控制输入才能更好地控制输出.md","20210411 控制输入，才能更好地控制输出","/pages/bc39b9/"],["07.体会感悟-产品沉思录观后有感/01.差异化才能生存，泯然众生是宇宙引力.md","20210426 差异化才能生存，泯然众生是宇宙引力","/pages/8ee315/"],["07.体会感悟-产品沉思录观后有感/02.做加法易，而做减法需要更多的认知努力.md","20210502 做加法易，而做减法需要更多的认知努力","/pages/e6bc11/"],["07.体会感悟-产品沉思录观后有感/03.时间的河入海流.md","时间的河入海流","/pages/18a53e/"]]},{title:"体会感悟-摄影",collapsable:!0,children:[["08.体会感悟-摄影/01.摄影文章收藏.md","摄影文章收藏","/pages/60f54c/"],["08.体会感悟-摄影/02.富士相机泛谈.md","富士相机泛谈","/pages/5f0c87/"],["08.体会感悟-摄影/03.胶片相机泛谈.md","胶片相机泛谈","/pages/0611eb/"],["08.体会感悟-摄影/04.后期泛谈.md","后期泛谈","/pages/736ec2/"]]},{title:"系列笔记-",collapsable:!0,children:[["09.系列笔记-.特征可视化/00.特征可视化简介.md","特征可视化简介","/pages/405167/"],["09.系列笔记-.特征可视化/01.类激活热力图可视化.md","类激活热力图可视化","/pages/26899e/"],["09.系列笔记-.特征可视化/02.特征图可视化.md","特征图可视化","/pages/7cc0fb/"],["09.系列笔记-.特征可视化/03.卷积核可视化.md","卷积核可视化","/pages/78c106/"]]},{title:"系列笔记-乐理和五线谱",collapsable:!0,children:[["10.系列笔记-乐理和五线谱/00.五线谱基础（上）.md","五线谱基础（上）","/pages/9867d4/"],["10.系列笔记-乐理和五线谱/01.五线谱基础（下）.md","五线谱基础（下）","/pages/4ac034/"]]},{title:"系列笔记-爬虫实践",collapsable:!0,children:[["11.系列笔记-爬虫实践/01.爬虫基础.md","爬虫基础","/pages/2cfd0e/"],["11.系列笔记-爬虫实践/02.Beautiful-soup4、Xpath、re.md","Beautiful-soup4、Xpath、re","/pages/ad2d26/"],["11.系列笔记-爬虫实践/03.session和cookie、代理、selenium自动化.md","session和cookie、代理、selenium自动化","/pages/a36ee1/"]]},{title:"系列笔记-Django学习笔记",collapsable:!0,children:[["12.系列笔记-Django学习笔记/00.安装云环境的mysql.md","安装云环境的mysql","/pages/359c09/"],["12.系列笔记-Django学习笔记/01.Django学习笔记（一）--简单入门.md","Django学习笔记（一）--简单入门","/pages/597550/"],["12.系列笔记-Django学习笔记/02.Django学习笔记（二）-- 连接MySQL数据库的小应用.md","Django学习笔记（二）-- 连接MySQL数据库的小应用","/pages/fc23c1/"]]},{title:"系列笔记-Git 使用笔记",collapsable:!0,children:[["13.系列笔记-Git 使用笔记/00.常见错误.md","常见错误","/pages/3dcbbc/"],["13.系列笔记-Git 使用笔记/01.本地分支的新建.md","本地分支的新建","/pages/c9c01b/"],["13.系列笔记-Git 使用笔记/02.fork 代码后想要同步至最新版.md","fork 代码后想要同步至最新版","/pages/27a54a/"],["13.系列笔记-Git 使用笔记/03.remote Support for password authentication was removed Please use a personal access token instead.md","remote Support for password authentication was removed Please use a personal access token instead","/pages/83cc49/"],["13.系列笔记-Git 使用笔记/04.撤销 git add.md","撤销 git add","/pages/ecdf24/"],["13.系列笔记-Git 使用笔记/05.撤销 git commit.md","撤销 git commit","/pages/1e41a6/"],["13.系列笔记-Git 使用笔记/06.ssh 与 Git 登录.md","ssh 与 Git 登录","/pages/02e47d/"],["13.系列笔记-Git 使用笔记/07.Git 设置代理.md","Git 设置代理","/pages/4000c9/"],["13.系列笔记-Git 使用笔记/08.Unverify.md","Unverify","/pages/950767/"],["13.系列笔记-Git 使用笔记/09.README 美化.md","README 美化","/pages/83977c/"]]},{title:"系列笔记-网站搭建",collapsable:!0,children:[["14.系列笔记-网站搭建/00.flarum 论坛搭建.md","flarum 论坛搭建","/pages/2da48e/"]]},{title:"系列笔记-图卷积网络",collapsable:!0,children:[["15.系列笔记-图卷积网络/00.图学习初印象.md","图学习初印象","/pages/1a1c0e/"],["15.系列笔记-图卷积网络/01.图游走类模型.md","图游走类模型","/pages/eef8e4/"],["15.系列笔记-图卷积网络/02.节点分类是如何训练的.md","节点分类是如何训练的","/pages/a12467/"]]},{title:"课程笔记-MIT-NULL",collapsable:!0,children:[["16.课程笔记-MIT-NULL/00.Course overview and the shell.md","Course overview and the shell","/pages/79bd3f/"]]},{title:"系列笔记-OpenCV-Python",collapsable:!0,children:[["17.系列笔记-OpenCV-Python/01.图像插值.md","图像插值","/pages/8cab5c/"],["17.系列笔记-OpenCV-Python/02.图像增强-几何及灰度变换.md","图像增强-几何及灰度变换","/pages/a70988/"],["17.系列笔记-OpenCV-Python/03.彩色空间互转.md","彩色空间互转","/pages/adde71/"],["17.系列笔记-OpenCV-Python/04.图像滤波.md","图像滤波","/pages/846a1d/"],["17.系列笔记-OpenCV-Python/05.阈值分割及二值化.md","阈值分割及二值化","/pages/b41db5/"]]},{title:"系列笔记-使用 Beancount 记账",collapsable:!0,children:[["18.系列笔记-使用 Beancount 记账/00.入门 Beancount.md","入门 Beancount","/pages/ac712c/"],["18.系列笔记-使用 Beancount 记账/01.利用 GitHub 以及商家的批量导入记账数据.md","利用 GitHub 以及商家的批量导入记账数据","/pages/3a4147/"],["18.系列笔记-使用 Beancount 记账/02.将 Cashwarden 的数据导入 Beancount.md","将 Cashwarden 的数据导入 Beancount","/pages/f53f1f/"]]},{title:"系列笔记-Python设计模式",collapsable:!0,children:[["19.系列笔记-Python设计模式/00.工厂方法模式.md","工厂方法模式","/pages/eee0ed/"]]},{title:"系列笔记-MLOps",collapsable:!0,children:[["20.系列笔记-MLOps/00.关于MLOps.md","关于MLOps","/pages/8a695b/"],["20.系列笔记-MLOps/01.(Notes) A Chat with Andrew on MLOps From Model-centric to Data-centric AI.md","(Notes) A Chat with Andrew on MLOps From Model-centric to Data-centric AI","/pages/2bcb1d/"],["20.系列笔记-MLOps/02.常用的数据治理手段.md","常用的数据治理手段","/pages/ac6238/"]]},{title:"系列笔记-Apollo自动驾驶",collapsable:!0,children:[["21.系列笔记-Apollo自动驾驶/00.Apollo 核心模块.md","Apollo 核心模块","/pages/899a9e/"],["21.系列笔记-Apollo自动驾驶/01.自动驾驶简介.md","自动驾驶简介","/pages/799e91/"],["21.系列笔记-Apollo自动驾驶/02.高精度地图.md","高精度地图","/pages/e423da/"],["21.系列笔记-Apollo自动驾驶/03.定位.md","定位","/pages/638dfe/"],["21.系列笔记-Apollo自动驾驶/04.感知.md","感知","/pages/1d8a53/"],["21.系列笔记-Apollo自动驾驶/05.预测.md","预测","/pages/7dc507/"],["21.系列笔记-Apollo自动驾驶/06.规划.md","规划","/pages/8beaa5/"],["21.系列笔记-Apollo自动驾驶/07.控制.md","控制","/pages/b43d07/"]]},{title:"系列笔记-PaddlePaddle",collapsable:!0,children:[["22.系列笔记-PaddlePaddle/00.整体评分.md","整体评分","/pages/0ba4e9/"]]},{title:"系列笔记-视频操作",collapsable:!0,children:[["23.系列笔记-视频操作/00.ffmpeg 库的使用.md","ffmpeg 库的使用","/pages/0f2c42/"]]},{title:"Vue+Django前后端分离开发",collapsable:!0,children:[]},{title:"深度学习及机器学习理论知识学习笔记",collapsable:!0,children:[["25.深度学习及机器学习理论知识学习笔记/00.极大似然函数.md","极大似然函数","/pages/b4bbc7/"],["25.深度学习及机器学习理论知识学习笔记/01.逻辑回归与sigmoid.md","逻辑回归与sigmoid","/pages/fbd258/"],["25.深度学习及机器学习理论知识学习笔记/02.softmax与交叉熵.md","softmax与交叉熵","/pages/bc5b38/"],["25.深度学习及机器学习理论知识学习笔记/03.矩估计.md","矩估计","/pages/38d46e/"],["25.深度学习及机器学习理论知识学习笔记/04.损失函数的前置知识.md","损失函数的前置知识","/pages/e646c8/"]]},{title:"PyTorch Tricks",collapsable:!0,children:[["26.PyTorch Tricks/00.PyTorch 常见代码片段.md","PyTorch 常见代码片段","/pages/60e26f/"],["26.PyTorch Tricks/01.使用单机多卡分布式训练.md","使用单机多卡分布式训练","/pages/eb4db7/"],["26.PyTorch Tricks/02.使用半精度训练.md","使用半精度训练","/pages/21270d/"],["26.PyTorch Tricks/03.数据增强.md","数据增强","/pages/15e77c/"],["26.PyTorch Tricks/04.常见 Tricks 代码片段.md","常见 Tricks 代码片段","/pages/53d32c/"]]}],"/03.生活杂谈/":[{title:"心情杂货",collapsable:!0,children:[["01.心情杂货/01.敬告青年.md","敬告青年","/pages/967c74/"],["01.心情杂货/02.年度计划模板.md","年度计划模板","/pages/fe5265/"],["01.心情杂货/03.年度总结模板.md","年度总结模板","/pages/32c4fa/"],["01.心情杂货/04.2020年终总结备份.md","2020年终总结备份","/pages/a32ea4/"],["01.心情杂货/05.六月的离别让人忧伤.md","六月的离别让人忧伤","/pages/daccdc/"],["01.心情杂货/06.2021 年终总结 | 时光飞逝的一年.md","2021 年终总结 | 时光飞逝的一年","/pages/aceb33/"],["01.心情杂货/07.年少可以听听李宗盛，只是容易上头.md","年少可以听听李宗盛，只是容易上头","/pages/de6315/"]]},{title:"学术杂谈",collapsable:!0,children:[["02.学术杂谈/00.Data-centric vs Model-centric 的个人拙见.md","Data-centric vs Model-centric 的个人拙见","/pages/4341a5/"],["02.学术杂谈/01.重参数化宇宙的起源.md","重参数化宇宙的起源","/pages/06b1ed/"],["02.学术杂谈/02.动态卷积.md","动态卷积","/pages/ba21cc/"],["02.学术杂谈/03.从 Tesla AI Day 看自动驾驶的进展.md","从 Tesla AI Day 看自动驾驶的进展","/pages/4de976/"]]}],"/04.wiki搬运/":[{title:"常见 bug 修复",collapsable:!0,children:[["01.常见 bug 修复/00.Vuepress deploy时的若干问题.md","Vuepress deploy时的若干问题","/pages/3379fb/"],["01.常见 bug 修复/01.npm项目启动报错.md","npm项目启动报错","/pages/b5ac46/"],["01.常见 bug 修复/02.连接远程服务器显示Host key verification failed.md","连接远程服务器显示Host key verification failed","/pages/63e1ef/"]]},{title:"环境配置",collapsable:!0,children:[["02.环境配置/00.GPU功率不一致.md","GPU功率不一致","/pages/5628db/"],["02.环境配置/01.Anaconda下载及配置.md","Anaconda下载及配置","/pages/df0682/"],["02.环境配置/02.从零开始配PyTorch GPU环境.md","从零开始配PyTorch GPU环境","/pages/b6a8b0/"],["02.环境配置/03.ubuntu 18-04 搭建 go 语言开发环境.md","ubuntu 18-04 搭建 go 语言开发环境","/pages/cf9a89/"],["02.环境配置/04.GPU速度太慢问题排查.md","GPU速度太慢问题排查","/pages/894290/"],["02.环境配置/05.Ubuntu系统安装.md","Ubuntu系统安装","/pages/0b52e1/"],["02.环境配置/06.服务器重装系统.md","服务器重装系统","/pages/0d3088/"],["02.环境配置/07.Clash 配置.md","Clash 配置","/pages/715b93/"]]},{title:"常用库的常见用法",collapsable:!0,children:[["03.常用库的常见用法/00.pandas 库常用用法.md","pandas 库常用用法","/pages/95cffd/"],["03.常用库的常见用法/01.glob 库常用用法.md","glob 库常用用法","/pages/52270e/"],["03.常用库的常见用法/02.sklearn 库常用用法.md","sklearn 库常用用法","/pages/a5960e/"],["03.常用库的常见用法/03.matplotlib 库常用用法--散点图绘制.md","matplotlib 库常用用法--散点图绘制","/pages/42f579/"],["03.常用库的常见用法/04.图像处理库的常用用法.md","图像处理库的常用用法","/pages/997b21/"],["03.常用库的常见用法/05.ubuntu 系统常用命令.md","ubuntu 系统常用命令","/pages/e7ed74/"],["03.常用库的常见用法/06.numpy 库常用用法.md","numpy 库常用用法","/pages/37da51/"],["03.常用库的常见用法/07.matplotlib 库常用用法.md","matplotlib 库常用用法","/pages/20455a/"],["03.常用库的常见用法/08.tmux 常用用法.md","tmux 常用用法","/pages/25adce/"],["03.常用库的常见用法/09.Docker 常用用法.md","Docker 常用用法","/pages/cdf916/"],["03.常用库的常见用法/10.zsh 相关用法.md","zsh 相关用法","/pages/018ff0/"],["03.常用库的常见用法/11.常见的专有名词.md","常见的专有名词","/pages/b96e43/"],["03.常用库的常见用法/12.sharelatex 部署.md","sharelatex 部署","/pages/023d1e/"],["03.常用库的常见用法/13.typecho 部署.md","typecho 部署","/pages/dfec85/"],["03.常用库的常见用法/14.cpp STL 常用用法.md","cpp STL 常用用法","/pages/8b291c/"],["03.常用库的常见用法/15.Tensorboard 常用用法.md","Tensorboard 常用用法","/pages/29671f/"]]}],"/05.资源收藏/":[{title:"面试资料",collapsable:!0,children:[["01.面试资料/00.面试问题集锦.md","面试问题集锦","/pages/aea6571b7a8bae86"],["01.面试资料/01.面试可能会用到的知识点.md","面试可能会用到的知识点","/pages/4f4178/"],["01.面试资料/02.Supermemo 面试知识点卡片-20210808.md","Supermemo 面试知识点卡片-20210808","/pages/85131e/"],["01.面试资料/03.LeetCode 面试算法题卡片-2021-0808.md","LeetCode 面试算法题卡片-2021-0808","/pages/d22623/"]]},{title:"资料搜集",collapsable:!0,children:[["02.资料搜集/00.资料搜集.md","资料搜集","/pages/32cc8f/"]]}],"/06.关于/":[["01.关于.md","关于","/about"]],"/07.友情链接/":[["01.友情链接.md","友情链接","/friends"]]},author:{name:"Muyun99",link:"https://github.com/Muyun99"},blogger:{avatar:"https://muyun-blog-pic.oss-cn-shanghai.aliyuncs.com/tutou.jpg",name:"Muyun99",slogan:"努力成为一个善良的人"},social:{iconfontCssFile:"//at.alicdn.com/t/font_2409154_6mouiwabqg.css",icons:[{iconClass:"icon-mail",title:"发邮件",link:"mailto:yundoo99@gmail.com"},{iconClass:"icon-github",title:"GitHub",link:"https://github.com/muyun99"},{iconClass:"icon-Blog",title:"博客",link:"https://muyun.work"},{iconClass:"icon-icon-test",title:"听音乐",link:"https://music.163.com/#/playlist?id=713385758"}]},footer:{createYear:2021,copyrightInfo:'Muyun99 | <a href="https://github.com/Muyun99/Wiki/blob/main/LICENSE" target="_blank">MIT License</a>'},htmlModules:{homeSidebarB:'\x3c!-- 纵向自适应 --\x3e\n    <ins class="adsbygoogle"\n        style="display:block;padding: 0.95rem;"\n        data-ad-client="ca-pub-7828333725993554"\n        data-ad-slot="7802654582"\n        data-ad-format="auto"\n        data-full-width-responsive="true"></ins>\n    <script>\n        (adsbygoogle = window.adsbygoogle || []).push({});\n    <\/script>',sidebarB:'\x3c!-- 正方形 --\x3e\n      <ins class="adsbygoogle"\n          style="display:block"\n          data-ad-client="ca-pub-7828333725993554"\n          data-ad-slot="3508773082"\n          data-ad-format="auto"\n          data-full-width-responsive="true"></ins>\n      <script>\n          (adsbygoogle = window.adsbygoogle || []).push({});\n      <\/script>',pageT:'\x3c!-- 固定100% * 90px可显示，max-height:90px未见显示--\x3e\n     <ins class="adsbygoogle"\n          style="display:inline-block;width:100%;max-height:90px"\n          data-ad-client="ca-pub-7828333725993554"\n          data-ad-slot="6625304284"></ins>\n      <script>\n          (adsbygoogle = window.adsbygoogle || []).push({});\n      <\/script>',pageB:'\x3c!-- 横向自适应 --\x3e\n      <ins class="adsbygoogle"\n          style="display:block"\n          data-ad-client="ca-pub-7828333725993554"\n          data-ad-slot="6620245489"\n          data-ad-format="auto"\n          data-full-width-responsive="true"></ins>\n      <script>\n          (adsbygoogle = window.adsbygoogle || []).push({});\n      <\/script>',windowRB:'\x3c!-- 固定160*160px --\x3e\n      <ins class="adsbygoogle"\n          style="display:inline-block;max-width:160px;max-height:160px"\n          data-ad-client="ca-pub-7828333725993554"\n          data-ad-slot="8377369658"></ins>\n      <script>\n          (adsbygoogle = window.adsbygoogle || []).push({});\n      <\/script>\n      '}}},bl=(t(130),t(181),t(116),t(190)),yl=t(191),_l=(t(328),t(35));var El={computed:{$filterPosts:function(){return this.$site.pages.filter((function(n){var e=n.frontmatter,t=e.pageComponent,a=e.article,i=e.home;return!(t||!1===a||!0===i)}))},$sortPosts:function(){return(n=this.$filterPosts).sort((function(n,e){var t=n.frontmatter.sticky,a=e.frontmatter.sticky;return t&&a?t==a?Object(_l.a)(n,e):t-a:t&&!a?-1:!t&&a?1:Object(_l.a)(n,e)})),n;var n},$sortPostsByDate:function(){return(n=this.$filterPosts).sort((function(n,e){return Object(_l.a)(n,e)})),n;var n},$groupPosts:function(){return function(n){for(var e={},t={},a=function(a,i){var r=n[a].frontmatter,o=r.categories,s=r.tags;"array"===Object(_l.n)(o)&&o.forEach((function(t){t&&(e[t]||(e[t]=[]),e[t].push(n[a]))})),"array"===Object(_l.n)(s)&&s.forEach((function(e){e&&(t[e]||(t[e]=[]),t[e].push(n[a]))}))},i=0,r=n.length;i<r;i++)a(i);return{categories:e,tags:t}}(this.$sortPosts)},$categoriesAndTags:function(){return function(n){var e=[],t=[];for(var a in n.categories)e.push({key:a,length:n.categories[a].length});for(var i in n.tags)t.push({key:i,length:n.tags[i].length});return{categories:e,tags:t}}(this.$groupPosts)}}};zi.component(bl.default),zi.component(yl.default);function Al(n){return n.toString().padStart(2,"0")}t(329);zi.component("Badge",(function(){return Promise.all([t.e(0),t.e(3)]).then(t.bind(null,672))})),zi.component("CodeBlock",(function(){return Promise.resolve().then(t.bind(null,190))})),zi.component("CodeGroup",(function(){return Promise.resolve().then(t.bind(null,191))}));t(330),t(331);var xl=[function(n){n.Vue,n.options,n.router,n.siteData},function(n){var e=n.Vue,t=(n.options,n.router,n.siteData);t.pages.map((function(n){var e=n.frontmatter,a=e.date,i=e.author;"string"==typeof a&&"Z"===a.charAt(a.length-1)&&(n.frontmatter.date=function(n){n instanceof Date||(n=new Date(n));return"".concat(n.getUTCFullYear(),"-").concat(Al(n.getUTCMonth()+1),"-").concat(Al(n.getUTCDate())," ").concat(Al(n.getUTCHours()),":").concat(Al(n.getUTCMinutes()),":").concat(Al(n.getUTCSeconds()))}(a)),i?n.author=i:t.themeConfig.author&&(n.author=t.themeConfig.author)})),e.mixin(El)},{},function(n){n.Vue.mixin({computed:{$dataBlock:function(){return this.$options.__data__block__}}})},{},{},function(n){n.router;"undefined"!=typeof window&&function(){var n=document.createElement("script"),e=window.location.protocol.split(":")[0];n.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(n,t)}()},function(n){n.Vue;Promise.all([t.e(0),t.e(247)]).then(t.t.bind(null,426,7)),Promise.all([t.e(0),t.e(248)]).then(t.t.bind(null,427,7))},function(n){var e=n.router;"undefined"!=typeof window&&(window._hmt=window._hmt||[],function(){var n=document.createElement("script");n.src="https://hm.baidu.com/hm.js?503f098e7e5b3a5b5d8c5fc2938af002";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(n,e)}(),e.afterEach((function(n){_hmt.push(["_trackPageview",n.fullPath])})))}],kl=[];t(184);function wl(n,e){return(wl=Object.setPrototypeOf||function(n,e){return n.__proto__=e,n})(n,e)}t(185);function Cl(n){return(Cl=Object.setPrototypeOf?Object.getPrototypeOf:function(n){return n.__proto__||Object.getPrototypeOf(n)})(n)}var Sl=t(189),Bl=t.n(Sl);function Tl(n,e){return!e||"object"!==Bl()(e)&&"function"!=typeof e?function(n){if(void 0===n)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return n}(n):e}function Pl(n){var e=function(){if("undefined"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(n){return!1}}();return function(){var t,a=Cl(n);if(e){var i=Cl(this).constructor;t=Reflect.construct(a,arguments,i)}else t=a.apply(this,arguments);return Tl(this,t)}}var Dl=function(n){!function(n,e){if("function"!=typeof e&&null!==e)throw new TypeError("Super expression must either be null or a function");n.prototype=Object.create(e&&e.prototype,{constructor:{value:n,writable:!0,configurable:!0}}),e&&wl(n,e)}(t,n);var e=Pl(t);function t(){return ys(this,t),e.apply(this,arguments)}return t}(function(){function n(){ys(this,n),this.store=new zi({data:{state:{}}})}return Es(n,[{key:"$get",value:function(n){return this.store.state[n]}},{key:"$set",value:function(n,e){zi.set(this.store.state,n,e)}},{key:"$emit",value:function(){var n;(n=this.store).$emit.apply(n,arguments)}},{key:"$on",value:function(){var n;(n=this.store).$on.apply(n,arguments)}}]),n}());Object.assign(Dl.prototype,{getPageAsyncComponent:Oo,getLayoutAsyncComponent:Uo,getAsyncComponent:Vo,getVueComponent:Go});var zl={install:function(n){var e=new Dl;n.$vuepress=e,n.prototype.$vuepress=e}};function Ll(n){n.beforeEach((function(e,t,a){if(Il(n,e.path))a();else if(/(\/|\.html)$/.test(e.path))if(/\/$/.test(e.path)){var i=e.path.replace(/\/$/,"")+".html";Il(n,i)?a(i):a()}else a();else{var r=e.path+"/",o=e.path+".html";Il(n,o)?a(o):Il(n,r)?a(r):a()}}))}function Il(n,e){return n.options.routes.filter((function(n){return n.path.toLowerCase()===e.toLowerCase()})).length>0}var Nl={props:{pageKey:String,slotKey:{type:String,default:"default"}},render:function(n){var e=this.pageKey||this.$parent.$page.key;return Ho("pageKey",e),zi.component(e)||zi.component(e,Oo(e)),zi.component(e)?n(e):n("")}},Fl={functional:!0,props:{slotKey:String,required:!0},render:function(n,e){var t=e.props,a=e.slots;return n("div",{class:["content__".concat(t.slotKey)]},a()[t.slotKey])}},jl={computed:{openInNewWindowTitle:function(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},Ml=(t(332),t(333),Object(hl.a)(jl,(function(){var n=this.$createElement,e=this._self._c||n;return e("span",[e("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[e("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),e("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),e("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports);function Rl(){return(Rl=Object(a.a)(regeneratorRuntime.mark((function n(e){var t,a,i,r;return regeneratorRuntime.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:return t="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:vl.routerBase||vl.base,Ll(a=new xo({base:t,mode:"history",fallback:!1,routes:fl,scrollBehavior:function(n,e,t){return t||(n.hash?!zi.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(n.hash)}:{x:0,y:0})}})),i={},n.prev=4,n.next=7,Promise.all(xl.filter((function(n){return"function"==typeof n})).map((function(n){return n({Vue:zi,options:i,router:a,siteData:vl,isServer:e})})));case 7:n.next=12;break;case 9:n.prev=9,n.t0=n.catch(4),console.error(n.t0);case 12:return r=new zi(Object.assign(i,{router:a,render:function(n){return n("div",{attrs:{id:"app"}},[n("RouterView",{ref:"layout"}),n("div",{class:"global-ui"},kl.map((function(e){return n(e)})))])}})),n.abrupt("return",{app:r,router:a});case 14:case"end":return n.stop()}}),n,null,[[4,9]])})))).apply(this,arguments)}zi.config.productionTip=!1,zi.use(xo),zi.use(zl),zi.mixin(function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:zi;ko(e),t.$vuepress.$set("siteData",e);var a=n(t.$vuepress.$get("siteData")),i=new a,r=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(i)),o={};return Object.keys(r).reduce((function(n,e){return e.startsWith("$")&&(n[e]=r[e].get),n}),o),{computed:o}}((function(n){return function(){function e(){ys(this,e)}return Es(e,[{key:"setPage",value:function(n){this.__page=n}},{key:"$site",get:function(){return n}},{key:"$themeConfig",get:function(){return this.$site.themeConfig}},{key:"$frontmatter",get:function(){return this.$page.frontmatter}},{key:"$localeConfig",get:function(){var n,e,t=this.$site.locales,a=void 0===t?{}:t;for(var i in a)"/"===i?e=a[i]:0===this.$page.path.indexOf(i)&&(n=a[i]);return n||e||{}}},{key:"$siteTitle",get:function(){return this.$localeConfig.title||this.$site.title||""}},{key:"$canonicalUrl",get:function(){var n=this.$page.frontmatter.canonicalUrl;return"string"==typeof n&&n}},{key:"$title",get:function(){var n=this.$page,e=this.$page.frontmatter.metaTitle;if("string"==typeof e)return e;var t=this.$siteTitle,a=n.frontmatter.home?null:n.frontmatter.title||n.title;return t?a?a+" | "+t:t:a||"VuePress"}},{key:"$description",get:function(){var n=function(n){if(n){var e=n.filter((function(n){return"description"===n.name}))[0];if(e)return e.content}}(this.$page.frontmatter.meta);return n||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}},{key:"$lang",get:function(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}},{key:"$localePath",get:function(){return this.$localeConfig.path||"/"}},{key:"$themeLocaleConfig",get:function(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}},{key:"$page",get:function(){return this.__page?this.__page:function(n,e){for(var t=0;t<n.length;t++){var a=n[t];if(a.path.toLowerCase()===e.toLowerCase())return a}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}}]),e}()}),vl)),zi.component("Content",Nl),zi.component("ContentSlotsDistributor",Fl),zi.component("OutboundLink",Ml),zi.component("ClientOnly",{functional:!0,render:function(n,e){var t=e.parent,a=e.children;if(t._isMounted)return a;t.$once("hook:mounted",(function(){t.$forceUpdate()}))}}),zi.component("Layout",Uo("Layout")),zi.component("NotFound",Uo("NotFound")),zi.prototype.$withBase=function(n){var e=this.$site.base;return"/"===n.charAt(0)?e+n.slice(1):n},window.__VUEPRESS__={version:"1.8.0",hash:"dfdc16c"},function(n){return Rl.apply(this,arguments)}(!1).then((function(n){var e=n.app;n.router.onReady((function(){e.$mount("#app")}))}))}]);