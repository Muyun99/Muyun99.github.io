(window.webpackJsonp=window.webpackJsonp||[]).push([[84],{519:function(e,t,r){"use strict";r.r(t);var n=r(25),a=Object(n.a)({},(function(){var e=this,t=e.$createElement,r=e._self._c||t;return r("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[r("h3",{attrs:{id:"语义分割入门"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#语义分割入门"}},[e._v("#")]),e._v(" 语义分割入门")]),e._v(" "),r("h4",{attrs:{id:"什么是语义分割"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#什么是语义分割"}},[e._v("#")]),e._v(" 什么是语义分割")]),e._v(" "),r("p",[e._v("图像分割是许多视觉理解系统中必不可少的组成部分，在医学影像分析、机器人感知、 视频监控、自动驾驶等领域都有着十分重要的地位。图像分割任务可以理解为基于语义信息和实例信息的像素级别的分类问题。")]),e._v(" "),r("p",[e._v("参考资料："),r("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/48670341",target:"_blank",rel:"noopener noreferrer"}},[e._v("语义分割-从入门到放弃"),r("OutboundLink")],1)]),e._v(" "),r("h4",{attrs:{id:"语义分割的发展现状"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#语义分割的发展现状"}},[e._v("#")]),e._v(" 语义分割的发展现状")]),e._v(" "),r("p",[e._v("FCN利用了全卷积网络产生特征，输入空间映射，实现了端到端的语义分割任务，成为深度学习技术应用于语义分割问题的基石。U-Net 通过上采样和 skip connection 融合高低层的特征信息，获得了更加精准的分割结果。SegNet 使用 Maxpooling indices 来增强位置信息，提高了 SegNet 的效率。")]),e._v(" "),r("p",[e._v("也有学者提出了 DeepLab 算法[3,20–22]，经过不断演进后共有四个版本。DeepLabv1 模 型[3] 将深度卷积神经网络和概率图模型进行结合。作者指出：深度卷积神经网络下采样导 致细节信息丢失，并且其结构会限制空间定位精度，该算法则使用空洞卷积以及条件随机 场对模型进行了改进。空洞卷积，在保证较大感受野的同时不过分下采样丢失过多细节信 息。条件随机场用于接收卷积神经网络的最后一层的响应进行后处理，以较少的时间内完 成细粒度的定位。DeepLabv1 将卷积神经网络和条件随机场进行耦合，并且使用多尺度预 测方法提高了边界定位精度，能够较好的恢复对象边缘信息，在 GPU 上能够达到 8 FPS 的速度。DeepLabv2[20] 将下采样的层全部替换为空洞卷积，以更高的采样密度来计算特征 映射，其特征提取模块也从 VGG[9] 换到了 ResNet[11]，加强了网络的特征提取能力。作者 还提出了基于空洞卷积的空间池化金字塔（ASPP）模块，以不同采样率的空洞卷积进行 采样，以多个比例学习图像的上下文信息，丰富了特征的维度。DeepLabv3[21] 取消了前两 个版本中的条件随机场的后处理，重点关注了四种利用上下文信息的网络模块，包括图像 金字塔、编码器-解码器、上下文模块、空间金字塔池化。该算法加深了网络深度，同时调 整了网络的下采样率，减少信息的丢失，级联模块进行特征提取后，将特征输入到结合图 像级别特征的空间金字塔池化模块，完成了整个网络结构的搭建。作者在这些模块的搭建 了进行了大量的实验验证，最终演化出最终的结构，在 PASCAL VOC 2012 数据集上的测 试性能的达到了当时的最优水平。DeepLabv3+[22] 使用空洞卷积的 Xception[15] 进行特征采 样，其网络结构见图 2.4。作者在 DeepLabv3 的基础上添加了 Decoder 模块，将 Xception 提取出的特征与 ASPP 模块采样后的特征进行特征融合后共同上采样恢复图像分辨率，使 整个模型成为编码器-解码器结构。解码器模块可以获得更好的边界分割效果，有助于模型 性能的提升。")]),e._v(" "),r("p",[e._v("RefineNet 精心设计了 Decoder 模块，并且增加了 residual connections，提升了网络的表达能力。讨论了空洞卷积的缺点。PSPNet 使用pyramid pooling整合context，使用auxiliary loss 提升网络的学习能力。DANet DANet是一种经典的应用self-Attention的网络，它引入了一种**自注意力机制来分别捕获空间维度和通道维度中的特征依赖关系。**提出了双重注意网络（DANet）来自适应地集成局部特征和全局依赖。在传统的扩张FCN之上附加两种类型的注意力模块，分别模拟空间和通道维度中的语义相互依赖性。")]),e._v(" "),r("p",[r("strong",[e._v("HRNet通过并行多个分辨率的分支，加上不断进行不同分支之间的信息交互，同时达到强语义信息和精准位置信息的目的。"),r("strong",[e._v("我觉得")]),e._v("最大的创新点还是能够从头到尾保持高分辨率，而不同分支的信息交互是为了补充通道数减少带来的信息损耗")]),e._v("。OCR 方法提出的物体上下文信息的目的在于显式地增强物体信息，通过计算一组物体的区域特征表达，根据物体区域特征表示与像素特征表示之间的相似度将这些物体区域特征表示传播给每一个像素。")]),e._v(" "),r("p",[e._v("PointRend 把语义分割以及实例分割问题（统称图像分割问题）当做一个渲染问题来解决。"),r("strong",[e._v("但本质上这篇论文其实是一个新型上采样方法，针对物体边缘的图像分割进行优化，使其在难以分割的物体边缘部分有更好的表现")]),e._v("。")]),e._v(" "),r("h3",{attrs:{id:"摘录了-cityscapes-数据集上的语义分割数据集的精度"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#摘录了-cityscapes-数据集上的语义分割数据集的精度"}},[e._v("#")]),e._v(" 摘录了 Cityscapes 数据集上的语义分割数据集的精度")]),e._v(" "),r("p",[e._v("https://paperswithcode.com/sota/semantic-segmentation-on-cityscapes")]),e._v(" "),r("p",[e._v("1、2014 DeepLab 63.1%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/semantic-image-segmentation-with-deep",target:"_blank",rel:"noopener noreferrer"}},[e._v("Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("2、2014 FCN 65.3%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/fully-convolutional-networks-for-semantic",target:"_blank",rel:"noopener noreferrer"}},[e._v("Fully Convolutional Networks for Semantic Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("3、2015 SegNet 57.0%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/segnet-a-deep-convolutional-encoder-decoder",target:"_blank",rel:"noopener noreferrer"}},[e._v("SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("5、2016 DeepLab-CRF (ResNet-101) 70.4%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/deeplab-semantic-image-segmentation-with-deep",target:"_blank",rel:"noopener noreferrer"}},[e._v("DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("6、2016 RefineNet (ResNet-101) 73.6%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/refinenet-multi-path-refinement-networks-for",target:"_blank",rel:"noopener noreferrer"}},[e._v("RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("7、2016 PSPNet (ResNet-101) 81.2%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/pyramid-scene-parsing-network",target:"_blank",rel:"noopener noreferrer"}},[e._v("Pyramid Scene Parsing Network"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("9、2017 DeepLabv3 (ResNet-101 coarse)")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/rethinking-atrous-convolution-for-semantic",target:"_blank",rel:"noopener noreferrer"}},[e._v("Rethinking Atrous Convolution for Semantic Image Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("13、2018 DenseASPP  (DenseNet-161) 80.6%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/denseaspp-for-semantic-segmentation-in-street",target:"_blank",rel:"noopener noreferrer"}},[e._v("DenseASPP for Semantic Segmentation in Street Scenes"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("14、2018 PSANet (ResNet-101) 81.4%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/psanet-point-wise-spatial-attention-network",target:"_blank",rel:"noopener noreferrer"}},[e._v("PSANet: Point-wise Spatial Attention Network for Scene Parsing"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("15、2018 CCNet 81.4%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/ccnet-criss-cross-attention-for-semantic",target:"_blank",rel:"noopener noreferrer"}},[e._v("CCNet: Criss-Cross Attention for Semantic Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("16、2018 DANet 81.5%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/dual-attention-network-for-scene-segmentation",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dual Attention Network for Scene Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("17、2018 OCNet 81.7%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/ocnet-object-context-network-for-scene",target:"_blank",rel:"noopener noreferrer"}},[e._v("OCNet: Object Context Network for Scene Parsing"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("18、2018 DeepLabv3+ (Xception-JFT) 82.1%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/encoder-decoder-with-atrous-separable",target:"_blank",rel:"noopener noreferrer"}},[e._v("Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("19、2018 DeepLabv3Plus + SDCNetAug 83.5%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/improving-semantic-segmentation-via-video",target:"_blank",rel:"noopener noreferrer"}},[e._v("Improving Semantic Segmentation via Video Propagation and Label Relaxation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("20、2019 Asymmetric ALNN 81.3%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/asymmetric-non-local-neural-networks-for",target:"_blank",rel:"noopener noreferrer"}},[e._v("Asymmetric Non-local Neural Networks for Semantic Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("21、2019 BFP 81.4%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/boundary-aware-feature-propagation-for-scene",target:"_blank",rel:"noopener noreferrer"}},[e._v("Boundary-Aware Feature Propagation for Scene Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("22、2019 HRNet (HRNetV2-W48) 81.6%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/high-resolution-representations-for-labeling",target:"_blank",rel:"noopener noreferrer"}},[e._v("High-Resolution Representations for Labeling Pixels and Regions"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("23、2019 OCR (ResNet-101) 81.8%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/object-contextual-representations-for",target:"_blank",rel:"noopener noreferrer"}},[e._v("Object-Contextual Representations for Semantic Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("24、2019 Auto-DeepLab-L 82.1%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/auto-deeplab-hierarchical-neural-architecture",target:"_blank",rel:"noopener noreferrer"}},[e._v("Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("25、2019 OCR (ResNet-101 coarse) 82.4%")]),e._v(" "),r("p",[e._v("见第 23 条")]),e._v(" "),r("p",[e._v("26、2019 OCR (HRNetV2-W48 coarse) 83.0%")]),e._v(" "),r("p",[e._v("见第 23 条")]),e._v(" "),r("p",[e._v("27、2019 HRNetV2+OCR (w/ASP) 83.7%")]),e._v(" "),r("p",[e._v("见第 23 条")]),e._v(" "),r("p",[e._v("28、2019 Panoptic-DeepLab 84.2%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/panoptic-deeplab-a-simple-strong-and-fast",target:"_blank",rel:"noopener noreferrer"}},[e._v("Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("29、2019 HRNetV2 + OCR + extra data 84.5%")]),e._v(" "),r("p",[e._v("见第 23 条")]),e._v(" "),r("p",[e._v("30、2020 ESANet-R34-NBt1D 80.09%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/efficient-rgb-d-semantic-segmentation-for",target:"_blank",rel:"noopener noreferrer"}},[e._v("Efficient RGB-D Semantic Segmentation for Indoor Scene Analysis"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("31、2020 CPN (ResNet-101) 81.3%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/context-prior-for-scene-segmentation",target:"_blank",rel:"noopener noreferrer"}},[e._v("Context Prior for Scene Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("32、2020 HANet 83.2%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/cars-cant-fly-up-in-the-sky-improving-urban",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cars Can't Fly up in the Sky: Improving Urban-Scene Segmentation via Height-driven Attention Networks"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("33、2020 ResNest200 83.3%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/resnest-split-attention-networks",target:"_blank",rel:"noopener noreferrer"}},[e._v("ResNeSt: Split-Attention Networks"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("34、2020 EfficientPS 84.21%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/efficientps-efficient-panoptic-segmentation",target:"_blank",rel:"noopener noreferrer"}},[e._v("EfficientPS: Efficient Panoptic Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("35、2020 HRNet-OCR(Hierarchical Multi-Scale Attention) 85.1%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/hierarchical-multi-scale-attention-for",target:"_blank",rel:"noopener noreferrer"}},[e._v("Hierarchical Multi-Scale Attention for Semantic Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("36、2021 DDRNet-39 1.5x 82.4%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/deep-dual-resolution-networks-for-real-time",target:"_blank",rel:"noopener noreferrer"}},[e._v("Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("37、2021 CCA (ResNet-101) 82.6%")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://paperswithcode.com/paper/caa-channelized-axial-attention-for-semantic",target:"_blank",rel:"noopener noreferrer"}},[e._v("CAA : Channelized Axial Attention for Semantic Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("p",[e._v("38、2021 SETR")]),e._v(" "),r("ul",[r("li",[e._v("paper: "),r("a",{attrs:{href:"https://github.com/fudan-zvg/SETR",target:"_blank",rel:"noopener noreferrer"}},[e._v("[CVPR 2021] Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers"),r("OutboundLink")],1)]),e._v(" "),r("li",[e._v("code: ["),r("a",{attrs:{href:"https://arxiv.org/abs/2012.15840",target:"_blank",rel:"noopener noreferrer"}},[e._v("Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers"),r("OutboundLink")],1)])]),e._v(" "),r("p",[e._v("参考资料：")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/36857546",target:"_blank",rel:"noopener noreferrer"}},[e._v("语义分割刷怪进阶"),r("OutboundLink")],1)]),e._v(" "),r("h3",{attrs:{id:"实时语义分割"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#实时语义分割"}},[e._v("#")]),e._v(" 实时语义分割")]),e._v(" "),r("p",[e._v("1、2016 ENet 58.3%")]),e._v(" "),r("ul",[r("li",[r("a",{attrs:{href:"https://paperswithcode.com/paper/enet-a-deep-neural-network-architecture-for",target:"_blank",rel:"noopener noreferrer"}},[e._v("ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation"),r("OutboundLink")],1)])]),e._v(" "),r("p",[e._v("2、2017 ICNet 70.6%")]),e._v(" "),r("ul",[r("li",[r("a",{attrs:{href:"https://paperswithcode.com/paper/icnet-for-real-time-semantic-segmentation-on",target:"_blank",rel:"noopener noreferrer"}},[e._v("ICNet for Real-Time Semantic Segmentation on High-Resolution Images"),r("OutboundLink")],1)])]),e._v(" "),r("p",[e._v("3、2018 ESPNet 60.3%")]),e._v(" "),r("ul",[r("li",[e._v("paper: "),r("a",{attrs:{href:"https://paperswithcode.com/paper/espnet-efficient-spatial-pyramid-of-dilated",target:"_blank",rel:"noopener noreferrer"}},[e._v("ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("li",[e._v("code: "),r("a",{attrs:{href:"https://github.com/sacmehta/ESPNet",target:"_blank",rel:"noopener noreferrer"}},[e._v("ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation"),r("OutboundLink")],1)])]),e._v(" "),r("p",[e._v("4、2018 ESPNetv2 66.2%")]),e._v(" "),r("ul",[r("li",[r("a",{attrs:{href:"https://paperswithcode.com/paper/espnetv2-a-light-weight-power-efficient-and",target:"_blank",rel:"noopener noreferrer"}},[e._v("ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network"),r("OutboundLink")],1)])]),e._v(" "),r("p",[e._v("5、2018 BiSeNet (ResNet-101) 78.9%")]),e._v(" "),r("ul",[r("li",[r("a",{attrs:{href:"https://paperswithcode.com/paper/bisenet-bilateral-segmentation-network-for",target:"_blank",rel:"noopener noreferrer"}},[e._v("BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation"),r("OutboundLink")],1)])]),e._v(" "),r("p",[e._v("6、2019 ESNet 70.7%")]),e._v(" "),r("ul",[r("li",[r("a",{attrs:{href:"https://paperswithcode.com/paper/esnet-an-efficient-symmetric-network-for-real",target:"_blank",rel:"noopener noreferrer"}},[e._v("ESNet: An Efficient Symmetric Network for Real-time Semantic Segmentation"),r("OutboundLink")],1)])]),e._v(" "),r("p",[e._v("7、2019 DFANet A 71.3%")]),e._v(" "),r("ul",[r("li",[r("a",{attrs:{href:"https://paperswithcode.com/paper/dfanet-deep-feature-aggregation-for-real-time",target:"_blank",rel:"noopener noreferrer"}},[e._v("DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation"),r("OutboundLink")],1)])]),e._v(" "),r("p",[e._v("8、2019 FasterSeg 71.5%")]),e._v(" "),r("ul",[r("li",[r("a",{attrs:{href:"https://paperswithcode.com/paper/fasterseg-searching-for-faster-real-time-1",target:"_blank",rel:"noopener noreferrer"}},[e._v("FasterSeg: Searching for Faster Real-time Semantic Segmentation"),r("OutboundLink")],1)])]),e._v(" "),r("p",[e._v("9、2020 SFSegNet (ECCV 2020 Oral)")]),e._v(" "),r("ul",[r("li",[e._v("paper: "),r("a",{attrs:{href:"https://arxiv.org/abs/2002.10120",target:"_blank",rel:"noopener noreferrer"}},[e._v("Semantic Flow for Fast and Accurate Scene Parsing"),r("OutboundLink")],1)]),e._v(" "),r("li",[e._v("code: "),r("a",{attrs:{href:"https://github.com/donnyyou/torchcv",target:"_blank",rel:"noopener noreferrer"}},[e._v("Implementation of Our ECCV-2020-oral paper: Semantic Flow for Fast and Accurate Scene Parsing"),r("OutboundLink")],1)])]),e._v(" "),r("p",[e._v("10、DecoupleSegNets (ECCV 2020)")]),e._v(" "),r("ul",[r("li",[r("p",[e._v("paper: "),r("a",{attrs:{href:"https://arxiv.org/abs/2007.10035",target:"_blank",rel:"noopener noreferrer"}},[e._v("Improving Semantic Segmentation via Decoupled Body and Edge Supervision"),r("OutboundLink")],1)])]),e._v(" "),r("li",[r("p",[e._v("code: "),r("a",{attrs:{href:"https://github.com/lxtGH/DecoupleSegNets",target:"_blank",rel:"noopener noreferrer"}},[e._v("Implementation of Our ECCV2020-work: Improving Semantic Segmentation via Decoupled Body and Edge Supervision"),r("OutboundLink")],1)])])]),e._v(" "),r("p",[e._v("10、2021 DDRNet")]),e._v(" "),r("ul",[r("li",[r("p",[e._v("paper: "),r("a",{attrs:{href:"https://arxiv.org/abs/2101.06085",target:"_blank",rel:"noopener noreferrer"}},[e._v("Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes"),r("OutboundLink")],1)])]),e._v(" "),r("li",[r("p",[e._v("code: "),r("a",{attrs:{href:"https://github.com/ydhongHIT/DDRNet",target:"_blank",rel:"noopener noreferrer"}},[e._v('The official implementation of "Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes"'),r("OutboundLink")],1)])])]),e._v(" "),r("p",[e._v("参考资料：")]),e._v(" "),r("p",[r("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/268409221",target:"_blank",rel:"noopener noreferrer"}},[e._v("实时语义分割 看这一篇就够了！涵盖24篇文章——上篇"),r("OutboundLink")],1)]),e._v(" "),r("p",[r("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/268405193",target:"_blank",rel:"noopener noreferrer"}},[e._v("实时语义分割 看这一篇就够了！涵盖24篇文章——下篇"),r("OutboundLink")],1)]),e._v(" "),r("h3",{attrs:{id:"实例分割"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#实例分割"}},[e._v("#")]),e._v(" 实例分割")]),e._v(" "),r("p",[e._v("1、2021 BCNet（CVPR2021）")]),e._v(" "),r("ul",[r("li",[e._v("paper："),r("a",{attrs:{href:"https://arxiv.org/abs/2103.12340",target:"_blank",rel:"noopener noreferrer"}},[e._v("Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers"),r("OutboundLink")],1)]),e._v(" "),r("li",[e._v("code："),r("a",{attrs:{href:"https://github.com/lkeab/BCNet",target:"_blank",rel:"noopener noreferrer"}},[e._v("Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers [CVPR 2021]"),r("OutboundLink")],1)])]),e._v(" "),r("h3",{attrs:{id:"全景分割"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#全景分割"}},[e._v("#")]),e._v(" 全景分割")]),e._v(" "),r("p",[e._v("1、2020 Axial-DeepLab (ECCV 2020 Spotlight)")]),e._v(" "),r("ul",[r("li",[r("p",[e._v("paper: "),r("a",{attrs:{href:"https://arxiv.org/abs/2003.07853",target:"_blank",rel:"noopener noreferrer"}},[e._v("Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation"),r("OutboundLink")],1)])]),e._v(" "),r("li",[r("p",[e._v("code: "),r("a",{attrs:{href:"https://github.com/csrhddlam/axial-deeplab",target:"_blank",rel:"noopener noreferrer"}},[e._v("This is a PyTorch re-implementation of Axial-DeepLab (ECCV 2020 Spotlight)"),r("OutboundLink")],1)])])]),e._v(" "),r("p",[e._v("2、2020 PanopticFCN（CVPR 2021 Oral）")]),e._v(" "),r("ul",[r("li",[e._v("paper："),r("a",{attrs:{href:"https://arxiv.org/abs/2012.00720",target:"_blank",rel:"noopener noreferrer"}},[e._v("Fully Convolutional Networks for Panoptic Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("li",[e._v("code："),r("a",{attrs:{href:"https://github.com/Jia-Research-Lab/PanopticFCN",target:"_blank",rel:"noopener noreferrer"}},[e._v("Fully Convolutional Networks for Panoptic Segmentation (CVPR2021 Oral)"),r("OutboundLink")],1)]),e._v(" "),r("li",[e._v("intro："),r("a",{attrs:{href:"https://mp.weixin.qq.com/s/UzgxAlPdW8BIGdnr_akqlg",target:"_blank",rel:"noopener noreferrer"}},[e._v("Panoptic FCN：真正End-to-End的全景分割"),r("OutboundLink")],1)])]),e._v(" "),r("h3",{attrs:{id:"语义分割的-domain-adaptation"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#语义分割的-domain-adaptation"}},[e._v("#")]),e._v(" 语义分割的 Domain Adaptation")]),e._v(" "),r("p",[e._v("1、2018 ADVENT（CVPR 2019）")]),e._v(" "),r("ul",[r("li",[e._v("paper: "),r("a",{attrs:{href:"https://arxiv.org/abs/1811.12833",target:"_blank",rel:"noopener noreferrer"}},[e._v("ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("li",[e._v("code: "),r("a",{attrs:{href:"https://github.com/valeoai/ADVENT",target:"_blank",rel:"noopener noreferrer"}},[e._v("Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation"),r("OutboundLink")],1)])]),e._v(" "),r("h3",{attrs:{id:"半监督分割"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#半监督分割"}},[e._v("#")]),e._v(" 半监督分割")]),e._v(" "),r("p",[e._v("1、2021 DTML（arxiv）")]),e._v(" "),r("ul",[r("li",[e._v("paper："),r("a",{attrs:{href:"https://arxiv.org/abs/2103.04708",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dual-Task Mutual Learning for Semi-Supervised Medical Image Segmentation"),r("OutboundLink")],1)]),e._v(" "),r("li",[e._v("code："),r("a",{attrs:{href:"https://github.com/YichiZhang98/DTML",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dual-Task Mutual Learning for Semi-Supervised Medical Image Segmentation"),r("OutboundLink")],1)])]),e._v(" "),r("h3",{attrs:{id:"分割的标注工具"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#分割的标注工具"}},[e._v("#")]),e._v(" 分割的标注工具")]),e._v(" "),r("p",[e._v("1、Semantic Segmentation Editor")]),e._v(" "),r("ul",[r("li",[e._v("link："),r("a",{attrs:{href:"https://github.com/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor",target:"_blank",rel:"noopener noreferrer"}},[e._v("Web labeling tool for bitmap images and point clouds"),r("OutboundLink")],1)])]),e._v(" "),r("p",[e._v("2、PixelAnnotationTool")]),e._v(" "),r("ul",[r("li",[e._v("link："),r("a",{attrs:{href:"https://github.com/abreheret/PixelAnnotationTool",target:"_blank",rel:"noopener noreferrer"}},[e._v("PixelAnnotationTool"),r("OutboundLink")],1)])]),e._v(" "),r("h3",{attrs:{id:"分割的性能评估"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#分割的性能评估"}},[e._v("#")]),e._v(" 分割的性能评估")]),e._v(" "),r("p",[e._v("1、Boundary IoU API")]),e._v(" "),r("ul",[r("li",[e._v("link: "),r("a",{attrs:{href:"https://github.com/bowenc0221/boundary-iou-api",target:"_blank",rel:"noopener noreferrer"}},[e._v("Boundary IoU API (Beta version)"),r("OutboundLink")],1)])])])}),[],!1,null,null,null);t.default=a.exports}}]);