(window.webpackJsonp=window.webpackJsonp||[]).push([[188],{611:function(_,v,t){"use strict";t.r(v);var a=t(25),s=Object(a.a)({},(function(){var _=this,v=_.$createElement,t=_._self._c||v;return t("ContentSlotsDistributor",{attrs:{"slot-key":_.$parent.slotKey}},[t("h2",{attrs:{id:"极大似然估计"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#极大似然估计"}},[_._v("#")]),_._v(" 极大似然估计")]),_._v(" "),t("h4",{attrs:{id:"_0-前言"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_0-前言"}},[_._v("#")]),_._v(" 0 前言")]),_._v(" "),t("p",[_._v("这是高数下册里面学到的知识，我印象中当时是比较简单的，做题的话套公式就可以了，但是在近期和一位师妹的学习沟通过程中，她问到了我很多细节，例如为什么在全连接层后面要用 softmax 函数呢？为什么 softmax 函数需要用指数的形式呢？我发现我检索到的很多回答很多都用到了极大似然估计的内容，好好再回顾以下相关的知识，记录于此。")]),_._v(" "),t("p",[_._v("值得注意的是，我们要把动机和优点区分开。动机是做这件事情的目的，优点是在完成这个目的时各个工具的优劣性，这两者不能混淆。例如：")]),_._v(" "),t("ul",[t("li",[_._v("在全连接层后使用 softmax 的动机是为了将全连接层的输出映射成一个概率分布。")]),_._v(" "),t("li",[_._v("那为什么现在常用的是 softmax 呢，其他的映射函数是否可以呢？")]),_._v(" "),t("li",[_._v("softmax 这么常用，其优点又是什么呢？")]),_._v(" "),t("li",[_._v("在什么样的场合我们可以使用其他的映射函数呢？")])]),_._v(" "),t("p",[_._v("私以为，这确实是一个研究生应当弄清楚的深度学习的理论基础，这有助于我们在今后撰写代码时不仅仅是网格搜索式地调参及改模型，也感谢我的师妹给我提了个醒，和我沟通确实有助于我进行知识的查漏补缺，让基础更加扎实一些。")]),_._v(" "),t("h4",{attrs:{id:"_01、什么是极大似然估计"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_01、什么是极大似然估计"}},[_._v("#")]),_._v(" 01、什么是极大似然估计？")]),_._v(" "),t("p",[_._v("极大似然估计（Maximum Likelihood Estimate, MLE），是参数估计的一种方法。")]),_._v(" "),t("ul",[t("li",[_._v("参数估计的解释是：已知某个随机样本满足某种概率分布，但其中的参数不确定。参数估计就是通过若干次实验，观察其结果，利用结果推出参数的大概值")]),_._v(" "),t("li",[_._v("极大似然估计的思想：已知某组参数能够使得这个样本出现的概率最大，我们就将该组概率作为该组估计的真实值。")]),_._v(" "),t("li",[_._v("极大似然估计的优点\n"),t("ul",[t("li",[_._v("渐进正确性：随着样本量的增加，估计值会最终趋向于真实值")]),_._v(" "),t("li",[_._v("渐进正态性：估计的抽样分布服从正态分布")]),_._v(" "),t("li",[_._v("有效性：极大似然估计在所有无偏估计中具有最小方差")])])]),_._v(" "),t("li",[_._v("极大似然估计的局限\n"),t("ul",[t("li",[_._v("是一种粗略的数学期望")]),_._v(" "),t("li",[_._v("一旦使用极大似然估计法，数据的产生过程必须严格完整地被假定并且描述，这意味着估计者需要对数据的产生过程有着较深的理解")]),_._v(" "),t("li",[_._v("极大似然估计一般不太适用于包含理性预期的结构模型，这类模型中的似然函数通常高度非线性化，这使得模型的估计应为搜索全局最优而变得及其困难复杂")])])])]),_._v(" "),t("h4",{attrs:{id:"_02、如何计算极大似然估计呢"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_02、如何计算极大似然估计呢"}},[_._v("#")]),_._v(" 02、如何计算极大似然估计呢？")]),_._v(" "),t("p",[_._v("求解极大似然函数估计的一般步骤，在印象中这也是高数里面的解题方法，套路性非常强，当时以为是送分题（")]),_._v(" "),t("ul",[t("li",[_._v("写出似然函数")]),_._v(" "),t("li",[_._v("对似然函数取对数并整理")]),_._v(" "),t("li",[_._v("求导数")]),_._v(" "),t("li",[_._v("解似然方程")])]),_._v(" "),t("p",[_._v("那么我们来解一道考研例题吧！")]),_._v(" "),t("p",[t("img",{attrs:{src:"https://muyun-blog-pic.oss-cn-shanghai.aliyuncs.com/picgo/image-20210914151646725.png",alt:"image-20210914151646725"}})]),_._v(" "),t("p",[_._v("似然：理念世界和现实世界")]),_._v(" "),t("p",[_._v("似然值：从真实世界估计的参数下的真实事件发生的概率")]),_._v(" "),t("p",[_._v("神经网络本质上就是计算神经网络里面的概率模型的似然值，找到那个极大似然值所对应的概率模型，应该就是最接近现实情况的那个概率模型")])])}),[],!1,null,null,null);v.default=s.exports}}]);