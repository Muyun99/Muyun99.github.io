(window.webpackJsonp=window.webpackJsonp||[]).push([[85],{507:function(a,t,r){"use strict";r.r(t);var e=r(25),s=Object(e.a)({},(function(){var a=this,t=a.$createElement,r=a._self._c||t;return r("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[r("h3",{attrs:{id:"_1、什么是domain-adaptation"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1、什么是domain-adaptation"}},[a._v("#")]),a._v(" 1、什么是Domain Adaptation？")]),a._v(" "),r("p",[a._v("Domain Adaptation 是源任务和目标任务一样，但是源域和目标域的数据分布不一样，并且源域有大量标记好的样本的迁移学习方法。这样就是如何把源域从大量的有标记样本中学习的知识迁移到目标域上解决相同的问题。")]),a._v(" "),r("p",[r("strong",[a._v("领域自适应")]),a._v("（Domain Adaptation）是迁移学习中的一种代表性方法，其定义为：源域（source domain）和目标域（target domain）共享相同的特征和类别，但是特征分布不同，如何利用信息丰富的源域样本来提升目标域模型的性能。"),r("strong",[a._v("源域")]),a._v("表示与测试样本不同的领域，具有丰富的监督标注信息；"),r("strong",[a._v("目标域")]),a._v("表示测试样本所在的领域，无标签或者只有少量标签。"),r("strong",[a._v("源域和目标域往往属于同一类任务，但是分布不同")]),a._v("。")]),a._v(" "),r("h3",{attrs:{id:"_2、domain-adaptation-的发展现状"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2、domain-adaptation-的发展现状"}},[a._v("#")]),a._v(" 2、Domain Adaptation 的发展现状")]),a._v(" "),r("h4",{attrs:{id:"_2-1-traditional-da-传统的非深度学习的da"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-traditional-da-传统的非深度学习的da"}},[a._v("#")]),a._v(" 2.1 Traditional DA（传统的非深度学习的DA）")]),a._v(" "),r("p",[a._v("TCA将源域和目标域一起映射到一个高维的再生核希尔伯特空间。在此空间中最小化源和目标的数据距离，同时最大程度地保留它们各自的内部属性。GFK 将源域和目标域视作 Grassmann流形 中的两个点，希望找到合适的变换组成一条测地线的路径。")]),a._v(" "),r("h4",{attrs:{id:"_2-2-discrepancy-based-da-基于差异的"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-discrepancy-based-da-基于差异的"}},[a._v("#")]),a._v(" 2.2 Discrepancy-based DA（基于差异的）")]),a._v(" "),r("p",[a._v("DDC 加入 MMD 距离来缩小源域和目标域的差异，有助于学习到对领域不敏感的特征表示。DAN加入多个适配层，并且用多核 MMD 替换单核 MMD，提升了域适应的性能。RTN 认为条件分布差异是由一个扰动函数，但是可以通过两层残差项（residual block）联系起来，利用Residual Function来区分source classifier和target classifier。JAN提出JMMD用于度量多个网络层的联合分布的差异。")]),a._v(" "),r("h4",{attrs:{id:"_2-3-adversarial-based-da-基于对抗的"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-adversarial-based-da-基于对抗的"}},[a._v("#")]),a._v(" 2.3 Adversarial-based DA（基于对抗的）")]),a._v(" "),r("p",[a._v("RevGrad主要提出了利用一个domain classifier来增强网络迁移性的方法，基于对抗的方法来进行优化。iCAN 结合了collaborative learning 和 adversarial learning对lower blocks 和 higher blocks 分别制定了学习目标。MADA为了避免使用单个判别器时，不同种类的样本被错误地对齐，作者提出了用多个判别器来捕捉多模式结构，判别器的个数正好等于原域样本的种类数。 Weighted Adversarial Nets "),r("strong",[a._v("选择出源域中与目标域那部分类别最接近的样本")]),a._v("，给它们赋予高权重，然后进行迁移。")]),a._v(" "),r("h4",{attrs:{id:"_2-4-reconstruction-based-da-基于重建的"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-reconstruction-based-da-基于重建的"}},[a._v("#")]),a._v(" 2.4 Reconstruction-based DA（基于重建的）")]),a._v(" "),r("p",[a._v("DRCN 利用源域和目标域共同提取特征，要求这些特征同时适合分类源域和重构目标域。DSN 提取不同域之间的公有特征以及利用公有特征进行迁移避免负迁移（negative transfer）")]),a._v(" "),r("h4",{attrs:{id:"_2-5-others"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-others"}},[a._v("#")]),a._v(" 2.5 Others")]),a._v(" "),r("p",[a._v("Asymmetric Tri-training 在使用2个Classifier去构建目标域的Pseudo Label，另外一个分类器学习伪标签的特征表达")]),a._v(" "),r("h3",{attrs:{id:"_3、参考资料"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_3、参考资料"}},[a._v("#")]),a._v(" 3、参考资料")]),a._v(" "),r("ul",[r("li",[r("p",[r("a",{attrs:{href:"https://github.com/jindongwang/transferlearning-tutorial",target:"_blank",rel:"noopener noreferrer"}},[a._v("《迁移学习简明手册》"),r("OutboundLink")],1)])]),a._v(" "),r("li",[r("p",[r("a",{attrs:{href:"https://arxiv.org/abs/1802.03601",target:"_blank",rel:"noopener noreferrer"}},[a._v("Deep Visual Domain Adaptation: A Survey"),r("OutboundLink")],1)])]),a._v(" "),r("li",[r("p",[a._v("[深度迁移学习综述 PPT](http://whdeng.cn/papers/deep%20domain%20adaptation%20tutorial-- small.pdf)")])])]),a._v(" "),r("p",[r("a",{attrs:{href:"https://www.bilibili.com/video/av39436440/",target:"_blank",rel:"noopener noreferrer"}},[a._v("- 深度迁移学习综述讲解视频"),r("OutboundLink")],1)])])}),[],!1,null,null,null);t.default=s.exports}}]);